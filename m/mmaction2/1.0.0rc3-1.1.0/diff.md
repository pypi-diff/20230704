# Comparing `tmp/mmaction2-1.0.0rc3.tar.gz` & `tmp/mmaction2-1.1.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/mmaction2-1.0.0rc3.tar", last modified: Fri Feb 10 14:19:16 2023, max compression
+gzip compressed data, was "dist/mmaction2-1.1.0.tar", last modified: Tue Jul  4 14:01:17 2023, max compression
```

## Comparing `mmaction2-1.0.0rc3.tar` & `mmaction2-1.1.0.tar`

### file list

```diff
@@ -1,672 +1,734 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/
--rw-r--r--   0 runner    (1001) docker     (123)      137 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    22922 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    19720 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/default_runtime.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/
--rw-r--r--   0 runner    (1001) docker     (123)      378 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/audioonly_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      275 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/bmn_400x100.py
--rw-r--r--   0 runner    (1001) docker     (123)      334 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/bsn_pem.py
--rw-r--r--   0 runner    (1001) docker     (123)      168 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/bsn_tem.py
--rw-r--r--   0 runner    (1001) docker     (123)      580 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/c2d_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      819 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/c3d_sports1m_pretrained.py
--rw-r--r--   0 runner    (1001) docker     (123)      972 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/i3d_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      772 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/ircsn_r152.py
--rw-r--r--   0 runner    (1001) docker     (123)      435 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/mvit_small.py
--rw-r--r--   0 runner    (1001) docker     (123)      915 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/r2plus1d_r34.py
--rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/slowfast_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      707 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/slowonly_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      751 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/swin_tiny.py
--rw-r--r--   0 runner    (1001) docker     (123)      636 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tanet_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      754 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tin_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)     1491 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tpn_slowonly_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)     1325 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tpn_tsm_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      677 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/trn_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      768 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tsm_mobilenet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)      678 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tsm_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      314 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tsn_r18_audio.py
--rw-r--r--   0 runner    (1001) docker     (123)      673 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tsn_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)      581 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/x3d.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/
--rw-r--r--   0 runner    (1001) docker     (123)      506 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/adam_20e.py
--rw-r--r--   0 runner    (1001) docker     (123)      470 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_100e.py
--rw-r--r--   0 runner    (1001) docker     (123)      548 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_150e_warmup.py
--rw-r--r--   0 runner    (1001) docker     (123)      468 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_50e.py
--rw-r--r--   0 runner    (1001) docker     (123)      553 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_100e.py
--rw-r--r--   0 runner    (1001) docker     (123)      551 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_50e.py
--rw-r--r--   0 runner    (1001) docker     (123)      554 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_100e.py
--rw-r--r--   0 runner    (1001) docker     (123)      552 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_50e.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/_base_/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/_base_/models/
--rw-r--r--   0 runner    (1001) docker     (123)     1743 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/_base_/models/slowonly_r50.py
--rw-r--r--   0 runner    (1001) docker     (123)     1623 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/_base_/models/slowonly_r50_nl.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/acrn/
--rw-r--r--   0 runner    (1001) docker     (123)     2222 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/acrn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4317 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2446 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/
--rw-r--r--   0 runner    (1001) docker     (123)    11140 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      207 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50-context_8xb16-4x16x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      170 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50-temporal-max_8xb6-8x8x1-cosine-10e_ava22-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3378 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3273 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb6-8x8x1-cosine-10e_ava22-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1764 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb8-8x8x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      242 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_r50-k400-pre-temporal-max-focal-alpha3-gamma1_8xb6-8x8x1-cosine-10e_ava22-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2503 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      719 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50-nl_8xb16-4x16x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2576 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50-nl_8xb16-8x8x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2974 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      433 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics700-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/
--rw-r--r--   0 runner    (1001) docker     (123)     3678 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k400-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4006 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k400-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      239 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-10e_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      256 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-focal-10e_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      224 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context-temporal-max_8xb8-8x8x1-10e_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      198 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context_8xb8-8x8x1-10e_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4195 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50_8xb8-16x4x1-10e-tricks_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      412 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      411 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/
--rw-r--r--   0 runner    (1001) docker     (123)     2188 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      221 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/slowonly-lfb-max_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4228 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1990 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/slowonly-lfb_ava-pretrained-r50_infer-4x16x1_ava21-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bmn/
--rw-r--r--   0 runner    (1001) docker     (123)     2981 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bmn/bmn_2xb8-400x100-9e_activitynet-feature.py
--rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bmn/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/
--rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/bsn_pem_1xb16-400x100-20e_activitynet-feature.py
--rw-r--r--   0 runner    (1001) docker     (123)     1096 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/bsn_pgm_400x100_activitynet-feature.py
--rw-r--r--   0 runner    (1001) docker     (123)     2703 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/bsn_tem_1xb16-400x100-20e_activitynet-feature.py
--rw-r--r--   0 runner    (1001) docker     (123)     1509 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/
--rw-r--r--   0 runner    (1001) docker     (123)      233 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/c2d_r101-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3149 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3155 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-16x4x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3152 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-8x8x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4129 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c3d/
--rw-r--r--   0 runner    (1001) docker     (123)     3516 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c3d/c3d_sports1m-pretrained_8xb30-16x1x1-45e_ucf101-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1170 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c3d/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/
--rw-r--r--   0 runner    (1001) docker     (123)      394 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ipcsn_ig65m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      371 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ipcsn_r152_32x2x1-180e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ipcsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r152-bnfrozen_8xb12-32x2x1-58e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      268 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r152_8xb12-32x2x1-58e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      350 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r50-bnfrozen_8xb12-32x2x1-58e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      371 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ircsn_r152_32x2x1-180e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      586 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ircsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     7995 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/
--rw-r--r--   0 runner    (1001) docker     (123)      238 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-heavy_8xb8-32x2x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3512 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-dot-product_8xb8-32x2x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-embedded-gaussian_8xb8-32x2x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      335 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-gaussian_8xb8-32x2x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3206 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-32x2x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2790 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-dense-32x2x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     6775 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/
--rw-r--r--   0 runner    (1001) docker     (123)     7548 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4727 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_32x3x1_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4224 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_u32_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4415 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_40x3x1_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4309 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_u40_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4626 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_32xb16-16x4x1-200e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     5088 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-maskfeat-pre_8xb32-16x4x1-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4079 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-pre_16xb16-u16-100e_sthv2-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/omnisource/
--rw-r--r--   0 runner    (1001) docker     (123)     1019 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/omnisource/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4872 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/omnisource/slowonly_r50_8xb16-8x8x1-256e_imagenet-kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/r2plus1d/
--rw-r--r--   0 runner    (1001) docker     (123)     2128 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/r2plus1d/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2649 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-32x2x1-180e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3480 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-8x8x1-180e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/
--rw-r--r--   0 runner    (1001) docker     (123)     5093 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      171 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/slowfast_r101-r50_32xb8-4x16x1-256e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      155 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/slowfast_r101_8xb8-8x8x1-256e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3447 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-4x16x1-256e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      242 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-8x8x1-256e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      454 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-8x8x1-steplr-256e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/
--rw-r--r--   0 runner    (1001) docker     (123)    10117 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3557 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-4x16x1-steplr-150e_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2630 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      745 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      744 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      490 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r101_8xb16-8x8x1-196e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      797 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      796 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3641 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-4x16x1-256e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2586 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-8x8x1-256e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/
--rw-r--r--   0 runner    (1001) docker     (123)     5495 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4230 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-base-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2992 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4236 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4195 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-small-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4144 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-tiny-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/
--rw-r--r--   0 runner    (1001) docker     (123)     3426 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3751 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb6-1x1x16-50e_sthv1-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-1x1x8-50e_sthv1-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3485 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-dense-1x1x8-100e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/timesformer/
--rw-r--r--   0 runner    (1001) docker     (123)     3247 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/timesformer/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      134 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/timesformer/timesformer_divST_8xb8-8x32x1-15e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      132 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/timesformer/timesformer_jointST_8xb8-8x32x1-15e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4478 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/timesformer/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/
--rw-r--r--   0 runner    (1001) docker     (123)     3037 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3577 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv1-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3473 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3181 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/tin_kinetics400-pretrained-tsm-r50_1x1x8-50e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/
--rw-r--r--   0 runner    (1001) docker     (123)     3117 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3201 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      156 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3105 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/trn/
--rw-r--r--   0 runner    (1001) docker     (123)     2299 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/trn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv1-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3669 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/
--rw-r--r--   0 runner    (1001) docker     (123)    10690 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      235 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      389 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-dot-product_8xb16-1x1x8-50e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      395 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-embedded-gaussian_8xb16-1x1x8-50e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      386 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-gaussian_8xb16-1x1x8-50e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      643 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3755 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3774 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3770 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-dense-1x1x8-50e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/custom_backbones/
--rw-r--r--   0 runner    (1001) docker     (123)     1110 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      552 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-rn101-32x4d_8xb32-1x1x3-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      250 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)    10729 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)      237 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r101_8xb32-1x1x8-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1710 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3148 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x5-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     3234 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     2941 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-dense-1x1x5-100e_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/
--rw-r--r--   0 runner    (1001) docker     (123)     2796 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_16x4x1_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_32x4x1_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/uniformer-small_imagenet1k-pre_16x4x1_kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/
--rw-r--r--   0 runner    (1001) docker     (123)    17947 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1863 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1859 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1859 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1859 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      968 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      973 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics600-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)      974 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1868 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1875 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res336_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/videomae/
--rw-r--r--   0 runner    (1001) docker     (123)     1683 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/videomae/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1632 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/videomae/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400.py
--rw-r--r--   0 runner    (1001) docker     (123)      192 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/videomae/vit-large-p16_videomae-k400-pre_16x4x1_kinetics-400.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/x3d/
--rw-r--r--   0 runner    (1001) docker     (123)     2068 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/x3d/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/x3d/x3d_m_16x5x1_facebook-kinetics400-rgb.py
--rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/x3d/x3d_s_13x6x1_facebook-kinetics400-rgb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/audioonly/
--rw-r--r--   0 runner    (1001) docker     (123)     2962 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/audioonly/audioonly_r50_8xb160-64x1x1-100e_kinetics400-audio-feature.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/resnet/
--rw-r--r--   0 runner    (1001) docker     (123)     1140 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/resnet/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2964 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio-feature.py
--rw-r--r--   0 runner    (1001) docker     (123)     3048 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3087 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3103 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/metafile.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/
--rw-r--r--   0 runner    (1001) docker     (123)     5659 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4220 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     4221 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     4009 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     4284 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4010 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py
--rw-r--r--   0 runner    (1001) docker     (123)     4285 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-limb.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/
--rw-r--r--   0 runner    (1001) docker     (123)    14524 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2019 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3030 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3046 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3044 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/
--rw-r--r--   0 runner    (1001) docker     (123)     7518 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/metafile.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3112 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3128 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py
--rw-r--r--   0 runner    (1001) docker     (123)      891 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction/.mim/model-index.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/
--rw-r--r--   0 runner    (1001) docker     (123)     5790 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/analyze_logs.py
--rw-r--r--   0 runner    (1001) docker     (123)     1956 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/bench_processing.py
--rw-r--r--   0 runner    (1001) docker     (123)     2756 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/benchmark.py
--rw-r--r--   0 runner    (1001) docker     (123)     5462 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/check_videos.py
--rw-r--r--   0 runner    (1001) docker     (123)     1527 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/eval_metric.py
--rw-r--r--   0 runner    (1001) docker     (123)     2809 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/get_flops.py
--rw-r--r--   0 runner    (1001) docker     (123)      643 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/print_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1892 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/report_accuracy.py
--rw-r--r--   0 runner    (1001) docker     (123)     2472 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/report_map.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/convert/
--rw-r--r--   0 runner    (1001) docker     (123)     7790 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/convert/convert_recognizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/
--rw-r--r--   0 runner    (1001) docker     (123)     3261 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/activitynet_feature_postprocessing.py
--rw-r--r--   0 runner    (1001) docker     (123)     6164 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/convert_proposal_format.py
--rw-r--r--   0 runner    (1001) docker     (123)     4949 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download.py
--rw-r--r--   0 runner    (1001) docker     (123)      264 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      284 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download_bsn_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      528 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download_feature_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      637 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download_features.sh
--rw-r--r--   0 runner    (1001) docker     (123)      278 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      259 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)     3511 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/generate_rawframes_filelist.py
--rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/process_annotations.py
--rw-r--r--   0 runner    (1001) docker     (123)     5256 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/tsn_feature_extraction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2888 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/anno_txt2json.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/
--rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/cut_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      339 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      506 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      570 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/download_videos_gnu_parallel.sh
--rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/download_videos_parallel.py
--rw-r--r--   0 runner    (1001) docker     (123)      410 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/download_videos_parallel.sh
--rw-r--r--   0 runner    (1001) docker     (123)      211 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      189 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)     1309 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/extract_rgb_frames_ffmpeg.sh
--rw-r--r--   0 runner    (1001) docker     (123)      425 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/fetch_ava_proposals.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/
--rw-r--r--   0 runner    (1001) docker     (123)     5146 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/X-101-64x4d-FPN.py
--rw-r--r--   0 runner    (1001) docker     (123)     6718 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/cut_kinetics.py
--rw-r--r--   0 runner    (1001) docker     (123)     1398 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/extract_rgb_frames.py
--rw-r--r--   0 runner    (1001) docker     (123)     4415 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/fetch_proposal.py
--rw-r--r--   0 runner    (1001) docker     (123)     1446 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/merge_annotations.py
--rw-r--r--   0 runner    (1001) docker     (123)     2809 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/prepare_annotation.py
--rw-r--r--   0 runner    (1001) docker     (123)      693 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/softlink_ava.py
--rw-r--r--   0 runner    (1001) docker     (123)    10580 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_audio_features.py
--rw-r--r--   0 runner    (1001) docker     (123)    10445 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_file_list.py
--rw-r--r--   0 runner    (1001) docker     (123)    10163 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_rawframes.py
--rw-r--r--   0 runner    (1001) docker     (123)     4152 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_videos.py
--rw-r--r--   0 runner    (1001) docker     (123)     2580 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/denormalize_proposal_file.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/
--rw-r--r--   0 runner    (1001) docker     (123)      404 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      325 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      211 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      189 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      201 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      402 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      387 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/diving48/generate_videos_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)     1998 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/extract_audio.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/
--rw-r--r--   0 runner    (1001) docker     (123)     3464 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/download.py
--rw-r--r--   0 runner    (1001) docker     (123)      531 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      334 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      232 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)     1883 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/generate_file_list.py
--rw-r--r--   0 runner    (1001) docker     (123)     1713 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/trim_event.py
--rw-r--r--   0 runner    (1001) docker     (123)     1629 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/trim_subaction.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/
--rw-r--r--   0 runner    (1001) docker     (123)      471 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      521 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      202 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      191 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      203 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      218 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      209 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/generate_videos_filelist.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/
--rw-r--r--   0 runner    (1001) docker     (123)     7014 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/download.py
--rw-r--r--   0 runner    (1001) docker     (123)      540 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      404 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      454 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)     4884 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/generate_file_list.py
--rw-r--r--   0 runner    (1001) docker     (123)      410 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)     1229 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/generate_sub_file_list.py
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/generate_videos_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/parse_tag_list.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/jester/
--rw-r--r--   0 runner    (1001) docker     (123)      195 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/jester/encode_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      207 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/jester/extract_flow.sh
--rw-r--r--   0 runner    (1001) docker     (123)      426 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/jester/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      377 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/jester/generate_videos_filelist.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/
--rw-r--r--   0 runner    (1001) docker     (123)     8191 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download.py
--rw-r--r--   0 runner    (1001) docker     (123)      794 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      876 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download_backup_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      687 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      750 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      705 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      729 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      734 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      712 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/generate_videos_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      764 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/rename_classnames.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/
--rw-r--r--   0 runner    (1001) docker     (123)      438 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      392 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      418 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      448 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      430 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/generate_videos_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      519 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/preprocess_data.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mmit/
--rw-r--r--   0 runner    (1001) docker     (123)      209 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mmit/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      185 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mmit/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      198 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mmit/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      433 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mmit/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      415 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mmit/generate_videos_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      392 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mmit/preprocess_data.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/omnisource/
--rw-r--r--   0 runner    (1001) docker     (123)     1194 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/omnisource/trim_raw_video.py
--rw-r--r--   0 runner    (1001) docker     (123)    18235 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/parse_file_list.py
--rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/resize_videos.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/
--rw-r--r--   0 runner    (1001) docker     (123)      770 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/babel2mma2.py
--rw-r--r--   0 runner    (1001) docker     (123)      545 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)     7465 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/gen_ntu_rgbd_raw.py
--rw-r--r--   0 runner    (1001) docker     (123)    11600 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/ntu_pose_extraction.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv1/
--rw-r--r--   0 runner    (1001) docker     (123)      192 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv1/encode_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      204 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv1/extract_flow.sh
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv1/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      372 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv1/generate_videos_filelist.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv2/
--rw-r--r--   0 runner    (1001) docker     (123)      211 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv2/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      189 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv2/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      201 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv2/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      387 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv2/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      372 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/sthv2/generate_videos_filelist.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/
--rw-r--r--   0 runner    (1001) docker     (123)      561 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/denormalize_proposal_file.sh
--rw-r--r--   0 runner    (1001) docker     (123)      781 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      442 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      396 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      422 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      421 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/fetch_tag_proposals.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/
--rw-r--r--   0 runner    (1001) docker     (123)      409 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/download_annotations.sh
--rw-r--r--   0 runner    (1001) docker     (123)      344 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/download_videos.sh
--rw-r--r--   0 runner    (1001) docker     (123)      202 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/extract_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      191 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/extract_rgb_frames.sh
--rw-r--r--   0 runner    (1001) docker     (123)      203 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/extract_rgb_frames_opencv.sh
--rw-r--r--   0 runner    (1001) docker     (123)      218 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/generate_rawframes_filelist.sh
--rw-r--r--   0 runner    (1001) docker     (123)      209 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ucf101/generate_videos_filelist.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/
--rw-r--r--   0 runner    (1001) docker     (123)     5800 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/export_onnx_stdet.py
--rw-r--r--   0 runner    (1001) docker     (123)     3942 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/mmaction2torchserve.py
--rw-r--r--   0 runner    (1001) docker     (123)     2727 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/mmaction_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/publish_model.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      496 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/dist_test.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      466 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/dist_train.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/misc/
--rw-r--r--   0 runner    (1001) docker     (123)     6716 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/misc/bsn_proposal_generation.py
--rw-r--r--   0 runner    (1001) docker     (123)     7589 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/misc/clip_feature_extraction.py
--rw-r--r--   0 runner    (1001) docker     (123)      363 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/misc/dist_clip_feature_extraction.sh
--rw-r--r--   0 runner    (1001) docker     (123)     6107 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/misc/flow_extraction.py
--rwxr-xr-x   0 runner    (1001) docker     (123)      620 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/slurm_test.sh
--rwxr-xr-x   0 runner    (1001) docker     (123)      644 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/slurm_train.sh
--rw-r--r--   0 runner    (1001) docker     (123)     4218 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/test.py
--rw-r--r--   0 runner    (1001) docker     (123)     4491 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/train.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/visualizations/
--rw-r--r--   0 runner    (1001) docker     (123)     8469 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/visualizations/browse_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     7072 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/visualizations/vis_cam.py
--rw-r--r--   0 runner    (1001) docker     (123)     8914 2023-02-10 14:19:11.000000 mmaction2-1.0.0rc3/mmaction/.mim/tools/visualizations/vis_scheduler.py
--rw-r--r--   0 runner    (1001) docker     (123)      957 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/apis/
--rw-r--r--   0 runner    (1001) docker     (123)      313 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/apis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7724 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/apis/inference.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/apis/inferencers/
--rw-r--r--   0 runner    (1001) docker     (123)      220 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/apis/inferencers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15087 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/apis/inferencers/actionrecog_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (123)     9215 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/apis/inferencers/mmaction2_inferencer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/datasets/
--rw-r--r--   0 runner    (1001) docker     (123)      690 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/activitynet_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3748 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/audio_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    25913 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/ava_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2150 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/pose_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     5535 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/rawframe_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     6041 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/repeat_aug_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/datasets/transforms/
--rw-r--r--   0 runner    (1001) docker     (123)     2605 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13071 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/transforms/formatting.py
--rw-r--r--   0 runner    (1001) docker     (123)    70437 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/transforms/loading.py
--rw-r--r--   0 runner    (1001) docker     (123)    45975 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/transforms/pose_transforms.py
--rw-r--r--   0 runner    (1001) docker     (123)    60552 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/transforms/processing.py
--rw-r--r--   0 runner    (1001) docker     (123)    14219 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/transforms/wrappers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3680 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/datasets/video_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/engine/
--rw-r--r--   0 runner    (1001) docker     (123)      218 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/engine/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)      176 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2088 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/hooks/output.py
--rw-r--r--   0 runner    (1001) docker     (123)     5325 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/hooks/visualization_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/engine/model/
--rw-r--r--   0 runner    (1001) docker     (123)      118 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1491 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/model/weight_init.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/engine/optimizers/
--rw-r--r--   0 runner    (1001) docker     (123)      409 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/optimizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4830 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/optimizers/layer_decay_optim_wrapper_constructor.py
--rw-r--r--   0 runner    (1001) docker     (123)     2651 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/optimizers/swin_optim_wrapper_constructor.py
--rw-r--r--   0 runner    (1001) docker     (123)     4276 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/optimizers/tsm_optim_wrapper_constructor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/engine/runner/
--rw-r--r--   0 runner    (1001) docker     (123)      149 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/runner/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2964 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/engine/runner/multi_loop.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/evaluation/
--rw-r--r--   0 runner    (1001) docker     (123)      135 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/
--rw-r--r--   0 runner    (1001) docker     (123)      979 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    22373 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/accuracy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/
--rw-r--r--   0 runner    (1001) docker     (123)       48 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5689 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     4923 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/np_box_list.py
--rw-r--r--   0 runner    (1001) docker     (123)     3462 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/np_box_ops.py
--rw-r--r--   0 runner    (1001) docker     (123)    24758 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/object_detection_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (123)    16949 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/per_image_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (123)     5313 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/standard_fields.py
--rw-r--r--   0 runner    (1001) docker     (123)     8544 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     9404 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/functional/eval_detection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/evaluation/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)      204 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5306 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/metrics/acc_metric.py
--rw-r--r--   0 runner    (1001) docker     (123)     6679 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/metrics/anet_metric.py
--rw-r--r--   0 runner    (1001) docker     (123)     3200 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/evaluation/metrics/ava_metric.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/
--rw-r--r--   0 runner    (1001) docker     (123)      529 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/
--rw-r--r--   0 runner    (1001) docker     (123)     1238 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8838 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/aagcn.py
--rw-r--r--   0 runner    (1001) docker     (123)     3040 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/c2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     5072 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/c3d.py
--rw-r--r--   0 runner    (1001) docker     (123)    11894 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/mobilenet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/mobilenet_v2_tsm.py
--rw-r--r--   0 runner    (1001) docker     (123)    36005 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/mvit.py
--rw-r--r--   0 runner    (1001) docker     (123)    23533 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     1532 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet2plus1d.py
--rw-r--r--   0 runner    (1001) docker     (123)    42862 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     6582 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet3d_csn.py
--rw-r--r--   0 runner    (1001) docker     (123)    22125 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet3d_slowfast.py
--rw-r--r--   0 runner    (1001) docker     (123)     1826 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet3d_slowonly.py
--rw-r--r--   0 runner    (1001) docker     (123)    14308 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet_audio.py
--rw-r--r--   0 runner    (1001) docker     (123)     8488 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet_omni.py
--rw-r--r--   0 runner    (1001) docker     (123)    12899 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet_tin.py
--rw-r--r--   0 runner    (1001) docker     (123)    11039 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/resnet_tsm.py
--rw-r--r--   0 runner    (1001) docker     (123)     8906 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/stgcn.py
--rw-r--r--   0 runner    (1001) docker     (123)    40458 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/swin.py
--rw-r--r--   0 runner    (1001) docker     (123)     3998 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/tanet.py
--rw-r--r--   0 runner    (1001) docker     (123)    12239 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/timesformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    22430 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/uniformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    22583 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/uniformerv2.py
--rw-r--r--   0 runner    (1001) docker     (123)    13903 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/vit_mae.py
--rw-r--r--   0 runner    (1001) docker     (123)    19483 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/backbones/x3d.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/common/
--rw-r--r--   0 runner    (1001) docker     (123)      481 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/common/conv2plus1d.py
--rw-r--r--   0 runner    (1001) docker     (123)     3721 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/common/conv_audio.py
--rw-r--r--   0 runner    (1001) docker     (123)     3021 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/common/sub_batchnorm3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     4767 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/common/tam.py
--rw-r--r--   0 runner    (1001) docker     (123)     9052 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/common/transformer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/data_preprocessors/
--rw-r--r--   0 runner    (1001) docker     (123)      140 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/data_preprocessors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5890 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/data_preprocessors/data_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/heads/
--rw-r--r--   0 runner    (1001) docker     (123)      666 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9018 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2279 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/gcn_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/i3d_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2982 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/mvit_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     4968 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/omni_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2756 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/slowfast_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2140 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/timesformer_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2778 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/tpn_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     8171 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/trn_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     4442 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/tsm_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     2591 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/tsn_audio_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     3546 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/tsn_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     3098 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/heads/x3d_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/
--rw-r--r--   0 runner    (1001) docker     (123)      128 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    19876 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/bmn.py
--rw-r--r--   0 runner    (1001) docker     (123)    20591 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/bsn.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      376 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11543 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/utils/bsn_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     5256 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/localizers/utils/proposal_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/losses/
--rw-r--r--   0 runner    (1001) docker     (123)      631 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1261 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/binary_logistic_regression_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7229 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/bmn_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7442 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/cross_entropy_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     6732 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/hvu_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)      744 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/nll_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     2666 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/ohem_hinge_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7336 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/losses/ssn_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/necks/
--rw-r--r--   0 runner    (1001) docker     (123)       88 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18247 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/necks/tpn.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/
--rw-r--r--   0 runner    (1001) docker     (123)      417 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9313 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     4644 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     4495 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     1278 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer_audio.py
--rw-r--r--   0 runner    (1001) docker     (123)     1425 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer_gcn.py
--rw-r--r--   0 runner    (1001) docker     (123)     6959 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer_omni.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/
--rw-r--r--   0 runner    (1001) docker     (123)      338 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/bbox_heads/
--rw-r--r--   0 runner    (1001) docker     (123)      110 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/bbox_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16716 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/bbox_heads/bbox_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/roi_extractors/
--rw-r--r--   0 runner    (1001) docker     (123)      136 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/roi_extractors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5238 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/roi_extractors/single_straight3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     9632 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/roi_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/
--rw-r--r--   0 runner    (1001) docker     (123)      202 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4302 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/acrn_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    14724 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/fbo_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     7780 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/lfb.py
--rw-r--r--   0 runner    (1001) docker     (123)     5715 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/lfb_infer_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/task_modules/
--rw-r--r--   0 runner    (1001) docker     (123)      122 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/task_modules/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/task_modules/assigners/
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/task_modules/assigners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6217 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/task_modules/assigners/max_iou_assigner_ava.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/models/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      362 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9012 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/utils/blending_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8484 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/utils/embed.py
--rw-r--r--   0 runner    (1001) docker     (123)    16041 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/utils/gcn_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     7077 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/models/utils/graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     5062 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/structures/
--rw-r--r--   0 runner    (1001) docker     (123)      217 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/structures/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2694 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/structures/action_data_sample.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/structures/bbox/
--rw-r--r--   0 runner    (1001) docker     (123)      163 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/structures/bbox/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1608 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/structures/bbox/bbox_target.py
--rw-r--r--   0 runner    (1001) docker     (123)     2160 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/structures/bbox/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/testing/
--rw-r--r--   0 runner    (1001) docker     (123)      655 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4182 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/testing/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      426 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      509 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/utils/collect_env.py
--rw-r--r--   0 runner    (1001) docker     (123)     9663 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/utils/gradcam_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1820 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)     1936 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/utils/setup_env.py
--rw-r--r--   0 runner    (1001) docker     (123)     1024 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/utils/typing.py
--rw-r--r--   0 runner    (1001) docker     (123)      777 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)      314 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/visualization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13253 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/visualization/action_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/mmaction/visualization/video_backend.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/mmaction2.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    22922 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction2.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    36492 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction2.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction2.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction2.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)      656 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction2.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        9 2023-02-10 14:19:15.000000 mmaction2-1.0.0rc3/mmaction2.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)      626 2023-02-10 14:19:16.000000 mmaction2-1.0.0rc3/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     7243 2023-02-10 14:19:10.000000 mmaction2-1.0.0rc3/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/
+-rw-r--r--   0 runner    (1001) docker     (123)      177 2023-07-04 14:01:15.000000 mmaction2-1.1.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)    30340 2023-07-04 14:01:17.000000 mmaction2-1.1.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    26349 2023-07-04 14:01:15.000000 mmaction2-1.1.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/default_runtime.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/
+-rw-r--r--   0 runner    (1001) docker     (123)      378 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/audioonly_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      275 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/bmn_400x100.py
+-rw-r--r--   0 runner    (1001) docker     (123)      334 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/bsn_pem.py
+-rw-r--r--   0 runner    (1001) docker     (123)      168 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/bsn_tem.py
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/c2d_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      819 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/c3d_sports1m_pretrained.py
+-rw-r--r--   0 runner    (1001) docker     (123)      972 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/i3d_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      772 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/ircsn_r152.py
+-rw-r--r--   0 runner    (1001) docker     (123)      435 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/mvit_small.py
+-rw-r--r--   0 runner    (1001) docker     (123)      915 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/r2plus1d_r34.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1225 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/slowfast_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      707 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/slowonly_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      751 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/swin_tiny.py
+-rw-r--r--   0 runner    (1001) docker     (123)      636 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tanet_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      754 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tin_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1491 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tpn_slowonly_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1325 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tpn_tsm_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      677 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/trn_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      768 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tsm_mobilenet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)      678 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tsm_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      314 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tsn_r18_audio.py
+-rw-r--r--   0 runner    (1001) docker     (123)      673 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tsn_r50.py
+-rw-r--r--   0 runner    (1001) docker     (123)      581 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/x3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/
+-rw-r--r--   0 runner    (1001) docker     (123)      506 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/adam_20e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      470 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_100e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      548 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_150e_warmup.py
+-rw-r--r--   0 runner    (1001) docker     (123)      468 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_50e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      553 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_100e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      551 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_50e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      554 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_100e.py
+-rw-r--r--   0 runner    (1001) docker     (123)      552 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_50e.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/acrn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2166 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/acrn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     5513 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5513 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/
+-rw-r--r--   0 runner    (1001) docker     (123)     2188 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3542 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/slowonly-lfb-infer_r50_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      221 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/slowonly-lfb-max_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5860 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3666 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/slowonly-lfb_ava-pretrained-r50_infer-4x16x1_ava21-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/
+-rw-r--r--   0 runner    (1001) docker     (123)     7226 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      207 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50-context_8xb16-4x16x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      170 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50-temporal-max_8xb6-8x8x1-cosine-10e_ava22-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5414 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4201 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5546 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb6-8x8x1-cosine-10e_ava22-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5439 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb8-8x8x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_r50-k400-pre-temporal-max-focal-alpha3-gamma1_8xb6-8x8x1-cosine-10e_ava22-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/
+-rw-r--r--   0 runner    (1001) docker     (123)     5994 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     5177 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k400-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5164 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k400-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      239 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      256 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-focal-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context-temporal-max_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3656 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50_8xb8-16x4x1-10e-tricks_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      412 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      411 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4871 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5277 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50-nl_8xb16-4x16x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5264 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50-nl_8xb16-8x8x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4990 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4730 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4991 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics700-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/videomae/
+-rw-r--r--   0 runner    (1001) docker     (123)     2320 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/videomae/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     5235 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/videomae/vit-base-p16_videomae-k400-pre_8xb8-16x4x1-20e-adamw_ava-kinetics-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5237 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/detection/videomae/vit-large-p16_videomae-k400-pre_8xb8-16x4x1-20e-adamw_ava-kinetics-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bmn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2981 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bmn/bmn_2xb8-400x100-9e_activitynet-feature.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1035 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bmn/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2563 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/bsn_pem_1xb16-400x100-20e_activitynet-feature.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1096 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/bsn_pgm_400x100_activitynet-feature.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2776 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/bsn_tem_1xb16-400x100-20e_activitynet-feature.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1509 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/tcanet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1017 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/tcanet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3320 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/localization/tcanet/tcanet_2xb8-700x100-9e_hacs-feature.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/
+-rw-r--r--   0 runner    (1001) docker     (123)      233 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/c2d_r101-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3149 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3155 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-16x4x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3152 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-8x8x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4129 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c3d/
+-rw-r--r--   0 runner    (1001) docker     (123)     3516 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c3d/c3d_sports1m-pretrained_8xb30-16x1x1-45e_ucf101-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1170 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/c3d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/
+-rw-r--r--   0 runner    (1001) docker     (123)      394 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ipcsn_ig65m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      371 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ipcsn_r152_32x2x1-180e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ipcsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r152-bnfrozen_8xb12-32x2x1-58e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      268 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r152_8xb12-32x2x1-58e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      350 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r50-bnfrozen_8xb12-32x2x1-58e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      371 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ircsn_r152_32x2x1-180e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      586 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ircsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7995 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/
+-rw-r--r--   0 runner    (1001) docker     (123)      238 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-heavy_8xb8-32x2x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3512 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-dot-product_8xb8-32x2x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-embedded-gaussian_8xb8-32x2x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      335 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-gaussian_8xb8-32x2x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3206 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-32x2x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2790 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-dense-32x2x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6775 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/
+-rw-r--r--   0 runner    (1001) docker     (123)     7548 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4727 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_32x3x1_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4224 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_u32_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4415 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_40x3x1_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4309 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_u40_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4626 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_32xb16-16x4x1-200e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5088 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-maskfeat-pre_8xb32-16x4x1-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4079 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-pre_16xb16-u16-100e_sthv2-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/omnisource/
+-rw-r--r--   0 runner    (1001) docker     (123)     1149 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/omnisource/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4872 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/omnisource/slowonly_r50_8xb16-8x8x1-256e_imagenet-kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/r2plus1d/
+-rw-r--r--   0 runner    (1001) docker     (123)     2128 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/r2plus1d/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2649 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-32x2x1-180e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3480 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-8x8x1-180e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/
+-rw-r--r--   0 runner    (1001) docker     (123)     5093 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      171 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/slowfast_r101-r50_32xb8-4x16x1-256e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      155 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/slowfast_r101_8xb8-8x8x1-256e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3447 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-4x16x1-256e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-8x8x1-256e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      454 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-8x8x1-steplr-256e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/
+-rw-r--r--   0 runner    (1001) docker     (123)    11256 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3557 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-4x16x1-steplr-150e_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2630 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4195 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      745 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      744 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      490 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r101_8xb16-8x8x1-196e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      797 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      796 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3641 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-4x16x1-256e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2586 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-8x8x1-256e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/
+-rw-r--r--   0 runner    (1001) docker     (123)     6561 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4230 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-base-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2992 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4236 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4465 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4195 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-small-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4144 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-tiny-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/
+-rw-r--r--   0 runner    (1001) docker     (123)     3426 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3751 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb6-1x1x16-50e_sthv1-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-1x1x8-50e_sthv1-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3485 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-dense-1x1x8-100e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/timesformer/
+-rw-r--r--   0 runner    (1001) docker     (123)     3247 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/timesformer/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      134 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/timesformer/timesformer_divST_8xb8-8x32x1-15e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      132 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/timesformer/timesformer_jointST_8xb8-8x32x1-15e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4478 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/timesformer/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/
+-rw-r--r--   0 runner    (1001) docker     (123)     3037 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3577 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv1-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3473 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3181 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/tin_kinetics400-pretrained-tsm-r50_1x1x8-50e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/
+-rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3201 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      156 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3211 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/trn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2299 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/trn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2976 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv1-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3669 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/
+-rw-r--r--   0 runner    (1001) docker     (123)    11643 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3702 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-mobilenetv2_8xb16-1x1x8-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      200 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      389 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-dot-product_8xb16-1x1x8-50e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      395 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-embedded-gaussian_8xb16-1x1x8-50e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      386 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-gaussian_8xb16-1x1x8-50e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2859 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1916 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3755 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3889 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3770 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-dense-1x1x8-50e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/custom_backbones/
+-rw-r--r--   0 runner    (1001) docker     (123)     1110 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      552 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-rn101-32x4d_8xb32-1x1x3-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      510 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-swin-transformer_32xb8-1x1x8-50e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      280 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11853 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      237 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r101_8xb32-1x1x8-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1823 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3148 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x5-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1989 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3347 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2941 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-dense-1x1x5-100e_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/
+-rw-r--r--   0 runner    (1001) docker     (123)     2796 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_16x4x1_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1515 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_32x4x1_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/uniformer-small_imagenet1k-pre_16x4x1_kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/
+-rw-r--r--   0 runner    (1001) docker     (123)    21610 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     5068 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5539 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5539 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5539 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4837 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4837 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6252 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_u8_kinetics710-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1865 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      973 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics600-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1864 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)      974 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1868 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1867 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res336_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomae/
+-rw-r--r--   0 runner    (1001) docker     (123)     1683 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomae/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1632 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomae/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400.py
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomae/vit-large-p16_videomae-k400-pre_16x4x1_kinetics-400.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomaev2/
+-rw-r--r--   0 runner    (1001) docker     (123)     1902 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomaev2/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1632 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomaev2/vit-base-p16_videomaev2-vit-g-dist-k710-pre_16x4x1_kinetics-400.py
+-rw-r--r--   0 runner    (1001) docker     (123)      206 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomaev2/vit-small-p16_videomaev2-vit-g-dist-k710-pre_16x4x1_kinetics-400.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/x3d/
+-rw-r--r--   0 runner    (1001) docker     (123)     2068 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/x3d/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/x3d/x3d_m_16x5x1_facebook-kinetics400-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1047 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition/x3d/x3d_s_13x6x1_facebook-kinetics400-rgb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/audioonly/
+-rw-r--r--   0 runner    (1001) docker     (123)     2962 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/audioonly/audioonly_r50_8xb160-64x1x1-100e_kinetics400-audio-feature.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/resnet/
+-rw-r--r--   0 runner    (1001) docker     (123)     1140 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/resnet/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2964 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio-feature.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3048 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/retrieval/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/retrieval/clip4clip/
+-rw-r--r--   0 runner    (1001) docker     (123)     3679 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/retrieval/clip4clip/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/retrieval/clip4clip/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3087 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3103 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/
+-rw-r--r--   0 runner    (1001) docker     (123)     5755 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/metafile.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/
+-rw-r--r--   0 runner    (1001) docker     (123)     3987 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/pose_only.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3782 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/rgb_only.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5686 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/rgbpose_conv3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4248 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4249 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4025 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4442 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4016 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4380 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-limb.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/
+-rw-r--r--   0 runner    (1001) docker     (123)    14524 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2019 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3030 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3046 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3028 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3044 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/
+-rw-r--r--   0 runner    (1001) docker     (123)     7518 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/metafile.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2009 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2021 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2012 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2024 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3112 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3128 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)      799 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/dataset-index.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1526 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/model-index.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/
+-rw-r--r--   0 runner    (1001) docker     (123)     5790 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/analyze_logs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1956 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/bench_processing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2756 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/benchmark.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5462 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/check_videos.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4549 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/confusion_matrix.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1550 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/eval_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2242 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/get_flops.py
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/print_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/report_accuracy.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2494 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/report_map.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/convert/
+-rw-r--r--   0 runner    (1001) docker     (123)     7790 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/convert/convert_recognizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/
+-rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/activitynet_feature_postprocessing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6182 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/convert_proposal_format.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4969 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download.py
+-rw-r--r--   0 runner    (1001) docker     (123)      264 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      284 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download_bsn_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      528 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download_feature_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      637 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download_features.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      278 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      259 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     3511 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/generate_rawframes_filelist.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1705 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/process_annotations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1251 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/tsn_extract_flow_feat_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1051 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/tsn_extract_rgb_feat_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1071 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/tsn_extract_video_feat_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2896 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/anno_txt2json.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/
+-rw-r--r--   0 runner    (1001) docker     (123)     1168 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/cut_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      339 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      506 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      570 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/download_videos_gnu_parallel.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     2178 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/download_videos_parallel.py
+-rw-r--r--   0 runner    (1001) docker     (123)      410 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/download_videos_parallel.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      211 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      189 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     1309 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/extract_rgb_frames_ffmpeg.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      425 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava/fetch_ava_proposals.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/
+-rw-r--r--   0 runner    (1001) docker     (123)     5146 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/X-101-64x4d-FPN.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6718 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/cut_kinetics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1398 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/extract_rgb_frames.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4415 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/fetch_proposal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1446 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/merge_annotations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2810 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/prepare_annotation.py
+-rw-r--r--   0 runner    (1001) docker     (123)      693 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/softlink_ava.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10588 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/build_audio_features.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10449 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/build_file_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10163 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/build_rawframes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4152 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/build_videos.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2580 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/denormalize_proposal_file.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/
+-rw-r--r--   0 runner    (1001) docker     (123)      404 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      325 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      211 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      189 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      402 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      387 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/generate_videos_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      215 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/diving48/preprocess.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     2006 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/extract_audio.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/
+-rw-r--r--   0 runner    (1001) docker     (123)     3476 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/download.py
+-rw-r--r--   0 runner    (1001) docker     (123)      531 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      338 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      232 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     1883 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/generate_file_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1729 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/trim_event.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1641 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/gym/trim_subaction.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hacs/
+-rw-r--r--   0 runner    (1001) docker     (123)     1529 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hacs/generate_anotations.py
+-rw-r--r--   0 runner    (1001) docker     (123)      393 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hacs/generate_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2327 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hacs/slowonly_feature_infer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      689 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hacs/write_feature_csv.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/
+-rw-r--r--   0 runner    (1001) docker     (123)      471 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      521 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      202 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      191 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      203 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      218 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      209 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/generate_videos_filelist.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/
+-rw-r--r--   0 runner    (1001) docker     (123)     7022 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/download.py
+-rw-r--r--   0 runner    (1001) docker     (123)      540 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      404 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      454 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     4896 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/generate_file_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)      410 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     1241 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/generate_sub_file_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)      417 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/generate_videos_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      436 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/parse_tag_list.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/jester/
+-rw-r--r--   0 runner    (1001) docker     (123)      195 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/jester/encode_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      207 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/jester/extract_flow.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      426 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/jester/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      377 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/jester/generate_videos_filelist.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/
+-rw-r--r--   0 runner    (1001) docker     (123)     8191 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download.py
+-rw-r--r--   0 runner    (1001) docker     (123)      794 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      876 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download_backup_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      687 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      750 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      705 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      729 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      734 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      712 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/generate_videos_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      190 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/preprocess_k400.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      188 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/preprocess_k600.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      190 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/preprocess_k700.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      764 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/rename_classnames.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mit/
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mit/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      392 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mit/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      418 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mit/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      448 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mit/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mit/generate_videos_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      519 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mit/preprocess_data.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mmit/
+-rw-r--r--   0 runner    (1001) docker     (123)      209 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mmit/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      185 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mmit/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mmit/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      433 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mmit/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      415 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mmit/generate_videos_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      392 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/mmit/preprocess_data.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/multisports/
+-rw-r--r--   0 runner    (1001) docker     (123)     2169 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/multisports/format_det_result.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3649 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/multisports/parse_anno.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/omnisource/
+-rw-r--r--   0 runner    (1001) docker     (123)     1202 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/omnisource/trim_raw_video.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18235 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/parse_file_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/resize_videos.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/
+-rw-r--r--   0 runner    (1001) docker     (123)      770 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/babel2mma2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1311 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/compress_nturgbd.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7465 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/gen_ntu_rgbd_raw.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10200 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/ntu_pose_extraction.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv1/
+-rw-r--r--   0 runner    (1001) docker     (123)      192 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv1/encode_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      204 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv1/extract_flow.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      421 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv1/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      372 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv1/generate_videos_filelist.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv2/
+-rw-r--r--   0 runner    (1001) docker     (123)      211 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv2/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      189 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv2/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv2/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      387 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv2/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      372 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv2/generate_videos_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      204 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/sthv2/preprocss.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/denormalize_proposal_file.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      781 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      612 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      442 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      396 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      422 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      421 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/fetch_tag_proposals.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/
+-rw-r--r--   0 runner    (1001) docker     (123)      409 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/download_annotations.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      344 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/download_videos.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      202 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/extract_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      191 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/extract_rgb_frames.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      203 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/extract_rgb_frames_opencv.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      218 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/generate_rawframes_filelist.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      209 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/ucf101/generate_videos_filelist.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/video_retrieval/
+-rw-r--r--   0 runner    (1001) docker     (123)     1531 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/video_retrieval/prepare_msrvtt.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1063 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/data/video_retrieval/prepare_msrvtt.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/deployment/
+-rw-r--r--   0 runner    (1001) docker     (123)     5109 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/deployment/export_onnx_gcn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4929 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/deployment/export_onnx_posec3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6362 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/deployment/export_onnx_stdet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3942 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/deployment/mmaction2torchserve.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2727 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/deployment/mmaction_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1801 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/deployment/publish_model.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      496 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/dist_test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      466 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/dist_train.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/misc/
+-rw-r--r--   0 runner    (1001) docker     (123)     6716 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/misc/bsn_proposal_generation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10672 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/misc/clip_feature_extraction.py
+-rw-r--r--   0 runner    (1001) docker     (123)      363 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/misc/dist_clip_feature_extraction.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     6107 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/misc/flow_extraction.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)      620 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/slurm_test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      644 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/slurm_train.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     4234 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/test.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4508 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/train.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/.mim/tools/visualizations/
+-rw-r--r--   0 runner    (1001) docker     (123)     8395 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/visualizations/browse_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7072 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/visualizations/vis_cam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9066 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/.mim/tools/visualizations/vis_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (123)      957 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/apis/
+-rw-r--r--   0 runner    (1001) docker     (123)      313 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8638 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/apis/inference.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/apis/inferencers/
+-rw-r--r--   0 runner    (1001) docker     (123)      220 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/apis/inferencers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15220 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/apis/inferencers/actionrecog_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9215 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/apis/inferencers/mmaction2_inferencer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)      759 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/activitynet_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3748 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/audio_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27104 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/ava_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2538 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2621 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/pose_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5597 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/rawframe_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6041 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/repeat_aug_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/datasets/transforms/
+-rw-r--r--   0 runner    (1001) docker     (123)     2791 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16467 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/transforms/formatting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    72689 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/transforms/loading.py
+-rw-r--r--   0 runner    (1001) docker     (123)    56732 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/transforms/pose_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)    60552 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/transforms/processing.py
+-rw-r--r--   0 runner    (1001) docker     (123)      902 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/transforms/text_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14380 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/transforms/wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3837 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/video_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1000 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/datasets/video_text_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/engine/
+-rw-r--r--   0 runner    (1001) docker     (123)      218 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/engine/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      176 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2088 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/hooks/output.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5325 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/hooks/visualization_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/engine/model/
+-rw-r--r--   0 runner    (1001) docker     (123)      118 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1491 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/model/weight_init.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/engine/optimizers/
+-rw-r--r--   0 runner    (1001) docker     (123)      409 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/optimizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5739 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/optimizers/layer_decay_optim_wrapper_constructor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2651 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/optimizers/swin_optim_wrapper_constructor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4276 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/optimizers/tsm_optim_wrapper_constructor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/engine/runner/
+-rw-r--r--   0 runner    (1001) docker     (123)      149 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/runner/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2964 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/engine/runner/multi_loop.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (123)      135 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/evaluation/functional/
+-rw-r--r--   0 runner    (1001) docker     (123)     1107 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22377 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/accuracy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/
+-rw-r--r--   0 runner    (1001) docker     (123)       48 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5690 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4923 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/np_box_list.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3462 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/np_box_ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11062 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/ava_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9408 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/eval_detection.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24229 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/functional/multisports_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/evaluation/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      386 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14451 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/metrics/acc_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6679 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/metrics/anet_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3238 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/metrics/ava_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3848 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/metrics/multisports_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4207 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/evaluation/metrics/retrieval_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/
+-rw-r--r--   0 runner    (1001) docker     (123)      574 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (123)     1300 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8838 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/aagcn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3040 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/c2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5072 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/c3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11923 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/mobilenet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3353 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/mobilenet_v2_tsm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36163 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/mvit.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23730 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1586 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet2plus1d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    43427 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6582 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet3d_csn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21594 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet3d_slowfast.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1355 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet3d_slowonly.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14308 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet_audio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8488 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet_omni.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12899 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet_tin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14237 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/resnet_tsm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8680 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/rgbposeconv3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8906 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/stgcn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40458 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/swin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3998 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/tanet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12259 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/timesformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22409 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/uniformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22560 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/uniformerv2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15043 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/vit_mae.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19483 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/backbones/x3d.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/common/
+-rw-r--r--   0 runner    (1001) docker     (123)      481 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/common/conv2plus1d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3721 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/common/conv_audio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3021 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/common/sub_batchnorm3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4767 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/common/tam.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9052 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/common/transformer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/data_preprocessors/
+-rw-r--r--   0 runner    (1001) docker     (123)      239 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/data_preprocessors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5972 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/data_preprocessors/data_preprocessor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1556 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/data_preprocessors/multimodal_data_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      831 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9087 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5362 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/feature_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2279 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/gcn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2662 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/i3d_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3345 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/mvit_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4968 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/omni_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9726 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/rgbpose_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2804 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/slowfast_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2140 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/timesformer_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2816 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/tpn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8171 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/trn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4470 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/tsm_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2591 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/tsn_audio_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3574 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/tsn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3779 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/uniformer_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3098 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/heads/x3d_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/localizers/
+-rw-r--r--   0 runner    (1001) docker     (123)      165 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19876 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/bmn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20657 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/bsn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19802 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/tcanet.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/localizers/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      689 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11543 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/utils/bsn_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5256 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/utils/proposal_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2205 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/localizers/utils/tcanet_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/losses/
+-rw-r--r--   0 runner    (1001) docker     (123)      631 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1261 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2117 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/binary_logistic_regression_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7229 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/bmn_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7885 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/cross_entropy_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6732 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/hvu_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)      744 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/nll_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2666 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/ohem_hinge_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7336 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/losses/ssn_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/necks/
+-rw-r--r--   0 runner    (1001) docker     (123)       88 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18247 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/necks/tpn.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/recognizers/
+-rw-r--r--   0 runner    (1001) docker     (123)      479 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11018 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7319 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/recognizer2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4521 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/recognizer3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1775 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/recognizer3d_mm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1278 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/recognizer_audio.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1425 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/recognizer_gcn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6959 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/recognizers/recognizer_omni.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/roi_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      338 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/roi_heads/bbox_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      110 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/bbox_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16716 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/bbox_heads/bbox_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/roi_heads/roi_extractors/
+-rw-r--r--   0 runner    (1001) docker     (123)      136 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/roi_extractors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5238 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/roi_extractors/single_straight3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9632 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/roi_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      202 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4302 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/acrn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14724 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/fbo_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7952 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/lfb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5715 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/lfb_infer_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/similarity/
+-rw-r--r--   0 runner    (1001) docker     (123)      225 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/similarity/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5445 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/similarity/adapters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6409 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/similarity/clip_similarity.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/task_modules/
+-rw-r--r--   0 runner    (1001) docker     (123)      122 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/task_modules/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/task_modules/assigners/
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/task_modules/assigners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6217 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/task_modules/assigners/max_iou_assigner_ava.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/models/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      362 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9709 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/utils/blending_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8484 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/utils/embed.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16041 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/utils/gcn_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7536 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/models/utils/graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5238 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/structures/
+-rw-r--r--   0 runner    (1001) docker     (123)      217 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/structures/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5936 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/structures/action_data_sample.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/structures/bbox/
+-rw-r--r--   0 runner    (1001) docker     (123)      163 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/structures/bbox/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1608 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/structures/bbox/bbox_target.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2160 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/structures/bbox/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/testing/
+-rw-r--r--   0 runner    (1001) docker     (123)      629 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4267 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/testing/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      540 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      805 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/utils/collect_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9663 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/utils/gradcam_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3815 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1936 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/utils/setup_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1024 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/utils/typing_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      774 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)      314 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/visualization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13332 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/visualization/action_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4911 2023-07-04 14:01:15.000000 mmaction2-1.1.0/mmaction/visualization/video_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction2.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    30340 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction2.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    40152 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction2.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction2.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction2.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)      666 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction2.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        9 2023-07-04 14:01:17.000000 mmaction2-1.1.0/mmaction2.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      691 2023-07-04 14:01:17.000000 mmaction2-1.1.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     7264 2023-07-04 14:01:15.000000 mmaction2-1.1.0/setup.py
```

### Comparing `mmaction2-1.0.0rc3/PKG-INFO` & `mmaction2-1.1.0/README.md`

 * *Files 22% similar despite different names*

```diff
@@ -1,314 +1,388 @@
-Metadata-Version: 2.1
-Name: mmaction2
-Version: 1.0.0rc3
-Summary: OpenMMLab Video Understanding Toolbox and Benchmark
-Home-page: https://github.com/open-mmlab/mmaction2
-Author: MMAction2 Contributors
-Author-email: openmmlab@gmail.com
-Maintainer: MMAction2 Contributors
-Maintainer-email: openmmlab@gmail.com
-License: Apache License 2.0
-Description: <div align="center">
-          <img src="https://github.com/open-mmlab/mmaction2/raw/1.x/resources/mmaction2_logo.png" width="600"/>
-          <div>&nbsp;</div>
-          <div align="center">
-            <b><font size="5">OpenMMLab website</font></b>
-            <sup>
-              <a href="https://openmmlab.com">
-                <i><font size="4">HOT</font></i>
-              </a>
-            </sup>
-            &nbsp;&nbsp;&nbsp;&nbsp;
-            <b><font size="5">OpenMMLab platform</font></b>
-            <sup>
-              <a href="https://platform.openmmlab.com">
-                <i><font size="4">TRY IT OUT</font></i>
-              </a>
-            </sup>
-          </div>
-        
-        [![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/1.x/)
-        [![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions)
-        [![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2)
-        [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/)
-        [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE)
-        [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
-        [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
-        
-        [📘Documentation](https://mmaction2.readthedocs.io/en/1.x/) |
-        [🛠️Installation](https://mmaction2.readthedocs.io/en/1.x/get_started.html) |
-        [👀Model Zoo](https://mmaction2.readthedocs.io/en/1.x/modelzoo.html) |
-        [🆕Update News](https://mmaction2.readthedocs.io/en/1.x/notes/changelog.html) |
-        [🚀Ongoing Projects](https://github.com/open-mmlab/mmaction2/projects) |
-        [🤔Reporting Issues](https://github.com/open-mmlab/mmaction2/issues/new/choose)
-        
-        </div>
-        
-        ## Introduction
-        
-        MMAction2 is an open-source toolbox for video understanding based on PyTorch.
-        It is a part of the [OpenMMLab](http://openmmlab.org/) project.
-        
-        The 1.x branch works with **PyTorch 1.6+**.
-        
-        <div align="center">
-          <div style="float:left;margin-right:10px;">
-          <img src="https://github.com/open-mmlab/mmaction2/raw/1.x/resources/mmaction2_overview.gif" width="380px"><br>
-            <p style="font-size:1.5vw;">Action Recognition Results on Kinetics-400</p>
-          </div>
-          <div style="float:right;margin-right:0px;">
-          <img src="https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif" width="380px"><br>
-            <p style="font-size:1.5vw;">Skeleton-based Action Recognition Results on NTU-RGB+D-120</p>
-          </div>
-        </div>
-        <div align="center">
-          <img src="https://user-images.githubusercontent.com/30782254/155710881-bb26863e-fcb4-458e-b0c4-33cd79f96901.gif" width="580px"/><br>
-            <p style="font-size:1.5vw;">Skeleton-based Spatio-Temporal Action Detection and Action Recognition Results on Kinetics-400</p>
-        </div>
-        <div align="center">
-          <img src="https://github.com/open-mmlab/mmaction2/raw/1.x/resources/spatio-temporal-det.gif" width="800px"/><br>
-            <p style="font-size:1.5vw;">Spatio-Temporal Action Detection Results on AVA-2.1</p>
-        </div>
-        
-        ## Major Features
-        
-        - **Modular design**: We decompose a video understanding framework into different components. One can easily construct a customized video understanding framework by combining different modules.
-        
-        - **Support four major video understanding tasks**: MMAction2 implements various algorithms for multiple video understanding tasks, including action recognition, action localization, spatio-temporal action detection, and skeleton-based action detection.
-        
-        - **Well tested and documented**: We provide detailed documentation and API reference, as well as unit tests.
-        
-        ## What's New
-        
-        **Release (2022.02.10)**: v1.0.0rc3 with the following new features:
-        
-        - Support Action Recognition model UniFormer V1(ICLR'2022), UniFormer V2(Arxiv'2022).
-        - Support training MViT V2(CVPR'2022), and MaskFeat(CVPR'2022) fine-tuning.
-        - Add a new handy interface for inference MMAction2 models ([demo](https://github.com/open-mmlab/mmaction2/blob/dev-1.x/demo/README.md#inferencer))
-        
-        ## Installation
-        
-        Please refer to [install.md](https://mmaction2.readthedocs.io/en/1.x/get_started.html) for more detailed instructions.
-        
-        ## Supported Methods
-        
-        <table style="margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;">
-          <tr>
-            <td colspan="5" style="font-weight:bold;">Action Recognition</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/c3d/README.md">C3D</a> (CVPR'2014)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tsn/README.md">TSN</a> (ECCV'2016)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/i3d/README.md">I3D</a> (CVPR'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/c2d/README.md">C2D</a> (CVPR'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/i3d/README.md">I3D Non-Local</a> (CVPR'2018)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/r2plus1d/README.md">R(2+1)D</a> (CVPR'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/trn/README.md">TRN</a> (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tsm/README.md">TSM</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tsm/README.md">TSM Non-Local</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/slowonly/README.md">SlowOnly</a> (ICCV'2019)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/slowfast/README.md">SlowFast</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/csn/README.md">CSN</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tin/README.md">TIN</a> (AAAI'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tpn/README.md">TPN</a> (CVPR'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/x3d/README.md">X3D</a> (CVPR'2020)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition_audio/resnet/README.md">MultiModality: Audio</a> (ArXiv'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tanet/README.md">TANet</a> (ArXiv'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/timesformer/README.md">TimeSformer</a> (ICML'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/swin/README.md">VideoSwin</a> (CVPR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/videomae/README.md">VideoMAE</a> (NeurIPS'2022)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/mvit/README.md">MViT V2</a> (CVPR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/uniformer/README.md">UniFormer V1</a> (ICLR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/uniformerv2/README.md">UniFormer V2</a> (Arxiv'2022)</td>
-            <td></td>
-            <td></td>
-          </tr>
-          <tr>
-            <td colspan="5" style="font-weight:bold;">Action Localization</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/localization/ssn/README.md">SSN</a> (ICCV'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/localization/bsn/README.md">BSN</a> (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/localization/bmn/README.md">BMN</a> (ICCV'2019)</td>
-            <td></td>
-            <td></td>
-          </tr>
-          <tr>
-            <td colspan="5" style="font-weight:bold;">Spatio-Temporal Action Detection</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/acrn/README.md">ACRN</a> (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/ava/README.md">SlowOnly+Fast R-CNN</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/ava/README.md">SlowFast+Fast R-CNN</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/lfb/README.md">LFB</a> (CVPR'2019)</td>
-            <td></td>
-          </tr>
-          <tr>
-            <td colspan="5" style="font-weight:bold;">Skeleton-based Action Recognition</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/stgcn/README.md">ST-GCN</a> (AAAI'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/2s-agcn/README.md">2s-AGCN</a> (CVPR'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/posec3d/README.md">PoseC3D</a> (CVPR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/stgcnpp/README.md">STGCN++</a> (ArXiv'2022)</td>
-            <td></td>
-          </tr>
-        </table>
-        
-        Results and models are available in the *README.md* of each method's config directory.
-        A summary can be found on the [**model zoo**](https://mmaction2.readthedocs.io/en/1.x/modelzoo.html) page.
-        
-        We will keep up with the latest progress of the community and support more popular algorithms and frameworks.
-        If you have any feature requests, please feel free to leave a comment in [Issues](https://github.com/open-mmlab/mmaction2/issues/19).
-        
-        ## Supported Datasets
-        
-        <table style="margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;">
-          <tr>
-            <td colspan="4" style="font-weight:bold;">Action Recognition</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/hmdb51/README.md">HMDB51</a> (<a href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">Homepage</a>) (ICCV'2011)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ucf101/README.md">UCF101</a> (<a href="https://www.crcv.ucf.edu/research/data-sets/ucf101/">Homepage</a>) (CRCV-IR-12-01)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/kinetics/README.md">Kinetics-[400/600/700]</a> (<a href="https://deepmind.com/research/open-source/kinetics/">Homepage</a>) (CVPR'2017)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/sthv1/README.md">SthV1</a>  (ICCV'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/sthv2/README.md">SthV2</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/something-something">Homepage</a>) (ICCV'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/diving48/README.md">Diving48</a> (<a href="http://www.svcl.ucsd.edu/projects/resound/dataset.html">Homepage</a>) (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/jester/README.md">Jester</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/jester">Homepage</a>) (ICCV'2019)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/mit/README.md">Moments in Time</a> (<a href="http://moments.csail.mit.edu/">Homepage</a>) (TPAMI'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/mmit/README.md">Multi-Moments in Time</a> (<a href="http://moments.csail.mit.edu/challenge_iccv_2019.html">Homepage</a>) (ArXiv'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/hvu/README.md">HVU</a> (<a href="https://github.com/holistic-video-understanding/HVU-Dataset">Homepage</a>) (ECCV'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/omnisource/README.md">OmniSource</a> (<a href="https://kennymckormick.github.io/omnisource/">Homepage</a>) (ECCV'2020)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/gym/README.md">FineGYM</a> (<a href="https://sdolivia.github.io/FineGym/">Homepage</a>) (CVPR'2020)</td>
-            <td></td>
-            <td></td>
-            <td></td>
-          </tr>
-          <tr>
-            <td colspan="4" style="font-weight:bold;">Action Localization</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/thumos14/README.md">THUMOS14</a> (<a href="https://www.crcv.ucf.edu/THUMOS14/download.html">Homepage</a>) (THUMOS Challenge 2014)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
-            <td></td>
-            <td></td>
-          </tr>
-          <tr>
-            <td colspan="4" style="font-weight:bold;">Spatio-Temporal Action Detection</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ucf101_24/README.md">UCF101-24*</a> (<a href="http://www.thumos.info/download.html">Homepage</a>) (CRCV-IR-12-01)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/jhmdb/README.md">JHMDB*</a> (<a href="http://jhmdb.is.tue.mpg.de/">Homepage</a>) (ICCV'2015)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ava/README.md">AVA</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (CVPR'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ava_kinetics/README.md">AVA-Kinetics</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (Arxiv'2020)</td>
-          </tr>
-          <tr>
-            <td colspan="4" style="font-weight:bold;">Skeleton-based Action Recognition</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-FineGYM</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-NTURGB+D</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-UCF101</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-HMDB51</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
-          </tr>
-        </table>
-        
-        Datasets marked with * are not fully supported yet, but related dataset preparation steps are provided. A summary can be found on the [**Supported Datasets**](https://mmaction2.readthedocs.io/en/latest/supported_datasets.html) page.
-        
-        ## Data Preparation
-        
-        Please refer to [data_preparation.md](docs/en/user_guides/2_data_prepare.md) for a general knowledge of data preparation.
-        
-        ## FAQ
-        
-        Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked questions.
-        
-        ## Projects built on MMAction2
-        
-        Currently, there are many research works and projects built on MMAction2 by users from community, such as:
-        
-        - Video Swin Transformer. [\[paper\]](https://arxiv.org/abs/2106.13230)[\[github\]](https://github.com/SwinTransformer/Video-Swin-Transformer)
-        - Evidential Deep Learning for Open Set Action Recognition, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/2107.10161)[\[github\]](https://github.com/Cogito2012/DEAR)
-        - Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/2103.17263)[\[github\]](https://github.com/xvjiarui/VFS)
-        
-        etc., check [projects.md](docs/en/notes/projects.md) to see all related projects.
-        
-        ## License
-        
-        This project is released under the [Apache 2.0 license](LICENSE).
-        
-        ## Citation
-        
-        If you find this project useful in your research, please consider cite:
-        
-        ```BibTeX
-        @misc{2020mmaction2,
-            title={OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark},
-            author={MMAction2 Contributors},
-            howpublished = {\url{https://github.com/open-mmlab/mmaction2}},
-            year={2020}
-        }
-        ```
-        
-        ## Contributing
-        
-        We appreciate all contributions to improve MMAction2. Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/1.x/CONTRIBUTING.md) in MMCV for more details about the contributing guideline.
-        
-        ## Acknowledgement
-        
-        MMAction2 is an open-source project that is contributed by researchers and engineers from various colleges and companies.
-        We appreciate all the contributors who implement their methods or add new features and users who give valuable feedback.
-        We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their new models.
-        
-        ## Projects in OpenMMLab
-        
-        - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
-        - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
-        - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-        - [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
-        - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
-        - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
-        - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
-        - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
-        - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
-        - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
-        - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
-        - [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
-        - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
-        - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
-        - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
-        - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
-        - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-        - [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
-        - [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
-        - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
-        
-Keywords: computer vision,video understanding
-Platform: UNKNOWN
-Classifier: Development Status :: 4 - Beta
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Description-Content-Type: text/markdown
-Provides-Extra: all
-Provides-Extra: tests
-Provides-Extra: optional
-Provides-Extra: mim
+<div align="center">
+  <img src="https://github.com/open-mmlab/mmaction2/raw/main/resources/mmaction2_logo.png" width="600"/>
+  <div>&nbsp;</div>
+  <div align="center">
+    <b><font size="5">OpenMMLab website</font></b>
+    <sup>
+      <a href="https://openmmlab.com">
+        <i><font size="4">HOT</font></i>
+      </a>
+    </sup>
+    &nbsp;&nbsp;&nbsp;&nbsp;
+    <b><font size="5">OpenMMLab platform</font></b>
+    <sup>
+      <a href="https://platform.openmmlab.com">
+        <i><font size="4">TRY IT OUT</font></i>
+      </a>
+    </sup>
+  </div>
+
+[![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/)
+[![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions)
+[![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2)
+[![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/)
+[![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/main/LICENSE)
+[![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
+[![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
+
+[📘Documentation](https://mmaction2.readthedocs.io/en/latest/) |
+[🛠️Installation](https://mmaction2.readthedocs.io/en/latest/get_started/installation.html) |
+[👀Model Zoo](https://mmaction2.readthedocs.io/en/latest/modelzoo_statistics.html) |
+[🆕Update News](https://mmaction2.readthedocs.io/en/latest/notes/changelog.html) |
+[🚀Ongoing Projects](https://github.com/open-mmlab/mmaction2/projects) |
+[🤔Reporting Issues](https://github.com/open-mmlab/mmaction2/issues/new/choose)
+
+</div>
+
+<div align="center">
+  <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
+</div>
+
+English | [简体中文](/README_zh-CN.md)
+
+## 📄 Table of Contents
+
+- [📄 Table of Contents](#-table-of-contents)
+- [🥳 🚀 What's New](#--whats-new-)
+- [📖 Introduction](#-introduction-)
+- [🎁 Major Features](#-major-features-)
+- [🛠️ Installation](#️-installation-)
+- [👀 Model Zoo](#-model-zoo-)
+- [👨‍🏫 Get Started](#-get-started-)
+- [🎫 License](#-license-)
+- [🖊️ Citation](#️-citation-)
+- [🙌 Contributing](#-contributing-)
+- [🤝 Acknowledgement](#-acknowledgement-)
+- [🏗️ Projects in OpenMMLab](#️-projects-in-openmmlab-)
+
+## 🥳 🚀 What's New [🔝](#-table-of-contents)
+
+**The default branch has been switched to `main`(previous `1.x`) from `master`(current `0.x`), and we encourage users to migrate to the latest version with more supported models, stronger pre-training checkpoints and simpler coding. Please refer to [Migration Guide](https://mmaction2.readthedocs.io/en/latest/migration.html) for more details.**
+
+**Release (2023.07.04)**: v1.1.0 with the following new features:
+
+- Support CLIP-based multi-modality models: ActionCLIP(Arxiv'2021) and CLIP4clip(ArXiv'2022)
+- Support rich projects: gesture recognition, spatio-temporal action detection tutorial, and knowledge distillation
+- Support HACS-segments dataset(ICCV'2019), MultiSports dataset(ICCV'2021), Kinetics-710 dataset(Arxiv'2022)
+- Support VideoMAE V2(CVPR'2023), and VideoMAE(NeurIPS'2022) on action detection
+- Support TCANet(CVPR'2021)
+- Support [Pure Python style Configuration File](https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-pure-python-style-configuration-file-beta) and downloading datasets by MIM with one command
+
+## 📖 Introduction [🔝](#-table-of-contents)
+
+MMAction2 is an open-source toolbox for video understanding based on PyTorch.
+It is a part of the [OpenMMLab](http://openmmlab.com/) project.
+
+<div align="center">
+  <img src="https://github.com/open-mmlab/mmaction2/raw/main/resources/mmaction2_overview.gif" width="380px">
+  <img src="https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif" width="380px">
+  <p style="font-size:1.5vw;"> Action Recognition on Kinetics-400 (left) and Skeleton-based Action Recognition on NTU-RGB+D-120 (right)</p>
+</div>
+
+<div align="center">
+  <img src="https://user-images.githubusercontent.com/30782254/155710881-bb26863e-fcb4-458e-b0c4-33cd79f96901.gif" width="580px"/><br>
+    <p style="font-size:1.5vw;">Skeleton-based Spatio-Temporal Action Detection and Action Recognition Results on Kinetics-400</p>
+</div>
+<div align="center">
+  <img src="https://github.com/open-mmlab/mmaction2/raw/main/resources/spatio-temporal-det.gif" width="800px"/><br>
+    <p style="font-size:1.5vw;">Spatio-Temporal Action Detection Results on AVA-2.1</p>
+</div>
+
+## 🎁 Major Features [🔝](#-table-of-contents)
+
+- **Modular design**: We decompose a video understanding framework into different components. One can easily construct a customized video understanding framework by combining different modules.
+
+- **Support five major video understanding tasks**: MMAction2 implements various algorithms for multiple video understanding tasks, including action recognition, action localization, spatio-temporal action detection, skeleton-based action detection and video retrieval.
+
+- **Well tested and documented**: We provide detailed documentation and API reference, as well as unit tests.
+
+## 🛠️ Installation [🔝](#-table-of-contents)
+
+MMAction2 depends on [PyTorch](https://pytorch.org/), [MMCV](https://github.com/open-mmlab/mmcv), [MMEngine](https://github.com/open-mmlab/mmengine), [MMDetection](https://github.com/open-mmlab/mmdetection) (optional) and [MMPose](https://github.com/open-mmlab/mmpose) (optional).
+
+Please refer to [install.md](https://mmaction2.readthedocs.io/en/latest/get_started/installation.html) for detailed instructions.
+
+<details close>
+<summary>Quick instructions</summary>
+
+```shell
+conda create --name openmmlab python=3.8 -y
+conda activate open-mmlab
+conda install pytorch torchvision -c pytorch  # This command will automatically install the latest version PyTorch and cudatoolkit, please check whether they match your environment.
+pip install -U openmim
+mim install mmengine
+mim install mmcv
+mim install mmdet  # optional
+mim install mmpose  # optional
+git clone https://github.com/open-mmlab/mmaction2.git
+cd mmaction2
+pip install -v -e .
+```
+
+</details>
+
+## 👀 Model Zoo [🔝](#-table-of-contents)
+
+Results and models are available in the [model zoo](https://mmaction2.readthedocs.io/en/latest/model_zoo/modelzoo.html).
+
+<details close>
+
+<summary>Supported model</summary>
+
+<table style="margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;">
+  <tr>
+    <td colspan="5" style="font-weight:bold;">Action Recognition</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/c3d/README.md">C3D</a> (CVPR'2014)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tsn/README.md">TSN</a> (ECCV'2016)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/i3d/README.md">I3D</a> (CVPR'2017)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/c2d/README.md">C2D</a> (CVPR'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/i3d/README.md">I3D Non-Local</a> (CVPR'2018)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/r2plus1d/README.md">R(2+1)D</a> (CVPR'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/trn/README.md">TRN</a> (ECCV'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tsm/README.md">TSM</a> (ICCV'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tsm/README.md">TSM Non-Local</a> (ICCV'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/slowonly/README.md">SlowOnly</a> (ICCV'2019)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/slowfast/README.md">SlowFast</a> (ICCV'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/csn/README.md">CSN</a> (ICCV'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tin/README.md">TIN</a> (AAAI'2020)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tpn/README.md">TPN</a> (CVPR'2020)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/x3d/README.md">X3D</a> (CVPR'2020)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition_audio/resnet/README.md">MultiModality: Audio</a> (ArXiv'2020)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tanet/README.md">TANet</a> (ArXiv'2020)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/timesformer/README.md">TimeSformer</a> (ICML'2021)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/projects/actionclip/README.md">ActionCLIP</a> (ArXiv'2021)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/swin/README.md">VideoSwin</a> (CVPR'2022)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/videomae/README.md">VideoMAE</a> (NeurIPS'2022)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/mvit/README.md">MViT V2</a> (CVPR'2022)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/uniformer/README.md">UniFormer V1</a> (ICLR'2022)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/uniformerv2/README.md">UniFormer V2</a> (Arxiv'2022)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/videomaev2/README.md">VideoMAE V2</a> (CVPR'2023)</td>
+  </tr>
+  <tr>
+    <td colspan="5" style="font-weight:bold;">Action Localization</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/localization/bsn/README.md">BSN</a> (ECCV'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/localization/bmn/README.md">BMN</a> (ICCV'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/localization/tcanet/README.md">TCANet</a> (CVPR'2021)</td>
+    <td></td>
+    <td></td>
+  </tr>
+  <tr>
+    <td colspan="5" style="font-weight:bold;">Spatio-Temporal Action Detection</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/acrn/README.md">ACRN</a> (ECCV'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/slowonly/README.md">SlowOnly+Fast R-CNN</a> (ICCV'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/slowfast/README.md">SlowFast+Fast R-CNN</a> (ICCV'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/lfb/README.md">LFB</a> (CVPR'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/videomae/README.md">VideoMAE</a> (NeurIPS'2022)</td>
+  </tr>
+  <tr>
+    <td colspan="5" style="font-weight:bold;">Skeleton-based Action Recognition</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/stgcn/README.md">ST-GCN</a> (AAAI'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/2s-agcn/README.md">2s-AGCN</a> (CVPR'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/posec3d/README.md">PoseC3D</a> (CVPR'2022)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/stgcnpp/README.md">STGCN++</a> (ArXiv'2022)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/projects/ctrgcn/README.md">CTRGCN</a> (CVPR'2021)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/projects/msg3d/README.md">MSG3D</a> (CVPR'2020)</td>
+    <td></td>
+    <td></td>
+    <td></td>
+    <td></td>
+  </tr>
+  <tr>
+    <td colspan="5" style="font-weight:bold;">Video Retrieval</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/retrieval/clip4clip/README.md">CLIP4Clip</a> (ArXiv'2022)</td>
+    <td></td>
+    <td></td>
+    <td></td>
+    <td></td>
+  </tr>
+
+</table>
+
+</details>
+
+<details close>
+
+<summary>Supported dataset</summary>
+
+<table style="margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;">
+  <tr>
+    <td colspan="4" style="font-weight:bold;">Action Recognition</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/hmdb51/README.md">HMDB51</a> (<a href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">Homepage</a>) (ICCV'2011)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ucf101/README.md">UCF101</a> (<a href="https://www.crcv.ucf.edu/research/data-sets/ucf101/">Homepage</a>) (CRCV-IR-12-01)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/kinetics/README.md">Kinetics-[400/600/700]</a> (<a href="https://deepmind.com/research/open-source/kinetics/">Homepage</a>) (CVPR'2017)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/sthv1/README.md">SthV1</a>  (ICCV'2017)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/sthv2/README.md">SthV2</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/something-something">Homepage</a>) (ICCV'2017)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/diving48/README.md">Diving48</a> (<a href="http://www.svcl.ucsd.edu/projects/resound/dataset.html">Homepage</a>) (ECCV'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/jester/README.md">Jester</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/jester">Homepage</a>) (ICCV'2019)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/mit/README.md">Moments in Time</a> (<a href="http://moments.csail.mit.edu/">Homepage</a>) (TPAMI'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/mmit/README.md">Multi-Moments in Time</a> (<a href="http://moments.csail.mit.edu/challenge_iccv_2019.html">Homepage</a>) (ArXiv'2019)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/hvu/README.md">HVU</a> (<a href="https://github.com/holistic-video-understanding/HVU-Dataset">Homepage</a>) (ECCV'2020)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/omnisource/README.md">OmniSource</a> (<a href="https://kennymckormick.github.io/omnisource/">Homepage</a>) (ECCV'2020)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/gym/README.md">FineGYM</a> (<a href="https://sdolivia.github.io/FineGym/">Homepage</a>) (CVPR'2020)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/kinetics710/README.md">Kinetics-710</a> (<a href="https://arxiv.org/pdf/2211.09552.pdf">Homepage</a>) (Arxiv'2022)</td>
+    <td></td>
+    <td></td>
+  </tr>
+  <tr>
+    <td colspan="4" style="font-weight:bold;">Action Localization</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/thumos14/README.md">THUMOS14</a> (<a href="https://www.crcv.ucf.edu/THUMOS14/download.html">Homepage</a>) (THUMOS Challenge 2014)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/hacs/README.md">HACS</a> (<a href="https://github.com/hangzhaomit/HACS-dataset">Homepage</a>) (ICCV'2019)</td>
+    <td></td>
+  </tr>
+  <tr>
+    <td colspan="4" style="font-weight:bold;">Spatio-Temporal Action Detection</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ucf101_24/README.md">UCF101-24*</a> (<a href="http://www.thumos.info/download.html">Homepage</a>) (CRCV-IR-12-01)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/jhmdb/README.md">JHMDB*</a> (<a href="http://jhmdb.is.tue.mpg.de/">Homepage</a>) (ICCV'2015)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ava/README.md">AVA</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (CVPR'2018)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ava_kinetics/README.md">AVA-Kinetics</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (Arxiv'2020)</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/multisports/README.md">MultiSports</a> (<a href="https://deeperaction.github.io/datasets/multisports.html">Homepage</a>) (ICCV'2021)</td>
+    <td></td>
+    <td></td>
+    <td></td>
+  </tr>
+  <tr>
+    <td colspan="4" style="font-weight:bold;">Skeleton-based Action Recognition</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-FineGYM</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-NTURGB+D</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-UCF101</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-HMDB51</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+  </tr>
+  <tr>
+    <td colspan="4" style="font-weight:bold;">Video Retrieval</td>
+  </tr>
+  <tr>
+    <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/video_retrieval/README.md">MSRVTT</a> (<a href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/">Homepage</a>) (CVPR'2016)</td>
+    <td></td>
+    <td></td>
+    <td></td>
+  </tr>
+
+</table>
+
+</details>
+
+## 👨‍🏫 Get Started [🔝](#-table-of-contents)
+
+For tutorials, we provide the following user guides for basic usage:
+
+- [Migration from MMAction2 0.X](https://mmaction2.readthedocs.io/en/latest/migration.html)
+- [Learn about Configs](https://mmaction2.readthedocs.io/en/latest/user_guides/config.html)
+- [Prepare Datasets](https://mmaction2.readthedocs.io/en/latest/user_guides/prepare_dataset.html)
+- [Inference with Existing Models](https://mmaction2.readthedocs.io/en/latest/user_guides/inference.html)
+- [Training and Testing](https://mmaction2.readthedocs.io/en/latest/user_guides/train_test.html)
+
+<details close>
+<summary>Research works built on MMAction2 by users from community</summary>
+
+- Video Swin Transformer. [\[paper\]](https://arxiv.org/abs/2106.13230)[\[github\]](https://github.com/SwinTransformer/Video-Swin-Transformer)
+- Evidential Deep Learning for Open Set Action Recognition, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/2107.10161)[\[github\]](https://github.com/Cogito2012/DEAR)
+- Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/2103.17263)[\[github\]](https://github.com/xvjiarui/VFS)
+
+</details>
+
+## 🎫 License [🔝](#-table-of-contents)
+
+This project is released under the [Apache 2.0 license](LICENSE).
+
+## 🖊️ Citation [🔝](#-table-of-contents)
+
+If you find this project useful in your research, please consider cite:
+
+```BibTeX
+@misc{2020mmaction2,
+    title={OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark},
+    author={MMAction2 Contributors},
+    howpublished = {\url{https://github.com/open-mmlab/mmaction2}},
+    year={2020}
+}
+```
+
+## 🙌 Contributing [🔝](#-table-of-contents)
+
+We appreciate all contributions to improve MMAction2. Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/2.x/CONTRIBUTING.md) in MMCV for more details about the contributing guideline.
+
+## 🤝 Acknowledgement [🔝](#-table-of-contents)
+
+MMAction2 is an open-source project that is contributed by researchers and engineers from various colleges and companies.
+We appreciate all the contributors who implement their methods or add new features and users who give valuable feedback.
+We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their new models.
+
+## 🏗️ Projects in OpenMMLab [🔝](#-table-of-contents)
+
+- [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
+- [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
+- [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
+- [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation library for multiple machine learning libraries.
+- [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
+- [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
+- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
+- [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
+- [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and benchmark.
+- [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
+- [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
+- [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
+- [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
+- [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
+- [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
+- [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
+- [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
+- [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
+- [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
+- [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
+- [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
+- [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
+- [Playground](https://github.com/open-mmlab/playground): A central hub for gathering and showcasing amazing projects built upon OpenMMLab.
```

#### html2text {}

```diff
@@ -1,176 +1,208 @@
-Metadata-Version: 2.1 Name: mmaction2 Version: 1.0.0rc3 Summary: OpenMMLab
-Video Understanding Toolbox and Benchmark Home-page: https://github.com/open-
-mmlab/mmaction2 Author: MMAction2 Contributors Author-email:
-openmmlab@gmail.com Maintainer: MMAction2 Contributors Maintainer-email:
-openmmlab@gmail.com License: Apache License 2.0 Description:
-[https://github.com/open-mmlab/mmaction2/raw/1.x/resources/mmaction2_logo.png]
+[https://github.com/open-mmlab/mmaction2/raw/main/resources/mmaction2_logo.png]
                                         
            OpenMMLab website HOT      OpenMMLab platform TRY_IT_OUT
       [![Documentation](https://readthedocs.org/projects/mmaction2/badge/
-?version=latest)](https://mmaction2.readthedocs.io/en/1.x/) [![actions](https:/
-     /github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://
+  ?version=latest)](https://mmaction2.readthedocs.io/en/latest/) [![actions]
+ (https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://
   github.com/open-mmlab/mmaction2/actions) [![codecov](https://codecov.io/gh/
-  open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/
-open-mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https:
-   //pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/
-  license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/
-      blob/master/LICENSE) [![Average time to resolve an issue](https://
-    isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://
-  github.com/open-mmlab/mmaction2/issues) [![Percentage of issues still open]
-  (https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://
-     github.com/open-mmlab/mmaction2/issues) [ðDocumentation](https://
-      mmaction2.readthedocs.io/en/1.x/) | [ð ï¸Installation](https://
- mmaction2.readthedocs.io/en/1.x/get_started.html) | [ðModel Zoo](https://
-  mmaction2.readthedocs.io/en/1.x/modelzoo.html) | [ðUpdate News](https://
-mmaction2.readthedocs.io/en/1.x/notes/changelog.html) | [ðOngoing Projects]
-  (https://github.com/open-mmlab/mmaction2/projects) | [ð¤Reporting Issues]
-          (https://github.com/open-mmlab/mmaction2/issues/new/choose)
-## Introduction MMAction2 is an open-source toolbox for video understanding
-based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.org/
-) project. The 1.x branch works with **PyTorch 1.6+**.
-          [https://github.com/open-mmlab/mmaction2/raw/1.x/resources/
-                            mmaction2_overview.gif]
-                  Action Recognition Results on Kinetics-400
- [https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-
-                          11eb-916b-b9db5563a9e5.gif]
-          Skeleton-based Action Recognition Results on NTU-RGB+D-120
+open-mmlab/mmaction2/branch/main/graph/badge.svg)](https://codecov.io/gh/open-
+ mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://
+pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/license/
+ open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/main/
+LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/
+resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/
+ issues) [![Percentage of issues still open](https://isitmaintained.com/badge/
+open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
+      [ðDocumentation](https://mmaction2.readthedocs.io/en/latest/) |
+ [ð ï¸Installation](https://mmaction2.readthedocs.io/en/latest/get_started/
+   installation.html) | [ðModel Zoo](https://mmaction2.readthedocs.io/en/
+         latest/modelzoo_statistics.html) | [ðUpdate News](https://
+    mmaction2.readthedocs.io/en/latest/notes/changelog.html) | [ðOngoing
+ Projects](https://github.com/open-mmlab/mmaction2/projects) | [ð¤Reporting
+      Issues](https://github.com/open-mmlab/mmaction2/issues/new/choose)
+
+English | [ç®ä½ä¸­æ](/README_zh-CN.md) ## ð Table of Contents - [ð
+Table of Contents](#-table-of-contents) - [ð¥³ ð What's New](#--whats-new-
+) - [ð Introduction](#-introduction-) - [ð Major Features](#-major-
+features-) - [ð ï¸ Installation](#ï¸-installation-) - [ð Model Zoo](#-
+model-zoo-) - [ð¨âð« Get Started](#-get-started-) - [ð« License](#-
+license-) - [ðï¸ Citation](#ï¸-citation-) - [ð Contributing](#-
+contributing-) - [ð¤ Acknowledgement](#-acknowledgement-) - [ðï¸ Projects
+in OpenMMLab](#ï¸-projects-in-openmmlab-) ## ð¥³ ð What's New [ð](#-
+table-of-contents) **The default branch has been switched to `main`(previous
+`1.x`) from `master`(current `0.x`), and we encourage users to migrate to the
+latest version with more supported models, stronger pre-training checkpoints
+and simpler coding. Please refer to [Migration Guide](https://
+mmaction2.readthedocs.io/en/latest/migration.html) for more details.**
+**Release (2023.07.04)**: v1.1.0 with the following new features: - Support
+CLIP-based multi-modality models: ActionCLIP(Arxiv'2021) and CLIP4clip
+(ArXiv'2022) - Support rich projects: gesture recognition, spatio-temporal
+action detection tutorial, and knowledge distillation - Support HACS-segments
+dataset(ICCV'2019), MultiSports dataset(ICCV'2021), Kinetics-710 dataset
+(Arxiv'2022) - Support VideoMAE V2(CVPR'2023), and VideoMAE(NeurIPS'2022) on
+action detection - Support TCANet(CVPR'2021) - Support [Pure Python style
+Configuration File](https://mmengine.readthedocs.io/en/latest/
+advanced_tutorials/config.html#a-pure-python-style-configuration-file-beta) and
+downloading datasets by MIM with one command ## ð Introduction [ð](#-
+table-of-contents) MMAction2 is an open-source toolbox for video understanding
+based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.com/
+) project.
+         [https://github.com/open-mmlab/mmaction2/raw/main/resources/
+ mmaction2_overview.gif] [https://user-images.githubusercontent.com/34324155/
+              123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif]
+Action Recognition on Kinetics-400 (left) and Skeleton-based Action Recognition
+                           on NTU-RGB+D-120 (right)
  [https://user-images.githubusercontent.com/30782254/155710881-bb26863e-fcb4-
                           458e-b0c4-33cd79f96901.gif]
 Skeleton-based Spatio-Temporal Action Detection and Action Recognition Results
                                 on Kinetics-400
-  [https://github.com/open-mmlab/mmaction2/raw/1.x/resources/spatio-temporal-
+ [https://github.com/open-mmlab/mmaction2/raw/main/resources/spatio-temporal-
                                    det.gif]
               Spatio-Temporal Action Detection Results on AVA-2.1
-## Major Features - **Modular design**: We decompose a video understanding
-framework into different components. One can easily construct a customized
-video understanding framework by combining different modules. - **Support four
-major video understanding tasks**: MMAction2 implements various algorithms for
-multiple video understanding tasks, including action recognition, action
-localization, spatio-temporal action detection, and skeleton-based action
-detection. - **Well tested and documented**: We provide detailed documentation
-and API reference, as well as unit tests. ## What's New **Release
-(2022.02.10)**: v1.0.0rc3 with the following new features: - Support Action
-Recognition model UniFormer V1(ICLR'2022), UniFormer V2(Arxiv'2022). - Support
-training MViT V2(CVPR'2022), and MaskFeat(CVPR'2022) fine-tuning. - Add a new
-handy interface for inference MMAction2 models ([demo](https://github.com/open-
-mmlab/mmaction2/blob/dev-1.x/demo/README.md#inferencer)) ## Installation Please
-refer to [install.md](https://mmaction2.readthedocs.io/en/1.x/get_started.html)
-for more detailed instructions. ## Supported Methods
+## ð Major Features [ð](#-table-of-contents) - **Modular design**: We
+decompose a video understanding framework into different components. One can
+easily construct a customized video understanding framework by combining
+different modules. - **Support five major video understanding tasks**:
+MMAction2 implements various algorithms for multiple video understanding tasks,
+including action recognition, action localization, spatio-temporal action
+detection, skeleton-based action detection and video retrieval. - **Well tested
+and documented**: We provide detailed documentation and API reference, as well
+as unit tests. ## ð ï¸ Installation [ð](#-table-of-contents) MMAction2
+depends on [PyTorch](https://pytorch.org/), [MMCV](https://github.com/open-
+mmlab/mmcv), [MMEngine](https://github.com/open-mmlab/mmengine), [MMDetection]
+(https://github.com/open-mmlab/mmdetection) (optional) and [MMPose](https://
+github.com/open-mmlab/mmpose) (optional). Please refer to [install.md](https://
+mmaction2.readthedocs.io/en/latest/get_started/installation.html) for detailed
+instructions.  Quick instructions ```shell conda create --name openmmlab
+python=3.8 -y conda activate open-mmlab conda install pytorch torchvision -
+c pytorch # This command will automatically install the latest version PyTorch
+and cudatoolkit, please check whether they match your environment. pip install
+-U openmim mim install mmengine mim install mmcv mim install mmdet # optional
+mim install mmpose # optional git clone https://github.com/open-mmlab/
+mmaction2.git cd mmaction2 pip install -v -e . ```  ## ð Model Zoo [ð](#-
+table-of-contents) Results and models are available in the [model zoo](https://
+mmaction2.readthedocs.io/en/latest/model_zoo/modelzoo.html).  Supported model
 Action Recognition
 C3D (CVPR'2014) TSN (ECCV'2016) I3D (CVPR'2017) C2D (CVPR'2018) I3D_Non-Local
                                                                 (CVPR'2018)
 R(2+1)D         TRN (ECCV'2018) TSM (ICCV'2019) TSM_Non-Local   SlowOnly
 (CVPR'2018)                                     (ICCV'2019)     (ICCV'2019)
 SlowFast        CSN (ICCV'2019) TIN (AAAI'2020) TPN (CVPR'2020) X3D (CVPR'2020)
 (ICCV'2019)
-MultiModality:  TANet           TimeSformer     VideoSwin       VideoMAE
-Audio           (ArXiv'2020)    (ICML'2021)     (CVPR'2022)     (NeurIPS'2022)
+MultiModality:  TANet           TimeSformer     ActionCLIP      VideoSwin
+Audio           (ArXiv'2020)    (ICML'2021)     (ArXiv'2021)    (CVPR'2022)
 (ArXiv'2020)
-MViT_V2         UniFormer_V1    UniFormer_V2
-(CVPR'2022)     (ICLR'2022)     (Arxiv'2022)
+VideoMAE        MViT_V2         UniFormer_V1    UniFormer_V2    VideoMAE_V2
+(NeurIPS'2022)  (CVPR'2022)     (ICLR'2022)     (Arxiv'2022)    (CVPR'2023)
 Action Localization
-SSN (ICCV'2017) BSN (ECCV'2018) BMN (ICCV'2019)
+BSN (ECCV'2018) BMN (ICCV'2019) TCANet
+                                (CVPR'2021)
 Spatio-Temporal Action Detection
-ACRN            SlowOnly+Fast   SlowFast+Fast
-(ECCV'2018)     R-CNN           R-CNN           LFB (CVPR'2019)
+ACRN            SlowOnly+Fast   SlowFast+Fast                   VideoMAE
+(ECCV'2018)     R-CNN           R-CNN           LFB (CVPR'2019) (NeurIPS'2022)
                 (ICCV'2019)     (ICCV'2019)
 Skeleton-based Action Recognition
-ST-GCN          2s-AGCN         PoseC3D         STGCN++
-(AAAI'2018)     (CVPR'2019)     (CVPR'2022)     (ArXiv'2022)
-Results and models are available in the *README.md* of each method's config
-directory. A summary can be found on the [**model zoo**](https://
-mmaction2.readthedocs.io/en/1.x/modelzoo.html) page. We will keep up with the
-latest progress of the community and support more popular algorithms and
-frameworks. If you have any feature requests, please feel free to leave a
-comment in [Issues](https://github.com/open-mmlab/mmaction2/issues/19). ##
-Supported Datasets
+ST-GCN          2s-AGCN         PoseC3D         STGCN++         CTRGCN
+(AAAI'2018)     (CVPR'2019)     (CVPR'2022)     (ArXiv'2022)    (CVPR'2021)
+MSG3D
+(CVPR'2020)
+Video Retrieval
+CLIP4Clip
+(ArXiv'2022)
+  Supported dataset
 Action Recognition
 HMDB51 (Homepage)    UCF101 (Homepage)  ActivityNet          Kinetics-[400/600/
 (ICCV'2011)          (CRCV-IR-12-01)    (Homepage)           700] (Homepage)
                                         (CVPR'2015)          (CVPR'2017)
 SthV1 (ICCV'2017)    SthV2 (Homepage)   Diving48 (Homepage)  Jester (Homepage)
                      (ICCV'2017)        (ECCV'2018)          (ICCV'2019)
 Moments_in_Time      Multi-Moments_in   HVU (Homepage)       OmniSource
 (Homepage)           Time (Homepage)    (ECCV'2020)          (Homepage)
 (TPAMI'2019)         (ArXiv'2019)                            (ECCV'2020)
-FineGYM (Homepage)
-(CVPR'2020)
+FineGYM (Homepage)   Kinetics-710
+(CVPR'2020)          (Homepage)
+                     (Arxiv'2022)
 Action Localization
-THUMOS14 (Homepage)  ActivityNet
-(THUMOS Challenge    (Homepage)
+THUMOS14 (Homepage)  ActivityNet        HACS (Homepage)
+(THUMOS Challenge    (Homepage)         (ICCV'2019)
 2014)                (CVPR'2015)
 Spatio-Temporal Action Detection
 UCF101-24*           JHMDB* (Homepage)  AVA (Homepage)       AVA-Kinetics
 (Homepage) (CRCV-IR- (ICCV'2015)        (CVPR'2018)          (Homepage)
 12-01)                                                       (Arxiv'2020)
+MultiSports
+(Homepage)
+(ICCV'2021)
 Skeleton-based Action Recognition
 PoseC3D-FineGYM      PoseC3D-NTURGB+D   PoseC3D-UCF101       PoseC3D-HMDB51
 (Homepage)           (Homepage)         (Homepage)           (Homepage)
 (ArXiv'2021)         (ArXiv'2021)       (ArXiv'2021)         (ArXiv'2021)
-Datasets marked with * are not fully supported yet, but related dataset
-preparation steps are provided. A summary can be found on the [**Supported
-Datasets**](https://mmaction2.readthedocs.io/en/latest/supported_datasets.html)
-page. ## Data Preparation Please refer to [data_preparation.md](docs/en/
-user_guides/2_data_prepare.md) for a general knowledge of data preparation. ##
-FAQ Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked questions.
-## Projects built on MMAction2 Currently, there are many research works and
-projects built on MMAction2 by users from community, such as: - Video Swin
-Transformer. [\[paper\]](https://arxiv.org/abs/2106.13230)[\[github\]](https://
-github.com/SwinTransformer/Video-Swin-Transformer) - Evidential Deep Learning
-for Open Set Action Recognition, ICCV 2021 **Oral**. [\[paper\]](https://
-arxiv.org/abs/2107.10161)[\[github\]](https://github.com/Cogito2012/DEAR) -
-Rethinking Self-supervised Correspondence Learning: A Video Frame-level
-Similarity Perspective, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/
-2103.17263)[\[github\]](https://github.com/xvjiarui/VFS) etc., check
-[projects.md](docs/en/notes/projects.md) to see all related projects. ##
-License This project is released under the [Apache 2.0 license](LICENSE). ##
-Citation If you find this project useful in your research, please consider
-cite: ```BibTeX @misc{2020mmaction2, title={OpenMMLab's Next Generation Video
-Understanding Toolbox and Benchmark}, author={MMAction2 Contributors},
-howpublished = {\url{https://github.com/open-mmlab/mmaction2}}, year={2020} }
-``` ## Contributing We appreciate all contributions to improve MMAction2.
-Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/1.x/
-CONTRIBUTING.md) in MMCV for more details about the contributing guideline. ##
-Acknowledgement MMAction2 is an open-source project that is contributed by
-researchers and engineers from various colleges and companies. We appreciate
-all the contributors who implement their methods or add new features and users
-who give valuable feedback. We wish that the toolbox and benchmark could serve
-the growing research community by providing a flexible toolkit to reimplement
-existing methods and develop their new models. ## Projects in OpenMMLab -
-[MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational
-library for training deep learning models. - [MMCV](https://github.com/open-
-mmlab/mmcv): OpenMMLab foundational library for computer vision. - [MIM](https:
-//github.com/open-mmlab/mim): MIM installs OpenMMLab packages. -
-[MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab
-image classification toolbox and benchmark. - [MMDetection](https://github.com/
-open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark. -
-[MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-
-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
+Video Retrieval
+MSRVTT (Homepage)
+(CVPR'2016)
+ ## ð¨âð« Get Started [ð](#-table-of-contents) For tutorials, we
+provide the following user guides for basic usage: - [Migration from MMAction2
+0.X](https://mmaction2.readthedocs.io/en/latest/migration.html) - [Learn about
+Configs](https://mmaction2.readthedocs.io/en/latest/user_guides/config.html) -
+[Prepare Datasets](https://mmaction2.readthedocs.io/en/latest/user_guides/
+prepare_dataset.html) - [Inference with Existing Models](https://
+mmaction2.readthedocs.io/en/latest/user_guides/inference.html) - [Training and
+Testing](https://mmaction2.readthedocs.io/en/latest/user_guides/
+train_test.html)  Research works built on MMAction2 by users from community -
+Video Swin Transformer. [\[paper\]](https://arxiv.org/abs/2106.13230)[\
+[github\]](https://github.com/SwinTransformer/Video-Swin-Transformer) -
+Evidential Deep Learning for Open Set Action Recognition, ICCV 2021 **Oral**.
+[\[paper\]](https://arxiv.org/abs/2107.10161)[\[github\]](https://github.com/
+Cogito2012/DEAR) - Rethinking Self-supervised Correspondence Learning: A Video
+Frame-level Similarity Perspective, ICCV 2021 **Oral**. [\[paper\]](https://
+arxiv.org/abs/2103.17263)[\[github\]](https://github.com/xvjiarui/VFS)  ## ð«
+License [ð](#-table-of-contents) This project is released under the [Apache
+2.0 license](LICENSE). ## ðï¸ Citation [ð](#-table-of-contents) If you
+find this project useful in your research, please consider cite: ```BibTeX
+@misc{2020mmaction2, title={OpenMMLab's Next Generation Video Understanding
+Toolbox and Benchmark}, author={MMAction2 Contributors}, howpublished = {\url
+{https://github.com/open-mmlab/mmaction2}}, year={2020} } ``` ## ð
+Contributing [ð](#-table-of-contents) We appreciate all contributions to
+improve MMAction2. Please refer to [CONTRIBUTING.md](https://github.com/open-
+mmlab/mmcv/blob/2.x/CONTRIBUTING.md) in MMCV for more details about the
+contributing guideline. ## ð¤ Acknowledgement [ð](#-table-of-contents)
+MMAction2 is an open-source project that is contributed by researchers and
+engineers from various colleges and companies. We appreciate all the
+contributors who implement their methods or add new features and users who give
+valuable feedback. We wish that the toolbox and benchmark could serve the
+growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their new models. ## ðï¸ Projects in OpenMMLab
+[ð](#-table-of-contents) - [MMEngine](https://github.com/open-mmlab/
+mmengine): OpenMMLab foundational library for training deep learning models. -
+[MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for
+computer vision. - [MIM](https://github.com/open-mmlab/mim): MIM installs
+OpenMMLab packages. - [MMEval](https://github.com/open-mmlab/mmeval): A unified
+evaluation library for multiple machine learning libraries. - [MMPreTrain]
+(https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and
+benchmark. - [MMDetection](https://github.com/open-mmlab/mmdetection):
+OpenMMLab detection toolbox and benchmark. - [MMDetection3D](https://
+github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for
+general 3D object detection. - [MMRotate](https://github.com/open-mmlab/
+mmrotate): OpenMMLab rotated object detection toolbox and benchmark. - [MMYOLO]
+(https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and
 benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
 OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
 github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
 understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
 OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
 github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
 and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
 OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
 github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
 benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
 fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
 mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
 benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
 video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
-Keywords: computer vision,video understanding Platform: UNKNOWN Classifier:
-Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Programming Language :: Python :: 3 Classifier: Programming Language :: Python
-:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
-Programming Language :: Python :: 3.9 Description-Content-Type: text/markdown
-Provides-Extra: all Provides-Extra: tests Provides-Extra: optional Provides-
-Extra: mim
+mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMagic](https:/
+/github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and
+**I**ntelligent **C**reation toolbox. - [MMGeneration](https://github.com/open-
+mmlab/mmgeneration): OpenMMLab image and video generative models toolbox. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment
+framework. - [Playground](https://github.com/open-mmlab/playground): A central
+hub for gathering and showcasing amazing projects built upon OpenMMLab.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/default_runtime.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/default_runtime.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/c2d_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/c2d_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/c3d_sports1m_pretrained.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/c3d_sports1m_pretrained.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/i3d_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/i3d_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/ircsn_r152.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/ircsn_r152.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/r2plus1d_r34.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/r2plus1d_r34.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/slowfast_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/slowfast_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/slowonly_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/slowonly_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/swin_tiny.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/swin_tiny.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tanet_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tanet_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tin_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tin_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tpn_slowonly_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tpn_slowonly_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tpn_tsm_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tpn_tsm_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/trn_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/trn_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tsm_mobilenet_v2.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tsm_mobilenet_v2.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tsm_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tsm_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/tsn_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/tsn_r50.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/models/x3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/models/x3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_150e_warmup.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_150e_warmup.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_100e.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_100e.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_50e.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_50e.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_100e.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_100e.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_50e.py` & `mmaction2-1.1.0/mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_50e.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/_base_/models/slowonly_r50.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,54 +1,70 @@
-url = ('https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/'
-       'slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-'
-       'rgb/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_'
-       'kinetics400-rgb_20220901-e7b65fad.pth')
+_base_ = ['../../_base_/default_runtime.py']
 
+# model settings
+num_frames = 32
 model = dict(
-    type='FastRCNN',
-    _scope_='mmdet',
-    init_cfg=dict(type='Pretrained', checkpoint=url),
+    type='Recognizer3D',
     backbone=dict(
-        type='ResNet3dSlowOnly',
-        depth=50,
-        pretrained=None,
-        pretrained2d=False,
-        lateral=False,
-        num_stages=4,
-        conv1_kernel=(1, 7, 7),
-        conv1_stride_t=1,
-        pool1_stride_t=1,
-        spatial_strides=(1, 2, 2, 1)),
-    roi_head=dict(
-        type='AVARoIHead',
-        bbox_roi_extractor=dict(
-            type='SingleRoIExtractor3D',
-            roi_layer_type='RoIAlign',
-            output_size=8,
-            with_temporal_pool=True),
-        bbox_head=dict(
-            type='BBoxHeadAVA',
-            in_channels=2048,
-            num_classes=81,
-            multilabel=True,
-            dropout_ratio=0.5)),
+        type='UniFormerV2',
+        input_resolution=336,
+        patch_size=14,
+        width=1024,
+        layers=24,
+        heads=16,
+        t_size=num_frames,
+        dw_reduction=1.5,
+        backbone_drop_path_rate=0.,
+        temporal_downsample=False,
+        no_lmhra=True,
+        double_lmhra=True,
+        return_list=[20, 21, 22, 23],
+        n_layers=4,
+        n_dim=1024,
+        n_head=16,
+        mlp_factor=4.,
+        drop_path_rate=0.,
+        mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
+    cls_head=dict(
+        type='TimeSformerHead',
+        dropout_ratio=0.5,
+        num_classes=700,
+        in_channels=1024,
+        average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
-        _scope_='mmaction',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        format_shape='NCTHW'),
-    train_cfg=dict(
-        rcnn=dict(
-            assigner=dict(
-                type='MaxIoUAssignerAVA',
-                pos_iou_thr=0.9,
-                neg_iou_thr=0.9,
-                min_pos_iou=0.9),
-            sampler=dict(
-                type='RandomSampler',
-                num=32,
-                pos_fraction=1,
-                neg_pos_ub=-1,
-                add_gt_as_proposals=True),
-            pos_weight=1.0)),
-    test_cfg=dict(rcnn=None))
+        mean=[114.75, 114.75, 114.75],
+        std=[57.375, 57.375, 57.375],
+        format_shape='NCTHW'))
+
+# dataset settings
+dataset_type = 'VideoDataset'
+data_root_val = 'data/k700'
+ann_file_test = 'data/k700/val.csv'
+
+test_pipeline = [
+    dict(type='DecordInit'),
+    dict(
+        type='UniformSample', clip_len=num_frames, num_clips=2,
+        test_mode=True),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 336)),
+    dict(type='ThreeCrop', crop_size=336),
+    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='PackActionInputs')
+]
+
+test_dataloader = dict(
+    batch_size=4,
+    num_workers=8,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_test,
+        data_prefix=dict(video=data_root_val),
+        pipeline=test_pipeline,
+        test_mode=True,
+        delimiter=','))
+
+test_evaluator = dict(type='AccMetric')
+test_cfg = dict(type='TestLoop')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/acrn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/acrn/metafile.yml`

 * *Files 13% similar despite different names*

```diff
@@ -1,47 +1,45 @@
 Collections:
-- Name: ACRN
-  README: configs/detection/acrn/README.md
-  Paper:
-    URL: https://arxiv.org/abs/1807.10982
-    Title: "Actor-Centric Relation Network"
+  - Name: ACRN
+    README: configs/detection/acrn/README.md
+    Paper:
+      URL: https://arxiv.org/abs/1807.10982
+      Title: "Actor-Centric Relation Network"
 
 Models:
   - Name: slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb
     Config: configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb.py
     In Collection: ACRN
     Metadata:
       Architecture: ResNet50
       Batch Size: 8
       Epochs: 10
       Pretrained: Kinetics-400
-      Resolution: short-side 320
       Training Data: AVA v2.1
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
       - Dataset: AVA v2.1
         Task: Action Detection
         Metrics:
-              mAP: 27.58
+              mAP: 27.65
     Training Log: https://download.openmmlab.com/mmaction/v1.0/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb_20220906-0dae1a90.pth
 
   - Name: slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb
     Config: configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb.py
     In Collection: ACRN
     Metadata:
       Architecture: ResNet50
       Batch Size: 8
       Epochs: 10
       Pretrained: Kinetics-400
-      Resolution: short-side 320
       Training Data: AVA v2.2
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
-      - Dataset: AVA v2.1
+      - Dataset: AVA v2.2
         Task: Action Detection
         Metrics:
-              mAP: 27.63
+              mAP: 27.71
     Training Log: https://download.openmmlab.com/mmaction/v1.0/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb_20220906-66ec24a2.pth
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 _base_ = [
-    '../../_base_/default_runtime.py', '../_base_/models/slowonly_r50.py'
+    '../slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb.py'  # noqa: E501
 ]
 
+url = ('https://download.openmmlab.com/mmaction/recognition/slowfast/'
+       'slowfast_r50_4x16x1_256e_kinetics400_rgb/'
+       'slowfast_r50_4x16x1_256e_kinetics400_rgb_20200704-bcde7ed7.pth')
+num_classes = 66
 model = dict(
+    init_cfg=dict(type='Pretrained', checkpoint=url),
     backbone=dict(
         _delete_=True,
-        type='ResNet3dSlowFast',
-        _scope_='mmaction',
-        pretrained=(
-            'https://download.openmmlab.com/mmaction/recognition/slowfast/'
-            'slowfast_r50_8x8x1_256e_kinetics400_rgb/'
-            'slowfast_r50_8x8x1_256e_kinetics400_rgb_20200716-73547d2b.pth'),
-        resample_rate=4,
-        speed_ratio=4,
+        type='mmaction.ResNet3dSlowFast',
+        pretrained=None,
+        resample_rate=8,
+        speed_ratio=8,
         channel_ratio=8,
         slow_pathway=dict(
             type='resnet3d',
             depth=50,
             pretrained=None,
             lateral=True,
-            fusion_kernel=7,
             conv1_kernel=(1, 7, 7),
             dilations=(1, 1, 1, 1),
             conv1_stride_t=1,
             pool1_stride_t=1,
             inflate=(0, 0, 1, 1),
             spatial_strides=(1, 2, 2, 1)),
         fast_pathway=dict(
@@ -32,108 +32,103 @@
             pretrained=None,
             lateral=False,
             base_channels=8,
             conv1_kernel=(5, 7, 7),
             conv1_stride_t=1,
             pool1_stride_t=1,
             spatial_strides=(1, 2, 2, 1))),
-    roi_head=dict(
-        shared_head=dict(type='ACRNHead', in_channels=4608, out_channels=2304),
-        bbox_head=dict(in_channels=2304)))
+    roi_head=dict(bbox_head=dict(in_channels=2304)))
 
 dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
+data_root = 'data/multisports/trainval'
+anno_root = 'data/multisports/annotations'
 
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
-proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
+ann_file_train = f'{anno_root}/multisports_train.csv'
+ann_file_val = f'{anno_root}/multisports_val.csv'
+gt_file = f'{anno_root}/multisports_GT.pkl'
 
-ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
+proposal_file_train = f'{anno_root}/multisports_dense_proposals_train.recall_96.13.pkl'  # noqa: E501
+proposal_file_val = f'{anno_root}/multisports_dense_proposals_val.recall_96.13.pkl'  # noqa: E501
 
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
-
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
+file_client_args = dict(io_backend='disk')
 
 train_pipeline = [
+    dict(type='DecordInit', **file_client_args),
     dict(type='SampleAVAFrames', clip_len=32, frame_interval=2),
-    dict(type='RawFrameDecode'),
+    dict(type='DecordDecode'),
     dict(type='RandomRescale', scale_range=(256, 320)),
     dict(type='RandomCrop', size=256),
     dict(type='Flip', flip_ratio=0.5),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
+
 # The testing is w/o. any cropping / flipping
 val_pipeline = [
+    dict(type='DecordInit', **file_client_args),
     dict(
         type='SampleAVAFrames', clip_len=32, frame_interval=2, test_mode=True),
-    dict(type='RawFrameDecode'),
+    dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=8,
+    batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
         pipeline=train_pipeline,
-        label_file=label_file,
+        num_classes=num_classes,
         proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
+        data_prefix=dict(img=data_root),
+        timestamp_start=1,
+        start_index=0,
+        use_frames=False,
+        fps=1,
+    ))
 val_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
-        exclude_file=exclude_file_val,
         pipeline=val_pipeline,
-        label_file=label_file,
+        num_classes=num_classes,
         proposal_file=proposal_file_val,
         data_prefix=dict(img=data_root),
-        test_mode=True))
+        test_mode=True,
+        timestamp_start=1,
+        start_index=0,
+        use_frames=False,
+        fps=1,
+    ))
 test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='AVAMetric',
-    ann_file=ann_file_val,
-    label_file=label_file,
-    exclude_file=exclude_file_val)
+val_evaluator = dict(type='MultiSportsMetric', ann_file=gt_file)
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=10, val_begin=1, val_interval=1)
+    type='EpochBasedTrainLoop', max_epochs=8, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
+    dict(type='LinearLR', start_factor=0.1, by_epoch=True, begin=0, end=5),
     dict(
-        type='LinearLR',
-        start_factor=0.1,
-        by_epoch=True,
+        type='MultiStepLR',
         begin=0,
-        end=2,
-        convert_to_iter_based=True),
-    dict(
-        type='CosineAnnealingLR',
-        T_max=8,
-        eta_min=0,
+        end=8,
         by_epoch=True,
-        begin=2,
-        end=10,
-        convert_to_iter_based=True)
+        milestones=[6, 7],
+        gamma=0.1)
 ]
 
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.00001),
-    clip_grad=dict(max_norm=40, norm_type=2))
+    optimizer=dict(type='SGD', lr=0.01125, momentum=0.9, weight_decay=0.00001),
+    clip_grad=dict(max_norm=5, norm_type=2))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50_8xb8-16x4x1-10e-tricks_ava-kinetics-rgb.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,40 +1,45 @@
-_base_ = [('slowfast-acrn_kinetics400-pretrained-r50'
-           '_8xb8-8x8x1-cosine-10e_ava21-rgb.py')]
+_base_ = ['slowonly_k700-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py']
 
-dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
+model = dict(
+    roi_head=dict(
+        bbox_roi_extractor=dict(with_global=True, temporal_pool_mode='max'),
+        bbox_head=dict(in_channels=4096, mlp_head=True, focal_gamma=1.0)))
+
+dataset_type = 'AVAKineticsDataset'
+data_root = 'data/ava_kinetics/rawframes'
+anno_root = 'data/ava_kinetics/annotations'
 
 ann_file_train = f'{anno_root}/ava_train_v2.2.csv'
 ann_file_val = f'{anno_root}/ava_val_v2.2.csv'
 
 exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.2.csv'
 exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.2.csv'
 
 label_file = f'{anno_root}/ava_action_list_v2.2_for_activitynet_2019.pbtxt'
 
 proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
                        'recall_93.9.pkl')
 proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
 
+file_client_args = dict(io_backend='disk')
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=32, frame_interval=2),
-    dict(type='RawFrameDecode'),
+    dict(type='SampleAVAFrames', clip_len=16, frame_interval=4),
+    dict(type='RawFrameDecode', **file_client_args),
     dict(type='RandomRescale', scale_range=(256, 320)),
     dict(type='RandomCrop', size=256),
     dict(type='Flip', flip_ratio=0.5),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 # The testing is w/o. any cropping / flipping
 val_pipeline = [
     dict(
-        type='SampleAVAFrames', clip_len=32, frame_interval=2, test_mode=True),
-    dict(type='RawFrameDecode'),
+        type='SampleAVAFrames', clip_len=16, frame_interval=4, test_mode=True),
+    dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=8,
@@ -67,7 +72,42 @@
 
 val_evaluator = dict(
     type='AVAMetric',
     ann_file=ann_file_val,
     label_file=label_file,
     exclude_file=exclude_file_val)
 test_evaluator = val_evaluator
+
+train_cfg = dict(
+    type='EpochBasedTrainLoop', max_epochs=10, val_begin=1, val_interval=1)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
+
+param_scheduler = [
+    dict(
+        type='LinearLR',
+        start_factor=0.1,
+        by_epoch=True,
+        begin=0,
+        end=2,
+        convert_to_iter_based=True),
+    dict(
+        type='CosineAnnealingLR',
+        T_max=8,
+        eta_min=0,
+        by_epoch=True,
+        begin=2,
+        end=10,
+        convert_to_iter_based=True)
+]
+
+optim_wrapper = dict(
+    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.00001),
+    clip_grad=dict(max_norm=40, norm_type=2))
+
+default_hooks = dict(checkpoint=dict(max_keep_ckpts=2))
+
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (8 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=64)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb6-1x1x16-50e_sthv1-rgb.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,101 +1,122 @@
-_base_ = ['slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py']
+_base_ = [
+    '../../_base_/models/tanet_r50.py', '../../_base_/default_runtime.py',
+    '../../_base_/schedules/sgd_tsm_50e.py'
+]
 
+# model settings
 model = dict(
-    backbone=dict(
-        _delete_=True,
-        type='ResNet3dSlowFast',
-        _scope_='mmaction',
-        pretrained=(
-            'https://download.openmmlab.com/mmaction/recognition/slowfast/'
-            'slowfast_r50_4x16x1_256e_kinetics400_rgb/'
-            'slowfast_r50_4x16x1_256e_kinetics400_rgb_20200704-bcde7ed7.pth'),
-        resample_rate=8,
-        speed_ratio=8,
-        channel_ratio=8,
-        slow_pathway=dict(
-            type='resnet3d',
-            depth=50,
-            pretrained=None,
-            lateral=True,
-            conv1_kernel=(1, 7, 7),
-            dilations=(1, 1, 1, 1),
-            conv1_stride_t=1,
-            pool1_stride_t=1,
-            inflate=(0, 0, 1, 1),
-            spatial_strides=(1, 2, 2, 1)),
-        fast_pathway=dict(
-            type='resnet3d',
-            depth=50,
-            pretrained=None,
-            lateral=False,
-            base_channels=8,
-            conv1_kernel=(5, 7, 7),
-            conv1_stride_t=1,
-            pool1_stride_t=1,
-            spatial_strides=(1, 2, 2, 1))),
-    roi_head=dict(bbox_head=dict(in_channels=2304)))
-
-dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
-
-ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
-
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
-
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
-
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
-proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
+    backbone=dict(num_segments=16),
+    cls_head=dict(num_classes=174, num_segments=16, dropout_ratio=0.6))
+
+# dataset settings
+dataset_type = 'RawframeDataset'
+data_root = 'data/sthv1/rawframes'
+data_root_val = 'data/sthv1/rawframes'
+ann_file_train = 'data/sthv1/sthv1_train_list_rawframes.txt'
+ann_file_val = 'data/sthv1/sthv1_val_list_rawframes.txt'
+ann_file_test = 'data/sthv1/sthv1_val_list_rawframes.txt'
 
+sthv1_flip_label_map = {2: 4, 4: 2, 30: 41, 41: 30, 52: 66, 66: 52}
+file_client_args = dict(io_backend='disk')
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=32, frame_interval=2),
-    dict(type='RawFrameDecode'),
-    dict(type='RandomRescale', scale_range=(256, 320)),
-    dict(type='RandomCrop', size=256),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=16),
+    dict(type='RawFrameDecode', **file_client_args),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(
+        type='MultiScaleCrop',
+        input_size=224,
+        scales=(1, 0.875, 0.75, 0.66),
+        random_crop=False,
+        max_wh_scale_gap=1,
+        num_fixed_crops=13),
+    dict(type='Resize', scale=(224, 224), keep_ratio=False),
+    dict(type='Flip', flip_ratio=0.5, flip_label_map=sthv1_flip_label_map),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
-
-# The testing is w/o. any cropping / flipping
 val_pipeline = [
     dict(
-        type='SampleAVAFrames', clip_len=32, frame_interval=2, test_mode=True),
-    dict(type='RawFrameDecode'),
+        type='SampleFrames',
+        clip_len=1,
+        frame_interval=1,
+        num_clips=16,
+        test_mode=True),
+    dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
-
+test_pipeline = [
+    dict(
+        type='SampleFrames',
+        clip_len=1,
+        frame_interval=1,
+        num_clips=16,
+        twice_sample=True,
+        test_mode=True),
+    dict(type='RawFrameDecode', **file_client_args),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='ThreeCrop', crop_size=256),
+    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='PackActionInputs')
+]
+test_pipeline = val_pipeline
 train_dataloader = dict(
-    batch_size=16,
+    batch_size=6,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
-        pipeline=train_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
+        data_prefix=dict(img=data_root),
+        filename_tmpl='{:05}.jpg',
+        pipeline=train_pipeline))
 val_dataloader = dict(
-    batch_size=1,
+    batch_size=6,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
-        exclude_file=exclude_file_val,
+        data_prefix=dict(img=data_root_val),
+        filename_tmpl='{:05}.jpg',
         pipeline=val_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_val,
-        data_prefix=dict(img=data_root),
         test_mode=True))
-test_dataloader = val_dataloader
+test_dataloader = dict(
+    batch_size=1,
+    num_workers=8,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_val,
+        data_prefix=dict(img=data_root_val),
+        filename_tmpl='{:05}.jpg',
+        pipeline=test_pipeline,
+        test_mode=True))
+
+val_evaluator = dict(type='AccMetric')
+test_evaluator = val_evaluator
+
+optim_wrapper = dict(optimizer=dict(lr=0.0075, weight_decay=0.001))
+
+param_scheduler = [
+    dict(
+        type='MultiStepLR',
+        begin=0,
+        end=50,
+        by_epoch=True,
+        milestones=[30, 40, 45],
+        gamma=0.1)
+]
+
+default_hooks = dict(checkpoint=dict(max_keep_ckpts=3))
+
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (6 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=48)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb6-8x8x1-cosine-10e_ava22-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/slowonly-lfb_ava-pretrained-r50_infer-4x16x1_ava21-rgb.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,104 +1,115 @@
-_base_ = ['slowfast_kinetics400-pretrained-r50_8xb8-8x8x1-20e_ava21-rgb.py']
+# This config is used to generate long-term feature bank.
+_base_ = ['../../_base_/default_runtime.py']
+
+# model settings
+lfb_prefix_path = 'data/ava/lfb_half'
+dataset_mode = 'val'  # ['train', 'val', 'test']
+
+url = ('https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/'
+       'slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-'
+       'rgb/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_'
+       'kinetics400-rgb_20220901-e7b65fad.pth')
 
 model = dict(
+    type='FastRCNN',
+    _scope_='mmdet',
+    init_cfg=dict(type='Pretrained', checkpoint=url),
     backbone=dict(
-        pretrained=(
-            'https://download.openmmlab.com/mmaction/recognition/slowfast/'
-            'slowfast_r50_8x8x1_256e_kinetics400_rgb/'
-            'slowfast_r50_8x8x1_256e_kinetics400_rgb_20200716-73547d2b.pth')))
+        type='mmaction.ResNet3dSlowOnly',
+        depth=50,
+        pretrained=None,
+        pretrained2d=False,
+        lateral=False,
+        num_stages=4,
+        conv1_kernel=(1, 7, 7),
+        conv1_stride_t=1,
+        pool1_stride_t=1,
+        spatial_strides=(1, 2, 2, 1)),
+    roi_head=dict(
+        type='AVARoIHead',
+        bbox_roi_extractor=dict(
+            type='SingleRoIExtractor3D',
+            roi_layer_type='RoIAlign',
+            output_size=8,
+            with_temporal_pool=True),
+        bbox_head=dict(
+            type='BBoxHeadAVA',
+            in_channels=2048,
+            num_classes=81,
+            multilabel=True,
+            dropout_ratio=0.5),
+        shared_head=dict(
+            type='LFBInferHead',
+            lfb_prefix_path=lfb_prefix_path,
+            dataset_mode=dataset_mode,
+            use_half_precision=True)),
+    data_preprocessor=dict(
+        type='ActionDataPreprocessor',
+        _scope_='mmaction',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        format_shape='NCTHW'),
+    train_cfg=dict(
+        rcnn=dict(
+            assigner=dict(
+                type='MaxIoUAssignerAVA',
+                pos_iou_thr=0.9,
+                neg_iou_thr=0.9,
+                min_pos_iou=0.9),
+            sampler=dict(
+                type='RandomSampler',
+                num=32,
+                pos_fraction=1,
+                neg_pos_ub=-1,
+                add_gt_as_proposals=True),
+            pos_weight=1.0)),
+    test_cfg=dict(rcnn=None))
 
+# dataset settings
 dataset_type = 'AVADataset'
 data_root = 'data/ava/rawframes'
 anno_root = 'data/ava/annotations'
 
-ann_file_train = f'{anno_root}/ava_train_v2.2.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.2.csv'
-
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.2.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.2.csv'
+ann_file_infer = f'{anno_root}/ava_{dataset_mode}_v2.1.csv'
+exclude_file_infer = (
+    f'{anno_root}/ava_{dataset_mode}_excluded_timestamps_v2.1.csv')
+label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
+proposal_file_infer = (
+    f'{anno_root}/ava_dense_proposals_{dataset_mode}.FAIR.recall_93.9.pkl')
+
+file_client_args = dict(
+    io_backend='petrel',
+    path_mapping=dict({'data/ava': 's3://openmmlab/datasets/action/ava'}))
 
-label_file = f'{anno_root}/ava_action_list_v2.2_for_activitynet_2019.pbtxt'
-
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
-proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
-
-train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=32, frame_interval=2),
-    dict(type='RawFrameDecode'),
-    dict(type='RandomRescale', scale_range=(256, 320)),
-    dict(type='RandomCrop', size=256),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
-    dict(type='PackActionInputs')
-]
-# The testing is w/o. any cropping / flipping
-val_pipeline = [
+infer_pipeline = [
     dict(
-        type='SampleAVAFrames', clip_len=32, frame_interval=2, test_mode=True),
-    dict(type='RawFrameDecode'),
+        type='SampleAVAFrames', clip_len=4, frame_interval=16, test_mode=True),
+    dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(
-    batch_size=6,
-    num_workers=8,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
-        pipeline=train_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
-val_dataloader = dict(
+test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
-        exclude_file=exclude_file_val,
-        pipeline=val_pipeline,
+        ann_file=ann_file_infer,
+        exclude_file=exclude_file_infer,
+        pipeline=infer_pipeline,
         label_file=label_file,
-        proposal_file=proposal_file_val,
+        proposal_file=proposal_file_infer,
         data_prefix=dict(img=data_root),
+        person_det_score_thr=0.9,
         test_mode=True))
-test_dataloader = val_dataloader
 
-val_evaluator = dict(
+test_evaluator = dict(
     type='AVAMetric',
-    ann_file=ann_file_val,
+    ann_file=ann_file_infer,
     label_file=label_file,
-    exclude_file=exclude_file_val)
-test_evaluator = val_evaluator
-
-train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=10, val_begin=1, val_interval=1)
-
-param_scheduler = [
-    dict(
-        type='LinearLR',
-        start_factor=0.1,
-        by_epoch=True,
-        begin=0,
-        end=2,
-        convert_to_iter_based=True),
-    dict(
-        type='CosineAnnealingLR',
-        T_max=8,
-        eta_min=0,
-        by_epoch=True,
-        begin=2,
-        end=10,
-        convert_to_iter_based=True)
-]
+    exclude_file=exclude_file_infer)
 
-optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.075, momentum=0.9, weight_decay=0.00001),
-    clip_grad=dict(max_norm=40, norm_type=2))
+test_cfg = dict(type='TestLoop')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb8-8x8x1-20e_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-2d.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,50 +1,67 @@
-_base_ = ['slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py']
-
-model = dict(
-    backbone=dict(
-        resample_rate=4,
-        speed_ratio=4,
-        slow_pathway=dict(fusion_kernel=7),
-        pretrained=(
-            'https://download.openmmlab.com/mmaction/recognition/slowfast/'
-            'slowfast_r50_8x8x1_256e_kinetics400_rgb/'
-            'slowfast_r50_8x8x1_256e_kinetics400_rgb_20200716-73547d2b.pth')))
-
-dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
-
-ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
-
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
+_base_ = 'stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py'
 
+dataset_type = 'PoseDataset'
+ann_file = 'data/skeleton/ntu120_2d.pkl'
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=32, frame_interval=2),
-    dict(type='RawFrameDecode'),
-    dict(type='RandomRescale', scale_range=(256, 320)),
-    dict(type='RandomCrop', size=256),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(type='UniformSampleFrames', clip_len=100),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
+    dict(type='PackActionInputs')
+]
+val_pipeline = [
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(
+        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
+    dict(type='PackActionInputs')
+]
+test_pipeline = [
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(
+        type='UniformSampleFrames', clip_len=100, num_clips=10,
+        test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=8,
-    num_workers=8,
+    batch_size=16,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
+        type='RepeatDataset',
+        times=5,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            pipeline=train_pipeline,
+            split='xsub_train')))
+val_dataloader = dict(
+    batch_size=16,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
-        pipeline=train_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
-
-optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.00001),
-    clip_grad=dict(max_norm=40, norm_type=2))
+        ann_file=ann_file,
+        pipeline=val_pipeline,
+        split='xsub_val',
+        test_mode=True))
+test_dataloader = dict(
+    batch_size=1,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file,
+        pipeline=test_pipeline,
+        split='xsub_val',
+        test_mode=True))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,72 +1,67 @@
-_base_ = ['slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py']
-
-model = dict(
-    backbone=dict(
-        depth=101,
-        pretrained=(
-            'https://download.openmmlab.com/mmaction/recognition/slowonly/'
-            'omni/slowonly_r101_without_omni_8x8x1_kinetics400_rgb_'
-            '20200926-0c730aef.pth')))
-
-dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
-
-ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
-
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
-
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
-
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
-proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
+_base_ = '2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
 
+dataset_type = 'PoseDataset'
+ann_file = 'data/skeleton/ntu60_2d.pkl'
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=8, frame_interval=8),
-    dict(type='RawFrameDecode'),
-    dict(type='RandomRescale', scale_range=(256, 320)),
-    dict(type='RandomCrop', size=256),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='UniformSampleFrames', clip_len=100),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
-# The testing is w/o. any cropping / flipping
 val_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=8, frame_interval=8, test_mode=True),
-    dict(type='RawFrameDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(
+        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
+    dict(type='PackActionInputs')
+]
+test_pipeline = [
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(
+        type='UniformSampleFrames', clip_len=100, num_clips=10,
+        test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
-    num_workers=8,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
-        pipeline=train_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
+        type='RepeatDataset',
+        times=5,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            pipeline=train_pipeline,
+            split='xsub_train')))
 val_dataloader = dict(
-    batch_size=1,
-    num_workers=8,
+    batch_size=16,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
-        exclude_file=exclude_file_val,
+        ann_file=ann_file,
         pipeline=val_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_val,
-        data_prefix=dict(img=data_root),
+        split='xsub_val',
+        test_mode=True))
+test_dataloader = dict(
+    batch_size=1,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file,
+        pipeline=test_pipeline,
+        split='xsub_val',
         test_mode=True))
-test_dataloader = val_dataloader
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50-nl_8xb16-8x8x1-20e_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,74 +1,59 @@
-_base_ = [
-    'slowonly_kinetics400-pretrained-r50-nl_8xb16-4x16x1-20e_ava21-rgb.py'
-]
+_base_ = ['tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py']
+
+model = dict(backbone=dict(num_segments=16), cls_head=dict(num_segments=16))
 
-model = dict(
-    backbone=dict(
-        pretrained=(
-            'https://download.openmmlab.com/mmaction/recognition/slowonly/'
-            'slowonly_nl_embedded_gaussian_r50_8x8x1_150e_kinetics400_rgb/'
-            'slowonly_nl_embedded_gaussian_r50_8x8x1_150e_kinetics400_rgb_'
-            '20210308-e8dd9e82.pth')))
-
-dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
-
-ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
-
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
-
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
-
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
-proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
+file_client_args = dict(io_backend='disk')
 
+sthv2_flip_label_map = {86: 87, 87: 86, 93: 94, 94: 93, 166: 167, 167: 166}
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=8, frame_interval=8),
-    dict(type='RawFrameDecode'),
-    dict(type='RandomRescale', scale_range=(256, 320)),
-    dict(type='RandomCrop', size=256),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='DecordInit', **file_client_args),
+    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=16),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(
+        type='MultiScaleCrop',
+        input_size=224,
+        scales=(1, 0.875, 0.75, 0.66),
+        random_crop=False,
+        max_wh_scale_gap=1,
+        num_fixed_crops=13),
+    dict(type='Resize', scale=(224, 224), keep_ratio=False),
+    dict(type='Flip', flip_ratio=0.5, flip_label_map=sthv2_flip_label_map),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
-# The testing is w/o. any cropping / flipping
 val_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=8, frame_interval=8, test_mode=True),
-    dict(type='RawFrameDecode'),
+    dict(type='DecordInit', **file_client_args),
+    dict(
+        type='SampleFrames',
+        clip_len=1,
+        frame_interval=1,
+        num_clips=16,
+        test_mode=True),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='PackActionInputs')
+]
+test_pipeline = [
+    dict(type='DecordInit', **file_client_args),
+    dict(
+        type='SampleFrames',
+        clip_len=1,
+        frame_interval=1,
+        num_clips=16,
+        twice_sample=True,
+        test_mode=True),
+    dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='ThreeCrop', crop_size=256),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(
-    batch_size=16,
-    num_workers=8,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
-        pipeline=train_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
-val_dataloader = dict(
-    batch_size=1,
-    num_workers=8,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file_val,
-        exclude_file=exclude_file_val,
-        pipeline=val_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_val,
-        data_prefix=dict(img=data_root),
-        test_mode=True))
-test_dataloader = val_dataloader
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
+
+val_dataloader = dict(dataset=dict(pipeline=val_pipeline))
+
+test_dataloader = dict(pipeline=test_pipeline, test_mode=True)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/audioonly/audioonly_r50_8xb160-64x1x1-100e_kinetics400-audio-feature.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,94 +1,97 @@
 _base_ = [
-    '../../_base_/default_runtime.py', '../_base_/models/slowonly_r50.py'
+    '../../_base_/models/audioonly_r50.py', '../../_base_/default_runtime.py'
 ]
 
-dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
-
-ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
-
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
-
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
-
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
-proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
-
+# dataset settings
+dataset_type = 'AudioDataset'
+data_root = 'data/kinetics400/audio_features_train'
+data_root_val = 'data/kinetics400/audio_features_val'
+ann_file_train = 'data/kinetics400/kinetics400_train_list_audio_features.txt'
+ann_file_val = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
+ann_file_test = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=4, frame_interval=16),
-    dict(type='RawFrameDecode'),
-    dict(type='RandomRescale', scale_range=(256, 320)),
-    dict(type='RandomCrop', size=256),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+    dict(type='LoadAudioFeature'),
+    dict(type='SampleFrames', clip_len=64, frame_interval=1, num_clips=1),
+    dict(type='AudioFeatureSelector'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
     dict(type='PackActionInputs')
 ]
-# The testing is w/o. any cropping / flipping
 val_pipeline = [
+    dict(type='LoadAudioFeature'),
     dict(
-        type='SampleAVAFrames', clip_len=4, frame_interval=16, test_mode=True),
-    dict(type='RawFrameDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+        type='SampleFrames',
+        clip_len=64,
+        frame_interval=1,
+        num_clips=1,
+        test_mode=True),
+    dict(type='AudioFeatureSelector'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='PackActionInputs')
+]
+test_pipeline = [
+    dict(type='LoadAudioFeature'),
+    dict(
+        type='SampleFrames',
+        clip_len=64,
+        frame_interval=1,
+        num_clips=10,
+        test_mode=True),
+    dict(type='AudioFeatureSelector'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=16,
-    num_workers=8,
+    batch_size=160,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
         pipeline=train_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
+        data_prefix=dict(audio=data_root),
+        suffix='.npy'))
 val_dataloader = dict(
-    batch_size=1,
-    num_workers=8,
+    batch_size=160,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
-        exclude_file=exclude_file_val,
         pipeline=val_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_val,
-        data_prefix=dict(img=data_root),
+        data_prefix=dict(audio=data_root_val),
+        suffix='.npy',
+        test_mode=True))
+test_dataloader = dict(
+    batch_size=1,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_test,
+        pipeline=test_pipeline,
+        data_prefix=dict(audio=data_root_val),
+        suffix='.npy',
         test_mode=True))
-test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='AVAMetric',
-    ann_file=ann_file_val,
-    label_file=label_file,
-    exclude_file=exclude_file_val)
+val_evaluator = dict(type='AccMetric')
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=20, val_begin=1, val_interval=1)
+    type='EpochBasedTrainLoop', max_epochs=100, val_begin=1, val_interval=5)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
-    dict(type='LinearLR', start_factor=0.1, by_epoch=True, begin=0, end=5),
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=20,
-        by_epoch=True,
-        milestones=[10, 15],
-        gamma=0.1)
+    dict(type='CosineAnnealingLR', eta_min=0, T_max=100, by_epoch=True)
 ]
 
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.00001),
+    optimizer=dict(type='SGD', lr=2.0, momentum=0.9, weight_decay=0.0001),
     clip_grad=dict(max_norm=40, norm_type=2))
+
+default_hooks = dict(
+    checkpoint=dict(max_keep_ckpts=3, interval=5), logger=dict(interval=20))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k400-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/slowonly-lfb-infer_r50_ava21-rgb.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,117 +1,114 @@
-_base_ = [
-    '../../_base_/default_runtime.py', '../_base_/models/slowonly_r50.py'
-]
+# This config is used to generate long-term feature bank.
+_base_ = '../../_base_/default_runtime.py'
 
-dataset_type = 'AVAKineticsDataset'
-data_root = 'data/ava_kinetics/rawframes'
-anno_root = 'data/ava_kinetics/annotations'
-
-ann_file_train = f'{anno_root}/ava_train_v2.2.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.2.csv'
-
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.2.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.2.csv'
-
-label_file = f'{anno_root}/ava_action_list_v2.2_for_activitynet_2019.pbtxt'
-
-proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
-                       'recall_93.9.pkl')
-proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
-
-# file_client_args = dict(
-#     io_backend='petrel',
-#     path_mapping=dict({
-#         'data/ava_kinetics/rawframes/':
-#         's3://openmmlab/datasets/action/ava/rawframes/'
-#     }))
-file_client_args = dict(io_backend='disk')
-
-train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=4, frame_interval=16),
-    dict(type='RawFrameDecode', **file_client_args),
-    dict(type='RandomRescale', scale_range=(256, 320)),
-    dict(type='RandomCrop', size=256),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
-    dict(type='PackActionInputs')
-]
-# The testing is w/o. any cropping / flipping
-val_pipeline = [
+# model settings
+lfb_prefix_path = 'data/ava/lfb_half'
+dataset_mode = 'train'  # ['train', 'val', 'test']
+
+url = ('https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/'
+       'slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-'
+       'rgb/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_'
+       'kinetics400-rgb_20220901-e7b65fad.pth')
+
+model = dict(
+    type='FastRCNN',
+    _scope_='mmdet',
+    init_cfg=dict(type='Pretrained', checkpoint=url),
+    backbone=dict(
+        type='mmaction.ResNet3dSlowOnly',
+        depth=50,
+        pretrained=None,
+        pretrained2d=False,
+        lateral=False,
+        num_stages=4,
+        conv1_kernel=(1, 7, 7),
+        conv1_stride_t=1,
+        pool1_stride_t=1,
+        spatial_strides=(1, 2, 2, 1)),
+    roi_head=dict(
+        type='AVARoIHead',
+        bbox_roi_extractor=dict(
+            type='SingleRoIExtractor3D',
+            roi_layer_type='RoIAlign',
+            output_size=8,
+            with_temporal_pool=True),
+        bbox_head=dict(
+            type='BBoxHeadAVA',
+            in_channels=2048,
+            num_classes=81,
+            multilabel=True,
+            dropout_ratio=0.5),
+        shared_head=dict(
+            type='LFBInferHead',
+            lfb_prefix_path=lfb_prefix_path,
+            dataset_mode=dataset_mode,
+            use_half_precision=True)),
+    data_preprocessor=dict(
+        type='ActionDataPreprocessor',
+        _scope_='mmaction',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        format_shape='NCTHW'),
+    train_cfg=dict(
+        rcnn=dict(
+            assigner=dict(
+                type='MaxIoUAssignerAVA',
+                pos_iou_thr=0.9,
+                neg_iou_thr=0.9,
+                min_pos_iou=0.9),
+            sampler=dict(
+                type='RandomSampler',
+                num=32,
+                pos_fraction=1,
+                neg_pos_ub=-1,
+                add_gt_as_proposals=True),
+            pos_weight=1.0)),
+    test_cfg=dict(rcnn=None))
+
+# dataset settings
+dataset_type = 'AVADataset'
+data_root = 'data/ava/rawframes'
+anno_root = 'data/ava/annotations'
+
+ann_file_infer = f'{anno_root}/ava_{dataset_mode}_v2.1.csv'
+
+exclude_file_infer = (
+    f'{anno_root}/ava_{dataset_mode}_excluded_timestamps_v2.1.csv')
+
+label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
+
+proposal_file_infer = (
+    f'{anno_root}/ava_dense_proposals_{dataset_mode}.FAIR.recall_93.9.pkl')
+
+infer_pipeline = [
     dict(
         type='SampleAVAFrames', clip_len=4, frame_interval=16, test_mode=True),
-    dict(type='RawFrameDecode', **file_client_args),
+    dict(type='RawFrameDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(
-    batch_size=8,
-    num_workers=8,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file_train,
-        exclude_file=exclude_file_train,
-        pipeline=train_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root)))
-val_dataloader = dict(
+test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
-        exclude_file=exclude_file_val,
-        pipeline=val_pipeline,
+        ann_file=ann_file_infer,
+        exclude_file=exclude_file_infer,
+        pipeline=infer_pipeline,
         label_file=label_file,
-        proposal_file=proposal_file_val,
+        proposal_file=proposal_file_infer,
         data_prefix=dict(img=data_root),
+        person_det_score_thr=0.9,
         test_mode=True))
-test_dataloader = val_dataloader
 
-val_evaluator = dict(
+test_cfg = dict(type='TestLoop')
+test_evaluator = dict(
     type='AVAMetric',
-    ann_file=ann_file_val,
+    ann_file=ann_file_infer,
     label_file=label_file,
-    exclude_file=exclude_file_val)
-test_evaluator = val_evaluator
-
-train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=10, val_begin=1, val_interval=1)
-val_cfg = dict(type='ValLoop')
-test_cfg = dict(type='TestLoop')
-
-param_scheduler = [
-    dict(
-        type='LinearLR',
-        start_factor=0.1,
-        by_epoch=True,
-        begin=0,
-        end=2,
-        convert_to_iter_based=True),
-    dict(
-        type='CosineAnnealingLR',
-        T_max=8,
-        eta_min=0,
-        by_epoch=True,
-        begin=2,
-        end=10,
-        convert_to_iter_based=True)
-]
-
-optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.00001),
-    clip_grad=dict(max_norm=40, norm_type=2))
-
-default_hooks = dict(checkpoint=dict(max_keep_ckpts=2))
-
-# Default setting for scaling LR automatically
-#   - `enable` means enable scaling LR automatically
-#       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (8 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=64)
+    exclude_file=exclude_file_infer,
+    action_thr=0.0)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k400-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,62 +1,101 @@
-_base_ = [
-    '../../_base_/default_runtime.py', '../_base_/models/slowonly_r50.py'
-]
+_base_ = '../../_base_/default_runtime.py'
 
 url = ('https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/'
-       'slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_kinetics400-'
-       'rgb/slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_'
-       'kinetics400-rgb_20220901-df42dc84.pth')
-
-model = dict(init_cfg=dict(type='Pretrained', checkpoint=url))
+       'slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-'
+       'rgb/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_'
+       'kinetics400-rgb_20220901-e7b65fad.pth')
+
+model = dict(
+    type='FastRCNN',
+    _scope_='mmdet',
+    init_cfg=dict(type='Pretrained', checkpoint=url),
+    backbone=dict(
+        type='mmaction.ResNet3dSlowOnly',
+        depth=50,
+        pretrained=None,
+        pretrained2d=False,
+        lateral=False,
+        num_stages=4,
+        conv1_kernel=(1, 7, 7),
+        conv1_stride_t=1,
+        pool1_stride_t=1,
+        spatial_strides=(1, 2, 2, 1)),
+    roi_head=dict(
+        type='AVARoIHead',
+        bbox_roi_extractor=dict(
+            type='SingleRoIExtractor3D',
+            roi_layer_type='RoIAlign',
+            output_size=8,
+            with_temporal_pool=True),
+        bbox_head=dict(
+            type='BBoxHeadAVA',
+            in_channels=2048,
+            num_classes=81,
+            multilabel=True,
+            dropout_ratio=0.5)),
+    data_preprocessor=dict(
+        type='mmaction.ActionDataPreprocessor',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        format_shape='NCTHW'),
+    train_cfg=dict(
+        rcnn=dict(
+            assigner=dict(
+                type='MaxIoUAssignerAVA',
+                pos_iou_thr=0.9,
+                neg_iou_thr=0.9,
+                min_pos_iou=0.9),
+            sampler=dict(
+                type='RandomSampler',
+                num=32,
+                pos_fraction=1,
+                neg_pos_ub=-1,
+                add_gt_as_proposals=True),
+            pos_weight=1.0)),
+    test_cfg=dict(rcnn=None))
+
+dataset_type = 'AVADataset'
+data_root = 'data/ava/rawframes'
+anno_root = 'data/ava/annotations'
 
-dataset_type = 'AVAKineticsDataset'
-data_root = 'data/ava_kinetics/rawframes'
-anno_root = 'data/ava_kinetics/annotations'
+ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
+ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
 
-ann_file_train = f'{anno_root}/ava_train_v2.2.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.2.csv'
+exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
+exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
 
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.2.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.2.csv'
-
-label_file = f'{anno_root}/ava_action_list_v2.2_for_activitynet_2019.pbtxt'
+label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
 
 proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
                        'recall_93.9.pkl')
 proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
 
-# file_client_args = dict(
-#     io_backend='petrel',
-#     path_mapping=dict({
-#         'data/ava_kinetics/rawframes/':
-#         's3://openmmlab/datasets/action/ava/rawframes/'
-#     }))
 file_client_args = dict(io_backend='disk')
-
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=8, frame_interval=8),
+    dict(type='SampleAVAFrames', clip_len=4, frame_interval=16),
     dict(type='RawFrameDecode', **file_client_args),
     dict(type='RandomRescale', scale_range=(256, 320)),
     dict(type='RandomCrop', size=256),
     dict(type='Flip', flip_ratio=0.5),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 # The testing is w/o. any cropping / flipping
 val_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=8, frame_interval=8, test_mode=True),
+    dict(
+        type='SampleAVAFrames', clip_len=4, frame_interval=16, test_mode=True),
     dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=8,
+    batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
         exclude_file=exclude_file_train,
@@ -84,40 +123,31 @@
     type='AVAMetric',
     ann_file=ann_file_val,
     label_file=label_file,
     exclude_file=exclude_file_val)
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=10, val_begin=1, val_interval=1)
+    type='EpochBasedTrainLoop', max_epochs=20, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
+    dict(type='LinearLR', start_factor=0.1, by_epoch=True, begin=0, end=5),
     dict(
-        type='LinearLR',
-        start_factor=0.1,
-        by_epoch=True,
+        type='MultiStepLR',
         begin=0,
-        end=2,
-        convert_to_iter_based=True),
-    dict(
-        type='CosineAnnealingLR',
-        T_max=8,
-        eta_min=0,
+        end=20,
         by_epoch=True,
-        begin=2,
-        end=10,
-        convert_to_iter_based=True)
+        milestones=[10, 15],
+        gamma=0.1)
 ]
 
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.00001),
+    optimizer=dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.00001),
     clip_grad=dict(max_norm=40, norm_type=2))
 
-default_hooks = dict(checkpoint=dict(max_keep_ckpts=2))
-
 # Default setting for scaling LR automatically
 #   - `enable` means enable scaling LR automatically
 #       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (8 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=64)
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50_8xb8-16x4x1-10e-tricks_ava-kinetics-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/slowonly/slowonly_kinetics700-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,67 +1,101 @@
-_base_ = [
-    '../../_base_/default_runtime.py', '../_base_/models/slowonly_r50.py'
-]
+_base_ = '../../_base_/default_runtime.py'
 
-url = ('https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/'
-       'slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-'
-       'rgb/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_'
-       'kinetics700-rgb_20221013-15b93b10.pth')
+url = ('https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly'
+       '/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_'
+       'kinetics700-rgb/slowonly_imagenet-pretrained-r50_16xb16-4x16x1-'
+       'steplr-150e_kinetics700-rgb_20220901-f73b3e89.pth')
 
 model = dict(
+    type='FastRCNN',
+    _scope_='mmdet',
     init_cfg=dict(type='Pretrained', checkpoint=url),
+    backbone=dict(
+        type='mmaction.ResNet3dSlowOnly',
+        depth=50,
+        pretrained=None,
+        pretrained2d=False,
+        lateral=False,
+        num_stages=4,
+        conv1_kernel=(1, 7, 7),
+        conv1_stride_t=1,
+        pool1_stride_t=1,
+        spatial_strides=(1, 2, 2, 1)),
     roi_head=dict(
-        bbox_roi_extractor=dict(with_global=True, temporal_pool_mode='max'),
-        bbox_head=dict(in_channels=4096, mlp_head=True, focal_gamma=1.0)))
-
-dataset_type = 'AVAKineticsDataset'
-data_root = 'data/ava_kinetics/rawframes'
-anno_root = 'data/ava_kinetics/annotations'
+        type='AVARoIHead',
+        bbox_roi_extractor=dict(
+            type='SingleRoIExtractor3D',
+            roi_layer_type='RoIAlign',
+            output_size=8,
+            with_temporal_pool=True),
+        bbox_head=dict(
+            type='BBoxHeadAVA',
+            in_channels=2048,
+            num_classes=81,
+            multilabel=True,
+            dropout_ratio=0.5)),
+    data_preprocessor=dict(
+        type='mmaction.ActionDataPreprocessor',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        format_shape='NCTHW'),
+    train_cfg=dict(
+        rcnn=dict(
+            assigner=dict(
+                type='MaxIoUAssignerAVA',
+                pos_iou_thr=0.9,
+                neg_iou_thr=0.9,
+                min_pos_iou=0.9),
+            sampler=dict(
+                type='RandomSampler',
+                num=32,
+                pos_fraction=1,
+                neg_pos_ub=-1,
+                add_gt_as_proposals=True),
+            pos_weight=1.0)),
+    test_cfg=dict(rcnn=None))
+
+dataset_type = 'AVADataset'
+data_root = 'data/ava/rawframes'
+anno_root = 'data/ava/annotations'
 
-ann_file_train = f'{anno_root}/ava_train_v2.2.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.2.csv'
+ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
+ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
 
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.2.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.2.csv'
+exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
+exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
 
-label_file = f'{anno_root}/ava_action_list_v2.2_for_activitynet_2019.pbtxt'
+label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
 
 proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
                        'recall_93.9.pkl')
 proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
 
-# file_client_args = dict(
-#     io_backend='petrel',
-#     path_mapping=dict({
-#         'data/ava_kinetics/rawframes/':
-#         's3://openmmlab/datasets/action/ava/rawframes/'
-#     }))
 file_client_args = dict(io_backend='disk')
-
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=16, frame_interval=4),
+    dict(type='SampleAVAFrames', clip_len=4, frame_interval=16),
     dict(type='RawFrameDecode', **file_client_args),
     dict(type='RandomRescale', scale_range=(256, 320)),
     dict(type='RandomCrop', size=256),
     dict(type='Flip', flip_ratio=0.5),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 # The testing is w/o. any cropping / flipping
 val_pipeline = [
     dict(
-        type='SampleAVAFrames', clip_len=16, frame_interval=4, test_mode=True),
+        type='SampleAVAFrames', clip_len=4, frame_interval=16, test_mode=True),
     dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=8,
+    batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
         exclude_file=exclude_file_train,
@@ -89,40 +123,31 @@
     type='AVAMetric',
     ann_file=ann_file_val,
     label_file=label_file,
     exclude_file=exclude_file_val)
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=10, val_begin=1, val_interval=1)
+    type='EpochBasedTrainLoop', max_epochs=20, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
+    dict(type='LinearLR', start_factor=0.1, by_epoch=True, begin=0, end=5),
     dict(
-        type='LinearLR',
-        start_factor=0.1,
-        by_epoch=True,
+        type='MultiStepLR',
         begin=0,
-        end=2,
-        convert_to_iter_based=True),
-    dict(
-        type='CosineAnnealingLR',
-        T_max=8,
-        eta_min=0,
+        end=20,
         by_epoch=True,
-        begin=2,
-        end=10,
-        convert_to_iter_based=True)
+        milestones=[10, 15],
+        gamma=0.1)
 ]
 
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.00001),
+    optimizer=dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.00001),
     clip_grad=dict(max_norm=40, norm_type=2))
 
-default_hooks = dict(checkpoint=dict(max_keep_ckpts=2))
-
 # Default setting for scaling LR automatically
 #   - `enable` means enable scaling LR automatically
 #       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (8 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=64)
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/lfb/metafile.yml`

 * *Files 0% similar despite different names*

```diff
@@ -18,15 +18,15 @@
       Training Data: AVA v2.1
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
       - Dataset: AVA v2.1
         Task: Action Detection
         Metrics:
-              mAP: 24.05
+              mAP: 24.11
     Training Log: https://download.openmmlab.com/mmaction/v1.0/detection/lfb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/detection/lfb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb_20220906-4c5b9f25.pth
 
   - Name: slowonly-lfb-max_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb
     Config: slowonly-lfb-max_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py
     In Collection: LFB
     Metadata:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb6-8x8x1-cosine-10e_ava22-rgb.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,139 +1,176 @@
-_base_ = [
-    '../../_base_/default_runtime.py', '../_base_/models/slowonly_r50.py'
-]
-
-# model settings
-lfb_prefix_path = 'data/ava/lfb_half'
+_base_ = '../../_base_/default_runtime.py'
 
-max_num_sampled_feat = 5
-window_size = 60
-lfb_channels = 2048
-dataset_modes = ('train', 'val')
+url = ('https://download.openmmlab.com/mmaction/recognition/slowfast/'
+       'slowfast_r50_8x8x1_256e_kinetics400_rgb/'
+       'slowfast_r50_8x8x1_256e_kinetics400_rgb_20200716-73547d2b.pth')
 
 model = dict(
+    type='FastRCNN',
+    _scope_='mmdet',
+    init_cfg=dict(type='Pretrained', checkpoint=url),
+    backbone=dict(
+        type='mmaction.ResNet3dSlowFast',
+        resample_rate=4,
+        speed_ratio=4,
+        channel_ratio=8,
+        pretrained=None,
+        slow_pathway=dict(
+            type='resnet3d',
+            depth=50,
+            pretrained=None,
+            lateral=True,
+            conv1_kernel=(1, 7, 7),
+            dilations=(1, 1, 1, 1),
+            conv1_stride_t=1,
+            pool1_stride_t=1,
+            inflate=(0, 0, 1, 1),
+            spatial_strides=(1, 2, 2, 1),
+            fusion_kernel=7),
+        fast_pathway=dict(
+            type='resnet3d',
+            depth=50,
+            pretrained=None,
+            lateral=False,
+            base_channels=8,
+            conv1_kernel=(5, 7, 7),
+            conv1_stride_t=1,
+            pool1_stride_t=1,
+            spatial_strides=(1, 2, 2, 1))),
     roi_head=dict(
-        shared_head=dict(
-            type='FBOHead',
-            lfb_cfg=dict(
-                lfb_prefix_path=lfb_prefix_path,
-                max_num_sampled_feat=max_num_sampled_feat,
-                window_size=window_size,
-                lfb_channels=lfb_channels,
-                dataset_modes=dataset_modes,
-                device='gpu'),
-            fbo_cfg=dict(
-                type='non_local',
-                st_feat_channels=2048,
-                lt_feat_channels=lfb_channels,
-                latent_channels=512,
-                num_st_feat=1,
-                num_lt_feat=window_size * max_num_sampled_feat,
-                num_non_local_layers=2,
-                st_feat_dropout_ratio=0.2,
-                lt_feat_dropout_ratio=0.2,
-                pre_activate=True)),
-        bbox_head=dict(in_channels=2560)))
+        type='AVARoIHead',
+        bbox_roi_extractor=dict(
+            type='SingleRoIExtractor3D',
+            roi_layer_type='RoIAlign',
+            output_size=8,
+            with_temporal_pool=True),
+        bbox_head=dict(
+            type='BBoxHeadAVA',
+            in_channels=2304,
+            num_classes=81,
+            multilabel=True,
+            dropout_ratio=0.5)),
+    data_preprocessor=dict(
+        type='mmaction.ActionDataPreprocessor',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        format_shape='NCTHW'),
+    train_cfg=dict(
+        rcnn=dict(
+            assigner=dict(
+                type='MaxIoUAssignerAVA',
+                pos_iou_thr=0.9,
+                neg_iou_thr=0.9,
+                min_pos_iou=0.9),
+            sampler=dict(
+                type='RandomSampler',
+                num=32,
+                pos_fraction=1,
+                neg_pos_ub=-1,
+                add_gt_as_proposals=True),
+            pos_weight=1.0)),
+    test_cfg=dict(rcnn=None))
 
 dataset_type = 'AVADataset'
 data_root = 'data/ava/rawframes'
 anno_root = 'data/ava/annotations'
 
-ann_file_train = f'{anno_root}/ava_train_v2.1.csv'
-ann_file_val = f'{anno_root}/ava_val_v2.1.csv'
+ann_file_train = f'{anno_root}/ava_train_v2.2.csv'
+ann_file_val = f'{anno_root}/ava_val_v2.2.csv'
 
-exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.1.csv'
-exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.1.csv'
+exclude_file_train = f'{anno_root}/ava_train_excluded_timestamps_v2.2.csv'
+exclude_file_val = f'{anno_root}/ava_val_excluded_timestamps_v2.2.csv'
 
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
+label_file = f'{anno_root}/ava_action_list_v2.2_for_activitynet_2019.pbtxt'
 
 proposal_file_train = (f'{anno_root}/ava_dense_proposals_train.FAIR.'
                        'recall_93.9.pkl')
 proposal_file_val = f'{anno_root}/ava_dense_proposals_val.FAIR.recall_93.9.pkl'
 
+file_client_args = dict(io_backend='disk')
 train_pipeline = [
-    dict(type='SampleAVAFrames', clip_len=4, frame_interval=16),
-    dict(type='RawFrameDecode'),
+    dict(type='SampleAVAFrames', clip_len=32, frame_interval=2),
+    dict(type='RawFrameDecode', **file_client_args),
     dict(type='RandomRescale', scale_range=(256, 320)),
     dict(type='RandomCrop', size=256),
     dict(type='Flip', flip_ratio=0.5),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 # The testing is w/o. any cropping / flipping
 val_pipeline = [
     dict(
-        type='SampleAVAFrames', clip_len=4, frame_interval=16, test_mode=True),
-    dict(type='RawFrameDecode'),
+        type='SampleAVAFrames', clip_len=32, frame_interval=2, test_mode=True),
+    dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='FormatShape', input_format='NCTHW', collapse=True),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=12,
+    batch_size=6,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
         exclude_file=exclude_file_train,
         pipeline=train_pipeline,
         label_file=label_file,
         proposal_file=proposal_file_train,
-        data_prefix=dict(img=data_root),
-        person_det_score_thr=0.9))
-
+        data_prefix=dict(img=data_root)))
 val_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
         exclude_file=exclude_file_val,
         pipeline=val_pipeline,
         label_file=label_file,
         proposal_file=proposal_file_val,
         data_prefix=dict(img=data_root),
-        person_det_score_thr=0.85,
         test_mode=True))
-
 test_dataloader = val_dataloader
 
 val_evaluator = dict(
     type='AVAMetric',
     ann_file=ann_file_val,
     label_file=label_file,
-    exclude_file=exclude_file_val,
-    action_thr=0.0)
+    exclude_file=exclude_file_val)
 test_evaluator = val_evaluator
 
-default_hooks = dict(checkpoint=dict(interval=3, max_keep_ckpts=3))
-
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=20, val_begin=1, val_interval=1)
+    type='EpochBasedTrainLoop', max_epochs=10, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
     dict(
         type='LinearLR',
         start_factor=0.1,
         by_epoch=True,
         begin=0,
-        end=5,
+        end=2,
         convert_to_iter_based=True),
     dict(
-        type='MultiStepLR',
-        begin=0,
-        end=20,
+        type='CosineAnnealingLR',
+        T_max=8,
+        eta_min=0,
         by_epoch=True,
-        milestones=[10, 15],
-        gamma=0.1)
+        begin=2,
+        end=10,
+        convert_to_iter_based=True)
 ]
 
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.15, momentum=0.9, weight_decay=1e-05),
-    clip_grad=dict(max_norm=20, norm_type=2))
+    optimizer=dict(type='SGD', lr=0.075, momentum=0.9, weight_decay=0.00001),
+    clip_grad=dict(max_norm=40, norm_type=2))
+
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (6 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=48)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/detection/lfb/slowonly-lfb_ava-pretrained-r50_infer-4x16x1_ava21-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/uniformer-small_imagenet1k-pre_16x4x1_kinetics400-rgb.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,65 +1,58 @@
-# This config is used to generate long-term feature bank.
-_base_ = [
-    '../../_base_/default_runtime.py', '../_base_/models/slowonly_r50.py'
-]
+_base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-lfb_prefix_path = 'data/ava/lfb_half'
-dataset_mode = 'val'  # ['train', 'val', 'test']
-
 model = dict(
-    roi_head=dict(
-        shared_head=dict(
-            type='LFBInferHead',
-            lfb_prefix_path=lfb_prefix_path,
-            dataset_mode=dataset_mode,
-            use_half_precision=True)))
+    type='Recognizer3D',
+    backbone=dict(
+        type='UniFormer',
+        depth=[3, 4, 8, 3],
+        embed_dim=[64, 128, 320, 512],
+        head_dim=64,
+        drop_path_rate=0.1),
+    cls_head=dict(
+        type='I3DHead',
+        dropout_ratio=0.,
+        num_classes=400,
+        in_channels=512,
+        average_clips='prob'),
+    data_preprocessor=dict(
+        type='ActionDataPreprocessor',
+        mean=[114.75, 114.75, 114.75],
+        std=[57.375, 57.375, 57.375],
+        format_shape='NCTHW'))
 
 # dataset settings
-dataset_type = 'AVADataset'
-data_root = 'data/ava/rawframes'
-anno_root = 'data/ava/annotations'
-
-ann_file_infer = f'{anno_root}/ava_{dataset_mode}_v2.1.csv'
-exclude_file_infer = (
-    f'{anno_root}/ava_{dataset_mode}_excluded_timestamps_v2.1.csv')
-label_file = f'{anno_root}/ava_action_list_v2.1_for_activitynet_2018.pbtxt'
-proposal_file_infer = (
-    f'{anno_root}/ava_dense_proposals_{dataset_mode}.FAIR.recall_93.9.pkl')
+dataset_type = 'VideoDataset'
+data_root_val = 'data/k400'
+ann_file_test = 'data/k400/val.csv'
 
-file_client_args = dict(
-    io_backend='petrel',
-    path_mapping=dict({'data/ava': 's3://openmmlab/datasets/action/ava'}))
-
-infer_pipeline = [
+test_pipeline = [
+    dict(type='DecordInit'),
     dict(
-        type='SampleAVAFrames', clip_len=4, frame_interval=16, test_mode=True),
-    dict(type='RawFrameDecode', **file_client_args),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='FormatShape', input_format='NCTHW', collapse=True),
+        type='SampleFrames',
+        clip_len=16,
+        frame_interval=4,
+        num_clips=4,
+        test_mode=True),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 224)),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=1,
+    batch_size=32,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_infer,
-        exclude_file=exclude_file_infer,
-        pipeline=infer_pipeline,
-        label_file=label_file,
-        proposal_file=proposal_file_infer,
-        data_prefix=dict(img=data_root),
-        person_det_score_thr=0.9,
-        test_mode=True))
-
-test_evaluator = dict(
-    type='AVAMetric',
-    ann_file=ann_file_infer,
-    label_file=label_file,
-    exclude_file=exclude_file_infer)
+        ann_file=ann_file_test,
+        data_prefix=dict(video=data_root_val),
+        pipeline=test_pipeline,
+        test_mode=True,
+        delimiter=','))
 
+test_evaluator = dict(type='AccMetric')
 test_cfg = dict(type='TestLoop')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bmn/bmn_2xb8-400x100-9e_activitynet-feature.py` & `mmaction2-1.1.0/mmaction/.mim/configs/localization/bmn/bmn_2xb8-400x100-9e_activitynet-feature.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bmn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/localization/bmn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/bsn_pem_1xb16-400x100-20e_activitynet-feature.py` & `mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/bsn_pem_1xb16-400x100-20e_activitynet-feature.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/bsn_pgm_400x100_activitynet-feature.py` & `mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/bsn_pgm_400x100_activitynet-feature.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/bsn_tem_1xb16-400x100-20e_activitynet-feature.py` & `mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/bsn_tem_1xb16-400x100-20e_activitynet-feature.py`

 * *Files 2% similar despite different names*

```diff
@@ -85,7 +85,9 @@
 tem_results_dir = f'{work_dir}/tem_results/'
 
 test_evaluator = dict(
     type='ANetMetric',
     metric_type='TEM',
     dump_config=dict(out=tem_results_dir, output_format='csv'))
 val_evaluator = test_evaluator
+
+default_hooks = dict(checkpoint=dict(filename_tmpl='tem_epoch_{}.pth'))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/localization/bsn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/localization/bsn/metafile.yml`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 Collections:
-- Name: BMN
+- Name: BSN
   README: configs/localization/bsn/README.md
   Paper:
     URL: https://arxiv.org/abs/1806.02964
     Title: "BSN: Boundary Sensitive Network for Temporal Action Proposal Generation"
 
 Models:
   - Name: bsn_400x100_1xb16_20e_activitynet_feature (cuhk_mean_100)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-16x4x1-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-16x4x1-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-8x8x1-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-8x8x1-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c2d/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/c2d/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c3d/c3d_sports1m-pretrained_8xb30-16x1x1-45e_ucf101-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/c3d/c3d_sports1m-pretrained_8xb30-16x1x1-45e_ucf101-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/c3d/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/c3d/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ipcsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ipcsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r152-bnfrozen_8xb12-32x2x1-58e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ircsn_ig65m-pretrained-r152-bnfrozen_8xb12-32x2x1-58e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/ircsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/ircsn_sports1m-pretrained-r152-bnfrozen_32x2x1-58e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/csn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/csn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-dot-product_8xb8-32x2x1-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50-nl-dot-product_8xb8-32x2x1-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-32x2x1-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-32x2x1-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-dense-32x2x1-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/i3d_imagenet-pretrained-r50_8xb8-dense-32x2x1-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/i3d/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/i3d/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/metafile.yml`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Collections:
 - Name: MViT
-  README: configs/recognition/MViT/README.md
+  README: configs/recognition/mvit/README.md
   Paper:
     URL: http://openaccess.thecvf.com//content/CVPR2022/papers/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.pdf
     Title: "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"
 
 Models:
   - Name: mvit-small-p244_32xb16-16x4x1-200e_kinetics400-rgb_infer
     Config: configs/recognition/mvit/mvit-small-p244_32xb16-16x4x1-200e_kinetics400-rgb.py
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_32x3x1_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_32x3x1_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_u32_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-base-p244_u32_sthv2-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_40x3x1_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_40x3x1_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_u40_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-large-p244_u40_sthv2-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_32xb16-16x4x1-200e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_32xb16-16x4x1-200e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-maskfeat-pre_8xb32-16x4x1-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-maskfeat-pre_8xb32-16x4x1-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-pre_16xb16-u16-100e_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/mvit/mvit-small-p244_k400-pre_16xb16-u16-100e_sthv2-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/omnisource/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/omnisource/metafile.yml`

 * *Files 16% similar despite different names*

```diff
@@ -1,11 +1,13 @@
 Collections:
   - Name: Omnisource
     README: configs/recognition/omnisource/README.md
-
+    Paper:
+      URL: https://arxiv.org/abs/2003.13042
+      Title: 'Omni-sourced Webly-supervised Learning for Video Recognition'
 
 Models:
   - Name: slowonly_r50_8xb16-8x8x1-256e_imagenet-kinetics400-rgb
     Config: configs/recognition/omnisource/slowonly_r50_8xb16-8x8x1-256e_imagenet-kinetics400-rgb.py
     In Collection: SlowOnly
     Metadata:
       Architecture: ResNet50
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/omnisource/slowonly_r50_8xb16-8x8x1-256e_imagenet-kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/omnisource/slowonly_r50_8xb16-8x8x1-256e_imagenet-kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/r2plus1d/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/r2plus1d/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-32x2x1-180e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-32x2x1-180e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-8x8x1-180e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/r2plus1d/r2plus1d_r34_8xb8-8x8x1-180e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-4x16x1-256e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-4x16x1-256e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/metafile.yml`

 * *Files 4% similar despite different names*

```diff
@@ -210,7 +210,30 @@
       - Dataset: Kinetics-700
         Task: Action Recognition
         Metrics:
           Top 1 Accuracy: 66.93
           Top 5 Accuracy: 87.47
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_kinetics700-rgb/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb_20220901-4098e1eb.pth
+
+  - Name: slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb
+    Config: configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb.py
+    In Collection: SlowOnly
+    Metadata:
+      Architecture: ResNet50
+      Batch Size: 8
+      Epochs: 150
+      FLOPs: 54.75G
+      Parameters: 32.45M
+      Pretrained: ImageNet
+      Resolution: short-side 320
+      Training Data: Kinetics-710
+      Training Resources: 32 GPUs
+    Modality: RGB
+    Results:
+      - Dataset: Kinetics-710
+        Task: Action Recognition
+        Metrics:
+          Top 1 Accuracy: 72.39
+          Top 5 Accuracy: 90.60
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb/slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/slowonly/slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb/slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb_20230612-12ce977c.pth
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-4x16x1-steplr-150e_kinetics700-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-4x16x1-steplr-150e_kinetics700-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-4x16x1-256e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-4x16x1-256e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-8x8x1-256e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-8x8x1-256e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/metafile.yml`

 * *Files 18% similar despite different names*

```diff
@@ -116,7 +116,30 @@
     - Dataset: Kinetics-700
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 75.92
         Top 5 Accuracy: 92.72
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/swin/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/swin/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb_20220930-f8d74db7.pth
+
+  - Name: swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb
+    Config: configs/recognition/swin/swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb.py
+    In Collection: Swin
+    Metadata:
+      Architecture: Swin-S
+      Batch Size: 4
+      Epochs: 30
+      FLOPs: 604G
+      Parameters: 197M
+      Pretrained: ImageNet-1K
+      Resolution: 224x224
+      Training Data: Kinetics-710
+      Training Resources: 32 GPUs
+    Modality: RGB
+    Results:
+    - Dataset: Kinetics-710
+      Task: Action Recognition
+      Metrics:
+        Top 1 Accuracy: 76.90
+        Top 5 Accuracy: 92.96
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/swin/swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb/swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/swin/swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb/swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb_20230612-8e082ff1.pth
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-base-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-base-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-small-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-small-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/swin/swin-tiny-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/swin/swin-tiny-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb6-1x1x16-50e_sthv1-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-1x1x8-50e_sthv1-rgb.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,29 +1,27 @@
 _base_ = [
     '../../_base_/models/tanet_r50.py', '../../_base_/default_runtime.py',
     '../../_base_/schedules/sgd_tsm_50e.py'
 ]
 
 # model settings
-model = dict(
-    backbone=dict(num_segments=16),
-    cls_head=dict(num_classes=174, num_segments=16, dropout_ratio=0.6))
+model = dict(cls_head=dict(num_classes=174, dropout_ratio=0.6))
 
 # dataset settings
 dataset_type = 'RawframeDataset'
 data_root = 'data/sthv1/rawframes'
 data_root_val = 'data/sthv1/rawframes'
 ann_file_train = 'data/sthv1/sthv1_train_list_rawframes.txt'
 ann_file_val = 'data/sthv1/sthv1_val_list_rawframes.txt'
 ann_file_test = 'data/sthv1/sthv1_val_list_rawframes.txt'
 
 sthv1_flip_label_map = {2: 4, 4: 2, 30: 41, 41: 30, 52: 66, 66: 52}
 file_client_args = dict(io_backend='disk')
 train_pipeline = [
-    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=16),
+    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
     dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(
         type='MultiScaleCrop',
         input_size=224,
         scales=(1, 0.875, 0.75, 0.66),
         random_crop=False,
@@ -35,50 +33,50 @@
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(
         type='SampleFrames',
         clip_len=1,
         frame_interval=1,
-        num_clips=16,
+        num_clips=8,
         test_mode=True),
     dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='CenterCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(
         type='SampleFrames',
         clip_len=1,
         frame_interval=1,
-        num_clips=16,
+        num_clips=8,
         twice_sample=True,
         test_mode=True),
     dict(type='RawFrameDecode', **file_client_args),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='ThreeCrop', crop_size=256),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
-test_pipeline = val_pipeline
+
 train_dataloader = dict(
-    batch_size=6,
+    batch_size=8,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
         data_prefix=dict(img=data_root),
         filename_tmpl='{:05}.jpg',
         pipeline=train_pipeline))
 val_dataloader = dict(
-    batch_size=6,
+    batch_size=8,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
         data_prefix=dict(img=data_root_val),
@@ -97,15 +95,15 @@
         filename_tmpl='{:05}.jpg',
         pipeline=test_pipeline,
         test_mode=True))
 
 val_evaluator = dict(type='AccMetric')
 test_evaluator = val_evaluator
 
-optim_wrapper = dict(optimizer=dict(lr=0.0075, weight_decay=0.001))
+optim_wrapper = dict(optimizer=dict(weight_decay=0.001))
 
 param_scheduler = [
     dict(
         type='MultiStepLR',
         begin=0,
         end=50,
         by_epoch=True,
@@ -114,9 +112,9 @@
 ]
 
 default_hooks = dict(checkpoint=dict(max_keep_ckpts=3))
 
 # Default setting for scaling LR automatically
 #   - `enable` means enable scaling LR automatically
 #       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (6 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=48)
+#   - `base_batch_size` = (8 GPUs) x (8 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=64)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-1x1x8-50e_sthv1-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_kinetics400-rgb.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,120 +1,122 @@
-_base_ = [
-    '../../_base_/models/tanet_r50.py', '../../_base_/default_runtime.py',
-    '../../_base_/schedules/sgd_tsm_50e.py'
-]
-
-# model settings
-model = dict(cls_head=dict(num_classes=174, dropout_ratio=0.6))
+_base_ = ['../../_base_/models/tsm_r50.py', '../../_base_/default_runtime.py']
 
 # dataset settings
-dataset_type = 'RawframeDataset'
-data_root = 'data/sthv1/rawframes'
-data_root_val = 'data/sthv1/rawframes'
-ann_file_train = 'data/sthv1/sthv1_train_list_rawframes.txt'
-ann_file_val = 'data/sthv1/sthv1_val_list_rawframes.txt'
-ann_file_test = 'data/sthv1/sthv1_val_list_rawframes.txt'
+dataset_type = 'VideoDataset'
+data_root = 'data/kinetics400/videos_train'
+data_root_val = 'data/kinetics400/videos_val'
+ann_file_train = 'data/kinetics400/kinetics400_train_list_videos.txt'
+ann_file_val = 'data/kinetics400/kinetics400_val_list_videos.txt'
 
-sthv1_flip_label_map = {2: 4, 4: 2, 30: 41, 41: 30, 52: 66, 66: 52}
 file_client_args = dict(io_backend='disk')
+
 train_pipeline = [
+    dict(type='DecordInit', **file_client_args),
     dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
-    dict(type='RawFrameDecode', **file_client_args),
+    dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(
         type='MultiScaleCrop',
         input_size=224,
         scales=(1, 0.875, 0.75, 0.66),
         random_crop=False,
         max_wh_scale_gap=1,
         num_fixed_crops=13),
     dict(type='Resize', scale=(224, 224), keep_ratio=False),
-    dict(type='Flip', flip_ratio=0.5, flip_label_map=sthv1_flip_label_map),
+    dict(type='Flip', flip_ratio=0.5),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
+    dict(type='DecordInit', **file_client_args),
     dict(
         type='SampleFrames',
         clip_len=1,
         frame_interval=1,
         num_clips=8,
         test_mode=True),
-    dict(type='RawFrameDecode', **file_client_args),
+    dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='CenterCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
+    dict(type='DecordInit', **file_client_args),
     dict(
         type='SampleFrames',
         clip_len=1,
         frame_interval=1,
         num_clips=8,
-        twice_sample=True,
         test_mode=True),
-    dict(type='RawFrameDecode', **file_client_args),
+    dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
-    dict(type='ThreeCrop', crop_size=256),
+    dict(type='TenCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=8,
+    batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
-        data_prefix=dict(img=data_root),
-        filename_tmpl='{:05}.jpg',
+        data_prefix=dict(video=data_root),
         pipeline=train_pipeline))
 val_dataloader = dict(
-    batch_size=8,
+    batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
-        data_prefix=dict(img=data_root_val),
-        filename_tmpl='{:05}.jpg',
+        data_prefix=dict(video=data_root_val),
         pipeline=val_pipeline,
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
-        data_prefix=dict(img=data_root_val),
-        filename_tmpl='{:05}.jpg',
+        data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
         test_mode=True))
 
 val_evaluator = dict(type='AccMetric')
 test_evaluator = val_evaluator
 
-optim_wrapper = dict(optimizer=dict(weight_decay=0.001))
+default_hooks = dict(checkpoint=dict(interval=3, max_keep_ckpts=3))
+
+train_cfg = dict(
+    type='EpochBasedTrainLoop', max_epochs=50, val_begin=1, val_interval=1)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
+    dict(type='LinearLR', start_factor=0.1, by_epoch=True, begin=0, end=5),
     dict(
         type='MultiStepLR',
         begin=0,
         end=50,
         by_epoch=True,
-        milestones=[30, 40, 45],
+        milestones=[25, 45],
         gamma=0.1)
 ]
 
-default_hooks = dict(checkpoint=dict(max_keep_ckpts=3))
+optim_wrapper = dict(
+    constructor='TSMOptimWrapperConstructor',
+    paramwise_cfg=dict(fc_lr5=True),
+    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001),
+    clip_grad=dict(max_norm=20, norm_type=2))
 
 # Default setting for scaling LR automatically
 #   - `enable` means enable scaling LR automatically
 #       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (8 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=64)
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-dense-1x1x8-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-dense-1x1x8-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/timesformer/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/timesformer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/timesformer/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/timesformer/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv1-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv1-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/tin_imagenet-pretrained-r50_8xb6-1x1x8-40e_sthv2-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tin/tin_kinetics400-pretrained-tsm-r50_1x1x8-50e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tin/tin_kinetics400-pretrained-tsm-r50_1x1x8-50e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/metafile.yml`

 * *Files 3% similar despite different names*

```diff
@@ -19,15 +19,15 @@
     Training Resources: 32 GPUs
   Modality: RGB
   Name: tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb
   Results:
   - Dataset: Kinetics-400
     Metrics:
       Top 1 Accuracy: 74.20
-      top5 accuracy: 91.48
+      Top 5 Accuracy: 91.48
     Task: Action Recognition
   Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tpn/tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb/tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb.log
   Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tpn/tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb/tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb_20220913-97d0835d.pth
 - Config: configs/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb.py
   In Collection: TPN
   Metadata:
     Architecture: ResNet50
@@ -41,15 +41,15 @@
     Training Resources: 32 GPUs
   Modality: RGB
   Name: tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb
   Results:
   - Dataset: Kinetics-400
     Metrics:
       Top 1 Accuracy: 76.74
-      top5 accuracy: 92.57
+      Top 5 Accuracy: 92.57
     Task: Action Recognition
   Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb.log
   Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb_20220913-fed3f4c1.pth
 - Config: configs/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb.py
   In Collection: TPN
   Metadata:
     Architecture: ResNet50
@@ -62,12 +62,12 @@
     Training Data: SthV1
     Training Resources: 48 GPUs
   Modality: RGB
   Name: tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb
   Results:
   - Dataset: SthV1
     Metrics:
-      Top 1 Accuracy: 48.98
-      Top 5 Accuracy: 78.91
+      Top 1 Accuracy: 51.87
+      Top 5 Accuracy: 79.67
     Task: Action Recognition
   Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb.log
-  Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb_20220913-d2f5c300.pth
+  Weights: (https://download.openmmlab.com/mmaction/v1.0/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb_20230221-940a3615.pth
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,20 +4,22 @@
 
 dataset_type = 'RawframeDataset'
 data_root = 'data/sthv1/rawframes'
 data_root_val = 'data/sthv1/rawframes'
 ann_file_train = 'data/sthv1/sthv1_train_list_rawframes.txt'
 ann_file_val = 'data/sthv1/sthv1_val_list_rawframes.txt'
 ann_file_test = 'data/sthv1/sthv1_val_list_rawframes.txt'
+
+sthv1_flip_label_map = {2: 4, 4: 2, 30: 41, 41: 30, 52: 66, 66: 52}
 train_pipeline = [
     dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
     dict(type='RawFrameDecode'),
     dict(type='RandomResizedCrop'),
     dict(type='Resize', scale=(224, 224), keep_ratio=False),
-    dict(type='Flip', flip_ratio=0.5),
+    dict(type='Flip', flip_ratio=0.5, flip_label_map=sthv1_flip_label_map),
     dict(type='ColorJitter'),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(
         type='SampleFrames',
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/trn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/trn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv1-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv1-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/metafile.yml`

 * *Files 4% similar despite different names*

```diff
@@ -163,75 +163,98 @@
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 73.66
         Top 5 Accuracy: 90.99
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50-nl-gaussian_8xb16-1x1x8-50e_kinetics400-rgb/tsm_imagenet-pretrained-r50-nl-gaussian_8xb16-1x1x8-50e_kinetics400-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50-nl-gaussian_8xb16-1x1x8-50e_kinetics400-rgb/tsm_imagenet-pretrained-r50-nl-gaussian_8xb16-1x1x8-50e_kinetics400-rgb_20220831-7e54dacf.pth
 
+  - Name: tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb
+    Config: configs/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb.py
+    In Collection: TSM
+    Metadata:
+      Architecture: MobileNetV2
+      Batch Size: 16
+      Epochs: 100
+      FLOPs: 3.269G
+      Parameters: 2.736M
+      Pretrained: ImageNet
+      Resolution: 224x224
+      Training Data: Kinetics-400
+      Training Resources: 8 GPUs
+    Modality: RGB
+    Results:
+    - Dataset: Kinetics-400
+      Task: Action Recognition
+      Metrics:
+        Top 1 Accuracy: 63.70
+        Top 5 Accuracy: 88.28
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb_20230320-efcc0d1b.pth
+
   - Name: tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb
     Config: configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py
     In Collection: TSM
     Metadata:
       Architecture: ResNet50
       Batch Size: 16
       Epochs: 100
       FLOPs: 32.88G
       Parameters: 23.87M
       Pretrained: ImageNet
       Resolution: 224x224
-      Training Data: Kinetics-400
+      Training Data: SthV2
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
-    - Dataset: Kinetics-400
+    - Dataset: SthV2
       Task: Action Recognition
       Metrics:
-        Top 1 Accuracy: 60.20
-        Top 5 Accuracy: 86.13
+        Top 1 Accuracy: 62.72
+        Top 5 Accuracy: 87.70
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.log
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb_20221122-446d261a.pth
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb_20230317-be0fc26e.pth
 
   - Name: tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb
     Config: configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb.py
     In Collection: TSM
     Metadata:
       Architecture: ResNet50
       Batch Size: 16
-      Epochs: 100
+      Epochs: 50
       FLOPs: 65.75G
       Parameters: 23.87M
       Pretrained: ImageNet
       Resolution: 224x224
-      Training Data: Kinetics-400
+      Training Data: SthV2
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
-    - Dataset: Kinetics-400
+    - Dataset: SthV2
       Task: Action Recognition
       Metrics:
-        Top 1 Accuracy: 62.46
-        Top 5 Accuracy: 87.75
+        Top 1 Accuracy: 64.16
+        Top 5 Accuracy: 88.61
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb.log
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb_20221122-b1fb8264.pth
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb_20230317-ec6696ad.pth
 
   - Name: tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb
     Config: configs/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb.py
     In Collection: TSM
     Metadata:
       Architecture: ResNet101
       Batch Size: 16
-      Epochs: 100
+      Epochs: 50
       FLOPs: 62.66G
       Parameters: 42.86M
       Pretrained: ImageNet
       Resolution: 224x224
-      Training Data: Kinetics-400
+      Training Data: SthV2
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
-    - Dataset: Kinetics-400
+    - Dataset: SthV2
       Task: Action Recognition
       Metrics:
-        Top 1 Accuracy: 60.49
-        Top 5 Accuracy: 85.99
+        Top 1 Accuracy: 63.70
+        Top 5 Accuracy: 88.28
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb.log
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb_20221122-cb2cc64e.pth
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb_20230320-efcc0d1b.pth
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,58 +1,70 @@
-_base_ = ['tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py']
+_base_ = ['../../_base_/default_runtime.py']
 
-model = dict(backbone=dict(num_segments=16), cls_head=dict(num_segments=16))
+# model settings
+num_frames = 16
+model = dict(
+    type='Recognizer3D',
+    backbone=dict(
+        type='UniFormerV2',
+        input_resolution=224,
+        patch_size=14,
+        width=1024,
+        layers=24,
+        heads=16,
+        t_size=num_frames,
+        dw_reduction=1.5,
+        backbone_drop_path_rate=0.,
+        temporal_downsample=False,
+        no_lmhra=True,
+        double_lmhra=True,
+        return_list=[20, 21, 22, 23],
+        n_layers=4,
+        n_dim=1024,
+        n_head=16,
+        mlp_factor=4.,
+        drop_path_rate=0.,
+        mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
+    cls_head=dict(
+        type='TimeSformerHead',
+        dropout_ratio=0.5,
+        num_classes=400,
+        in_channels=1024,
+        average_clips='prob'),
+    data_preprocessor=dict(
+        type='ActionDataPreprocessor',
+        mean=[114.75, 114.75, 114.75],
+        std=[57.375, 57.375, 57.375],
+        format_shape='NCTHW'))
+
+# dataset settings
+dataset_type = 'VideoDataset'
+data_root_val = 'data/k400'
+ann_file_test = 'data/k400/val.csv'
 
-file_client_args = dict(io_backend='disk')
-
-train_pipeline = [
-    dict(type='DecordInit', **file_client_args),
-    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=16),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(
-        type='MultiScaleCrop',
-        input_size=224,
-        scales=(1, 0.875, 0.75, 0.66),
-        random_crop=False,
-        max_wh_scale_gap=1,
-        num_fixed_crops=13),
-    dict(type='Resize', scale=(224, 224), keep_ratio=False),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCHW'),
-    dict(type='PackActionInputs')
-]
-val_pipeline = [
-    dict(type='DecordInit', **file_client_args),
-    dict(
-        type='SampleFrames',
-        clip_len=1,
-        frame_interval=1,
-        num_clips=16,
-        test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='CenterCrop', crop_size=224),
-    dict(type='FormatShape', input_format='NCHW'),
-    dict(type='PackActionInputs')
-]
 test_pipeline = [
-    dict(type='DecordInit', **file_client_args),
+    dict(type='DecordInit'),
     dict(
-        type='SampleFrames',
-        clip_len=1,
-        frame_interval=1,
-        num_clips=16,
-        twice_sample=True,
+        type='UniformSample', clip_len=num_frames, num_clips=4,
         test_mode=True),
     dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='TenCrop', crop_size=224),
-    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='Resize', scale=(-1, 224)),
+    dict(type='ThreeCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
-
-val_dataloader = dict(dataset=dict(pipeline=val_pipeline))
+test_dataloader = dict(
+    batch_size=16,
+    num_workers=8,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_test,
+        data_prefix=dict(video=data_root_val),
+        pipeline=test_pipeline,
+        test_mode=True,
+        delimiter=','))
 
-test_dataloader = dict(pipeline=test_pipeline, test_mode=True)
+test_evaluator = dict(type='AccMetric')
+test_cfg = dict(type='TestLoop')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-dense-1x1x8-50e_kinetics400-rgb.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 ann_file_train = 'data/kinetics400/kinetics400_train_list_videos.txt'
 ann_file_val = 'data/kinetics400/kinetics400_val_list_videos.txt'
 
 file_client_args = dict(io_backend='disk')
 
 train_pipeline = [
     dict(type='DecordInit', **file_client_args),
-    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
+    dict(type='DenseSampleFrames', clip_len=1, frame_interval=1, num_clips=8),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(
         type='MultiScaleCrop',
         input_size=224,
         scales=(1, 0.875, 0.75, 0.66),
         random_crop=False,
@@ -25,29 +25,29 @@
     dict(type='Flip', flip_ratio=0.5),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='DecordInit', **file_client_args),
     dict(
-        type='SampleFrames',
+        type='DenseSampleFrames',
         clip_len=1,
         frame_interval=1,
         num_clips=8,
         test_mode=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='CenterCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(type='DecordInit', **file_client_args),
     dict(
-        type='SampleFrames',
+        type='DenseSampleFrames',
         clip_len=1,
         frame_interval=1,
         num_clips=8,
         test_mode=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='TenCrop', crop_size=224),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,28 +7,29 @@
 dataset_type = 'VideoDataset'
 data_root = 'data/sthv2/videos'
 ann_file_train = 'data/sthv2/sthv2_train_list_videos.txt'
 ann_file_val = 'data/sthv2/sthv2_val_list_videos.txt'
 
 file_client_args = dict(io_backend='disk')
 
+sthv2_flip_label_map = {86: 87, 87: 86, 93: 94, 94: 93, 166: 167, 167: 166}
 train_pipeline = [
     dict(type='DecordInit', **file_client_args),
     dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(
         type='MultiScaleCrop',
         input_size=224,
         scales=(1, 0.875, 0.75, 0.66),
         random_crop=False,
         max_wh_scale_gap=1,
         num_fixed_crops=13),
     dict(type='Resize', scale=(224, 224), keep_ratio=False),
-    dict(type='Flip', flip_ratio=0.5),
+    dict(type='Flip', flip_ratio=0.5, flip_label_map=sthv2_flip_label_map),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='DecordInit', **file_client_args),
     dict(
         type='SampleFrames',
@@ -49,15 +50,15 @@
         clip_len=1,
         frame_interval=1,
         num_clips=8,
         test_mode=True,
         twice_sample=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
-    dict(type='TenCrop', crop_size=224),
+    dict(type='ThreeCrop', crop_size=256),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=8,
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-dense-1x1x8-50e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,122 +1,108 @@
-_base_ = ['../../_base_/models/tsm_r50.py', '../../_base_/default_runtime.py']
+_base_ = [
+    '../../_base_/models/tsn_r50.py', '../../_base_/schedules/sgd_50e.py',
+    '../../_base_/default_runtime.py'
+]
+
+# model settings
+model = dict(cls_head=dict(num_classes=174, dropout_ratio=0.5))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root = 'data/kinetics400/videos_train'
-data_root_val = 'data/kinetics400/videos_val'
-ann_file_train = 'data/kinetics400/kinetics400_train_list_videos.txt'
-ann_file_val = 'data/kinetics400/kinetics400_val_list_videos.txt'
+data_root = 'data/sthv2/videos'
+ann_file_train = 'data/sthv2/sthv2_train_list_videos.txt'
+ann_file_val = 'data/sthv2/sthv2_val_list_videos.txt'
 
 file_client_args = dict(io_backend='disk')
 
+sthv2_flip_label_map = {86: 87, 87: 86, 93: 94, 94: 93, 166: 167, 167: 166}
 train_pipeline = [
     dict(type='DecordInit', **file_client_args),
-    dict(type='DenseSampleFrames', clip_len=1, frame_interval=1, num_clips=8),
+    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(
         type='MultiScaleCrop',
         input_size=224,
         scales=(1, 0.875, 0.75, 0.66),
         random_crop=False,
-        max_wh_scale_gap=1,
-        num_fixed_crops=13),
+        max_wh_scale_gap=1),
     dict(type='Resize', scale=(224, 224), keep_ratio=False),
-    dict(type='Flip', flip_ratio=0.5),
+    dict(type='Flip', flip_ratio=0.5, flip_label_map=sthv2_flip_label_map),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='DecordInit', **file_client_args),
     dict(
-        type='DenseSampleFrames',
+        type='SampleFrames',
         clip_len=1,
         frame_interval=1,
         num_clips=8,
         test_mode=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='CenterCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(type='DecordInit', **file_client_args),
     dict(
-        type='DenseSampleFrames',
+        type='SampleFrames',
         clip_len=1,
         frame_interval=1,
-        num_clips=8,
+        num_clips=25,
         test_mode=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 256)),
     dict(type='TenCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
         data_prefix=dict(video=data_root),
         pipeline=train_pipeline))
 val_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
-        data_prefix=dict(video=data_root_val),
+        data_prefix=dict(video=data_root),
         pipeline=val_pipeline,
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
-        data_prefix=dict(video=data_root_val),
+        data_prefix=dict(video=data_root),
         pipeline=test_pipeline,
         test_mode=True))
 
 val_evaluator = dict(type='AccMetric')
 test_evaluator = val_evaluator
 
 default_hooks = dict(checkpoint=dict(interval=3, max_keep_ckpts=3))
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=50, val_begin=1, val_interval=1)
-val_cfg = dict(type='ValLoop')
-test_cfg = dict(type='TestLoop')
-
-param_scheduler = [
-    dict(type='LinearLR', start_factor=0.1, by_epoch=True, begin=0, end=5),
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=50,
-        by_epoch=True,
-        milestones=[25, 45],
-        gamma=0.1)
-]
-
-optim_wrapper = dict(
-    constructor='TSMOptimWrapperConstructor',
-    paramwise_cfg=dict(fc_lr5=True),
-    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001),
-    clip_grad=dict(max_norm=20, norm_type=2))
+    type='EpochBasedTrainLoop', max_epochs=50, val_begin=1, val_interval=5)
 
 # Default setting for scaling LR automatically
 #   - `enable` means enable scaling LR automatically
 #       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=128)
+#   - `base_batch_size` = (8 GPUs) x (32 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=256)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-rn101-32x4d_8xb32-1x1x3-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-rn101-32x4d_8xb32-1x1x3-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/metafile.yml`

 * *Files 2% similar despite different names*

```diff
@@ -169,15 +169,15 @@
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb_20220906-5f4c0daf.pth
 
   - Name: tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb
     Config: configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb.py
     In Collection: TSN
     Metadata:
-      Architecture: ResNet50
+      Architecture: Swin-base
       Batch Size: 32
       Epochs: 100
       FLOPs: 386.7G
       Parameters: 87.15M
       Pretrained: ImageNet
       Resolution: 224x224
       Training Data: Kinetics-400
@@ -188,14 +188,37 @@
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 77.03
         Top 5 Accuracy: 92.61
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb_20220906-65ed814e.pth
 
+  - Name: tsn_imagenet-pretrained-swin-transformer_32xb8-1x1x8-50e_kinetics400-rgb
+    Config: configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-swin-transformer_32xb8-1x1x8-50e_kinetics400-rgb.py
+    In Collection: TSN
+    Metadata:
+      Architecture: Swin-base
+      Batch Size: 32
+      Epochs: 100
+      FLOPs: 386.7G
+      Parameters: 87.15M
+      Pretrained: ImageNet
+      Resolution: 224x224
+      Training Data: Kinetics-400
+      Training Resources: 32 GPUs
+    Modality: RGB
+    Results:
+    - Dataset: Kinetics-400
+      Task: Action Recognition
+      Metrics:
+        Top 1 Accuracy: 79.22
+        Top 5 Accuracy: 94.20
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-swin-transformer_32xb8-1x1x8-50e_kinetics400-rgb/tsn_imagenet-pretrained-swin-transformer_32xb8-1x1x8-50e_kinetics400-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb_20220906-65ed814e.pth
+
   - Name: tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb
     Config: configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py
     In Collection: TSN
     Metadata:
       Architecture: ResNet50
       Batch Size: 32
       Epochs: 100
@@ -206,18 +229,18 @@
       Training Data: Kinetics-400
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
     - Dataset: Kinetics-400
       Task: Action Recognition
       Metrics:
-        Top 1 Accuracy: 34.85
-        Top 5 Accuracy: 66.37
+        Top 1 Accuracy: 35.51
+        Top 5 Accuracy: 67.09
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.log
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb_20221122-ad2dbb37.pth
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb_20230313-06ad7d03.pth
 
   - Name: tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb
     Config: configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb.py
     In Collection: TSN
     Metadata:
       Architecture: ResNet50
       Batch Size: 32
@@ -229,11 +252,11 @@
       Training Data: Kinetics-400
       Training Resources: 8 GPUs
     Modality: RGB
     Results:
     - Dataset: Kinetics-400
       Task: Action Recognition
       Metrics:
-        Top 1 Accuracy: 36.55
-        Top 5 Accuracy: 68.00
+        Top 1 Accuracy: 36.91
+        Top 5 Accuracy: 68.77
     Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb.log
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb_20221122-ee13c8e2.pth
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb_20230221-85bcc1c3.pth
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,55 +1,67 @@
-_base_ = ['tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py']
-
-file_client_args = dict(io_backend='disk')
+_base_ = 'stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
 
+dataset_type = 'PoseDataset'
+ann_file = 'data/skeleton/ntu60_2d.pkl'
 train_pipeline = [
-    dict(type='DecordInit', **file_client_args),
-    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=16),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(
-        type='MultiScaleCrop',
-        input_size=224,
-        scales=(1, 0.875, 0.75, 0.66),
-        random_crop=False,
-        max_wh_scale_gap=1,
-        num_fixed_crops=13),
-    dict(type='Resize', scale=(224, 224), keep_ratio=False),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='UniformSampleFrames', clip_len=100),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='DecordInit', **file_client_args),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
     dict(
-        type='SampleFrames',
-        clip_len=1,
-        frame_interval=1,
-        num_clips=16,
-        test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='CenterCrop', crop_size=224),
-    dict(type='FormatShape', input_format='NCHW'),
+        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='DecordInit', **file_client_args),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
     dict(
-        type='SampleFrames',
-        clip_len=1,
-        frame_interval=1,
-        num_clips=25,
+        type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='TenCrop', crop_size=224),
-    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
-
-val_dataloader = dict(dataset=dict(pipeline=val_pipeline))
-
-test_dataloader = dict(dataset=dict(pipeline=test_pipeline, test_mode=True))
+train_dataloader = dict(
+    batch_size=16,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=True),
+    dataset=dict(
+        type='RepeatDataset',
+        times=5,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            pipeline=train_pipeline,
+            split='xsub_train')))
+val_dataloader = dict(
+    batch_size=16,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file,
+        pipeline=val_pipeline,
+        split='xsub_val',
+        test_mode=True))
+test_dataloader = dict(
+    batch_size=1,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file,
+        pipeline=test_pipeline,
+        split='xsub_val',
+        test_mode=True))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x5-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x5-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,107 +1,106 @@
-_base_ = [
-    '../../_base_/models/tsn_r50.py', '../../_base_/schedules/sgd_50e.py',
-    '../../_base_/default_runtime.py'
-]
-
-# model settings
-model = dict(cls_head=dict(num_classes=174, dropout_ratio=0.5))
-
-# dataset settings
-dataset_type = 'VideoDataset'
-data_root = 'data/sthv2/videos'
-ann_file_train = 'data/sthv2/sthv2_train_list_videos.txt'
-ann_file_val = 'data/sthv2/sthv2_val_list_videos.txt'
+_base_ = '../../_base_/default_runtime.py'
 
-file_client_args = dict(io_backend='disk')
+model = dict(
+    type='RecognizerGCN',
+    backbone=dict(
+        type='STGCN',
+        gcn_adaptive='init',
+        gcn_with_res=True,
+        tcn_type='mstcn',
+        graph_cfg=dict(layout='nturgb+d', mode='spatial')),
+    cls_head=dict(type='GCNHead', num_classes=60, in_channels=256))
 
+dataset_type = 'PoseDataset'
+ann_file = 'data/skeleton/ntu60_3d.pkl'
 train_pipeline = [
-    dict(type='DecordInit', **file_client_args),
-    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(
-        type='MultiScaleCrop',
-        input_size=224,
-        scales=(1, 0.875, 0.75, 0.66),
-        random_crop=False,
-        max_wh_scale_gap=1),
-    dict(type='Resize', scale=(224, 224), keep_ratio=False),
-    dict(type='Flip', flip_ratio=0.5),
-    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['j']),
+    dict(type='UniformSampleFrames', clip_len=100),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='DecordInit', **file_client_args),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['j']),
     dict(
-        type='SampleFrames',
-        clip_len=1,
-        frame_interval=1,
-        num_clips=8,
-        test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='CenterCrop', crop_size=224),
-    dict(type='FormatShape', input_format='NCHW'),
+        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='DecordInit', **file_client_args),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['j']),
     dict(
-        type='SampleFrames',
-        clip_len=1,
-        frame_interval=1,
-        num_clips=25,
+        type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 256)),
-    dict(type='TenCrop', crop_size=224),
-    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=32,
-    num_workers=8,
+    batch_size=16,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file_train,
-        data_prefix=dict(video=data_root),
-        pipeline=train_pipeline))
+        type='RepeatDataset',
+        times=5,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            pipeline=train_pipeline,
+            split='xsub_train')))
 val_dataloader = dict(
-    batch_size=32,
-    num_workers=8,
+    batch_size=16,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
-        data_prefix=dict(video=data_root),
+        ann_file=ann_file,
         pipeline=val_pipeline,
+        split='xsub_val',
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
-    num_workers=8,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
-        data_prefix=dict(video=data_root),
+        ann_file=ann_file,
         pipeline=test_pipeline,
+        split='xsub_val',
         test_mode=True))
 
-val_evaluator = dict(type='AccMetric')
+val_evaluator = [dict(type='AccMetric')]
 test_evaluator = val_evaluator
 
-default_hooks = dict(checkpoint=dict(interval=3, max_keep_ckpts=3))
-
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=50, val_begin=1, val_interval=5)
+    type='EpochBasedTrainLoop', max_epochs=16, val_begin=1, val_interval=1)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
+
+param_scheduler = [
+    dict(
+        type='CosineAnnealingLR',
+        eta_min=0,
+        T_max=16,
+        by_epoch=True,
+        convert_to_iter_based=True)
+]
+
+optim_wrapper = dict(
+    optimizer=dict(
+        type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0005, nesterov=True))
+
+default_hooks = dict(checkpoint=dict(interval=1), logger=dict(interval=100))
 
 # Default setting for scaling LR automatically
 #   - `enable` means enable scaling LR automatically
 #       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (32 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=256)
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-dense-1x1x5-100e_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-dense-1x1x5-100e_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_16x4x1_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_16x4x1_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_32x4x1_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_32x4x1_kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformer/uniformer-small_imagenet1k-pre_16x4x1_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,52 +1,64 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
+num_frames = 16
 model = dict(
     type='Recognizer3D',
     backbone=dict(
-        type='UniFormer',
-        depth=[3, 4, 8, 3],
-        embed_dim=[64, 128, 320, 512],
-        head_dim=64,
-        drop_path_rate=0.1),
+        type='UniFormerV2',
+        input_resolution=224,
+        patch_size=14,
+        width=1024,
+        layers=24,
+        heads=16,
+        t_size=num_frames,
+        dw_reduction=1.5,
+        backbone_drop_path_rate=0.,
+        temporal_downsample=False,
+        no_lmhra=True,
+        double_lmhra=True,
+        return_list=[20, 21, 22, 23],
+        n_layers=4,
+        n_dim=1024,
+        n_head=16,
+        mlp_factor=4.,
+        drop_path_rate=0.,
+        mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
-        type='I3DHead',
-        dropout_ratio=0.,
-        num_classes=400,
-        in_channels=512,
+        type='TimeSformerHead',
+        dropout_ratio=0.5,
+        num_classes=700,
+        in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/k400'
-ann_file_test = 'data/k400/val.csv'
+data_root_val = 'data/k700'
+ann_file_test = 'data/k700/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
-        type='SampleFrames',
-        clip_len=16,
-        frame_interval=4,
-        num_clips=4,
+        type='UniformSample', clip_len=num_frames, num_clips=4,
         test_mode=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 224)),
-    dict(type='CenterCrop', crop_size=224),
+    dict(type='ThreeCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=32,
+    batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/metafile.yml`

 * *Files 17% similar despite different names*

```diff
@@ -2,42 +2,65 @@
 - Name: UniFormerV2
   README: configs/recognition/uniformerv2/README.md
   Paper:
     URL: https://arxiv.org/abs/2211.09552
     Title: "UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer"
 
 Models:
-  - Name: uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics400-rgb
-    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py
-    In Collection: UniFormer
+  - Name: uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb
+    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb.py
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-B/16
+      Batch Size: 32
+      Pretrained: CLIP-400M
+      Frame: 8
+      Sampling method: Uniform
+      Resolution: 224x224
+      Training Data: Kinetics-400
+      Training Resources: 8 GPUs
+    Modality: RGB
+    Results:
+    - Dataset: Kinetics-400
+      Task: Action Recognition
+      Metrics:
+        Top 1 Accuracy: 84.3
+        Top 5 Accuracy: 96.4
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb_20230313-e29fc968.pth
+
+  - Name: uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb
+    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb.py
+    In Collection: UniFormerV2
+    Metadata:
+      Architecture: UniFormerV2-B/16
+      Batch Size: 32
       Pretrained: Kinetics-710
-      Resolution: short-side 320
       Frame: 8
       Sampling method: Uniform
+      Resolution: 224x224
+      Training Data: Kinetics-400
+      Training Resources: 8 GPUs
     Modality: RGB
-    Converted From:
-      Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
-      Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
     - Dataset: Kinetics-400
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 85.8
         Top 5 Accuracy: 97.1
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics400/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics400-rgb_20221219-203d6aac.pth
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb_20230313-75be0806.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
-      Resolution: short-side 320
+      Resolution: 224x224
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
@@ -46,19 +69,19 @@
       Metrics:
         Top 1 Accuracy: 88.7
         Top 5 Accuracy: 98.1
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics400/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb_20221219-972ea063.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
-      Resolution: short-side 320
+      Resolution: 224x224
       Frame: 16
       Sampling method: Uniform
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
@@ -67,19 +90,19 @@
       Metrics:
         Top 1 Accuracy: 89.0
         Top 5 Accuracy: 98.2
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics400/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb_20221219-6dc86d05.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
-      Resolution: short-side 320
+      Resolution: 224x224
       Frame: 32
       Sampling method: Uniform
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
@@ -88,56 +111,58 @@
       Metrics:
         Top 1 Accuracy: 89.3
         Top 5 Accuracy: 98.2
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics400/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb_20221219-56a46f64.pth
 
   - Name: uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics400-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics400-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14@336
       Pretrained: Kinetics-710
-      Resolution: short-side 320
+      Resolution: 224x224
       Frame: 32
       Sampling method: Uniform
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
     - Dataset: Kinetics-400
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 89.5
         Top 5 Accuracy: 98.4
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics400/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics400-rgb_20221219-1dd7650f.pth
 
-  - Name: uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics600-rgb
-    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py
-    In Collection: UniFormer
+  - Name: uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb
+    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb.py
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-B/16
       Pretrained: Kinetics-710
       Frame: 8
       Sampling method: Uniform
+      Training Resources: 8 GPUs
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
     - Dataset: Kinetics-600
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 86.4
         Top 5 Accuracy: 97.3
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics600/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics600-rgb_20221219-c62c4da4.pth
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb_20230313-544f06f0.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics600-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -149,15 +174,15 @@
       Metrics:
         Top 1 Accuracy: 89.0
         Top 5 Accuracy: 98.3
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics600/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics600-rgb_20221219-cf88e4c2.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
       Frame: 16
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -169,15 +194,15 @@
       Metrics:
         Top 1 Accuracy: 89.4
         Top 5 Accuracy: 98.3
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics600/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb_20221219-38ff0e3e.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
       Frame: 32
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -189,15 +214,15 @@
       Metrics:
         Top 1 Accuracy: 89.2
         Top 5 Accuracy: 98.3
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics600/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb_20221219-d450d071.pth
 
   - Name: uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics600-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics600-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14@336
       Pretrained: Kinetics-710
       Frame: 32
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -207,37 +232,61 @@
     - Dataset: Kinetics-600
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 89.8
         Top 5 Accuracy: 98.5
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics600/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics600-rgb_20221219-f984f5d2.pth
 
-  - Name: uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics700-rgb
-    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py
+  - Name: uniformerv2-base-p16-res224_clip-pre_8xb32-u8_kinetics700-rgb
+    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_8xb32-u8_kinetics700-rgb.py
+    In Collection: UniFormer
+    Metadata:
+      Architecture: UniFormerV2-B/16
+      Pretrained: CLIP-400M
+      Frame: 8
+      Sampling method: Uniform
+      Training Resources: 8 GPUs
+    Modality: RGB
+    Converted From:
+      Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
+      Code: https://github.com/OpenGVLab/UniFormerV2
+    Results:
+    - Dataset: Kinetics-700
+      Task: Action Recognition
+      Metrics:
+        Top 1 Accuracy: 75.9
+        Top 5 Accuracy: 92.9
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics700-rgb/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics700-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics700-rgb/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics700-rgb_20230313-f02e48ad.pth
+
+  - Name: uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb
+    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb.py
     In Collection: UniFormer
     Metadata:
       Architecture: UniFormerV2-B/16
       Pretrained: Kinetics-710
       Frame: 8
       Sampling method: Uniform
+      Training Resources: 8 GPUs
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
     - Dataset: Kinetics-700
       Task: Action Recognition
       Metrics:
         Top 1 Accuracy: 76.3
         Top 5 Accuracy: 92.9
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics700/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics700-rgb_20221219-8a7c4ac4.pth
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb_20230313-69070837.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -249,15 +298,15 @@
       Metrics:
         Top 1 Accuracy: 80.8
         Top 5 Accuracy: 95.2
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics700/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb_20221219-bfb9f401.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
       Frame: 16
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -269,15 +318,15 @@
       Metrics:
         Top 1 Accuracy: 81.2
         Top 5 Accuracy: 95.6
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics700/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb_20221219-745209d2.pth
 
   - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710
       Frame: 32
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -289,15 +338,15 @@
       Metrics:
         Top 1 Accuracy: 81.4
         Top 5 Accuracy: 95.7
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics700/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb_20221219-eebe7056.pth
 
   - Name: uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14@336
       Pretrained: Kinetics-710
       Frame: 32
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -309,77 +358,80 @@
       Metrics:
         Top 1 Accuracy: 82.1
         Top 5 Accuracy: 96.0
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics700/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb_20221219-bfb9f401.pth
 
   - Name: uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-B/16
       Pretrained: CLIP-400M
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics710/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb_20221219-77d34f81.pth
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb_20230612-63cdbad9.pth
 
-  - Name: uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb
-    Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py
+  - Name: uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb
+    Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb.py
     In Collection: UniFormer
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: CLIP-400M
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics710/uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb_20221219-bfaae587.pth
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics710/uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb_20230612-d002a407.pth
 
   - Name: uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14@336
       Pretrained: Kinetics-710
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics710/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb_20221219-55878cdc.pth
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/kinetics710/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb_20230612-d723ddc1.pth
 
-  - Name: uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb
-    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
-    In Collection: UniFormer
+  - Name: uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb
+    Config: configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb.py
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-B/16
       Pretrained: Kinetics-710 + Kinetics-400
       Frame: 8
       Sampling method: Uniform
+      Training Resources: 16 GPUs
     Modality: RGB
     Converted From:
       Weights: https://github.com/OpenGVLab/UniFormerV2/blob/main/MODEL_ZOO.md
       Code: https://github.com/OpenGVLab/UniFormerV2
     Results:
     - Dataset: Moments in Time V1
       Task: Action Recognition
       Metrics:
-        Top 1 Accuracy: 42.7
-        Top 5 Accuracy: 71.6
-    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/mitv1/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb_20221219-fddbc786.pth
+        Top 1 Accuracy: 42.3
+        Top 5 Accuracy: 71.5
+    Training Log: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb.log
+    Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb_20230313-a6f4a567.pth
 
   - Name: uniformerv2-large-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14
       Pretrained: Kinetics-710 + Kinetics-400
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
@@ -391,15 +443,15 @@
       Metrics:
         Top 1 Accuracy: 47.0
         Top 5 Accuracy: 76.1
     Weights: https://download.openmmlab.com/mmaction/v1.0/recognition/uniformerv2/mitv1/uniformerv2-large-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb_20221219-882c0598.pth
 
   - Name: uniformerv2-large-p16-res336_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb
     Config: configs/recognition/uniformerv2/uniformerv2-large-p16-res336_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
-    In Collection: UniFormer
+    In Collection: UniFormerV2
     Metadata:
       Architecture: UniFormerV2-L/14@336
       Pretrained: Kinetics-710 + Kinetics-400
       Frame: 8
       Sampling method: Uniform
     Modality: RGB
     Converted From:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics600-rgb.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,70 +1,70 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-num_frames = 8
+num_frames = 32
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
-        input_resolution=224,
-        patch_size=16,
-        width=768,
-        layers=12,
-        heads=12,
+        input_resolution=336,
+        patch_size=14,
+        width=1024,
+        layers=24,
+        heads=16,
         t_size=num_frames,
         dw_reduction=1.5,
         backbone_drop_path_rate=0.,
         temporal_downsample=False,
         no_lmhra=True,
         double_lmhra=True,
-        return_list=[8, 9, 10, 11],
+        return_list=[20, 21, 22, 23],
         n_layers=4,
-        n_dim=768,
-        n_head=12,
+        n_dim=1024,
+        n_head=16,
         mlp_factor=4.,
         drop_path_rate=0.,
         mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
         type='TimeSformerHead',
         dropout_ratio=0.5,
-        num_classes=339,
-        in_channels=768,
+        num_classes=600,
+        in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/mit_v1'
-ann_file_test = 'data/mit_v1/val.csv'
+data_root_val = 'data/k600'
+ann_file_test = 'data/k600/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
-        type='UniformSample', clip_len=num_frames, num_clips=4,
+        type='UniformSample', clip_len=num_frames, num_clips=2,
         test_mode=True),
     dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 224)),
-    dict(type='ThreeCrop', crop_size=224),
+    dict(type='Resize', scale=(-1, 336)),
+    dict(type='ThreeCrop', crop_size=336),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=32,
+    batch_size=4,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
         test_mode=True,
-        delimiter=' '))
+        delimiter=','))
 
 test_evaluator = dict(type='AccMetric')
 test_cfg = dict(type='TestLoop')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,64 +1,64 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-num_frames = 8
+num_frames = 32
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
         input_resolution=224,
-        patch_size=16,
-        width=768,
-        layers=12,
-        heads=12,
+        patch_size=14,
+        width=1024,
+        layers=24,
+        heads=16,
         t_size=num_frames,
         dw_reduction=1.5,
         backbone_drop_path_rate=0.,
         temporal_downsample=False,
         no_lmhra=True,
         double_lmhra=True,
-        return_list=[8, 9, 10, 11],
+        return_list=[20, 21, 22, 23],
         n_layers=4,
-        n_dim=768,
-        n_head=12,
+        n_dim=1024,
+        n_head=16,
         mlp_factor=4.,
         drop_path_rate=0.,
         mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
         type='TimeSformerHead',
         dropout_ratio=0.5,
-        num_classes=400,
-        in_channels=768,
+        num_classes=700,
+        in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/k400'
-ann_file_test = 'data/k400/val.csv'
+data_root_val = 'data/k700'
+ann_file_test = 'data/k700/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
         type='UniformSample', clip_len=num_frames, num_clips=4,
         test_mode=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 224)),
     dict(type='ThreeCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=32,
+    batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics400-rgb.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,64 +1,64 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-num_frames = 8
+num_frames = 32
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
-        input_resolution=224,
-        patch_size=16,
-        width=768,
-        layers=12,
-        heads=12,
+        input_resolution=336,
+        patch_size=14,
+        width=1024,
+        layers=24,
+        heads=16,
         t_size=num_frames,
         dw_reduction=1.5,
         backbone_drop_path_rate=0.,
         temporal_downsample=False,
         no_lmhra=True,
         double_lmhra=True,
-        return_list=[8, 9, 10, 11],
+        return_list=[20, 21, 22, 23],
         n_layers=4,
-        n_dim=768,
-        n_head=12,
+        n_dim=1024,
+        n_head=16,
         mlp_factor=4.,
         drop_path_rate=0.,
         mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
         type='TimeSformerHead',
         dropout_ratio=0.5,
-        num_classes=600,
-        in_channels=768,
+        num_classes=400,
+        in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/k600'
-ann_file_test = 'data/k600/val.csv'
+data_root_val = 'data/k400'
+ann_file_test = 'data/k400/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
-        type='UniformSample', clip_len=num_frames, num_clips=4,
+        type='UniformSample', clip_len=num_frames, num_clips=2,
         test_mode=True),
     dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 224)),
-    dict(type='ThreeCrop', crop_size=224),
+    dict(type='Resize', scale=(-1, 336)),
+    dict(type='ThreeCrop', crop_size=336),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=32,
+    batch_size=4,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res336_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,69 +2,69 @@
 
 # model settings
 num_frames = 8
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
-        input_resolution=224,
-        patch_size=16,
-        width=768,
-        layers=12,
-        heads=12,
+        input_resolution=336,
+        patch_size=14,
+        width=1024,
+        layers=24,
+        heads=16,
         t_size=num_frames,
         dw_reduction=1.5,
         backbone_drop_path_rate=0.,
         temporal_downsample=False,
         no_lmhra=True,
         double_lmhra=True,
-        return_list=[8, 9, 10, 11],
+        return_list=[20, 21, 22, 23],
         n_layers=4,
-        n_dim=768,
-        n_head=12,
+        n_dim=1024,
+        n_head=16,
         mlp_factor=4.,
         drop_path_rate=0.,
         mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
         type='TimeSformerHead',
         dropout_ratio=0.5,
-        num_classes=700,
-        in_channels=768,
+        num_classes=339,
+        in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/k700'
-ann_file_test = 'data/k700/val.csv'
+data_root_val = 'data/mit_v1'
+ann_file_test = 'data/mit_v1/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
         type='UniformSample', clip_len=num_frames, num_clips=4,
         test_mode=True),
     dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 224)),
-    dict(type='ThreeCrop', crop_size=224),
+    dict(type='Resize', scale=(-1, 336)),
+    dict(type='ThreeCrop', crop_size=336),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=32,
+    batch_size=8,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
         test_mode=True,
-        delimiter=','))
+        delimiter=' '))
 
 test_evaluator = dict(type='AccMetric')
 test_cfg = dict(type='TestLoop')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-num_frames = 16
+num_frames = 32
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
         input_resolution=224,
         patch_size=14,
         width=1024,
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-num_frames = 16
+num_frames = 8
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
         input_resolution=224,
         patch_size=14,
         width=1024,
@@ -50,15 +50,15 @@
     dict(type='Resize', scale=(-1, 224)),
     dict(type='ThreeCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb.py`

 * *Files 1% similar despite different names*

```diff
@@ -23,27 +23,27 @@
         n_head=16,
         mlp_factor=4.,
         drop_path_rate=0.,
         mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
         type='TimeSformerHead',
         dropout_ratio=0.5,
-        num_classes=400,
+        num_classes=600,
         in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/k400'
-ann_file_test = 'data/k400/val.csv'
+data_root_val = 'data/k600'
+ann_file_test = 'data/k600/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
         type='UniformSample', clip_len=num_frames, num_clips=4,
         test_mode=True),
     dict(type='DecordDecode'),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-num_frames = 32
+num_frames = 8
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
         input_resolution=224,
         patch_size=14,
         width=1024,
@@ -50,15 +50,15 @@
     dict(type='Resize', scale=(-1, 224)),
     dict(type='ThreeCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../_base_/default_runtime.py']
 
 # model settings
-num_frames = 32
+num_frames = 8
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='UniFormerV2',
         input_resolution=224,
         patch_size=14,
         width=1024,
@@ -23,42 +23,42 @@
         n_head=16,
         mlp_factor=4.,
         drop_path_rate=0.,
         mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
         type='TimeSformerHead',
         dropout_ratio=0.5,
-        num_classes=700,
+        num_classes=400,
         in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/k700'
-ann_file_test = 'data/k700/val.csv'
+data_root_val = 'data/k400'
+ann_file_test = 'data/k400/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
         type='UniformSample', clip_len=num_frames, num_clips=4,
         test_mode=True),
     dict(type='DecordDecode'),
     dict(type='Resize', scale=(-1, 224)),
     dict(type='ThreeCrop', crop_size=224),
     dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
 test_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,27 +23,27 @@
         n_head=16,
         mlp_factor=4.,
         drop_path_rate=0.,
         mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
     cls_head=dict(
         type='TimeSformerHead',
         dropout_ratio=0.5,
-        num_classes=400,
+        num_classes=339,
         in_channels=1024,
         average_clips='prob'),
     data_preprocessor=dict(
         type='ActionDataPreprocessor',
         mean=[114.75, 114.75, 114.75],
         std=[57.375, 57.375, 57.375],
         format_shape='NCTHW'))
 
 # dataset settings
 dataset_type = 'VideoDataset'
-data_root_val = 'data/k400'
-ann_file_test = 'data/k400/val.csv'
+data_root_val = 'data/mit_v1'
+ann_file_test = 'data/mit_v1/val.csv'
 
 test_pipeline = [
     dict(type='DecordInit'),
     dict(
         type='UniformSample', clip_len=num_frames, num_clips=4,
         test_mode=True),
     dict(type='DecordDecode'),
@@ -60,11 +60,11 @@
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
         data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
         test_mode=True,
-        delimiter=','))
+        delimiter=' '))
 
 test_evaluator = dict(type='AccMetric')
 test_cfg = dict(type='TestLoop')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio-feature.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,70 +1,97 @@
-_base_ = ['../../_base_/default_runtime.py']
-
-# model settings
-num_frames = 8
-model = dict(
-    type='Recognizer3D',
-    backbone=dict(
-        type='UniFormerV2',
-        input_resolution=224,
-        patch_size=14,
-        width=1024,
-        layers=24,
-        heads=16,
-        t_size=num_frames,
-        dw_reduction=1.5,
-        backbone_drop_path_rate=0.,
-        temporal_downsample=False,
-        no_lmhra=True,
-        double_lmhra=True,
-        return_list=[20, 21, 22, 23],
-        n_layers=4,
-        n_dim=1024,
-        n_head=16,
-        mlp_factor=4.,
-        drop_path_rate=0.,
-        mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
-    cls_head=dict(
-        type='TimeSformerHead',
-        dropout_ratio=0.5,
-        num_classes=600,
-        in_channels=1024,
-        average_clips='prob'),
-    data_preprocessor=dict(
-        type='ActionDataPreprocessor',
-        mean=[114.75, 114.75, 114.75],
-        std=[57.375, 57.375, 57.375],
-        format_shape='NCTHW'))
+_base_ = [
+    '../../_base_/models/tsn_r18_audio.py', '../../_base_/default_runtime.py'
+]
 
 # dataset settings
-dataset_type = 'VideoDataset'
-data_root_val = 'data/k600'
-ann_file_test = 'data/k600/val.csv'
-
+dataset_type = 'AudioDataset'
+data_root = 'data/kinetics400/audio_features_train'
+data_root_val = 'data/kinetics400/audio_features_val'
+ann_file_train = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
+ann_file_val = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
+ann_file_test = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
+train_pipeline = [
+    dict(type='LoadAudioFeature'),
+    dict(type='SampleFrames', clip_len=64, frame_interval=1, num_clips=1),
+    dict(type='AudioFeatureSelector'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='PackActionInputs')
+]
+val_pipeline = [
+    dict(type='LoadAudioFeature'),
+    dict(
+        type='SampleFrames',
+        clip_len=64,
+        frame_interval=1,
+        num_clips=1,
+        test_mode=True),
+    dict(type='AudioFeatureSelector'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='PackActionInputs')
+]
 test_pipeline = [
-    dict(type='DecordInit'),
+    dict(type='LoadAudioFeature'),
     dict(
-        type='UniformSample', clip_len=num_frames, num_clips=4,
+        type='SampleFrames',
+        clip_len=64,
+        frame_interval=1,
+        num_clips=10,
         test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 224)),
-    dict(type='ThreeCrop', crop_size=224),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='AudioFeatureSelector'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
     dict(type='PackActionInputs')
 ]
 
+train_dataloader = dict(
+    batch_size=320,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=True),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_train,
+        data_prefix=dict(audio=data_root_val),
+        suffix='.npy',
+        pipeline=train_pipeline))
+val_dataloader = dict(
+    batch_size=320,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_val,
+        pipeline=val_pipeline,
+        data_prefix=dict(audio=data_root_val),
+        suffix='.npy',
+        test_mode=True))
 test_dataloader = dict(
-    batch_size=32,
-    num_workers=8,
+    batch_size=1,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
-        data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
-        test_mode=True,
-        delimiter=','))
+        data_prefix=dict(audio=data_root_val),
+        suffix='.npy',
+        test_mode=True))
 
-test_evaluator = dict(type='AccMetric')
+val_evaluator = dict(type='AccMetric')
+test_evaluator = val_evaluator
+
+train_cfg = dict(
+    type='EpochBasedTrainLoop', max_epochs=100, val_begin=1, val_interval=5)
+val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
+
+param_scheduler = [
+    dict(type='CosineAnnealingLR', eta_min=0, T_max=100, by_epoch=True)
+]
+
+optim_wrapper = dict(
+    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001),
+    clip_grad=dict(max_norm=40, norm_type=2))
+
+default_hooks = dict(
+    checkpoint=dict(max_keep_ckpts=3, interval=5), logger=dict(interval=20))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-pre_u8_kinetics710-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,70 +1,100 @@
-_base_ = ['../../_base_/default_runtime.py']
-
-# model settings
-num_frames = 32
-model = dict(
-    type='Recognizer3D',
-    backbone=dict(
-        type='UniFormerV2',
-        input_resolution=336,
-        patch_size=14,
-        width=1024,
-        layers=24,
-        heads=16,
-        t_size=num_frames,
-        dw_reduction=1.5,
-        backbone_drop_path_rate=0.,
-        temporal_downsample=False,
-        no_lmhra=True,
-        double_lmhra=True,
-        return_list=[20, 21, 22, 23],
-        n_layers=4,
-        n_dim=1024,
-        n_head=16,
-        mlp_factor=4.,
-        drop_path_rate=0.,
-        mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
-    cls_head=dict(
-        type='TimeSformerHead',
-        dropout_ratio=0.5,
-        num_classes=400,
-        in_channels=1024,
-        average_clips='prob'),
-    data_preprocessor=dict(
-        type='ActionDataPreprocessor',
-        mean=[114.75, 114.75, 114.75],
-        std=[57.375, 57.375, 57.375],
-        format_shape='NCTHW'))
+_base_ = [
+    '../../_base_/models/tsn_r18_audio.py', '../../_base_/default_runtime.py'
+]
 
 # dataset settings
-dataset_type = 'VideoDataset'
-data_root_val = 'data/k400'
-ann_file_test = 'data/k400/val.csv'
-
+dataset_type = 'AudioDataset'
+data_root = 'data/kinetics400/audios_train'
+data_root_val = 'data/kinetics400/audios_val'
+ann_file_train = 'data/kinetics400/kinetics400_train_list_audios.txt'
+ann_file_val = 'data/kinetics400/kinetics400_val_list_audios.txt'
+ann_file_test = 'data/kinetics400/kinetics400_val_list_audios.txt'
+train_pipeline = [
+    dict(type='AudioDecodeInit'),
+    dict(type='SampleFrames', clip_len=64, frame_interval=1, num_clips=1),
+    dict(type='AudioDecode'),
+    dict(type='AudioAmplify', ratio=1.5),
+    dict(type='MelSpectrogram'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='PackActionInputs')
+]
+val_pipeline = [
+    dict(type='AudioDecodeInit'),
+    dict(
+        type='SampleFrames',
+        clip_len=64,
+        frame_interval=1,
+        num_clips=1,
+        test_mode=True),
+    dict(type='AudioDecode'),
+    dict(type='AudioAmplify', ratio=1.5),
+    dict(type='MelSpectrogram'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='PackActionInputs')
+]
 test_pipeline = [
-    dict(type='DecordInit'),
+    dict(type='AudioDecodeInit'),
     dict(
-        type='UniformSample', clip_len=num_frames, num_clips=2,
+        type='SampleFrames',
+        clip_len=64,
+        frame_interval=1,
+        num_clips=10,
         test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 336)),
-    dict(type='ThreeCrop', crop_size=336),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='AudioDecode'),
+    dict(type='AudioAmplify', ratio=1.5),
+    dict(type='MelSpectrogram'),
+    dict(type='FormatAudioShape', input_format='NCTF'),
     dict(type='PackActionInputs')
 ]
 
+train_dataloader = dict(
+    batch_size=320,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=True),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_train,
+        data_prefix=dict(audio=data_root),
+        pipeline=train_pipeline))
+val_dataloader = dict(
+    batch_size=320,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file_val,
+        pipeline=val_pipeline,
+        data_prefix=dict(audio=data_root_val),
+        test_mode=True))
 test_dataloader = dict(
-    batch_size=4,
-    num_workers=8,
+    batch_size=1,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
-        data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
-        test_mode=True,
-        delimiter=','))
+        data_prefix=dict(audio=data_root_val),
+        test_mode=True))
 
-test_evaluator = dict(type='AccMetric')
+val_evaluator = dict(type='AccMetric')
+test_evaluator = val_evaluator
+
+train_cfg = dict(
+    type='EpochBasedTrainLoop', max_epochs=100, val_begin=1, val_interval=5)
+val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
+
+param_scheduler = [
+    dict(type='CosineAnnealingLR', eta_min=0, T_max=100, by_epoch=True)
+]
+
+optim_wrapper = dict(
+    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001),
+    clip_grad=dict(max_norm=40, norm_type=2))
+
+default_hooks = dict(
+    checkpoint=dict(max_keep_ckpts=3, interval=5), logger=dict(interval=20))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,70 +1,67 @@
-_base_ = ['../../_base_/default_runtime.py']
-
-# model settings
-num_frames = 32
-model = dict(
-    type='Recognizer3D',
-    backbone=dict(
-        type='UniFormerV2',
-        input_resolution=336,
-        patch_size=14,
-        width=1024,
-        layers=24,
-        heads=16,
-        t_size=num_frames,
-        dw_reduction=1.5,
-        backbone_drop_path_rate=0.,
-        temporal_downsample=False,
-        no_lmhra=True,
-        double_lmhra=True,
-        return_list=[20, 21, 22, 23],
-        n_layers=4,
-        n_dim=1024,
-        n_head=16,
-        mlp_factor=4.,
-        drop_path_rate=0.,
-        mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
-    cls_head=dict(
-        type='TimeSformerHead',
-        dropout_ratio=0.5,
-        num_classes=700,
-        in_channels=1024,
-        average_clips='prob'),
-    data_preprocessor=dict(
-        type='ActionDataPreprocessor',
-        mean=[114.75, 114.75, 114.75],
-        std=[57.375, 57.375, 57.375],
-        format_shape='NCTHW'))
-
-# dataset settings
-dataset_type = 'VideoDataset'
-data_root_val = 'data/k700'
-ann_file_test = 'data/k700/val.csv'
+_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
 
+dataset_type = 'PoseDataset'
+ann_file = 'data/skeleton/ntu60_2d.pkl'
+train_pipeline = [
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='UniformSampleFrames', clip_len=100),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
+    dict(type='PackActionInputs')
+]
+val_pipeline = [
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(
+        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
+    dict(type='PackActionInputs')
+]
 test_pipeline = [
-    dict(type='DecordInit'),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
     dict(
-        type='UniformSample', clip_len=num_frames, num_clips=2,
+        type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 336)),
-    dict(type='ThreeCrop', crop_size=336),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 
+train_dataloader = dict(
+    batch_size=16,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=True),
+    dataset=dict(
+        type='RepeatDataset',
+        times=5,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            pipeline=train_pipeline,
+            split='xsub_train')))
+val_dataloader = dict(
+    batch_size=16,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file,
+        pipeline=val_pipeline,
+        split='xsub_val',
+        test_mode=True))
 test_dataloader = dict(
-    batch_size=4,
-    num_workers=8,
+    batch_size=1,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_test,
-        data_prefix=dict(video=data_root_val),
+        ann_file=ann_file,
         pipeline=test_pipeline,
-        test_mode=True,
-        delimiter=','))
-
-test_evaluator = dict(type='AccMetric')
-test_cfg = dict(type='TestLoop')
+        split='xsub_val',
+        test_mode=True))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res336_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-2d.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,70 +1,67 @@
-_base_ = ['../../configs/_base_/default_runtime.py']
-
-# model settings
-num_frames = 8
-model = dict(
-    type='Recognizer3D',
-    backbone=dict(
-        type='UniFormerV2',
-        input_resolution=336,
-        patch_size=14,
-        width=1024,
-        layers=24,
-        heads=16,
-        t_size=num_frames,
-        dw_reduction=1.5,
-        backbone_drop_path_rate=0.,
-        temporal_downsample=False,
-        no_lmhra=True,
-        double_lmhra=True,
-        return_list=[20, 21, 22, 23],
-        n_layers=4,
-        n_dim=1024,
-        n_head=16,
-        mlp_factor=4.,
-        drop_path_rate=0.,
-        mlp_dropout=[0.5, 0.5, 0.5, 0.5]),
-    cls_head=dict(
-        type='TimeSformerHead',
-        dropout_ratio=0.5,
-        num_classes=339,
-        in_channels=1024,
-        average_clips='prob'),
-    data_preprocessor=dict(
-        type='ActionDataPreprocessor',
-        mean=[114.75, 114.75, 114.75],
-        std=[57.375, 57.375, 57.375],
-        format_shape='NCTHW'))
-
-# dataset settings
-dataset_type = 'VideoDataset'
-data_root_val = 'data/mit_v1'
-ann_file_test = 'data/mit_v1/val.csv'
+_base_ = 'stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py'
 
+dataset_type = 'PoseDataset'
+ann_file = 'data/skeleton/ntu120_2d.pkl'
+train_pipeline = [
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='UniformSampleFrames', clip_len=100),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
+    dict(type='PackActionInputs')
+]
+val_pipeline = [
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(
+        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
+    dict(type='PackActionInputs')
+]
 test_pipeline = [
-    dict(type='DecordInit'),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
     dict(
-        type='UniformSample', clip_len=num_frames, num_clips=4,
+        type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
-    dict(type='DecordDecode'),
-    dict(type='Resize', scale=(-1, 336)),
-    dict(type='ThreeCrop', crop_size=336),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='PoseDecode'),
+    dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 
+train_dataloader = dict(
+    batch_size=16,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=True),
+    dataset=dict(
+        type='RepeatDataset',
+        times=5,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            pipeline=train_pipeline,
+            split='xsub_train')))
+val_dataloader = dict(
+    batch_size=16,
+    num_workers=2,
+    persistent_workers=True,
+    sampler=dict(type='DefaultSampler', shuffle=False),
+    dataset=dict(
+        type=dataset_type,
+        ann_file=ann_file,
+        pipeline=val_pipeline,
+        split='xsub_val',
+        test_mode=True))
 test_dataloader = dict(
-    batch_size=8,
-    num_workers=8,
+    batch_size=1,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_test,
-        data_prefix=dict(video=data_root_val),
+        ann_file=ann_file,
         pipeline=test_pipeline,
-        test_mode=True,
-        delimiter=' '))
-
-test_evaluator = dict(type='AccMetric')
-test_cfg = dict(type='TestLoop')
+        split='xsub_val',
+        test_mode=True))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/videomae/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomae/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/videomae/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomae/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/x3d/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/x3d/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/x3d/x3d_m_16x5x1_facebook-kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/x3d/x3d_m_16x5x1_facebook-kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition/x3d/x3d_s_13x6x1_facebook-kinetics400-rgb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/x3d/x3d_s_13x6x1_facebook-kinetics400-rgb.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/audioonly/audioonly_r50_8xb160-64x1x1-100e_kinetics400-audio-feature.py` & `mmaction2-1.1.0/mmaction/.mim/configs/localization/tcanet/tcanet_2xb8-700x100-9e_hacs-feature.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,97 +1,131 @@
-_base_ = [
-    '../../_base_/models/audioonly_r50.py', '../../_base_/default_runtime.py'
-]
+_base_ = '../../_base_/default_runtime.py'
+
+# model settings
+model = dict(
+    type='TCANet',
+    feat_dim=700,
+    se_sample_num=32,
+    action_sample_num=64,
+    temporal_dim=100,
+    window_size=9,
+    lgte_num=2,
+    soft_nms_alpha=0.4,
+    soft_nms_low_threshold=0.0,
+    soft_nms_high_threshold=0.0,
+    post_process_top_k=100,
+    feature_extraction_interval=16)
 
 # dataset settings
-dataset_type = 'AudioDataset'
-data_root = 'data/kinetics400/audio_features_train'
-data_root_val = 'data/kinetics400/audio_features_val'
-ann_file_train = 'data/kinetics400/kinetics400_train_list_audio_features.txt'
-ann_file_val = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
-ann_file_test = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
+dataset_type = 'ActivityNetDataset'
+data_root = 'data/HACS/slowonly_feature/'
+data_root_val = 'data/HACS/slowonly_feature/'
+ann_file_train = 'data/HACS/hacs_anno_train.json'
+ann_file_val = 'data/HACS/hacs_anno_val.json'
+ann_file_test = 'data/HACS/hacs_anno_val.json'
+
 train_pipeline = [
-    dict(type='LoadAudioFeature'),
-    dict(type='SampleFrames', clip_len=64, frame_interval=1, num_clips=1),
-    dict(type='AudioFeatureSelector'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
-    dict(type='PackActionInputs')
+    dict(type='LoadLocalizationFeature'),
+    dict(type='GenerateLocalizationLabels'),
+    dict(
+        type='PackLocalizationInputs',
+        keys=('gt_bbox', 'proposals'),
+        meta_keys=('video_name', ))
 ]
+
 val_pipeline = [
-    dict(type='LoadAudioFeature'),
+    dict(type='LoadLocalizationFeature'),
+    dict(type='GenerateLocalizationLabels'),
     dict(
-        type='SampleFrames',
-        clip_len=64,
-        frame_interval=1,
-        num_clips=1,
-        test_mode=True),
-    dict(type='AudioFeatureSelector'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
-    dict(type='PackActionInputs')
+        type='PackLocalizationInputs',
+        keys=('gt_bbox', 'proposals'),
+        meta_keys=(
+            'video_name',
+            'duration_second',
+            'duration_frame',
+            'annotations',
+            'feature_frame',
+        ))
 ]
+
 test_pipeline = [
-    dict(type='LoadAudioFeature'),
+    dict(type='LoadLocalizationFeature'),
     dict(
-        type='SampleFrames',
-        clip_len=64,
-        frame_interval=1,
-        num_clips=10,
-        test_mode=True),
-    dict(type='AudioFeatureSelector'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
-    dict(type='PackActionInputs')
+        type='PackLocalizationInputs',
+        keys=('gt_bbox', 'proposals'),
+        meta_keys=(
+            'video_name',
+            'duration_second',
+            'duration_frame',
+            'annotations',
+            'feature_frame',
+        ))
 ]
 
 train_dataloader = dict(
-    batch_size=160,
-    num_workers=2,
+    batch_size=8,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
+    drop_last=True,
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
-        pipeline=train_pipeline,
-        data_prefix=dict(audio=data_root),
-        suffix='.npy'))
+        data_prefix=dict(video=data_root),
+        pipeline=train_pipeline))
+
 val_dataloader = dict(
-    batch_size=160,
-    num_workers=2,
+    batch_size=1,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
+        data_prefix=dict(video=data_root_val),
         pipeline=val_pipeline,
-        data_prefix=dict(audio=data_root_val),
-        suffix='.npy',
         test_mode=True))
+
 test_dataloader = dict(
     batch_size=1,
-    num_workers=2,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_test,
+        data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
-        data_prefix=dict(audio=data_root_val),
-        suffix='.npy',
         test_mode=True))
 
-val_evaluator = dict(type='AccMetric')
-test_evaluator = val_evaluator
-
+max_epochs = 9
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=100, val_begin=1, val_interval=5)
+    type='EpochBasedTrainLoop',
+    max_epochs=max_epochs,
+    val_begin=1,
+    val_interval=1)
+
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
-param_scheduler = [
-    dict(type='CosineAnnealingLR', eta_min=0, T_max=100, by_epoch=True)
-]
-
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=2.0, momentum=0.9, weight_decay=0.0001),
+    optimizer=dict(type='Adam', lr=0.001, weight_decay=0.0001),
     clip_grad=dict(max_norm=40, norm_type=2))
 
-default_hooks = dict(
-    checkpoint=dict(max_keep_ckpts=3, interval=5), logger=dict(interval=20))
+param_scheduler = [
+    dict(
+        type='MultiStepLR',
+        begin=0,
+        end=max_epochs,
+        by_epoch=True,
+        milestones=[
+            7,
+        ],
+        gamma=0.1)
+]
+
+work_dir = './work_dirs/tcanet_2xb8-2048x100-9e_hacs-feature/'
+test_evaluator = dict(
+    type='ANetMetric',
+    metric_type='AR@AN',
+    dump_config=dict(out=f'{work_dir}/results.json', output_format='json'))
+val_evaluator = test_evaluator
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/resnet/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition_audio/resnet/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio-feature.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-mobilenetv2_8xb16-1x1x8-100e_kinetics400-rgb.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,97 +1,124 @@
 _base_ = [
-    '../../_base_/models/tsn_r18_audio.py', '../../_base_/default_runtime.py'
+    '../../_base_/models/tsm_mobilenet_v2.py',
+    '../../_base_/default_runtime.py'
 ]
 
 # dataset settings
-dataset_type = 'AudioDataset'
-data_root = 'data/kinetics400/audio_features_train'
-data_root_val = 'data/kinetics400/audio_features_val'
-ann_file_train = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
-ann_file_val = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
-ann_file_test = 'data/kinetics400/kinetics400_val_list_audio_features.txt'
+dataset_type = 'VideoDataset'
+data_root = 'data/kinetics400/videos_train'
+data_root_val = 'data/kinetics400/videos_val'
+ann_file_train = 'data/kinetics400/kinetics400_train_list_videos.txt'
+ann_file_val = 'data/kinetics400/kinetics400_val_list_videos.txt'
+
+file_client_args = dict(io_backend='disk')
+
 train_pipeline = [
-    dict(type='LoadAudioFeature'),
-    dict(type='SampleFrames', clip_len=64, frame_interval=1, num_clips=1),
-    dict(type='AudioFeatureSelector'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='DecordInit', **file_client_args),
+    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(
+        type='MultiScaleCrop',
+        input_size=224,
+        scales=(1, 0.875, 0.75, 0.66),
+        random_crop=False,
+        max_wh_scale_gap=1,
+        num_fixed_crops=13),
+    dict(type='Resize', scale=(224, 224), keep_ratio=False),
+    dict(type='Flip', flip_ratio=0.5),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='LoadAudioFeature'),
+    dict(type='DecordInit', **file_client_args),
     dict(
         type='SampleFrames',
-        clip_len=64,
+        clip_len=1,
         frame_interval=1,
-        num_clips=1,
+        num_clips=8,
         test_mode=True),
-    dict(type='AudioFeatureSelector'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='LoadAudioFeature'),
+    dict(type='DecordInit', **file_client_args),
     dict(
         type='SampleFrames',
-        clip_len=64,
+        clip_len=1,
         frame_interval=1,
-        num_clips=10,
+        num_clips=8,
         test_mode=True),
-    dict(type='AudioFeatureSelector'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='ThreeCrop', crop_size=256),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
-    batch_size=320,
-    num_workers=2,
+    batch_size=16,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_train,
-        data_prefix=dict(audio=data_root_val),
-        suffix='.npy',
+        data_prefix=dict(video=data_root),
         pipeline=train_pipeline))
 val_dataloader = dict(
-    batch_size=320,
-    num_workers=2,
+    batch_size=16,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file_val,
+        data_prefix=dict(video=data_root_val),
         pipeline=val_pipeline,
-        data_prefix=dict(audio=data_root_val),
-        suffix='.npy',
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
-    num_workers=2,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_test,
+        ann_file=ann_file_val,
+        data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
-        data_prefix=dict(audio=data_root_val),
-        suffix='.npy',
         test_mode=True))
 
 val_evaluator = dict(type='AccMetric')
 test_evaluator = val_evaluator
 
+default_hooks = dict(checkpoint=dict(interval=3, max_keep_ckpts=3))
+
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=100, val_begin=1, val_interval=5)
+    type='EpochBasedTrainLoop', max_epochs=100, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
-    dict(type='CosineAnnealingLR', eta_min=0, T_max=100, by_epoch=True)
+    dict(
+        type='MultiStepLR',
+        begin=0,
+        end=100,
+        by_epoch=True,
+        milestones=[40, 80],
+        gamma=0.1)
 ]
 
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001),
-    clip_grad=dict(max_norm=40, norm_type=2))
-
-default_hooks = dict(
-    checkpoint=dict(max_keep_ckpts=3, interval=5), logger=dict(interval=20))
+    constructor='TSMOptimWrapperConstructor',
+    paramwise_cfg=dict(fc_lr5=True),
+    optimizer=dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.00002),
+    clip_grad=dict(max_norm=20, norm_type=2))
+
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=True, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio.py` & `mmaction2-1.1.0/mmaction/.mim/configs/retrieval/clip4clip/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,100 +1,122 @@
-_base_ = [
-    '../../_base_/models/tsn_r18_audio.py', '../../_base_/default_runtime.py'
-]
+_base_ = '../../_base_/default_runtime.py'
 
-# dataset settings
-dataset_type = 'AudioDataset'
-data_root = 'data/kinetics400/audios_train'
-data_root_val = 'data/kinetics400/audios_val'
-ann_file_train = 'data/kinetics400/kinetics400_train_list_audios.txt'
-ann_file_val = 'data/kinetics400/kinetics400_val_list_audios.txt'
-ann_file_test = 'data/kinetics400/kinetics400_val_list_audios.txt'
+model = dict(
+    type='CLIPSimilarity',
+    clip_arch='ViT-B/32',
+    to_float32=True,
+    frozen_layers=0,
+    data_preprocessor=dict(
+        type='MultiModalDataPreprocessor',
+        preprocessors=dict(
+            imgs=dict(
+                type='ActionDataPreprocessor',
+                mean=[122.771, 116.746, 104.093],
+                std=[68.500, 66.632, 70.323],
+                format_shape='NCHW'),
+            text=dict(type='ActionDataPreprocessor', to_float32=False))),
+    adapter=dict(type='SimpleMeanAdapter'))
+
+dataset_type = 'VideoTextDataset'
+data_root = 'data/video_retrieval/msrvtt'
+file_client_args = dict(io_backend='disk')
 train_pipeline = [
-    dict(type='AudioDecodeInit'),
-    dict(type='SampleFrames', clip_len=64, frame_interval=1, num_clips=1),
-    dict(type='AudioDecode'),
-    dict(type='AudioAmplify', ratio=1.5),
-    dict(type='MelSpectrogram'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
-    dict(type='PackActionInputs')
+    dict(type='DecordInit', **file_client_args),
+    dict(type='UniformSample', clip_len=12, num_clips=1),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='CLIPTokenize'),
+    dict(type='PackActionInputs', collect_keys=('imgs', 'text'))
 ]
 val_pipeline = [
-    dict(type='AudioDecodeInit'),
-    dict(
-        type='SampleFrames',
-        clip_len=64,
-        frame_interval=1,
-        num_clips=1,
-        test_mode=True),
-    dict(type='AudioDecode'),
-    dict(type='AudioAmplify', ratio=1.5),
-    dict(type='MelSpectrogram'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
-    dict(type='PackActionInputs')
-]
-test_pipeline = [
-    dict(type='AudioDecodeInit'),
-    dict(
-        type='SampleFrames',
-        clip_len=64,
-        frame_interval=1,
-        num_clips=10,
-        test_mode=True),
-    dict(type='AudioDecode'),
-    dict(type='AudioAmplify', ratio=1.5),
-    dict(type='MelSpectrogram'),
-    dict(type='FormatAudioShape', input_format='NCTF'),
-    dict(type='PackActionInputs')
+    dict(type='DecordInit', **file_client_args),
+    dict(type='UniformSample', clip_len=12, num_clips=1, test_mode=True),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCHW'),
+    dict(type='CLIPTokenize'),
+    dict(type='PackActionInputs', collect_keys=('imgs', 'text'))
 ]
+test_pipeline = val_pipeline
 
 train_dataloader = dict(
-    batch_size=320,
-    num_workers=2,
+    batch_size=16,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_train,
-        data_prefix=dict(audio=data_root),
+        ann_file='train_9k.json',
+        data_root=data_root,
+        data_prefix=dict(video='videos'),
         pipeline=train_pipeline))
 val_dataloader = dict(
-    batch_size=320,
-    num_workers=2,
+    batch_size=16,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file='test_JSFUSION.json',
+        data_root=data_root,
+        data_prefix=dict(video='videos'),
         pipeline=val_pipeline,
-        data_prefix=dict(audio=data_root_val),
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
-    num_workers=2,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_test,
+        ann_file='test_JSFUSION.json',
+        data_root=data_root,
+        data_prefix=dict(video='videos'),
         pipeline=test_pipeline,
-        data_prefix=dict(audio=data_root_val),
         test_mode=True))
 
-val_evaluator = dict(type='AccMetric')
+val_evaluator = dict(type='RetrievalMetric')
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=100, val_begin=1, val_interval=5)
+    type='EpochBasedTrainLoop', max_epochs=5, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
-    dict(type='CosineAnnealingLR', eta_min=0, T_max=100, by_epoch=True)
+    dict(
+        type='LinearLR',
+        start_factor=0.05,
+        by_epoch=True,
+        begin=0,
+        end=0.5,
+        convert_to_iter_based=True),
+    dict(
+        type='CosineAnnealingLR',
+        T_max=4.5,
+        eta_min=0,
+        by_epoch=True,
+        begin=0.5,
+        end=5,
+        convert_to_iter_based=True)
 ]
 
 optim_wrapper = dict(
-    optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001),
-    clip_grad=dict(max_norm=40, norm_type=2))
-
-default_hooks = dict(
-    checkpoint=dict(max_keep_ckpts=3, interval=5), logger=dict(interval=20))
+    optimizer=dict(
+        type='AdamW',
+        lr=1e-05,
+        betas=(0.9, 0.999),
+        eps=1e-08,
+        weight_decay=0.05),
+    paramwise_cfg=dict(norm_decay_mult=0., bias_decay_mult=0.),
+)
+
+default_hooks = dict(checkpoint=dict(save_best=None))
+
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-_base_ = '2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
+_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
 
 dataset_type = 'PoseDataset'
 ann_file = 'data/skeleton/ntu60_2d.pkl'
 train_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['bm']),
     dict(type='UniformSampleFrames', clip_len=100),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['bm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['bm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/2s-agcn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/2s-agcn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/metafile.yml`

 * *Files 4% similar despite different names*

```diff
@@ -9,53 +9,56 @@
   - Name: slowonly_r50_8xb16-u48-240e_gym-keypoint
     Config: configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint.py
     In Collection: PoseC3D
     Metadata:
       Architecture: SlowOnly-R50
       Batch Size: 16
       Epochs: 240
-      Parameters: 2044867
+      FLOPs: 20.6G
+      Parameters: 2.0M
       Training Data: FineGYM
       Training Resources: 8 GPUs
       pseudo heatmap: keypoint
     Results:
     - Dataset: FineGYM
       Task: Skeleton-based Action Recognition
       Metrics:
-        mean Top 1 Accuracy: 93.4
+        mean Top 1 Accuracy: 93.5
     Training Log: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint/slowonly_r50_8xb16-u48-240e_gym-keypoint.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint/slowonly_r50_8xb16-u48-240e_gym-keypoint_20220815-da338c58.pth
 
   - Name: slowonly_r50_8xb16-u48-240e_gym-limb
     Config: configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb.py
     In Collection: PoseC3D
     Metadata:
       Architecture: SlowOnly-R50
       Batch Size: 16
       Epochs: 240
-      Parameters: 2044867
+      FLOPs: 20.6G
+      Parameters: 2.0M
       Training Data: FineGYM
       Training Resources: 8 GPUs
       pseudo heatmap: limb
     Results:
     - Dataset: FineGYM
       Task: Skeleton-based Action Recognition
       Metrics:
-        mean Top 1 Accuracy: 93.7
+        mean Top 1 Accuracy: 93.6
     Training Log: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb/slowonly_r50_8xb16-u48-240e_gym-limb.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb/slowonly_r50_8xb16-u48-240e_gym-limb_20220815-2e6e3c5c.pth
 
   - Name: slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint
     Config: configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py
     In Collection: PoseC3D
     Metadata:
       Architecture: SlowOnly-R50
       Batch Size: 16
       Epochs: 240
-      Parameters: 2024860
+      FLOPs: 20.6G
+      Parameters: 2.0M
       Training Data: NTU60-XSub
       Training Resources: 8 GPUs
       pseudo heatmap: keypoint
     Results:
     - Dataset: NTU60-XSub
       Task: Skeleton-based Action Recognition
       Metrics:
@@ -66,15 +69,16 @@
   - Name: slowonly_r50_8xb16-u48-240e_ntu60-xsub-limb
     Config: configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-limb.py
     In Collection: PoseC3D
     Metadata:
       Architecture: SlowOnly-R50
       Batch Size: 16
       Epochs: 240
-      Parameters: 2024860
+      FLOPs: 20.6G
+      Parameters: 2.0M
       Training Data: NTU60-XSub
       Training Resources: 8 GPUs
       pseudo heatmap: limb
     Results:
     - Dataset: NTU60-XSub
       Task: Skeleton-based Action Recognition
       Metrics:
@@ -85,37 +89,39 @@
   - Name: slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint
     Config: configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint.py
     In Collection: PoseC3D
     Metadata:
       Architecture: SlowOnly-R50
       Batch Size: 16
       Epochs: 120
-      Parameters: 3029984
+      FLOPs: 14.6G
+      Parameters: 3.0M
       Training Data: HMDB51
       Training Resources: 8 GPUs
       pseudo heatmap: keypoint
     Results:
     - Dataset: HMDB51
       Task: Skeleton-based Action Recognition
       Metrics:
-        Top 1 Accuracy: 69.2
+        Top 1 Accuracy: 69.6
     Training Log: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint_20220815-17eaa484.pth
 
   - Name: slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint
     Config: configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint.py
     In Collection: PoseC3D
     Metadata:
       Architecture: SlowOnly-R50
       Batch Size: 16
       Epochs: 120
-      Parameters: 3055584
+      FLOPs: 14.6G
+      Parameters: 3.1M
       Training Data: UCF101
       Training Resources: 8 GPUs
       pseudo heatmap: keypoint
     Results:
     - Dataset: UCF101
       Task: Skeleton-based Action Recognition
       Metrics:
-        Top 1 Accuracy: 86.9
+        Top 1 Accuracy: 86.8
     Training Log: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint.log
     Weights: https://download.openmmlab.com/mmaction/v1.0/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint_20220815-9972260d.pth
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,15 +24,15 @@
         spatial_type='avg',
         dropout_ratio=0.5,
         average_clips='prob'),
     train_cfg=None,
     test_cfg=None)
 
 dataset_type = 'PoseDataset'
-ann_file = 'data/posec3d/hmdb51.pkl'
+ann_file = 'data/skeleton/hmdb51_2d.pkl'
 left_kp = [1, 3, 5, 7, 9, 11, 13, 15]
 right_kp = [2, 4, 6, 8, 10, 12, 14, 16]
 train_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
@@ -41,30 +41,30 @@
     dict(type='Flip', flip_ratio=0.5, left_kp=left_kp, right_kp=right_kp),
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 56)),
     dict(type='CenterCrop', crop_size=56),
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(
         type='UniformSampleFrames', clip_len=48, num_clips=10, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
@@ -75,15 +75,15 @@
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False,
         double=True,
         left_kp=left_kp,
         right_kp=right_kp),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,15 +24,15 @@
         spatial_type='avg',
         dropout_ratio=0.5,
         average_clips='prob'),
     train_cfg=None,
     test_cfg=None)
 
 dataset_type = 'PoseDataset'
-ann_file = 'data/posec3d/ucf101.pkl'
+ann_file = 'data/skeleton/ucf101_2d.pkl'
 left_kp = [1, 3, 5, 7, 9, 11, 13, 15]
 right_kp = [2, 4, 6, 8, 10, 12, 14, 16]
 train_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
@@ -41,30 +41,30 @@
     dict(type='Flip', flip_ratio=0.5, left_kp=left_kp, right_kp=right_kp),
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 56)),
     dict(type='CenterCrop', crop_size=56),
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(
         type='UniformSampleFrames', clip_len=48, num_clips=10, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
@@ -75,15 +75,15 @@
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False,
         double=True,
         left_kp=left_kp,
         right_kp=right_kp),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,21 +19,18 @@
         dilations=(1, 1, 1)),
     cls_head=dict(
         type='I3DHead',
         in_channels=512,
         num_classes=99,
         spatial_type='avg',
         dropout_ratio=0.5,
-        average_clips='prob'),
-    train_cfg=None,
-    test_cfg=None)
+        average_clips='prob'))
 
 dataset_type = 'PoseDataset'
-ann_file_train = 'data/posec3d/gym_train.pkl'
-ann_file_val = 'data/posec3d/gym_val.pkl'
+ann_file = 'data/skeleton/gym_2d.pkl'
 left_kp = [1, 3, 5, 7, 9, 11, 13, 15]
 right_kp = [2, 4, 6, 8, 10, 12, 14, 16]
 train_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
@@ -42,30 +39,30 @@
     dict(type='Flip', flip_ratio=0.5, left_kp=left_kp, right_kp=right_kp),
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
     dict(type='CenterCrop', crop_size=64),
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(
         type='UniformSampleFrames', clip_len=48, num_clips=10, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
@@ -76,61 +73,67 @@
         sigma=0.6,
         use_score=True,
         with_kp=True,
         with_limb=False,
         double=True,
         left_kp=left_kp,
         right_kp=right_kp),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type, ann_file=ann_file_train, pipeline=train_pipeline))
+        type='RepeatDataset',
+        times=10,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            split='train',
+            pipeline=train_pipeline)))
 val_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='val',
         pipeline=val_pipeline,
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='val',
         pipeline=test_pipeline,
         test_mode=True))
 
 val_evaluator = dict(type='AccMetric')
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=240, val_begin=1, val_interval=10)
+    type='EpochBasedTrainLoop', max_epochs=24, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
     dict(
         type='CosineAnnealingLR',
         eta_min=0,
-        T_max=240,
+        T_max=24,
         by_epoch=True,
         convert_to_iter_based=True)
 ]
 
 optim_wrapper = dict(
     optimizer=dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.0003),
     clip_grad=dict(max_norm=40, norm_type=2))
-
-default_hooks = dict(checkpoint=dict(interval=10, max_keep_ckpts=3))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-limb.py`

 * *Files 6% similar despite different names*

```diff
@@ -16,29 +16,27 @@
         inflate=(0, 1, 1),
         spatial_strides=(2, 2, 2),
         temporal_strides=(1, 1, 2),
         dilations=(1, 1, 1)),
     cls_head=dict(
         type='I3DHead',
         in_channels=512,
-        num_classes=99,
-        spatial_type='avg',
+        num_classes=60,
         dropout_ratio=0.5,
-        average_clips='prob'),
-    train_cfg=None,
-    test_cfg=None)
+        average_clips='prob'))
 
 dataset_type = 'PoseDataset'
-ann_file_train = 'data/posec3d/gym_train.pkl'
-ann_file_val = 'data/posec3d/gym_val.pkl'
+ann_file = 'data/skeleton/ntu60_2d.pkl'
 left_kp = [1, 3, 5, 7, 9, 11, 13, 15]
 right_kp = [2, 4, 6, 8, 10, 12, 14, 16]
 skeletons = [[0, 5], [0, 6], [5, 7], [7, 9], [6, 8], [8, 10], [5, 11],
              [11, 13], [13, 15], [6, 12], [12, 14], [14, 16], [0, 1], [0, 2],
              [1, 3], [2, 4], [11, 12]]
+left_limb = [0, 2, 3, 6, 7, 8, 12, 14]
+right_limb = [1, 4, 5, 9, 10, 11, 13, 15]
 train_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
     dict(type='RandomResizedCrop', area_range=(0.56, 1.0)),
     dict(type='Resize', scale=(56, 56), keep_ratio=False),
@@ -46,15 +44,15 @@
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=False,
         with_limb=True,
         skeletons=skeletons),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
@@ -62,15 +60,15 @@
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=False,
         with_limb=True,
         skeletons=skeletons),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(
         type='UniformSampleFrames', clip_len=48, num_clips=10, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
@@ -80,63 +78,69 @@
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=False,
         with_limb=True,
         skeletons=skeletons,
         double=True,
-        left_kp=left_kp,
-        right_kp=right_kp),
-    dict(type='FormatShape', input_format='NCTHW'),
+        left_limb=left_limb,
+        right_limb=right_limb),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type, ann_file=ann_file_train, pipeline=train_pipeline))
+        type='RepeatDataset',
+        times=10,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            split='xsub_train',
+            pipeline=train_pipeline)))
 val_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='xsub_val',
         pipeline=val_pipeline,
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='xsub_val',
         pipeline=test_pipeline,
         test_mode=True))
 
-val_evaluator = dict(type='AccMetric')
+val_evaluator = [dict(type='AccMetric')]
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=240, val_begin=1, val_interval=10)
+    type='EpochBasedTrainLoop', max_epochs=24, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
     dict(
         type='CosineAnnealingLR',
         eta_min=0,
-        T_max=240,
+        T_max=24,
         by_epoch=True,
         convert_to_iter_based=True)
 ]
 
 optim_wrapper = dict(
     optimizer=dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.0003),
     clip_grad=dict(max_norm=40, norm_type=2))
-
-default_hooks = dict(checkpoint=dict(interval=10, max_keep_ckpts=3))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/pose_only.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,136 +1,127 @@
-_base_ = '../../_base_/default_runtime.py'
+_base_ = '../../../_base_/default_runtime.py'
 
 model = dict(
     type='Recognizer3D',
     backbone=dict(
         type='ResNet3dSlowOnly',
-        depth=50,
-        pretrained=None,
         in_channels=17,
         base_channels=32,
         num_stages=3,
         out_indices=(2, ),
         stage_blocks=(4, 6, 3),
         conv1_stride_s=1,
         pool1_stride_s=1,
         inflate=(0, 1, 1),
         spatial_strides=(2, 2, 2),
-        temporal_strides=(1, 1, 2),
+        temporal_strides=(1, 1, 1),
         dilations=(1, 1, 1)),
     cls_head=dict(
         type='I3DHead',
         in_channels=512,
         num_classes=60,
-        spatial_type='avg',
         dropout_ratio=0.5,
-        average_clips='prob'),
-    train_cfg=None,
-    test_cfg=None)
+        average_clips='prob'))
 
 dataset_type = 'PoseDataset'
-ann_file_train = 'data/posec3d/ntu60_xsub_train.pkl'
-ann_file_val = 'data/posec3d/ntu60_xsub_val.pkl'
+ann_file = 'data/skeleton/ntu60_2d.pkl'
 left_kp = [1, 3, 5, 7, 9, 11, 13, 15]
 right_kp = [2, 4, 6, 8, 10, 12, 14, 16]
 train_pipeline = [
-    dict(type='UniformSampleFrames', clip_len=48),
+    dict(type='UniformSampleFrames', clip_len=32),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
-    dict(type='Resize', scale=(-1, 64)),
+    dict(type='Resize', scale=(64, 64), keep_ratio=False),
     dict(type='RandomResizedCrop', area_range=(0.56, 1.0)),
     dict(type='Resize', scale=(56, 56), keep_ratio=False),
     dict(type='Flip', flip_ratio=0.5, left_kp=left_kp, right_kp=right_kp),
-    dict(
-        type='GeneratePoseTarget',
-        sigma=0.6,
-        use_score=True,
-        with_kp=True,
-        with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='GeneratePoseTarget', with_kp=True, with_limb=False),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='UniformSampleFrames', clip_len=48, num_clips=1, test_mode=True),
+    dict(type='UniformSampleFrames', clip_len=32, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
-    dict(type='Resize', scale=(-1, 64)),
-    dict(type='CenterCrop', crop_size=64),
-    dict(
-        type='GeneratePoseTarget',
-        sigma=0.6,
-        use_score=True,
-        with_kp=True,
-        with_limb=False),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='Resize', scale=(64, 64), keep_ratio=False),
+    dict(type='GeneratePoseTarget', with_kp=True, with_limb=False),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(
-        type='UniformSampleFrames', clip_len=48, num_clips=10, test_mode=True),
+        type='UniformSampleFrames', clip_len=32, num_clips=10, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
-    dict(type='Resize', scale=(-1, 64)),
-    dict(type='CenterCrop', crop_size=64),
+    dict(type='Resize', scale=(64, 64), keep_ratio=False),
     dict(
         type='GeneratePoseTarget',
-        sigma=0.6,
-        use_score=True,
         with_kp=True,
         with_limb=False,
-        double=True,
         left_kp=left_kp,
         right_kp=right_kp),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type, ann_file=ann_file_train, pipeline=train_pipeline))
+        type='RepeatDataset',
+        times=10,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            split='xsub_train',
+            pipeline=train_pipeline)))
 val_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='xsub_val',
         pipeline=val_pipeline,
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='xsub_val',
         pipeline=test_pipeline,
         test_mode=True))
 
-val_evaluator = dict(type='AccMetric')
+val_evaluator = [dict(type='AccMetric')]
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=240, val_begin=1, val_interval=10)
+    type='EpochBasedTrainLoop', max_epochs=18, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
     dict(
         type='CosineAnnealingLR',
         eta_min=0,
-        T_max=240,
+        T_max=18,
         by_epoch=True,
         convert_to_iter_based=True)
 ]
 
 optim_wrapper = dict(
     optimizer=dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.0003),
     clip_grad=dict(max_norm=40, norm_type=2))
 
-default_hooks = dict(checkpoint=dict(max_keep_ckpts=3))
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-limb.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb.py`

 * *Files 5% similar despite different names*

```diff
@@ -16,29 +16,28 @@
         inflate=(0, 1, 1),
         spatial_strides=(2, 2, 2),
         temporal_strides=(1, 1, 2),
         dilations=(1, 1, 1)),
     cls_head=dict(
         type='I3DHead',
         in_channels=512,
-        num_classes=60,
+        num_classes=99,
         spatial_type='avg',
         dropout_ratio=0.5,
-        average_clips='prob'),
-    train_cfg=None,
-    test_cfg=None)
+        average_clips='prob'))
 
 dataset_type = 'PoseDataset'
-ann_file_train = 'data/posec3d/ntu60_xsub_train.pkl'
-ann_file_val = 'data/posec3d/ntu60_xsub_val.pkl'
+ann_file = 'data/skeleton/gym_2d.pkl'
 left_kp = [1, 3, 5, 7, 9, 11, 13, 15]
 right_kp = [2, 4, 6, 8, 10, 12, 14, 16]
 skeletons = [[0, 5], [0, 6], [5, 7], [7, 9], [6, 8], [8, 10], [5, 11],
              [11, 13], [13, 15], [6, 12], [12, 14], [14, 16], [0, 1], [0, 2],
              [1, 3], [2, 4], [11, 12]]
+left_limb = [0, 2, 3, 6, 7, 8, 12, 14]
+right_limb = [1, 4, 5, 9, 10, 11, 13, 15]
 train_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
     dict(type='RandomResizedCrop', area_range=(0.56, 1.0)),
     dict(type='Resize', scale=(56, 56), keep_ratio=False),
@@ -46,15 +45,15 @@
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=False,
         with_limb=True,
         skeletons=skeletons),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='UniformSampleFrames', clip_len=48, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
     dict(type='Resize', scale=(-1, 64)),
@@ -62,15 +61,15 @@
     dict(
         type='GeneratePoseTarget',
         sigma=0.6,
         use_score=True,
         with_kp=False,
         with_limb=True,
         skeletons=skeletons),
-    dict(type='FormatShape', input_format='NCTHW'),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(
         type='UniformSampleFrames', clip_len=48, num_clips=10, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='PoseCompact', hw_ratio=1., allow_imgpad=True),
@@ -81,62 +80,70 @@
         sigma=0.6,
         use_score=True,
         with_kp=False,
         with_limb=True,
         skeletons=skeletons,
         double=True,
         left_kp=left_kp,
-        right_kp=right_kp),
-    dict(type='FormatShape', input_format='NCTHW'),
+        right_kp=right_kp,
+        left_limb=left_limb,
+        right_limb=right_limb),
+    dict(type='FormatShape', input_format='NCTHW_Heatmap'),
     dict(type='PackActionInputs')
 ]
 
 train_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type, ann_file=ann_file_train, pipeline=train_pipeline))
+        type='RepeatDataset',
+        times=10,
+        dataset=dict(
+            type=dataset_type,
+            ann_file=ann_file,
+            split='train',
+            pipeline=train_pipeline))),
 val_dataloader = dict(
     batch_size=16,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='val',
         pipeline=val_pipeline,
         test_mode=True))
 test_dataloader = dict(
     batch_size=1,
     num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file_val,
+        ann_file=ann_file,
+        split='val',
         pipeline=test_pipeline,
         test_mode=True))
 
 val_evaluator = dict(type='AccMetric')
 test_evaluator = val_evaluator
 
 train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=240, val_begin=1, val_interval=10)
+    type='EpochBasedTrainLoop', max_epochs=24, val_begin=1, val_interval=1)
 val_cfg = dict(type='ValLoop')
 test_cfg = dict(type='TestLoop')
 
 param_scheduler = [
     dict(
         type='CosineAnnealingLR',
         eta_min=0,
-        T_max=240,
+        T_max=24,
         by_epoch=True,
         convert_to_iter_based=True)
 ]
 
 optim_wrapper = dict(
     optimizer=dict(type='SGD', lr=0.2, momentum=0.9, weight_decay=0.0003),
     clip_grad=dict(max_norm=40, norm_type=2))
-
-default_hooks = dict(checkpoint=dict(max_keep_ckpts=3))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-_base_ = 'stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py'
+_base_ = 'stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
 
 dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu120_2d.pkl'
+ann_file = 'data/skeleton/ntu60_2d.pkl'
 train_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
     dict(type='UniformSampleFrames', clip_len=100),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-_base_ = 'stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
+_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
 
 dataset_type = 'PoseDataset'
 ann_file = 'data/skeleton/ntu60_2d.pkl'
 train_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
     dict(type='UniformSampleFrames', clip_len=100),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
     dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['b']),
+    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-_base_ = 'stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py'
+_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py'
 
 dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu120_2d.pkl'
+ann_file = 'data/skeleton/ntu60_3d.pkl'
 train_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['b']),
     dict(type='UniformSampleFrames', clip_len=100),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['b']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['b']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu120-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-_base_ = 'stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
+_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py'
 
 dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu60_2d.pkl'
+ann_file = 'data/skeleton/ntu60_3d.pkl'
 train_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['bm']),
     dict(type='UniformSampleFrames', clip_len=100),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['bm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['jm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['bm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu120-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/metafile.yml` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/metafile.yml`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,31 +1,31 @@
-_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py'
+_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py'
 
 dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu60_2d.pkl'
+ann_file = 'data/skeleton/ntu60_3d.pkl'
 train_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['bm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['jm']),
     dict(type='UniformSampleFrames', clip_len=100),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['bm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['jm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['bm']),
+    dict(type='PreNormalize3D'),
+    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['jm']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x16-50e_sthv2-rgb.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,67 +1,56 @@
-_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py'
+_base_ = ['tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py']
 
-dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu60_3d.pkl'
+file_client_args = dict(io_backend='disk')
+
+sthv2_flip_label_map = {86: 87, 87: 86, 93: 94, 94: 93, 166: 167, 167: 166}
 train_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['bm']),
-    dict(type='UniformSampleFrames', clip_len=100),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
+    dict(type='DecordInit', **file_client_args),
+    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=16),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(
+        type='MultiScaleCrop',
+        input_size=224,
+        scales=(1, 0.875, 0.75, 0.66),
+        random_crop=False,
+        max_wh_scale_gap=1,
+        num_fixed_crops=13),
+    dict(type='Resize', scale=(224, 224), keep_ratio=False),
+    dict(type='Flip', flip_ratio=0.5, flip_label_map=sthv2_flip_label_map),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['bm']),
+    dict(type='DecordInit', **file_client_args),
     dict(
-        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
+        type='SampleFrames',
+        clip_len=1,
+        frame_interval=1,
+        num_clips=16,
+        test_mode=True),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='CenterCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['bm']),
+    dict(type='DecordInit', **file_client_args),
     dict(
-        type='UniformSampleFrames', clip_len=100, num_clips=10,
+        type='SampleFrames',
+        clip_len=1,
+        frame_interval=1,
+        num_clips=25,
         test_mode=True),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='TenCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCHW'),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(
-    batch_size=16,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type='RepeatDataset',
-        times=5,
-        dataset=dict(
-            type=dataset_type,
-            ann_file=ann_file,
-            pipeline=train_pipeline,
-            split='xsub_train')))
-val_dataloader = dict(
-    batch_size=16,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file,
-        pipeline=val_pipeline,
-        split='xsub_val',
-        test_mode=True))
-test_dataloader = dict(
-    batch_size=1,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file,
-        pipeline=test_pipeline,
-        split='xsub_val',
-        test_mode=True))
+train_dataloader = dict(dataset=dict(pipeline=train_pipeline))
+
+val_dataloader = dict(dataset=dict(pipeline=val_pipeline))
+
+test_dataloader = dict(dataset=dict(pipeline=test_pipeline, test_mode=True))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,31 +1,41 @@
-_base_ = 'stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py'
+_base_ = '../../_base_/default_runtime.py'
+
+model = dict(
+    type='RecognizerGCN',
+    backbone=dict(
+        type='STGCN',
+        gcn_adaptive='init',
+        gcn_with_res=True,
+        tcn_type='mstcn',
+        graph_cfg=dict(layout='coco', mode='spatial')),
+    cls_head=dict(type='GCNHead', num_classes=60, in_channels=256))
 
 dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu60_3d.pkl'
+ann_file = 'data/skeleton/ntu60_2d.pkl'
 train_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['jm']),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['j']),
     dict(type='UniformSampleFrames', clip_len=100),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 val_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['jm']),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['j']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
 test_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['jm']),
+    dict(type='PreNormalize2D'),
+    dict(type='GenSkeFeat', dataset='coco', feats=['j']),
     dict(
         type='UniformSampleFrames', clip_len=100, num_clips=10,
         test_mode=True),
     dict(type='PoseDecode'),
     dict(type='FormatGCNInput', num_person=2),
     dict(type='PackActionInputs')
 ]
@@ -61,7 +71,36 @@
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
         ann_file=ann_file,
         pipeline=test_pipeline,
         split='xsub_val',
         test_mode=True))
+
+val_evaluator = [dict(type='AccMetric')]
+test_evaluator = val_evaluator
+
+train_cfg = dict(
+    type='EpochBasedTrainLoop', max_epochs=16, val_begin=1, val_interval=1)
+val_cfg = dict(type='ValLoop')
+test_cfg = dict(type='TestLoop')
+
+param_scheduler = [
+    dict(
+        type='CosineAnnealingLR',
+        eta_min=0,
+        T_max=16,
+        by_epoch=True,
+        convert_to_iter_based=True)
+]
+
+optim_wrapper = dict(
+    optimizer=dict(
+        type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0005, nesterov=True))
+
+default_hooks = dict(checkpoint=dict(interval=1), logger=dict(interval=100))
+
+# Default setting for scaling LR automatically
+#   - `enable` means enable scaling LR automatically
+#       or not by default.
+#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
+auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/hacs/slowonly_feature_infer.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,106 +1,77 @@
-_base_ = '../../_base_/default_runtime.py'
+# Copyright (c) OpenMMLab. All rights reserved.
+default_hooks = dict(
+    runtime_info=dict(type='RuntimeInfoHook'),
+    timer=dict(type='IterTimerHook'),
+    logger=dict(type='LoggerHook', interval=20, ignore_last=False),
+    param_scheduler=dict(type='ParamSchedulerHook'),
+    checkpoint=dict(type='CheckpointHook', interval=4, save_best='auto'),
+    sampler_seed=dict(type='DistSamplerSeedHook'),
+    sync_buffers=dict(type='SyncBuffersHook'))
+
+env_cfg = dict(
+    cudnn_benchmark=False,
+    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
+    dist_cfg=dict(backend='nccl'))
+
+log_processor = dict(type='LogProcessor', window_size=20, by_epoch=True)
+vis_backends = [dict(type='LocalVisBackend')]
+visualizer = dict(
+    type='ActionVisualizer', vis_backends=[dict(type='LocalVisBackend')])
+log_level = 'INFO'
 
 model = dict(
-    type='RecognizerGCN',
+    type='Recognizer3D',
     backbone=dict(
-        type='STGCN',
-        gcn_adaptive='init',
-        gcn_with_res=True,
-        tcn_type='mstcn',
-        graph_cfg=dict(layout='coco', mode='spatial')),
-    cls_head=dict(type='GCNHead', num_classes=60, in_channels=256))
-
-dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu60_2d.pkl'
-train_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['j']),
-    dict(type='UniformSampleFrames', clip_len=100),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
-    dict(type='PackActionInputs')
-]
-val_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['j']),
-    dict(
-        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
-    dict(type='PackActionInputs')
-]
+        type='ResNet3dSlowOnly',
+        depth=50,
+        lateral=False,
+        conv1_kernel=(1, 7, 7),
+        conv1_stride_t=1,
+        pool1_stride_t=1,
+        inflate=(0, 0, 1, 1),
+        norm_eval=False),
+    cls_head=dict(
+        type='I3DHead',
+        in_channels=2048,
+        num_classes=700,
+        spatial_type='avg',
+        dropout_ratio=0.5,
+        average_clips=None),
+    data_preprocessor=dict(
+        type='ActionDataPreprocessor',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        format_shape='NCTHW'))
+
+data_root = './data'
+ann_file = 'hacs_data.txt'
+
 test_pipeline = [
-    dict(type='PreNormalize2D'),
-    dict(type='GenSkeFeat', dataset='coco', feats=['j']),
+    dict(type='DecordInit', io_backend='disk'),
     dict(
-        type='UniformSampleFrames', clip_len=100, num_clips=10,
+        type='SampleFrames',
+        clip_len=8,
+        frame_interval=8,
+        num_clips=100,
         test_mode=True),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 256)),
+    dict(type='CenterCrop', crop_size=256),
+    dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(
-    batch_size=16,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type='RepeatDataset',
-        times=5,
-        dataset=dict(
-            type=dataset_type,
-            ann_file=ann_file,
-            pipeline=train_pipeline,
-            split='xsub_train')))
-val_dataloader = dict(
-    batch_size=16,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file,
-        pipeline=val_pipeline,
-        split='xsub_val',
-        test_mode=True))
 test_dataloader = dict(
     batch_size=1,
-    num_workers=2,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
-        type=dataset_type,
+        type='VideoDataset',
         ann_file=ann_file,
+        data_prefix=dict(video=data_root),
         pipeline=test_pipeline,
-        split='xsub_val',
         test_mode=True))
 
-val_evaluator = [dict(type='AccMetric')]
-test_evaluator = val_evaluator
-
-train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=16, val_begin=1, val_interval=1)
-val_cfg = dict(type='ValLoop')
+test_evaluator = dict(type='DumpResults', out_file_path='result.pkl')
 test_cfg = dict(type='TestLoop')
-
-param_scheduler = [
-    dict(
-        type='CosineAnnealingLR',
-        eta_min=0,
-        T_max=16,
-        by_epoch=True,
-        convert_to_iter_based=True)
-]
-
-optim_wrapper = dict(
-    optimizer=dict(
-        type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0005, nesterov=True))
-
-default_hooks = dict(checkpoint=dict(interval=1), logger=dict(interval=100))
-
-# Default setting for scaling LR automatically
-#   - `enable` means enable scaling LR automatically
-#       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/configs/skeleton/stgcnpp/stgcnpp_8xb16-joint-u100-80e_ntu60-xsub-keypoint-3d.py` & `mmaction2-1.1.0/mmaction/.mim/configs/recognition/videomaev2/vit-base-p16_videomaev2-vit-g-dist-k710-pre_16x4x1_kinetics-400.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,106 +1,61 @@
-_base_ = '../../_base_/default_runtime.py'
+_base_ = ['../../_base_/default_runtime.py']
 
+# model settings
 model = dict(
-    type='RecognizerGCN',
+    type='Recognizer3D',
     backbone=dict(
-        type='STGCN',
-        gcn_adaptive='init',
-        gcn_with_res=True,
-        tcn_type='mstcn',
-        graph_cfg=dict(layout='nturgb+d', mode='spatial')),
-    cls_head=dict(type='GCNHead', num_classes=60, in_channels=256))
+        type='VisionTransformer',
+        img_size=224,
+        patch_size=16,
+        embed_dims=768,
+        depth=12,
+        num_heads=12,
+        mlp_ratio=4,
+        qkv_bias=True,
+        num_frames=16,
+        norm_cfg=dict(type='LN', eps=1e-6)),
+    cls_head=dict(
+        type='TimeSformerHead',
+        num_classes=400,
+        in_channels=768,
+        average_clips='prob'),
+    data_preprocessor=dict(
+        type='ActionDataPreprocessor',
+        mean=[123.675, 116.28, 103.53],
+        std=[58.395, 57.12, 57.375],
+        format_shape='NCTHW'))
+
+# dataset settings
+dataset_type = 'VideoDataset'
+data_root_val = 'data/kinetics400/videos_val'
+ann_file_test = 'data/kinetics400/kinetics400_val_list_videos.txt'
 
-dataset_type = 'PoseDataset'
-ann_file = 'data/skeleton/ntu60_3d.pkl'
-train_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['j']),
-    dict(type='UniformSampleFrames', clip_len=100),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
-    dict(type='PackActionInputs')
-]
-val_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['j']),
-    dict(
-        type='UniformSampleFrames', clip_len=100, num_clips=1, test_mode=True),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
-    dict(type='PackActionInputs')
-]
 test_pipeline = [
-    dict(type='PreNormalize3D'),
-    dict(type='GenSkeFeat', dataset='nturgb+d', feats=['j']),
+    dict(type='DecordInit'),
     dict(
-        type='UniformSampleFrames', clip_len=100, num_clips=10,
+        type='SampleFrames',
+        clip_len=16,
+        frame_interval=4,
+        num_clips=5,
         test_mode=True),
-    dict(type='PoseDecode'),
-    dict(type='FormatGCNInput', num_person=2),
+    dict(type='DecordDecode'),
+    dict(type='Resize', scale=(-1, 224)),
+    dict(type='ThreeCrop', crop_size=224),
+    dict(type='FormatShape', input_format='NCTHW'),
     dict(type='PackActionInputs')
 ]
 
-train_dataloader = dict(
-    batch_size=16,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type='RepeatDataset',
-        times=5,
-        dataset=dict(
-            type=dataset_type,
-            ann_file=ann_file,
-            pipeline=train_pipeline,
-            split='xsub_train')))
-val_dataloader = dict(
-    batch_size=16,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=False),
-    dataset=dict(
-        type=dataset_type,
-        ann_file=ann_file,
-        pipeline=val_pipeline,
-        split='xsub_val',
-        test_mode=True))
 test_dataloader = dict(
     batch_size=1,
-    num_workers=2,
+    num_workers=8,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
-        ann_file=ann_file,
+        ann_file=ann_file_test,
+        data_prefix=dict(video=data_root_val),
         pipeline=test_pipeline,
-        split='xsub_val',
         test_mode=True))
 
-val_evaluator = [dict(type='AccMetric')]
-test_evaluator = val_evaluator
-
-train_cfg = dict(
-    type='EpochBasedTrainLoop', max_epochs=16, val_begin=1, val_interval=1)
-val_cfg = dict(type='ValLoop')
+test_evaluator = dict(type='AccMetric')
 test_cfg = dict(type='TestLoop')
-
-param_scheduler = [
-    dict(
-        type='CosineAnnealingLR',
-        eta_min=0,
-        T_max=16,
-        by_epoch=True,
-        convert_to_iter_based=True)
-]
-
-optim_wrapper = dict(
-    optimizer=dict(
-        type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0005, nesterov=True))
-
-default_hooks = dict(checkpoint=dict(interval=1), logger=dict(interval=100))
-
-# Default setting for scaling LR automatically
-#   - `enable` means enable scaling LR automatically
-#       or not by default.
-#   - `base_batch_size` = (8 GPUs) x (16 samples per GPU).
-auto_scale_lr = dict(enable=False, base_batch_size=128)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/analyze_logs.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/analyze_logs.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/bench_processing.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/bench_processing.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/benchmark.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/benchmark.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/check_videos.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/check_videos.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/eval_metric.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/eval_metric.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
 import mmengine
 from mmengine import Config, DictAction
 from mmengine.evaluator import Evaluator
 from mmengine.registry import init_default_scope
+from rich import print
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Evaluate metric of the '
                                      'results saved in pkl format')
     parser.add_argument('config', help='Config of the model')
     parser.add_argument('pkl_results', help='Results in pickle format')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/print_config.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/print_config.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/report_accuracy.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/report_accuracy.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
+import numpy as np
 from mmengine import load
 from scipy.special import softmax
 
 from mmaction.evaluation.functional import (get_weighted_score,
                                             mean_class_accuracy,
+                                            mmit_mean_average_precision,
                                             top_k_accuracy)
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Fusing multiple scores')
     parser.add_argument(
         '--preds',
@@ -19,42 +21,58 @@
     parser.add_argument(
         '--coefficients',
         nargs='+',
         type=float,
         help='coefficients of each score file',
         default=[1.0, 1.0])
     parser.add_argument('--apply-softmax', action='store_true')
+    parser.add_argument(
+        '--multi-label',
+        action='store_true',
+        help='whether the task is multi label classification')
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
     assert len(args.preds) == len(args.coefficients)
     data_sample_list = [load(f) for f in args.preds]
     score_list = []
     for data_samples in data_sample_list:
         scores = [
             sample['pred_scores']['item'].numpy() for sample in data_samples
         ]
         score_list.append(scores)
-    labels = [
-        sample['gt_labels']['item'].item() for sample in data_sample_list[0]
-    ]
+
+    if args.multi_label:
+        labels = [
+            sample['gt_labels']['item'] for sample in data_sample_list[0]
+        ]
+    else:
+        labels = [
+            sample['gt_labels']['item'].item()
+            for sample in data_sample_list[0]
+        ]
 
     if args.apply_softmax:
 
         def apply_softmax(scores):
             return [softmax(score) for score in scores]
 
         score_list = [apply_softmax(scores) for scores in score_list]
 
     weighted_scores = get_weighted_score(score_list, args.coefficients)
-    mean_class_acc = mean_class_accuracy(weighted_scores, labels)
-    top_1_acc, top_5_acc = top_k_accuracy(weighted_scores, labels, (1, 5))
-    print(f'Mean Class Accuracy: {mean_class_acc:.04f}')
-    print(f'Top 1 Accuracy: {top_1_acc:.04f}')
-    print(f'Top 5 Accuracy: {top_5_acc:.04f}')
+    if args.multi_label:
+        mean_avg_prec = mmit_mean_average_precision(
+            np.array(weighted_scores), np.stack([t.numpy() for t in labels]))
+        print(f'MMit Average Precision: {mean_avg_prec:.04f}')
+    else:
+        mean_class_acc = mean_class_accuracy(weighted_scores, labels)
+        top_1_acc, top_5_acc = top_k_accuracy(weighted_scores, labels, (1, 5))
+        print(f'Mean Class Accuracy: {mean_class_acc:.04f}')
+        print(f'Top 1 Accuracy: {top_1_acc:.04f}')
+        print(f'Top 5 Accuracy: {top_5_acc:.04f}')
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/analysis_tools/report_map.py` & `mmaction2-1.1.0/mmaction/.mim/tools/analysis_tools/report_map.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
 
-import mmcv
+import mmengine
 import numpy as np
 
-from mmaction.core import ActivityNetLocalization
+from mmaction.evaluation import ActivityNetLocalization
 
 args = None
 
 
 def cuhk17_top1():
     """Assign label for each proposal with the cuhk17 result, which is the #2
     entry in http://activity-net.org/challenges/2017/evaluation.html."""
     if not osp.exists('cuhk_anet17_pred.json'):
         os.system('wget https://download.openmmlab.com/'
                   'mmaction/localization/cuhk_anet17_pred.json')
-    proposal = mmcv.load(args.proposal)
+    proposal = mmengine.load(args.proposal)
     results = proposal['results']
-    cuhk_pred = mmcv.load('cuhk_anet17_pred.json')['results']
+    cuhk_pred = mmengine.load('cuhk_anet17_pred.json')['results']
 
     def get_topk(preds, k):
         preds.sort(key=lambda x: x['score'])
         return preds[-k:]
 
     for k, v in results.items():
         action_pred = cuhk_pred[k]
@@ -32,15 +32,15 @@
         new_value = []
         for item in v:
             x = dict(label=top1_label)
             x.update(item)
             new_value.append(x)
         results[k] = new_value
     proposal['results'] = results
-    mmcv.dump(proposal, args.det_output)
+    mmengine.dump(proposal, args.det_output)
 
 
 cls_funcs = {'cuhk17_top1': cuhk17_top1}
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Report detection mAP for'
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/convert/convert_recognizer.py` & `mmaction2-1.1.0/mmaction/.mim/tools/convert/convert_recognizer.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/activitynet_feature_postprocessing.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/activitynet_feature_postprocessing.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import argparse
 import multiprocessing
 import os
 import os.path as osp
 
 import numpy as np
 import scipy.interpolate
-from mmcv import dump, load
+from mmengine import dump, load
 
 args = None
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='ANet Feature Prepare')
     parser.add_argument('--rgb', default='', help='rgb feature root')
@@ -84,16 +84,20 @@
         with open(osp.join(args.dest, name.replace('.pkl', '.csv')), 'w') as f:
             f.write('\n'.join(lines))
 
 
 def main():
     global args
     args = parse_args()
-    rgb_feat = os.listdir(args.rgb)
-    flow_feat = os.listdir(args.flow)
+    rgb_feat = [file for file in os.listdir(args.rgb) if file.endswith('.pkl')]
+    flow_feat = [
+        file for file in os.listdir(args.flow) if file.endswith('.pkl')
+    ]
     assert set(rgb_feat) == set(flow_feat)
+    # for feat in rgb_feat:
+    #     merge_feat(feat)
     pool = multiprocessing.Pool(32)
     pool.map(merge_feat, rgb_feat)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/convert_proposal_format.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/convert_proposal_format.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 """This file converts the output proposal file of proposal generator (BSN, BMN)
 into the input proposal file of action classifier (Currently supports SSN and
 P-GCN, not including TSN, I3D etc.)."""
 import argparse
 
-import mmcv
+import mmengine
 import numpy as np
 
-from mmaction.core import pairwise_temporal_iou
+from mmaction.evaluation import pairwise_temporal_iou
 
 
 def load_annotations(ann_file):
     """Load the annotation according to ann_file into video_infos."""
     video_infos = []
-    anno_database = mmcv.load(ann_file)
+    anno_database = mmengine.load(ann_file)
     for video_name in anno_database:
         video_info = anno_database[video_name]
         video_info['video_name'] = video_name
         video_infos.append(video_info)
     return video_infos
 
 
@@ -139,15 +139,15 @@
     for line in open(args.activity_index_file).readlines():
         activity_index[line.strip()] = class_idx
         class_idx += 1
 
     video_infos = load_annotations(args.ann_file)
     ground_truth = import_ground_truth(video_infos, activity_index)
     proposal, num_proposals = import_proposals(
-        mmcv.load(args.proposal_file)['results'])
+        mmengine.load(args.proposal_file)['results'])
     video_idx = 0
 
     for video_info in video_infos:
         video_id = video_info['video_name'][2:]
         num_frames = video_info['duration_frame']
         fps = video_info['fps']
         tiou, t_overlap = pairwise_temporal_iou(
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # https://github.com/activitynet/ActivityNet/blob/master/Crawler/Kinetics/download.py  # noqa: E501
 # The code is licensed under the MIT licence.
 import argparse
 import os
 import ssl
 import subprocess
 
-import mmcv
+import mmengine
 from joblib import Parallel, delayed
 
 ssl._create_default_https_context = ssl._create_unverified_context
 data_file = '../../../data/ActivityNet'
 output_dir = f'{data_file}/videos'
 
 
@@ -97,15 +97,15 @@
     """
     if is_bsn_case:
         lines = open(input_csv).readlines()
         lines = lines[1:]
         # YoutubeIDs do not have prefix `v_`
         youtube_ids = [x.split(',')[0][2:] for x in lines]
     else:
-        data = mmcv.load(anno_file)['database']
+        data = mmengine.load(anno_file)['database']
         youtube_ids = list(data.keys())
 
     return youtube_ids
 
 
 def main(input_csv, output_dir, anno_file, num_jobs=24, is_bsn_case=False):
     # Reading and parsing ActivityNet.
@@ -121,23 +121,23 @@
             status_list.append(download_clip_wrapper(index, output_dir))
     else:
         status_list = Parallel(n_jobs=num_jobs)(
             delayed(download_clip_wrapper)(index, output_dir)
             for index in youtube_ids)
 
     # Save download report.
-    mmcv.dump(status_list, 'download_report.json')
-    annotation = mmcv.load(anno_file)
+    mmengine.dump(status_list, 'download_report.json')
+    annotation = mmengine.load(anno_file)
     downloaded = {status[0]: status[1] for status in status_list}
     annotation = {k: v for k, v in annotation.items() if downloaded[k]}
 
     if is_bsn_case:
         anno_file_bak = anno_file.replace('.json', '_bak.json')
         os.rename(anno_file, anno_file_bak)
-        mmcv.dump(annotation, anno_file)
+        mmengine.dump(annotation, anno_file)
 
 
 if __name__ == '__main__':
     args = parse_args()
     is_bsn_case = args.bsn
     if is_bsn_case:
         video_list = f'{data_file}/video_info_new.csv'
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download_feature_annotations.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download_feature_annotations.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/download_features.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/download_features.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/generate_rawframes_filelist.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/generate_rawframes_filelist.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/activitynet/process_annotations.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/activitynet/process_annotations.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,27 +14,27 @@
 
 data_file = '../../../data/ActivityNet'
 info_file = f'{data_file}/video_info_new.csv'
 ann_file = f'{data_file}/anet_anno_action.json'
 
 anno_database = load_json(ann_file)
 
-video_record = np.loadtxt(info_file, dtype=np.str, delimiter=',', skiprows=1)
+video_record = np.loadtxt(info_file, dtype=str, delimiter=',', skiprows=1)
 
 video_dict_train = {}
 video_dict_val = {}
 video_dict_test = {}
 video_dict_full = {}
 
 for _, video_item in enumerate(video_record):
     video_name = video_item[0]
     video_info = anno_database[video_name]
     video_subset = video_item[5]
-    video_info['fps'] = video_item[3].astype(np.float)
-    video_info['rfps'] = video_item[4].astype(np.float)
+    video_info['fps'] = video_item[3].astype(np.float64)
+    video_info['rfps'] = video_item[4].astype(np.float64)
     video_dict_full[video_name] = video_info
     if video_subset == 'training':
         video_dict_train[video_name] = video_info
     elif video_subset == 'testing':
         video_dict_test[video_name] = video_info
     elif video_subset == 'validation':
         video_dict_val[video_name] = video_info
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/anno_txt2json.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/anno_txt2json.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 
-import mmcv
+import mmengine
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         description='Convert txt annotation list to json')
     parser.add_argument(
         'annofile', type=str, help='the txt annotation file to convert')
@@ -96,8 +96,8 @@
     # convert txt anno list to json
     args = parse_args()
     lines = open(args.annofile).readlines()
     lines = [x.strip() for x in lines]
     result = lines2dictlist(lines, args.format)
     if args.output is None:
         args.output = args.annofile.replace('.txt', '.json')
-    mmcv.dump(result, args.output)
+    mmengine.dump(result, args.output)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/cut_videos.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava/cut_videos.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/download_videos_gnu_parallel.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava/download_videos_gnu_parallel.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/download_videos_parallel.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava/download_videos_parallel.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os.path as osp
 import subprocess
 
-import mmcv
+import mmengine
 from joblib import Parallel, delayed
 
 URL_PREFIX = 'https://s3.amazonaws.com/ava-dataset/trainval/'
 
 
 def download_video(video_url, output_dir, num_attempts=5):
     video_file = osp.basename(video_url)
@@ -32,28 +32,28 @@
                 break
 
     status = osp.exists(output_file)
     return status, 'Downloaded'
 
 
 def main(source_file, output_dir, num_jobs=24, num_attempts=5):
-    mmcv.mkdir_or_exist(output_dir)
+    mmengine.mkdir_or_exist(output_dir)
     video_list = open(source_file).read().strip().split('\n')
     video_list = [osp.join(URL_PREFIX, video) for video in video_list]
 
     if num_jobs == 1:
         status_list = []
         for video in video_list:
             video_list.append(download_video(video, output_dir, num_attempts))
     else:
         status_list = Parallel(n_jobs=num_jobs)(
             delayed(download_video)(video, output_dir, num_attempts)
             for video in video_list)
 
-    mmcv.dump(status_list, 'download_report.json')
+    mmengine.dump(status_list, 'download_report.json')
 
 
 if __name__ == '__main__':
     description = 'Helper script for downloading AVA videos'
     parser = argparse.ArgumentParser(description=description)
     parser.add_argument(
         'source_file', type=str, help='TXT file containing the video filename')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava/extract_rgb_frames_ffmpeg.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava/extract_rgb_frames_ffmpeg.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/X-101-64x4d-FPN.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/X-101-64x4d-FPN.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/cut_kinetics.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/cut_kinetics.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/extract_rgb_frames.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/extract_rgb_frames.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/fetch_proposal.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/fetch_proposal.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/merge_annotations.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/merge_annotations.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/prepare_annotation.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/prepare_annotation.py`

 * *Files 0% similar despite different names*

```diff
@@ -56,15 +56,15 @@
 
 if __name__ == '__main__':
     p = argparse.ArgumentParser()
     p.add_argument(
         '--avakinetics_anotation',
         type=str,
         default='./ava_kinetics_v1_0',
-        help='the directory to ava-kinetics anotations')
+        help='the directory to ava-kinetics annotations')
     p.add_argument(
         '--num_workers',
         type=int,
         default=-1,
         help='number of workers used for multiprocessing')
     p.add_argument(
         '--avakinetics_root',
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/ava_kinetics/softlink_ava.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/ava_kinetics/softlink_ava.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_audio_features.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/build_audio_features.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import argparse
 import glob
 import os
 import os.path as osp
 import sys
 from multiprocessing import Pool
 
-import mmcv
+import mmengine
 import numpy as np
 from scipy.io import wavfile
 
 try:
     import librosa
     import lws
 except ImportError:
@@ -291,15 +291,15 @@
     parser.add_argument('spectrogram_save_path', type=str)
     parser.add_argument('--level', type=int, default=1)
     parser.add_argument('--ext', default='.m4a')
     parser.add_argument('--num-workers', type=int, default=4)
     parser.add_argument('--part', type=str, default='1/1')
     args = parser.parse_args()
 
-    mmcv.mkdir_or_exist(args.spectrogram_save_path)
+    mmengine.mkdir_or_exist(args.spectrogram_save_path)
 
     files = glob.glob(
         osp.join(args.audio_home_path, '*/' * args.level, '*' + args.ext))
     print(f'found {len(files)} files.')
     files = sorted(files)
     if args.part is not None:
         [this_part, num_parts] = [int(i) for i in args.part.split('/')]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_file_list.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/build_file_list.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import glob
 import json
 import os.path as osp
 import random
 
-from mmcv.runner import set_random_seed
+from mmengine.runner import set_random_seed
 
 from tools.data.anno_txt2json import lines2dictlist
 from tools.data.parse_file_list import (parse_directory, parse_diving48_splits,
                                         parse_hmdb51_split,
                                         parse_jester_splits,
                                         parse_kinetics_splits,
                                         parse_mit_splits, parse_mmit_splits,
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_rawframes.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/build_rawframes.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/build_videos.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/build_videos.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/denormalize_proposal_file.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/denormalize_proposal_file.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/extract_audio.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/extract_audio.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import glob
 import os
 import os.path as osp
 from multiprocessing import Pool
 
-import mmcv
+import mmengine
 
 
 def extract_audio_wav(line):
     """Extract the audio wave from video streams using FFMPEG."""
     video_id, _ = osp.splitext(osp.basename(line))
     video_dir = osp.dirname(line)
     video_rel_dir = osp.relpath(video_dir, args.root)
@@ -43,15 +43,15 @@
 
     return args
 
 
 if __name__ == '__main__':
     args = parse_args()
 
-    mmcv.mkdir_or_exist(args.dst_root)
+    mmengine.mkdir_or_exist(args.dst_root)
 
     print('Reading videos from folder: ', args.root)
     print('Extension of videos: ', args.ext)
     fullpath_list = glob.glob(args.root + '/*' * args.level + '.' + args.ext)
     done_fullpath_list = glob.glob(args.dst_root + '/*' * args.level + '.wav')
     print('Total number of videos found: ', len(fullpath_list))
     print('Total number of videos extracted finished: ',
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/download.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/gym/download.py`

 * *Files 3% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 # https://github.com/activitynet/ActivityNet/blob/master/Crawler/Kinetics/download.py  # noqa: E501
 # The code is licensed under the MIT licence.
 import argparse
 import os
 import ssl
 import subprocess
 
-import mmcv
+import mmengine
 from joblib import Parallel, delayed
 
 ssl._create_default_https_context = ssl._create_unverified_context
 
 
 def download(video_identifier,
              output_filename,
@@ -68,30 +68,30 @@
     downloaded, log = download(youtube_id, output_filename)
     status = tuple([youtube_id, downloaded, log])
     return status
 
 
 def main(input, output_dir, num_jobs=24):
     # Reading and parsing ActivityNet.
-    youtube_ids = mmcv.load(input).keys()
+    youtube_ids = mmengine.load(input).keys()
     # Creates folders where videos will be saved later.
     if not os.path.exists(output_dir):
         os.makedirs(output_dir)
     # Download all clips.
     if num_jobs == 1:
         status_list = []
         for index in youtube_ids:
             status_list.append(download_wrapper(index, output_dir))
     else:
         status_list = Parallel(n_jobs=num_jobs)(
             delayed(download_wrapper)(index, output_dir)
             for index in youtube_ids)
 
     # Save download report.
-    mmcv.dump(status_list, 'download_report.json')
+    mmengine.dump(status_list, 'download_report.json')
 
 
 if __name__ == '__main__':
     description = 'Helper script for downloading GYM videos.'
     p = argparse.ArgumentParser(description=description)
     p.add_argument('input', type=str, help='The gym annotation file')
     p.add_argument(
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/download_annotations.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/gym/download_annotations.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/generate_file_list.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/gym/generate_file_list.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/trim_event.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/gym/trim_event.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,28 +1,28 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os
 import os.path as osp
 import subprocess
 
-import mmcv
+import mmengine
 
 data_root = '../../../data/gym'
 video_root = f'{data_root}/videos'
 anno_root = f'{data_root}/annotations'
 anno_file = f'{anno_root}/annotation.json'
 
 event_anno_file = f'{anno_root}/event_annotation.json'
 event_root = f'{data_root}/events'
 
 videos = os.listdir(video_root)
 videos = set(videos)
-annotation = mmcv.load(anno_file)
+annotation = mmengine.load(anno_file)
 event_annotation = {}
 
-mmcv.mkdir_or_exist(event_root)
+mmengine.mkdir_or_exist(event_root)
 
 for k, v in annotation.items():
     if k + '.mp4' not in videos:
         print(f'video {k} has not been downloaded')
         continue
 
     video_path = osp.join(video_root, k + '.mp4')
@@ -51,8 +51,8 @@
                 f'Trimming of the Event {event_name} of Video {k} Failed',
                 flush=True)
 
         segments = event_anno['segments']
         if segments is not None:
             event_annotation[event_name] = segments
 
-mmcv.dump(event_annotation, event_anno_file)
+mmengine.dump(event_annotation, event_anno_file)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/gym/trim_subaction.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/gym/trim_subaction.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os
 import os.path as osp
 import subprocess
 
-import mmcv
+import mmengine
 
 data_root = '../../../data/gym'
 anno_root = f'{data_root}/annotations'
 
 event_anno_file = f'{anno_root}/event_annotation.json'
 event_root = f'{data_root}/events'
 subaction_root = f'{data_root}/subactions'
 
 events = os.listdir(event_root)
 events = set(events)
-annotation = mmcv.load(event_anno_file)
+annotation = mmengine.load(event_anno_file)
 
-mmcv.mkdir_or_exist(subaction_root)
+mmengine.mkdir_or_exist(subaction_root)
 
 for k, v in annotation.items():
     if k + '.mp4' not in events:
         print(f'video {k[:11]} has not been downloaded '
               f'or the event clip {k} not generated')
         continue
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hmdb51/download_videos.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/hmdb51/download_videos.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/download.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/download.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 import glob
 import os
 import shutil
 import ssl
 import subprocess
 import uuid
 
-import mmcv
+import mmengine
 from joblib import Parallel, delayed
 
 ssl._create_default_https_context = ssl._create_unverified_context
 args = None
 
 
 def create_video_folders(output_dir, tmp_dir):
@@ -169,15 +169,15 @@
         status_lst = Parallel(n_jobs=num_jobs)(
             delayed(download_clip_wrapper)(item, trim_format, tmp_dir,
                                            output_dir) for item in dataset)
 
     # Clean tmp dir.
     shutil.rmtree(tmp_dir)
     # Save download report.
-    mmcv.dump(status_lst, 'download_report.json')
+    mmengine.dump(status_lst, 'download_report.json')
 
 
 if __name__ == '__main__':
     description = 'Helper script for downloading and trimming HVU videos.'
     p = argparse.ArgumentParser(description=description)
     p.add_argument(
         'input_csv',
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/download_annotations.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/download_annotations.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/generate_file_list.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/generate_file_list.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import fnmatch
 import glob
 import os
 import os.path as osp
 
-import mmcv
+import mmengine
 
 annotation_root = '../../data/hvu/annotations'
 tag_file = 'hvu_tags.json'
 args = None
 
 
 def parse_directory(path,
@@ -105,15 +105,15 @@
 
     args = parser.parse_args()
     return args
 
 
 if __name__ == '__main__':
     args = parse_args()
-    tag_cates = mmcv.load(tag_file)
+    tag_cates = mmengine.load(tag_file)
     tag2category = {}
     for k in tag_cates:
         for tag in tag_cates[k]:
             tag2category[tag] = k
 
     data_list = open(args.input_csv).readlines()
     data_list = [x.strip().split(',') for x in data_list[1:]]
@@ -145,8 +145,8 @@
         result = [
             dict(
                 frame_dir=k[0], total_frames=parse_result[k[0]][1], label=k[1])
             for k in data_list
         ]
     elif args.mode == 'videos':
         result = [dict(filename=k[0] + '.mp4', label=k[1]) for k in data_list]
-    mmcv.dump(result, args.output)
+    mmengine.dump(result, args.output)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/hvu/generate_sub_file_list.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/hvu/generate_sub_file_list.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,34 +1,34 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os.path as osp
 
-import mmcv
+import mmengine
 
 
 def main(annotation_file, category):
     assert category in [
         'action', 'attribute', 'concept', 'event', 'object', 'scene'
     ]
 
-    data = mmcv.load(annotation_file)
+    data = mmengine.load(annotation_file)
     basename = osp.basename(annotation_file)
     dirname = osp.dirname(annotation_file)
     basename = basename.replace('hvu', f'hvu_{category}')
 
     target_file = osp.join(dirname, basename)
 
     result = []
     for item in data:
         label = item['label']
         if category in label:
             item['label'] = label[category]
             result.append(item)
 
-    mmcv.dump(data, target_file)
+    mmengine.dump(data, target_file)
 
 
 if __name__ == '__main__':
     description = 'Helper script for generating HVU per-category file list.'
     p = argparse.ArgumentParser(description=description)
     p.add_argument(
         'annotation_file',
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download_annotations.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download_annotations.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download_backup_annotations.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download_backup_annotations.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/download_videos.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/download_videos.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/extract_frames.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/extract_frames.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/extract_rgb_frames.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/extract_rgb_frames.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/extract_rgb_frames_opencv.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/extract_rgb_frames_opencv.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/generate_rawframes_filelist.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/generate_rawframes_filelist.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/generate_videos_filelist.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/generate_videos_filelist.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/kinetics/rename_classnames.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/kinetics/rename_classnames.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/mit/preprocess_data.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/mit/preprocess_data.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/omnisource/trim_raw_video.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/omnisource/trim_raw_video.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os
 import os.path as osp
 import sys
 from subprocess import check_output
 
-import mmcv
+import mmengine
 
 
 def get_duration(vid_name):
     command = f'ffprobe -i {vid_name} 2>&1 | grep "Duration"'
     output = str(check_output(command, shell=True))
     output = output.split(',')[0].split('Duration:')[1].strip()
     h, m, s = output.split(':')
@@ -24,15 +24,15 @@
         return
 
     i = 0
     name, _ = osp.splitext(vid_name)
 
     # We output 10-second clips into the folder `name`
     dest = name
-    mmcv.mkdir_or_exist(dest)
+    mmengine.mkdir_or_exist(dest)
 
     command_tmpl = ('ffmpeg -y loglevel error -i {} -ss {} -t {} -crf 18 '
                     '-c:v libx264 {}/part_{}.mp4')
     while i * 10 < lt:
         os.system(command_tmpl.format(vid_name, i * 10, 10, dest, i))
         i += 1
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/parse_file_list.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/parse_file_list.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/resize_videos.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/resize_videos.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/babel2mma2.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/babel2mma2.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/gen_ntu_rgbd_raw.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/gen_ntu_rgbd_raw.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/skeleton/ntu_pose_extraction.py` & `mmaction2-1.1.0/mmaction/.mim/tools/data/skeleton/ntu_pose_extraction.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,86 +1,28 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import abc
 import argparse
-import os
 import os.path as osp
-import random as rd
-import shutil
-import string
 from collections import defaultdict
+from tempfile import TemporaryDirectory
 
-import cv2
-import mmcv
+import mmengine
 import numpy as np
 
-try:
-    from mmdet.apis import inference_detector, init_detector
-except (ImportError, ModuleNotFoundError):
-    raise ImportError('Failed to import `inference_detector` and '
-                      '`init_detector` form `mmdet.apis`. These apis are '
-                      'required in this script! ')
-
-try:
-    from mmpose.apis import inference_top_down_pose_model, init_pose_model
-except (ImportError, ModuleNotFoundError):
-    raise ImportError('Failed to import `inference_top_down_pose_model` and '
-                      '`init_pose_model` form `mmpose.apis`. These apis are '
-                      'required in this script! ')
-
-mmdet_root = ''
-mmpose_root = ''
+from mmaction.apis import detection_inference, pose_inference
+from mmaction.utils import frame_extract
 
 args = abc.abstractproperty()
-args.det_config = f'{mmdet_root}/configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco-person.py'  # noqa: E501
+args.det_config = 'demo/demo_configs/faster-rcnn_r50-caffe_fpn_ms-1x_coco-person.py'  # noqa: E501
 args.det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco-person/faster_rcnn_r50_fpn_1x_coco-person_20201216_175929-d022e227.pth'  # noqa: E501
 args.det_score_thr = 0.5
-args.pose_config = f'{mmpose_root}/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w32_coco_256x192.py'  # noqa: E501
+args.pose_config = 'demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'  # noqa: E501
 args.pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'  # noqa: E501
 
 
-def gen_id(size=8):
-    chars = string.ascii_uppercase + string.digits
-    return ''.join(rd.choice(chars) for _ in range(size))
-
-
-def extract_frame(video_path):
-    dname = gen_id()
-    os.makedirs(dname, exist_ok=True)
-    frame_tmpl = osp.join(dname, 'img_{:05d}.jpg')
-    vid = cv2.VideoCapture(video_path)
-    frame_paths = []
-    flag, frame = vid.read()
-    cnt = 0
-    while flag:
-        frame_path = frame_tmpl.format(cnt + 1)
-        frame_paths.append(frame_path)
-
-        cv2.imwrite(frame_path, frame)
-        cnt += 1
-        flag, frame = vid.read()
-
-    return frame_paths
-
-
-def detection_inference(args, frame_paths):
-    model = init_detector(args.det_config, args.det_checkpoint, args.device)
-    assert model.CLASSES[0] == 'person', ('We require you to use a detector '
-                                          'trained on COCO')
-    results = []
-    print('Performing Human Detection for each frame')
-    prog_bar = mmcv.ProgressBar(len(frame_paths))
-    for frame_path in frame_paths:
-        result = inference_detector(model, frame_path)
-        # We only keep human detections with score larger than det_score_thr
-        result = result[0][result[0][:, 4] >= args.det_score_thr]
-        results.append(result)
-        prog_bar.update()
-    return results
-
-
 def intersection(b0, b1):
     l, r = max(b0[0], b1[0]), min(b0[2], b1[2])
     u, d = max(b0[1], b1[1]), min(b0[3], b1[3])
     return max(0, r - l) * max(0, d - u)
 
 
 def iou(b0, b1):
@@ -223,15 +165,15 @@
             mind = np.Inf
             mink = None
             for k in bboxd:
                 if np.abs(k - idx) < mind:
                     mind = np.abs(k - idx)
                     mink = k
             bbox[idx] = bboxd[mink]
-    return bad, bbox
+    return bad, bbox[:, None, :]
 
 
 def bboxes2bbox(bbox, num_frame):
     ret = np.zeros((num_frame, 2, 5))
     for t, item in enumerate(bbox):
         if item.shape[0] <= 2:
             ret[t, :item.shape[0]] = item
@@ -283,49 +225,65 @@
             bboxes.append(tracklet2bbox(tracklet, len(det_results))[:, None])
         bbox = np.concatenate(bboxes, axis=1)
         return bbox
     else:
         return bboxes2bbox(det_results, len(det_results))
 
 
-def pose_inference(args, frame_paths, det_results):
-    model = init_pose_model(args.pose_config, args.pose_checkpoint,
-                            args.device)
-    print('Performing Human Pose Estimation for each frame')
-    prog_bar = mmcv.ProgressBar(len(frame_paths))
-
-    num_frame = len(det_results)
-    num_person = max([len(x) for x in det_results])
-    kp = np.zeros((num_person, num_frame, 17, 3), dtype=np.float32)
-
-    for i, (f, d) in enumerate(zip(frame_paths, det_results)):
-        # Align input format
-        d = [dict(bbox=x) for x in list(d) if x[-1] > 0.5]
-        pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]
-        for j, item in enumerate(pose):
-            kp[j, i] = item['keypoints']
-        prog_bar.update()
-    return kp
+def pose_inference_with_align(args, frame_paths, det_results):
+    # filter frame without det bbox
+    det_results = [
+        frm_dets for frm_dets in det_results if frm_dets.shape[0] > 0
+    ]
+
+    pose_results, _ = pose_inference(args.pose_config, args.pose_checkpoint,
+                                     frame_paths, det_results, args.device)
+    # align the num_person among frames
+    num_persons = max([pose['keypoints'].shape[0] for pose in pose_results])
+    num_points = pose_results[0]['keypoints'].shape[1]
+    num_frames = len(pose_results)
+    keypoints = np.zeros((num_persons, num_frames, num_points, 2),
+                         dtype=np.float32)
+    scores = np.zeros((num_persons, num_frames, num_points), dtype=np.float32)
+
+    for f_idx, frm_pose in enumerate(pose_results):
+        frm_num_persons = frm_pose['keypoints'].shape[0]
+        for p_idx in range(frm_num_persons):
+            keypoints[p_idx, f_idx] = frm_pose['keypoints'][p_idx]
+            scores[p_idx, f_idx] = frm_pose['keypoint_scores'][p_idx]
+
+    return keypoints, scores
 
 
 def ntu_pose_extraction(vid, skip_postproc=False):
-    frame_paths = extract_frame(vid)
-    det_results = detection_inference(args, frame_paths)
+    tmp_dir = TemporaryDirectory()
+    frame_paths, _ = frame_extract(vid, out_dir=tmp_dir.name)
+    det_results, _ = detection_inference(
+        args.det_config,
+        args.det_checkpoint,
+        frame_paths,
+        args.det_score_thr,
+        device=args.device,
+        with_score=True)
+
     if not skip_postproc:
         det_results = ntu_det_postproc(vid, det_results)
-    pose_results = pose_inference(args, frame_paths, det_results)
+
     anno = dict()
-    anno['keypoint'] = pose_results[..., :2]
-    anno['keypoint_score'] = pose_results[..., 2]
+
+    keypoints, scores = pose_inference_with_align(args, frame_paths,
+                                                  det_results)
+    anno['keypoint'] = keypoints
+    anno['keypoint_score'] = scores
     anno['frame_dir'] = osp.splitext(osp.basename(vid))[0]
     anno['img_shape'] = (1080, 1920)
     anno['original_shape'] = (1080, 1920)
-    anno['total_frames'] = pose_results.shape[1]
+    anno['total_frames'] = keypoints.shape[1]
     anno['label'] = int(osp.basename(vid).split('A')[1][:3]) - 1
-    shutil.rmtree(osp.dirname(frame_paths[0]))
+    tmp_dir.cleanup()
 
     return anno
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         description='Generate Pose Annotation for a single NTURGB-D video')
@@ -340,8 +298,8 @@
 if __name__ == '__main__':
     global_args = parse_args()
     args.device = global_args.device
     args.video = global_args.video
     args.output = global_args.output
     args.skip_postproc = global_args.skip_postproc
     anno = ntu_pose_extraction(args.video, args.skip_postproc)
-    mmcv.dump(anno, args.output)
+    mmengine.dump(anno, args.output)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/denormalize_proposal_file.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/denormalize_proposal_file.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/download_annotations.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/download_annotations.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/data/thumos14/download_videos.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/data/thumos14/download_videos.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/export_onnx_stdet.py` & `mmaction2-1.1.0/mmaction/.mim/tools/deployment/export_onnx_stdet.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,11 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+# This script serves the sole purpose of converting spatial-temporal detection
+# models supported in MMAction2 to ONNX files. Please note that attempting to
+# convert other models using this script may not yield successful results.
 import argparse
 
 import onnxruntime
 import torch
 import torch.nn as nn
 from mmdet.structures.bbox import bbox2roi
 from mmengine import Config
@@ -121,14 +124,21 @@
         cls_score = self.bbox_head(bbox_feats)
         return cls_score
 
 
 def main():
     args = parse_args()
     config = Config.fromfile(args.config)
+
+    if config.model.type != 'FastRCNN':
+        print('This script serves the sole purpose of converting spatial '
+              'temporal detection models in MMAction2 to ONNX files. Please '
+              'note that attempting to convert other models using this script '
+              'may not yield successful results.\n\n')
+
     init_default_scope(config.get('default_scope', 'mmaction'))
 
     base_model = MODELS.build(config.model)
     load_checkpoint(base_model, args.checkpoint, map_location='cpu')
     base_model.to(args.device)
 
     if len(args.shape) == 1:
@@ -151,17 +161,17 @@
     print(f'Model output shape: {cls_score.shape}')
 
     torch.onnx.export(
         model, (input_tensor, rois),
         args.output_file,
         input_names=['input_tensor', 'rois'],
         output_names=['cls_score'],
-        export_params=False,
+        export_params=True,
         do_constant_folding=True,
-        verbose=True,
+        verbose=False,
         opset_version=11,
         dynamic_axes={
             'input_tensor': {
                 0: 'batch_size',
                 3: 'height',
                 4: 'width'
             },
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/mmaction2torchserve.py` & `mmaction2-1.1.0/mmaction/.mim/tools/deployment/mmaction2torchserve.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/mmaction_handler.py` & `mmaction2-1.1.0/mmaction/.mim/tools/deployment/mmaction_handler.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/deployment/publish_model.py` & `mmaction2-1.1.0/mmaction/.mim/tools/deployment/publish_model.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/misc/bsn_proposal_generation.py` & `mmaction2-1.1.0/mmaction/.mim/tools/misc/bsn_proposal_generation.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/misc/flow_extraction.py` & `mmaction2-1.1.0/mmaction/.mim/tools/misc/flow_extraction.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/slurm_test.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/slurm_test.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/slurm_train.sh` & `mmaction2-1.1.0/mmaction/.mim/tools/slurm_train.sh`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/test.py` & `mmaction2-1.1.0/mmaction/.mim/tools/test.py`

 * *Files 1% similar despite different names*

```diff
@@ -47,15 +47,15 @@
         default=2,
         help='display time of every window. (second)')
     parser.add_argument(
         '--launcher',
         choices=['none', 'pytorch', 'slurm', 'mpi'],
         default='none',
         help='job launcher')
-    parser.add_argument('--local_rank', type=int, default=0)
+    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)
     args = parser.parse_args()
     if 'LOCAL_RANK' not in os.environ:
         os.environ['LOCAL_RANK'] = str(args.local_rank)
     return args
 
 
 def merge_args(cfg, args):
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/train.py` & `mmaction2-1.1.0/mmaction/.mim/tools/train.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,15 +12,15 @@
     parser.add_argument('config', help='train config file path')
     parser.add_argument('--work-dir', help='the dir to save logs and models')
     parser.add_argument(
         '--resume',
         nargs='?',
         type=str,
         const='auto',
-        help='If specify checkpint path, resume from it, while if not '
+        help='If specify checkpoint path, resume from it, while if not '
         'specify, try to auto resume from the latest checkpoint '
         'in the work directory.')
     parser.add_argument(
         '--amp',
         action='store_true',
         help='enable automatic-mixed-precision training')
     parser.add_argument(
@@ -52,15 +52,15 @@
         'Note that the quotation marks are necessary and that no white space '
         'is allowed.')
     parser.add_argument(
         '--launcher',
         choices=['none', 'pytorch', 'slurm', 'mpi'],
         default='none',
         help='job launcher')
-    parser.add_argument('--local_rank', type=int, default=0)
+    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)
     args = parser.parse_args()
     if 'LOCAL_RANK' not in os.environ:
         os.environ['LOCAL_RANK'] = str(args.local_rank)
 
     return args
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/visualizations/browse_dataset.py` & `mmaction2-1.1.0/mmaction/.mim/tools/visualizations/browse_dataset.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,21 +17,17 @@
 from mmaction.visualization import ActionVisualizer
 from mmaction.visualization.action_visualizer import _get_adaptive_scale
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Browse a dataset')
     parser.add_argument('config', help='train config file path')
-    parser.add_argument('--label', default=None, type=str, help='label file')
     parser.add_argument(
-        '--output-dir',
-        '-o',
-        default=None,
-        type=str,
-        help='If there is no display interface, you can save it.')
+        'output_dir', default=None, type=str, help='output directory')
+    parser.add_argument('--label', default=None, type=str, help='label file')
     parser.add_argument(
         '--phase',
         '-p',
         default='train',
         type=str,
         choices=['train', 'test', 'val'],
         help='phase of dataset to visualize, accept "train" "test" and "val".'
```

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/visualizations/vis_cam.py` & `mmaction2-1.1.0/mmaction/.mim/tools/visualizations/vis_cam.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/.mim/tools/visualizations/vis_scheduler.py` & `mmaction2-1.1.0/mmaction/.mim/tools/visualizations/vis_scheduler.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,66 +12,17 @@
 from mmengine.config import Config, DictAction
 from mmengine.hooks import Hook
 from mmengine.model import BaseModel
 from mmengine.registry import init_default_scope
 from mmengine.runner import Runner
 from mmengine.visualization import Visualizer
 from rich.progress import BarColumn, MofNCompleteColumn, Progress, TextColumn
+from torch.utils.data import DataLoader
 
-
-class SimpleModel(BaseModel):
-    """simple model that do nothing in train_step."""
-
-    def __init__(self):
-        super(SimpleModel, self).__init__()
-        self.data_preprocessor = nn.Identity()
-        self.conv = nn.Conv2d(1, 1, 1)
-
-    def forward(self, inputs, data_samples, mode='tensor'):
-        pass
-
-    def train_step(self, data, optim_wrapper):
-        pass
-
-
-class ParamRecordHook(Hook):
-
-    def __init__(self, by_epoch):
-        super().__init__()
-        self.by_epoch = by_epoch
-        self.lr_list = []
-        self.momentum_list = []
-        self.task_id = 0
-        self.progress = Progress(BarColumn(), MofNCompleteColumn(),
-                                 TextColumn('{task.description}'))
-
-    def before_train(self, runner):
-        if self.by_epoch:
-            total = runner.train_loop.max_epochs
-            self.task_id = self.progress.add_task(
-                'epochs', start=True, total=total)
-        else:
-            total = runner.train_loop.max_iters
-            self.task_id = self.progress.add_task(
-                'iters', start=True, total=total)
-        self.progress.start()
-
-    def after_train_epoch(self, runner):
-        if self.by_epoch:
-            self.progress.update(self.task_id, advance=1)
-
-    def after_train_iter(self, runner, batch_idx, data_batch, outputs):
-        if not self.by_epoch:
-            self.progress.update(self.task_id, advance=1)
-        self.lr_list.append(runner.optim_wrapper.get_lr()['lr'][0])
-        self.momentum_list.append(
-            runner.optim_wrapper.get_momentum()['momentum'][0])
-
-    def after_train(self, runner):
-        self.progress.stop()
+from mmaction.utils import get_str_type
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         description='Visualize a Dataset Pipeline')
     parser.add_argument('config', help='config file path')
     parser.add_argument(
@@ -126,14 +77,66 @@
     if args.window_size != '':
         assert re.match(r'\d+\*\d+', args.window_size), \
             "'window-size' must be in format 'W*H'."
 
     return args
 
 
+class SimpleModel(BaseModel):
+    """simple model that do nothing in train_step."""
+
+    def __init__(self):
+        super(SimpleModel, self).__init__()
+        self.data_preprocessor = nn.Identity()
+        self.conv = nn.Conv2d(1, 1, 1)
+
+    def forward(self, inputs, data_samples, mode='tensor'):
+        pass
+
+    def train_step(self, data, optim_wrapper):
+        pass
+
+
+class ParamRecordHook(Hook):
+
+    def __init__(self, by_epoch):
+        super().__init__()
+        self.by_epoch = by_epoch
+        self.lr_list = []
+        self.momentum_list = []
+        self.task_id = 0
+        self.progress = Progress(BarColumn(), MofNCompleteColumn(),
+                                 TextColumn('{task.description}'))
+
+    def before_train(self, runner):
+        if self.by_epoch:
+            total = runner.train_loop.max_epochs
+            self.task_id = self.progress.add_task(
+                'epochs', start=True, total=total)
+        else:
+            total = runner.train_loop.max_iters
+            self.task_id = self.progress.add_task(
+                'iters', start=True, total=total)
+        self.progress.start()
+
+    def after_train_epoch(self, runner):
+        if self.by_epoch:
+            self.progress.update(self.task_id, advance=1)
+
+    def after_train_iter(self, runner, batch_idx, data_batch, outputs):
+        if not self.by_epoch:
+            self.progress.update(self.task_id, advance=1)
+        self.lr_list.append(runner.optim_wrapper.get_lr()['lr'][0])
+        self.momentum_list.append(
+            runner.optim_wrapper.get_momentum()['momentum'][0])
+
+    def after_train(self, runner):
+        self.progress.stop()
+
+
 def plot_curve(lr_list, args, param_name, iters_per_epoch, by_epoch=True):
     """Plot learning rate vs iter graph."""
     try:
         import seaborn as sns
         sns.set_style(args.style)
     except ImportError:
         pass
@@ -182,14 +185,15 @@
         train_dataloader=data_loader,
         train_cfg=cfg.train_cfg,
         log_level=cfg.log_level,
         optim_wrapper=cfg.optim_wrapper,
         param_scheduler=cfg.param_scheduler,
         default_scope=cfg.default_scope,
         default_hooks=default_hooks,
+        auto_scale_lr=cfg.get('auto_scale_lr'),
         visualizer=MagicMock(spec=Visualizer),
         custom_hooks=cfg.get('custom_hooks', None))
 
     runner.train()
 
     return param_record_hook.lr_list, param_record_hook.momentum_list
 
@@ -219,30 +223,30 @@
 
     # prepare data loader
     batch_size = cfg.train_dataloader.batch_size * args.ngpus
 
     if 'by_epoch' in cfg.train_cfg:
         by_epoch = cfg.train_cfg.get('by_epoch')
     elif 'type' in cfg.train_cfg:
-        by_epoch = cfg.train_cfg.get('type') == 'EpochBasedTrainLoop'
+        by_epoch = get_str_type(cfg.train_cfg.get('by_epoch')) \
+                    == 'EpochBasedTrainLoop'
     else:
         raise ValueError('please set `train_cfg`.')
 
     if args.dataset_size is None and by_epoch:
         from mmaction.registry import DATASETS
         dataset_size = len(DATASETS.build(cfg.train_dataloader.dataset))
         print(f'dataset is {dataset_size}')
-        # dataset_size = len(build_dataset(cfg.train_dataloader.dataset))
     else:
         dataset_size = args.dataset_size or batch_size
 
-    class FakeDataloader(list):
-        dataset = MagicMock(metainfo=None)
-
-    data_loader = FakeDataloader(range(dataset_size // batch_size))
+    data_loader = DataLoader(range(dataset_size), batch_size)
+    assert len(data_loader) > 0, \
+        'Please decrease batchsize to make sure that ' \
+        'a epoch at least have one iteration!'
     dataset_info = (
         f'\nDataset infos:'
         f'\n - Dataset size: {dataset_size}'
         f'\n - Batch size per GPU: {cfg.train_dataloader.batch_size}'
         f'\n - Number of GPUs: {args.ngpus}'
         f'\n - Total batch size: {batch_size}')
     if by_epoch:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/__init__.py` & `mmaction2-1.1.0/mmaction/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import mmcv
 import mmengine
 from mmengine.utils import digit_version
 
 from .version import __version__
 
-mmcv_minimum_version = '2.0.0rc0'
+mmcv_minimum_version = '2.0.0rc4'
 mmcv_maximum_version = '2.1.0'
 mmcv_version = digit_version(mmcv.__version__)
 
-mmengine_minimum_version = '0.5.0'
+mmengine_minimum_version = '0.7.1'
 mmengine_maximum_version = '1.0.0'
 mmengine_version = digit_version(mmengine.__version__)
 
 assert (digit_version(mmcv_minimum_version) <= mmcv_version
         < digit_version(mmcv_maximum_version)), \
     f'MMCV=={mmcv.__version__} is used but incompatible. ' \
     f'Please install mmcv>={mmcv_minimum_version}, <{mmcv_maximum_version}.'
```

### Comparing `mmaction2-1.0.0rc3/mmaction/apis/inference.py` & `mmaction2-1.1.0/mmaction/apis/inference.py`

 * *Files 7% similar despite different names*

```diff
@@ -35,15 +35,16 @@
         config = mmengine.Config.fromfile(config)
     elif not isinstance(config, mmengine.Config):
         raise TypeError('config must be a filename or Config object, '
                         f'but got {type(config)}')
 
     init_default_scope(config.get('default_scope', 'mmaction'))
 
-    if config.model.backbone.get('pretrained', None):
+    if hasattr(config.model, 'backbone') and config.model.backbone.get(
+            'pretrained', None):
         config.model.backbone.pretrained = None
     model = MODELS.build(config.model)
 
     if checkpoint is not None:
         load_checkpoint(model, checkpoint, map_location='cpu')
     model.cfg = config
     model.to(device)
@@ -68,14 +69,15 @@
     Returns:
         :obj:`ActionDataSample`: The inference results. Specifically, the
         predicted scores are saved at ``result.pred_scores.item``.
     """
 
     if test_pipeline is None:
         cfg = model.cfg
+        init_default_scope(cfg.get('default_scope', 'mmaction'))
         test_pipeline_cfg = cfg.test_pipeline
         test_pipeline = Compose(test_pipeline_cfg)
 
     input_flag = None
     if isinstance(video, dict):
         input_flag = 'dict'
     elif isinstance(video, str):
@@ -95,76 +97,92 @@
     # Forward the model
     with torch.no_grad():
         result = model.test_step(data)[0]
 
     return result
 
 
-def detection_inference(det_config: Union[str, Path, mmengine.Config],
+def detection_inference(det_config: Union[str, Path, mmengine.Config,
+                                          nn.Module],
                         det_checkpoint: str,
                         frame_paths: List[str],
                         det_score_thr: float = 0.9,
                         det_cat_id: int = 0,
-                        device: Union[str, torch.device] = 'cuda:0') -> tuple:
+                        device: Union[str, torch.device] = 'cuda:0',
+                        with_score: bool = False) -> tuple:
     """Detect human boxes given frame paths.
 
     Args:
-        det_config (Union[str, :obj:`Path`, :obj:`mmengine.Config`]): Config
-            file path, :obj:`Path` or the config object.
+        det_config (Union[str, :obj:`Path`, :obj:`mmengine.Config`,
+            :obj:`torch.nn.Module`]):
+            Det config file path or Detection model object. It can be
+            a :obj:`Path`, a config object, or a module object.
         det_checkpoint: Checkpoint path/url.
         frame_paths (List[str]): The paths of frames to do detection inference.
         det_score_thr (float): The threshold of human detection score.
             Defaults to 0.9.
         det_cat_id (int): The category id for human detection. Defaults to 0.
         device (Union[str, torch.device]): The desired device of returned
             tensor. Defaults to ``'cuda:0'``.
+        with_score (bool): Whether to append detection score after box.
+            Defaults to None.
 
     Returns:
         List[np.ndarray]: List of detected human boxes.
         List[:obj:`DetDataSample`]: List of data samples, generally used
             to visualize data.
     """
     try:
         from mmdet.apis import inference_detector, init_detector
         from mmdet.structures import DetDataSample
     except (ImportError, ModuleNotFoundError):
         raise ImportError('Failed to import `inference_detector` and '
                           '`init_detector` from `mmdet.apis`. These apis are '
                           'required in this inference api! ')
-
-    model = init_detector(
-        config=det_config, checkpoint=det_checkpoint, device=device)
+    if isinstance(det_config, nn.Module):
+        model = det_config
+    else:
+        model = init_detector(
+            config=det_config, checkpoint=det_checkpoint, device=device)
 
     results = []
     data_samples = []
     print('Performing Human Detection for each frame')
     for frame_path in track_iter_progress(frame_paths):
         det_data_sample: DetDataSample = inference_detector(model, frame_path)
         pred_instance = det_data_sample.pred_instances.cpu().numpy()
         bboxes = pred_instance.bboxes
+        scores = pred_instance.scores
         # We only keep human detection bboxs with score larger
         # than `det_score_thr` and category id equal to `det_cat_id`.
-        bboxes = bboxes[np.logical_and(pred_instance.labels == det_cat_id,
-                                       pred_instance.scores > det_score_thr)]
+        valid_idx = np.logical_and(pred_instance.labels == det_cat_id,
+                                   pred_instance.scores > det_score_thr)
+        bboxes = bboxes[valid_idx]
+        scores = scores[valid_idx]
+
+        if with_score:
+            bboxes = np.concatenate((bboxes, scores[:, None]), axis=-1)
         results.append(bboxes)
         data_samples.append(det_data_sample)
 
     return results, data_samples
 
 
-def pose_inference(pose_config: Union[str, Path, mmengine.Config],
+def pose_inference(pose_config: Union[str, Path, mmengine.Config, nn.Module],
                    pose_checkpoint: str,
                    frame_paths: List[str],
                    det_results: List[np.ndarray],
                    device: Union[str, torch.device] = 'cuda:0') -> tuple:
     """Perform Top-Down pose estimation.
 
     Args:
-        pose_config (Union[str, :obj:`Path`, :obj:`mmengine.Config`]): Config
-            file path, :obj:`Path` or the config object.
+        pose_config (Union[str, :obj:`Path`, :obj:`mmengine.Config`,
+            :obj:`torch.nn.Module`]): Pose config file path or
+            pose model object. It can be a :obj:`Path`, a config object,
+            or a module object.
         pose_checkpoint: Checkpoint path/url.
         frame_paths (List[str]): The paths of frames to do pose inference.
         det_results (List[np.ndarray]): List of detected human boxes.
         device (Union[str, torch.device]): The desired device of returned
             tensor. Defaults to ``'cuda:0'``.
 
     Returns:
@@ -175,23 +193,25 @@
     try:
         from mmpose.apis import inference_topdown, init_model
         from mmpose.structures import PoseDataSample, merge_data_samples
     except (ImportError, ModuleNotFoundError):
         raise ImportError('Failed to import `inference_topdown` and '
                           '`init_model` from `mmpose.apis`. These apis '
                           'are required in this inference api! ')
-
-    model = init_model(pose_config, pose_checkpoint, device)
+    if isinstance(pose_config, nn.Module):
+        model = pose_config
+    else:
+        model = init_model(pose_config, pose_checkpoint, device)
 
     results = []
     data_samples = []
     print('Performing Human Pose Estimation for each frame')
     for f, d in track_iter_progress(list(zip(frame_paths, det_results))):
         pose_data_samples: List[PoseDataSample] \
-            = inference_topdown(model, f, d, bbox_format='xyxy')
+            = inference_topdown(model, f, d[..., :4], bbox_format='xyxy')
         pose_data_sample = merge_data_samples(pose_data_samples)
         pose_data_sample.dataset_meta = model.dataset_meta
         poses = pose_data_sample.pred_instances.to_dict()
         results.append(poses)
         data_samples.append(pose_data_sample)
 
     return results, data_samples
```

### Comparing `mmaction2-1.0.0rc3/mmaction/apis/inferencers/actionrecog_inferencer.py` & `mmaction2-1.1.0/mmaction/apis/inferencers/actionrecog_inferencer.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from mmengine.fileio import list_from_file
 from mmengine.infer.infer import BaseInferencer, ModelType
 from mmengine.registry import init_default_scope
 from mmengine.structures import InstanceData
 
 from mmaction.registry import INFERENCERS
 from mmaction.structures import ActionDataSample
-from mmaction.utils import ConfigType
+from mmaction.utils import ConfigType, get_str_type
 
 InstanceList = List[InstanceData]
 InputType = Union[str, np.ndarray]
 InputsType = Union[InputType, Sequence[InputType]]
 PredType = Union[InstanceData, InstanceList]
 ImgType = Union[np.ndarray, Sequence[np.ndarray]]
 ResType = Union[Dict, List[Dict], InstanceData, List[InstanceData]]
@@ -163,42 +163,43 @@
 
     def _init_pipeline(self, cfg: ConfigType) -> Compose:
         """Initialize the test pipeline."""
         test_pipeline = cfg.test_dataloader.dataset.pipeline
         # Alter data pipelines for decode
         if self.input_format == 'array':
             for i in range(len(test_pipeline)):
-                if 'Decode' in test_pipeline[i]['type']:
+                if 'Decode' in get_str_type(test_pipeline[i]['type']):
                     test_pipeline[i] = dict(type='ArrayDecode')
             test_pipeline = [
                 x for x in test_pipeline if 'Init' not in x['type']
             ]
         elif self.input_format == 'video':
-            if 'Init' not in test_pipeline[0]['type']:
+            if 'Init' not in get_str_type(test_pipeline[0]['type']):
                 test_pipeline = [dict(type='DecordInit')] + test_pipeline
             else:
                 test_pipeline[0] = dict(type='DecordInit')
             for i in range(len(test_pipeline)):
-                if 'Decode' in test_pipeline[i]['type']:
+                if 'Decode' in get_str_type(test_pipeline[i]['type']):
                     test_pipeline[i] = dict(type='DecordDecode')
         elif self.input_format == 'rawframes':
-            if 'Init' in test_pipeline[0]['type']:
+            if 'Init' in get_str_type(test_pipeline[0]['type']):
                 test_pipeline = test_pipeline[1:]
             for i in range(len(test_pipeline)):
-                if 'Decode' in test_pipeline[i]['type']:
+                if 'Decode' in get_str_type(test_pipeline[i]['type']):
                     test_pipeline[i] = dict(type='RawFrameDecode')
         # Alter data pipelines to close TTA, avoid OOM
         # Use center crop instead of multiple crop
         for i in range(len(test_pipeline)):
-            if test_pipeline[i]['type'] in ['ThreeCrop', 'TenCrop']:
+            if get_str_type(
+                    test_pipeline[i]['type']) in ['ThreeCrop', 'TenCrop']:
                 test_pipeline[i]['type'] = 'CenterCrop'
         # Use single clip for `Recognizer3D`
         if cfg.model.type == 'Recognizer3D':
             for i in range(len(test_pipeline)):
-                if test_pipeline[i]['type'] == 'SampleFrames':
+                if get_str_type(test_pipeline[i]['type']) == 'SampleFrames':
                     test_pipeline[i]['num_clips'] = 1
         # Pack multiple types of input format
         test_pipeline.insert(
             0,
             dict(
                 type='InferencerPackInput',
                 input_format=self.input_format,
```

### Comparing `mmaction2-1.0.0rc3/mmaction/apis/inferencers/mmaction2_inferencer.py` & `mmaction2-1.1.0/mmaction/apis/inferencers/mmaction2_inferencer.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/__init__.py` & `mmaction2-1.1.0/mmaction/datasets/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -4,13 +4,14 @@
 from .ava_dataset import AVADataset, AVAKineticsDataset
 from .base import BaseActionDataset
 from .pose_dataset import PoseDataset
 from .rawframe_dataset import RawframeDataset
 from .repeat_aug_dataset import RepeatAugDataset, repeat_pseudo_collate
 from .transforms import *  # noqa: F401, F403
 from .video_dataset import VideoDataset
+from .video_text_dataset import VideoTextDataset
 
 __all__ = [
     'AVADataset', 'AVAKineticsDataset', 'ActivityNetDataset', 'AudioDataset',
     'BaseActionDataset', 'PoseDataset', 'RawframeDataset', 'RepeatAugDataset',
-    'VideoDataset', 'repeat_pseudo_collate'
+    'VideoDataset', 'repeat_pseudo_collate', 'VideoTextDataset'
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/activitynet_dataset.py` & `mmaction2-1.1.0/mmaction/datasets/activitynet_dataset.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/audio_dataset.py` & `mmaction2-1.1.0/mmaction/datasets/audio_dataset.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/ava_dataset.py` & `mmaction2-1.1.0/mmaction/datasets/ava_dataset.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,18 +11,18 @@
 from mmaction.registry import DATASETS
 from mmaction.utils import ConfigType
 from .base import BaseActionDataset
 
 
 @DATASETS.register_module()
 class AVADataset(BaseActionDataset):
-    """AVA dataset for spatial temporal detection.
+    """STAD dataset for spatial temporal action detection.
 
-    Based on official AVA annotation files, the dataset loads raw frames,
-    bounding boxes, proposals and applies specified transformations to return
+    The dataset loads raw frames/video files, bounding boxes,
+    proposals and applies specified transformations to return
     a dict containing the frame tensors and other information.
 
     This datasets can load information from the following files:
 
     .. code-block:: txt
 
         ann_file -> ava_{train, val}_{v2.1, v2.2}.csv
@@ -58,16 +58,16 @@
         label_file (str): Path to the label file like
             ``ava_action_list_{v2.1, v2.2}.pbtxt`` or
             ``ava_action_list_{v2.1, v2.2}_for_activitynet_2019.pbtxt``.
             Defaults to None.
         filename_tmpl (str): Template for each filename.
             Defaults to 'img_{:05}.jpg'.
         start_index (int): Specify a start index for frames in consideration of
-            different filename format. However, when taking frames as input,
-            it should be set to 0, since frames from 0. Defaults to 0.
+            different filename format. It should be set to 1 for AVA, since
+            frame index start from 1 in AVA dataset. Defaults to 1.
         proposal_file (str): Path to the proposal file like
             ``ava_dense_proposals_{train, val}.FAIR.recall_93.9.pkl``.
             Defaults to None.
         person_det_score_thr (float): The threshold of person detection scores,
             bboxes with scores above the threshold will be used.
             Note that 0 <= person_det_score_thr <= 1. If no proposal has
             detection score larger than the threshold, the one with the largest
@@ -87,35 +87,43 @@
         num_max_proposals (int): Max proposals number to store.
             Defaults to 1000.
         timestamp_start (int): The start point of included timestamps. The
             default value is referred from the official website.
             Defaults to 902.
         timestamp_end (int): The end point of included timestamps. The default
             value is referred from the official website. Defaults to 1798.
-        fps (int): Overrides the default FPS for the dataset. Defaults to 30.
+        use_frames (bool): Whether to use rawframes as input.
+            Defaults to True.
+        fps (int): Overrides the default FPS for the dataset. If set to 1,
+            means counting timestamp by frame, e.g. MultiSports dataset.
+            Otherwise by second. Defaults to 30.
+        multilabel (bool): Determines whether it is a multilabel recognition
+            task. Defaults to True.
     """
 
     def __init__(self,
                  ann_file: str,
-                 exclude_file: str,
                  pipeline: List[Union[ConfigType, Callable]],
-                 label_file: str,
+                 exclude_file: Optional[str] = None,
+                 label_file: Optional[str] = None,
                  filename_tmpl: str = 'img_{:05}.jpg',
-                 start_index: int = 0,
+                 start_index: int = 1,
                  proposal_file: str = None,
                  person_det_score_thr: float = 0.9,
                  num_classes: int = 81,
                  custom_classes: Optional[List[int]] = None,
                  data_prefix: ConfigType = dict(img=''),
                  modality: str = 'RGB',
                  test_mode: bool = False,
                  num_max_proposals: int = 1000,
                  timestamp_start: int = 900,
                  timestamp_end: int = 1800,
+                 use_frames: bool = True,
                  fps: int = 30,
+                 multilabel: bool = True,
                  **kwargs) -> None:
         self._FPS = fps  # Keep this as standard
         self.custom_classes = custom_classes
         if custom_classes is not None:
             assert num_classes == len(custom_classes) + 1
             assert 0 not in custom_classes
             _, class_whitelist = read_labelmap(open(label_file))
@@ -129,14 +137,16 @@
             'The value of '
             'person_det_score_thr should in [0, 1]. ')
         self.person_det_score_thr = person_det_score_thr
         self.timestamp_start = timestamp_start
         self.timestamp_end = timestamp_end
         self.num_max_proposals = num_max_proposals
         self.filename_tmpl = filename_tmpl
+        self.use_frames = use_frames
+        self.multilabel = multilabel
 
         super().__init__(
             ann_file,
             pipeline=pipeline,
             data_prefix=data_prefix,
             test_mode=test_mode,
             num_classes=num_classes,
@@ -181,16 +191,19 @@
             bboxes.append(img_record['entity_box'])
             valid_labels = np.array([
                 selected_record['label']
                 for selected_record in selected_records
             ])
 
             # The format can be directly used by BCELossWithLogits
-            label = np.zeros(self.num_classes, dtype=np.float32)
-            label[valid_labels] = 1.
+            if self.multilabel:
+                label = np.zeros(self.num_classes, dtype=np.float32)
+                label[valid_labels] = 1.
+            else:
+                label = valid_labels
 
             labels.append(label)
             entity_ids.append(img_record['entity_id'])
 
         bboxes = np.stack(bboxes)
         labels = np.stack(labels)
         entity_ids = np.stack(entity_ids)
@@ -208,21 +221,25 @@
             label = int(line_split[6])
             if self.custom_classes is not None:
                 if label not in self.custom_classes:
                     continue
                 label = self.custom_classes.index(label)
 
             video_id = line_split[0]
-            timestamp = int(line_split[1])
+            timestamp = int(line_split[1])  # count by second or frame.
             img_key = f'{video_id},{timestamp:04d}'
 
             entity_box = np.array(list(map(float, line_split[2:6])))
             entity_id = int(line_split[7])
-            shot_info = (0, (self.timestamp_end - self.timestamp_start) *
-                         self._FPS)
+            if self.use_frames:
+                shot_info = (0, (self.timestamp_end - self.timestamp_start) *
+                             self._FPS)
+            # for video data, automatically get shot info when decoding
+            else:
+                shot_info = None
 
             video_info = dict(
                 video_id=video_id,
                 timestamp=timestamp,
                 entity_box=entity_box,
                 label=label,
                 entity_id=entity_id,
@@ -242,14 +259,16 @@
                 frame_dir=frame_dir,
                 video_id=video_id,
                 timestamp=int(timestamp),
                 img_key=img_key,
                 shot_info=shot_info,
                 fps=self._FPS,
                 ann=ann)
+            if not self.use_frames:
+                video_info['filename'] = video_info.pop('frame_dir')
             data_list.append(video_info)
 
         return data_list
 
     def filter_data(self) -> List[dict]:
         """Filter out records in the exclude_file."""
         valid_indexes = []
@@ -297,14 +316,20 @@
                     proposals = proposals[:self.num_max_proposals]
                     data_info['proposals'] = proposals[:, :4]
                     data_info['scores'] = proposals[:, 4]
                 else:
                     proposals = proposals[:self.num_max_proposals]
                     data_info['proposals'] = proposals
 
+                assert data_info['proposals'].max() <= 1 and \
+                    data_info['proposals'].min() >= 0, \
+                    (f'relative proposals invalid: max value '
+                     f'{data_info["proposals"].max()}, min value '
+                     f'{data_info["proposals"].min()}')
+
         ann = data_info.pop('ann')
         data_info['gt_bboxes'] = ann['gt_bboxes']
         data_info['gt_labels'] = ann['gt_labels']
         data_info['entity_ids'] = ann['entity_ids']
 
         return data_info
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/base.py` & `mmaction2-1.1.0/mmaction/datasets/base.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/pose_dataset.py` & `mmaction2-1.1.0/mmaction/datasets/pose_dataset.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Callable, List, Optional, Union
+import os.path as osp
+from typing import Callable, Dict, List, Optional, Union
 
-from mmengine.fileio import exists, load
+import mmengine
 
 from mmaction.registry import DATASETS
-from mmaction.utils import ConfigType
 from .base import BaseActionDataset
 
 
 @DATASETS.register_module()
 class PoseDataset(BaseActionDataset):
     """Pose dataset for action recognition.
 
@@ -17,42 +17,48 @@
 
     The ann_file is a pickle file, the json file contains a list of
     annotations, the fields of an annotation include frame_dir(video_id),
     total_frames, label, kp, kpscore.
 
     Args:
         ann_file (str): Path to the annotation file.
-        pipeline (list): A sequence of data transforms.
-        split (str, optional): The dataset split used. Only applicable to
-            ``UCF`` or ``HMDB``. Allowed choices are ``train1``, ``test1``,
-            ``train2``, ``test2``, ``train3``, ``test3``. Defaults to None.
-        start_index (int): Specify a start index for frames in consideration of
-            different filename format. Defaults to 0.
+        pipeline (list[dict | callable]): A sequence of data transforms.
+        split (str, optional): The dataset split used. For UCF101 and
+            HMDB51, allowed choices are 'train1', 'test1', 'train2',
+            'test2', 'train3', 'test3'. For NTURGB+D, allowed choices
+            are 'xsub_train', 'xsub_val', 'xview_train', 'xview_val'.
+            For NTURGB+D 120, allowed choices are 'xsub_train',
+            'xsub_val', 'xset_train', 'xset_val'. For FineGYM,
+            allowed choices are 'train', 'val'. Defaults to None.
     """
 
     def __init__(self,
                  ann_file: str,
-                 pipeline: List[Union[ConfigType, Callable]],
+                 pipeline: List[Union[Dict, Callable]],
                  split: Optional[str] = None,
-                 start_index: int = 0,
                  **kwargs) -> None:
-        # split, applicable to ``ucf101`` or ``hmdb51``
         self.split = split
         super().__init__(
-            ann_file,
-            pipeline=pipeline,
-            start_index=start_index,
-            modality='Pose',
-            **kwargs)
+            ann_file, pipeline=pipeline, modality='Pose', **kwargs)
 
-    def load_data_list(self) -> List[dict]:
+    def load_data_list(self) -> List[Dict]:
         """Load annotation file to get skeleton information."""
         assert self.ann_file.endswith('.pkl')
-        exists(self.ann_file)
-        data_list = load(self.ann_file)
+        mmengine.exists(self.ann_file)
+        data_list = mmengine.load(self.ann_file)
 
         if self.split is not None:
-            split, data = data_list['split'], data_list['annotations']
-            identifier = 'filename' if 'filename' in data[0] else 'frame_dir'
-            data_list = [x for x in data if x[identifier] in split[self.split]]
-
+            split, annos = data_list['split'], data_list['annotations']
+            identifier = 'filename' if 'filename' in annos[0] else 'frame_dir'
+            split = set(split[self.split])
+            data_list = [x for x in annos if x[identifier] in split]
+
+        # Sometimes we may need to load video from the file
+        if 'video' in self.data_prefix:
+            for item in data_list:
+                if 'filename' in item:
+                    item['filename'] = osp.join(self.data_prefix['video'],
+                                                item['filename'])
+                if 'frame_dir' in item:
+                    item['frame_dir'] = osp.join(self.data_prefix['video'],
+                                                 item['frame_dir'])
         return data_list
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/rawframe_dataset.py` & `mmaction2-1.1.0/mmaction/datasets/rawframe_dataset.py`

 * *Files 2% similar despite different names*

```diff
@@ -129,15 +129,17 @@
                 idx += 2
             else:
                 # idx for total_frames
                 video_info['total_frames'] = int(line_split[idx])
                 idx += 1
             # idx for label[s]
             label = [int(x) for x in line_split[idx:]]
-            assert label, f'missing label in line: {line}'
+            # add fake label for inference datalist without label
+            if not label:
+                label = [-1]
             if self.multi_class:
                 assert self.num_classes is not None
                 video_info['label'] = label
             else:
                 assert len(label) == 1
                 video_info['label'] = label[0]
             data_list.append(video_info)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/repeat_aug_dataset.py` & `mmaction2-1.1.0/mmaction/datasets/repeat_aug_dataset.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/transforms/__init__.py` & `mmaction2-1.1.0/mmaction/datasets/transforms/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,21 +7,23 @@
                       GenerateLocalizationLabels, ImageDecode,
                       LoadAudioFeature, LoadHVULabel, LoadLocalizationFeature,
                       LoadProposals, LoadRGBFromFile, OpenCVDecode, OpenCVInit,
                       PIMSDecode, PIMSInit, PyAVDecode, PyAVDecodeMotionVector,
                       PyAVInit, RawFrameDecode, SampleAVAFrames, SampleFrames,
                       UniformSample, UntrimmedSampleFrames)
 from .pose_transforms import (GeneratePoseTarget, GenSkeFeat, JointToBone,
-                              LoadKineticsPose, MergeSkeFeat, PadTo,
+                              LoadKineticsPose, MergeSkeFeat, MMCompact,
+                              MMDecode, MMUniformSampleFrames, PadTo,
                               PoseCompact, PoseDecode, PreNormalize2D,
                               PreNormalize3D, ToMotion, UniformSampleFrames)
 from .processing import (AudioAmplify, CenterCrop, ColorJitter, Flip, Fuse,
                          MelSpectrogram, MultiScaleCrop, RandomCrop,
                          RandomRescale, RandomResizedCrop, Resize, TenCrop,
                          ThreeCrop)
+from .text_transforms import CLIPTokenize
 from .wrappers import ImgAug, PytorchVideoWrapper, TorchVisionWrapper
 
 __all__ = [
     'ArrayDecode', 'AudioAmplify', 'AudioDecode', 'AudioDecodeInit',
     'AudioFeatureSelector', 'BuildPseudoClip', 'CenterCrop', 'ColorJitter',
     'DecordDecode', 'DecordInit', 'DecordInit', 'DenseSampleFrames', 'Flip',
     'FormatAudioShape', 'FormatGCNInput', 'FormatShape', 'Fuse', 'GenSkeFeat',
@@ -32,9 +34,10 @@
     'OpenCVDecode', 'OpenCVInit', 'OpenCVInit', 'PIMSDecode', 'PIMSInit',
     'PackActionInputs', 'PackLocalizationInputs', 'PadTo', 'PoseCompact',
     'PoseDecode', 'PreNormalize2D', 'PreNormalize3D', 'PyAVDecode',
     'PyAVDecodeMotionVector', 'PyAVInit', 'PyAVInit', 'PytorchVideoWrapper',
     'RandomCrop', 'RandomRescale', 'RandomResizedCrop', 'RawFrameDecode',
     'Resize', 'SampleAVAFrames', 'SampleFrames', 'TenCrop', 'ThreeCrop',
     'ToMotion', 'TorchVisionWrapper', 'Transpose', 'UniformSample',
-    'UniformSampleFrames', 'UntrimmedSampleFrames'
+    'UniformSampleFrames', 'UntrimmedSampleFrames', 'MMUniformSampleFrames',
+    'MMDecode', 'MMCompact', 'CLIPTokenize'
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/transforms/formatting.py` & `mmaction2-1.1.0/mmaction/datasets/transforms/formatting.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Dict, Sequence
+from typing import Dict, Optional, Sequence, Tuple
 
 import numpy as np
 import torch
 from mmcv.transforms import BaseTransform, to_tensor
 from mmengine.structures import InstanceData, LabelData
 
 from mmaction.registry import TRANSFORMS
@@ -34,68 +34,84 @@
     mapping_table = {
         'gt_bboxes': 'bboxes',
         'gt_labels': 'labels',
     }
 
     def __init__(
         self,
+        collect_keys: Optional[Tuple[str]] = None,
         meta_keys: Sequence[str] = ('img_shape', 'img_key', 'video_id',
                                     'timestamp')
     ) -> None:
+        self.collect_keys = collect_keys
         self.meta_keys = meta_keys
 
     def transform(self, results: Dict) -> Dict:
         """The transform function of :class:`PackActionInputs`.
 
         Args:
             results (dict): The result dict.
 
         Returns:
             dict: The result dict.
         """
         packed_results = dict()
-        if 'imgs' in results:
-            imgs = results['imgs']
-            packed_results['inputs'] = to_tensor(imgs)
-        elif 'keypoint' in results:
-            keypoint = results['keypoint']
-            packed_results['inputs'] = to_tensor(keypoint)
-        elif 'audios' in results:
-            audios = results['audios']
-            packed_results['inputs'] = to_tensor(audios)
+        if self.collect_keys is not None:
+            packed_results['inputs'] = dict()
+            for key in self.collect_keys:
+                packed_results['inputs'][key] = to_tensor(results[key])
         else:
-            raise ValueError(
-                'Cannot get `imgs`, `keypoint` or `audios` in the input dict '
-                'of `PackActionInputs`.')
+            if 'imgs' in results:
+                imgs = results['imgs']
+                packed_results['inputs'] = to_tensor(imgs)
+            elif 'heatmap_imgs' in results:
+                heatmap_imgs = results['heatmap_imgs']
+                packed_results['inputs'] = to_tensor(heatmap_imgs)
+            elif 'keypoint' in results:
+                keypoint = results['keypoint']
+                packed_results['inputs'] = to_tensor(keypoint)
+            elif 'audios' in results:
+                audios = results['audios']
+                packed_results['inputs'] = to_tensor(audios)
+            elif 'text' in results:
+                text = results['text']
+                packed_results['inputs'] = to_tensor(text)
+            else:
+                raise ValueError(
+                    'Cannot get `imgs`, `keypoint`, `heatmap_imgs`, '
+                    '`audios` or `text` in the input dict of '
+                    '`PackActionInputs`.')
 
         data_sample = ActionDataSample()
 
         if 'gt_bboxes' in results:
             instance_data = InstanceData()
             for key in self.mapping_table.keys():
                 instance_data[self.mapping_table[key]] = to_tensor(
                     results[key])
             data_sample.gt_instances = instance_data
 
             if 'proposals' in results:
                 data_sample.proposals = InstanceData(
                     bboxes=to_tensor(results['proposals']))
-        else:
+
+        if 'label' in results:
             label_data = LabelData()
             label_data.item = to_tensor(results['label'])
             data_sample.gt_labels = label_data
 
         img_meta = {k: results[k] for k in self.meta_keys if k in results}
         data_sample.set_metainfo(img_meta)
         packed_results['data_samples'] = data_sample
         return packed_results
 
     def __repr__(self) -> str:
         repr_str = self.__class__.__name__
-        repr_str += f'(meta_keys={self.meta_keys})'
+        repr_str += f'(collect_keys={self.collect_keys}, '
+        repr_str += f'meta_keys={self.meta_keys})'
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class PackLocalizationInputs(BaseTransform):
 
     def __init__(self, keys=(), meta_keys=('video_name', )):
@@ -123,19 +139,29 @@
             packed_results['inputs'] = torch.tensor(0.)
         else:
             raise ValueError(
                 'Cannot get "raw_feature" or "bsp_feature" in the input '
                 'dict of `PackActionInputs`.')
 
         data_sample = ActionDataSample()
-        instance_data = InstanceData()
         for key in self.keys:
-            if key in results:
+            if key not in results:
+                continue
+            if key == 'gt_bbox':
+                instance_data = InstanceData()
                 instance_data[key] = to_tensor(results[key])
-        data_sample.gt_instances = instance_data
+                data_sample.gt_instances = instance_data
+            elif key == 'proposals':
+                instance_data = InstanceData()
+                instance_data[key] = to_tensor(results[key])
+                data_sample.proposals = instance_data
+            else:
+                raise NotImplementedError(
+                    f"Key '{key}' is not supported in `PackLocalizationInputs`"
+                )
 
         img_meta = {k: results[k] for k in self.meta_keys if k in results}
         data_sample.set_metainfo(img_meta)
         packed_results['data_samples'] = data_sample
         return packed_results
 
     def __repr__(self) -> str:
@@ -174,65 +200,114 @@
 
 
 @TRANSFORMS.register_module()
 class FormatShape(BaseTransform):
     """Format final imgs shape to the given input_format.
 
     Required keys:
-        - imgs
+        - imgs (optional)
+        - heatmap_imgs (optional)
         - num_clips
         - clip_len
 
     Modified Keys:
-        - img
-        - input_shape
+        - imgs (optional)
+        - input_shape (optional)
+
+    Added Keys:
+        - heatmap_input_shape (optional)
 
     Args:
-        input_format (str): Define the final imgs format.
+        input_format (str): Define the final data format.
         collapse (bool): To collapse input_format N... to ... (NCTHW to CTHW,
             etc.) if N is 1. Should be set as True when training and testing
             detectors. Defaults to False.
     """
 
     def __init__(self, input_format: str, collapse: bool = False) -> None:
         self.input_format = input_format
         self.collapse = collapse
-        if self.input_format not in ['NCTHW', 'NCHW', 'NCHW_Flow', 'NPTCHW']:
+        if self.input_format not in [
+                'NCTHW', 'NCHW', 'NCHW_Flow', 'NCTHW_Heatmap', 'NPTCHW'
+        ]:
             raise ValueError(
                 f'The input format {self.input_format} is invalid.')
 
-    def transform(self, results: dict) -> dict:
+    def transform(self, results: Dict) -> Dict:
         """Performs the FormatShape formatting.
 
         Args:
             results (dict): The resulting dict to be modified and passed
                 to the next transform in pipeline.
         """
         if not isinstance(results['imgs'], np.ndarray):
             results['imgs'] = np.array(results['imgs'])
-        imgs = results['imgs']
+
         # [M x H x W x C]
         # M = 1 * N_crops * N_clips * T
         if self.collapse:
             assert results['num_clips'] == 1
 
         if self.input_format == 'NCTHW':
+            if 'imgs' in results:
+                imgs = results['imgs']
+                num_clips = results['num_clips']
+                clip_len = results['clip_len']
+                if isinstance(clip_len, dict):
+                    clip_len = clip_len['RGB']
+
+                imgs = imgs.reshape((-1, num_clips, clip_len) + imgs.shape[1:])
+                # N_crops x N_clips x T x H x W x C
+                imgs = np.transpose(imgs, (0, 1, 5, 2, 3, 4))
+                # N_crops x N_clips x C x T x H x W
+                imgs = imgs.reshape((-1, ) + imgs.shape[2:])
+                # M' x C x T x H x W
+                # M' = N_crops x N_clips
+                results['imgs'] = imgs
+                results['input_shape'] = imgs.shape
+
+            if 'heatmap_imgs' in results:
+                imgs = results['heatmap_imgs']
+                num_clips = results['num_clips']
+                clip_len = results['clip_len']
+                # clip_len must be a dict
+                clip_len = clip_len['Pose']
+
+                imgs = imgs.reshape((-1, num_clips, clip_len) + imgs.shape[1:])
+                # N_crops x N_clips x T x C x H x W
+                imgs = np.transpose(imgs, (0, 1, 3, 2, 4, 5))
+                # N_crops x N_clips x C x T x H x W
+                imgs = imgs.reshape((-1, ) + imgs.shape[2:])
+                # M' x C x T x H x W
+                # M' = N_crops x N_clips
+                results['heatmap_imgs'] = imgs
+                results['heatmap_input_shape'] = imgs.shape
+
+        elif self.input_format == 'NCTHW_Heatmap':
             num_clips = results['num_clips']
             clip_len = results['clip_len']
+            imgs = results['imgs']
 
             imgs = imgs.reshape((-1, num_clips, clip_len) + imgs.shape[1:])
-            # N_crops x N_clips x T x H x W x C
-            imgs = np.transpose(imgs, (0, 1, 5, 2, 3, 4))
+            # N_crops x N_clips x T x C x H x W
+            imgs = np.transpose(imgs, (0, 1, 3, 2, 4, 5))
             # N_crops x N_clips x C x T x H x W
             imgs = imgs.reshape((-1, ) + imgs.shape[2:])
             # M' x C x T x H x W
             # M' = N_crops x N_clips
+            results['imgs'] = imgs
+            results['input_shape'] = imgs.shape
+
         elif self.input_format == 'NCHW':
+            imgs = results['imgs']
             imgs = np.transpose(imgs, (0, 3, 1, 2))
             # M x C x H x W
+            results['imgs'] = imgs
+            results['input_shape'] = imgs.shape
+
         elif self.input_format == 'NCHW_Flow':
             num_imgs = len(results['imgs'])
             assert num_imgs % 2 == 0
             n = num_imgs // 2
             h, w = results['imgs'][0].shape
             x_flow = np.empty((n, h, w), dtype=np.float32)
             y_flow = np.empty((n, h, w), dtype=np.float32)
@@ -248,34 +323,39 @@
             imgs = np.transpose(imgs, (0, 1, 2, 5, 3, 4))
             # N_crops x N_clips x T x C x H x W
             imgs = imgs.reshape((-1, imgs.shape[2] * imgs.shape[3]) +
                                 imgs.shape[4:])
             # M' x C' x H x W
             # M' = N_crops x N_clips
             # C' = T x C
+            results['imgs'] = imgs
+            results['input_shape'] = imgs.shape
+
         elif self.input_format == 'NPTCHW':
             num_proposals = results['num_proposals']
             num_clips = results['num_clips']
             clip_len = results['clip_len']
+            imgs = results['imgs']
             imgs = imgs.reshape((num_proposals, num_clips * clip_len) +
                                 imgs.shape[1:])
             # P x M x H x W x C
             # M = N_clips x T
             imgs = np.transpose(imgs, (0, 1, 4, 2, 3))
             # P x M x C x H x W
+            results['imgs'] = imgs
+            results['input_shape'] = imgs.shape
 
         if self.collapse:
-            assert imgs.shape[0] == 1
-            imgs = imgs.squeeze(0)
+            assert results['imgs'].shape[0] == 1
+            results['imgs'] = results['imgs'].squeeze(0)
+            results['input_shape'] = results['imgs'].shape
 
-        results['imgs'] = imgs
-        results['input_shape'] = imgs.shape
         return results
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr_str = self.__class__.__name__
         repr_str += f"(input_format='{self.input_format}')"
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class FormatAudioShape(BaseTransform):
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/transforms/loading.py` & `mmaction2-1.1.0/mmaction/datasets/transforms/loading.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy as cp
 import io
 import os
 import os.path as osp
 import shutil
-from typing import Optional, Union
+from typing import Dict, List, Optional, Union
 
 import mmcv
 import numpy as np
 import torch
 from mmcv.transforms import BaseTransform
 from mmengine.fileio import FileClient
 
@@ -352,15 +352,15 @@
         Args:
             results (dict): The resulting dict to be modified and passed
                 to the next transform in pipeline.
         """
         total_frames = results['total_frames']
         # if can't get fps, same value of `fps` and `target_fps`
         # will perform nothing
-        fps = results.get('fps')
+        fps = results.get('avg_fps')
         if self.target_fps is None or not fps:
             fps_scale_ratio = 1.0
         else:
             fps_scale_ratio = fps / self.target_fps
         ori_clip_len = self._get_ori_clip_len(fps_scale_ratio)
         clip_offsets = self._sample_clips(total_frames, ori_clip_len)
 
@@ -407,62 +407,69 @@
                     f'out_of_bound_opt={self.out_of_bound_opt}, '
                     f'test_mode={self.test_mode})')
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class UniformSample(BaseTransform):
-    """Uniformly sample frames from the video. Currently used for Something-
-    Something V2 dataset. Modified from
-    https://github.com/facebookresearch/SlowFast/blob/64a
+    """Uniformly sample frames from the video.
+
+    Modified from https://github.com/facebookresearch/SlowFast/blob/64a
     bcc90ccfdcbb11cf91d6e525bed60e92a8796/slowfast/datasets/ssv2.py#L159.
 
-    To sample an n-frame clip from the video. UniformSampleFrames basically
+    To sample an n-frame clip from the video. UniformSample basically
     divides the video into n segments of equal length and randomly samples one
     frame from each segment.
 
     Required keys:
 
-    - total_frames
-    - start_index
+        - total_frames
+        - start_index
 
     Added keys:
 
-    - frame_inds
-    - clip_len
-    - frame_interval
-    - num_clips
+        - frame_inds
+        - clip_len
+        - frame_interval
+        - num_clips
 
     Args:
         clip_len (int): Frames of each sampled output clip.
-        num_clips (int): Number of clips to be sampled. Default: 1.
+        num_clips (int): Number of clips to be sampled. Defaults to 1.
         test_mode (bool): Store True when building test or validation dataset.
-            Default: False.
+            Defaults to False.
     """
 
     def __init__(self,
                  clip_len: int,
                  num_clips: int = 1,
                  test_mode: bool = False) -> None:
 
         self.clip_len = clip_len
         self.num_clips = num_clips
         self.test_mode = test_mode
 
-    def _get_sample_clips(self, num_frames: int) -> np.array:
-        """When video frames is shorter than target clip len, this strategy
-        would repeat sample frame, rather than loop sample in 'loop' mode. In
-        test mode, this strategy would sample the middle frame of each segment,
-        rather than set a random seed, and therefore only support sample 1
-        clip.
+    def _get_sample_clips(self, num_frames: int) -> np.ndarray:
+        """To sample an n-frame clip from the video. UniformSample basically
+        divides the video into n segments of equal length and randomly samples
+        one frame from each segment. When the duration of video frames is
+        shorter than the desired length of the target clip, this approach will
+        duplicate the sampled frame instead of looping the sample in "loop"
+        mode. In the test mode, when we need to sample multiple clips,
+        specifically 'n' clips, this method will further divide the segments
+        based on the number of clips to be sampled. The 'i-th' clip will.
+
+        sample the frame located at the position 'i * len(segment) / n'
+        within the segment.
 
         Args:
             num_frames (int): Total number of frame in the video.
+
         Returns:
-            seq (list): the indexes of frames of sampled from the video.
+            seq (np.ndarray): the indexes of frames of sampled from the video.
         """
         seg_size = float(num_frames - 1) / self.clip_len
         inds = []
         if not self.test_mode:
             for i in range(self.clip_len):
                 start = int(np.round(seg_size * i))
                 end = int(np.round(seg_size * (i + 1)))
@@ -473,81 +480,96 @@
                 for i in range(self.clip_len):
                     start = int(np.round(seg_size * i))
                     frame_index = start + int(duration * (k + 1))
                     inds.append(frame_index)
 
         return np.array(inds)
 
-    def transform(self, results: dict):
+    def transform(self, results: Dict) -> Dict:
+        """Perform the Uniform Sampling.
+
+        Args:
+            results (dict): The result dict.
+
+        Returns:
+            dict: The result dict.
+        """
         num_frames = results['total_frames']
 
         inds = self._get_sample_clips(num_frames)
         start_index = results['start_index']
         inds = inds + start_index
 
         results['frame_inds'] = inds.astype(np.int32)
         results['clip_len'] = self.clip_len
         results['frame_interval'] = None
         results['num_clips'] = self.num_clips
         return results
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr_str = (f'{self.__class__.__name__}('
                     f'clip_len={self.clip_len}, '
                     f'num_clips={self.num_clips}, '
                     f'test_mode={self.test_mode}')
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class UntrimmedSampleFrames(BaseTransform):
     """Sample frames from the untrimmed video.
 
     Required keys are "filename", "total_frames", added or modified keys are
-    "frame_inds", "frame_interval" and "num_clips".
+    "frame_inds", "clip_interval" and "num_clips".
 
     Args:
-        clip_len (int): The length of sampled clips. Default: 1.
+        clip_len (int): The length of sampled clips. Defaults to  1.
+        clip_interval (int): Clip interval of adjacent center of sampled
+            clips. Defaults to 16.
         frame_interval (int): Temporal interval of adjacent sampled frames.
-            Default: 16.
+            Defaults to 1.
     """
 
-    def __init__(self, clip_len=1, frame_interval=16):
+    def __init__(self, clip_len=1, clip_interval=16, frame_interval=1):
         self.clip_len = clip_len
+        self.clip_interval = clip_interval
         self.frame_interval = frame_interval
 
     def transform(self, results):
         """Perform the SampleFrames loading.
 
         Args:
             results (dict): The resulting dict to be modified and passed
                 to the next transform in pipeline.
         """
         total_frames = results['total_frames']
         start_index = results['start_index']
 
-        clip_centers = np.arange(self.frame_interval // 2, total_frames,
-                                 self.frame_interval)
+        clip_centers = np.arange(self.clip_interval // 2, total_frames,
+                                 self.clip_interval)
         num_clips = clip_centers.shape[0]
         frame_inds = clip_centers[:, None] + np.arange(
-            -(self.clip_len // 2), self.clip_len -
-            (self.clip_len // 2))[None, :]
+            -(self.clip_len // 2 * self.frame_interval),
+            self.frame_interval *
+            (self.clip_len -
+             (self.clip_len // 2)), self.frame_interval)[None, :]
         # clip frame_inds to legal range
         frame_inds = np.clip(frame_inds, 0, total_frames - 1)
 
         frame_inds = np.concatenate(frame_inds) + start_index
         results['frame_inds'] = frame_inds.astype(np.int32)
         results['clip_len'] = self.clip_len
+        results['clip_interval'] = self.clip_interval
         results['frame_interval'] = self.frame_interval
         results['num_clips'] = num_clips
         return results
 
     def __repr__(self):
         repr_str = (f'{self.__class__.__name__}('
                     f'clip_len={self.clip_len}, '
+                    f'clip_interval={self.clip_interval}, '
                     f'frame_interval={self.frame_interval})')
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class DenseSampleFrames(SampleFrames):
     """Select frames from the video by dense sample strategy.
@@ -727,23 +749,26 @@
         Args:
             results (dict): The resulting dict to be modified and passed
                 to the next transform in pipeline.
         """
         fps = results['fps']
         timestamp = results['timestamp']
         timestamp_start = results['timestamp_start']
-        shot_info = results['shot_info']
+        start_index = results.get('start_index', 0)
+        if results.get('total_frames') is not None:
+            shot_info = (0, results['total_frames'])
+        else:
+            shot_info = results['shot_info']
 
-        center_index = fps * (timestamp - timestamp_start) + 1
+        center_index = fps * (timestamp - timestamp_start) + start_index
 
         skip_offsets = np.random.randint(
             -self.frame_interval // 2, (self.frame_interval + 1) // 2,
             size=self.clip_len)
         frame_inds = self._get_clips(center_index, skip_offsets, shot_info)
-        start_index = results.get('start_index', 0)
 
         frame_inds = np.array(frame_inds, dtype=np.int32) + start_index
         results['frame_inds'] = frame_inds
         results['clip_len'] = self.clip_len
         results['frame_interval'] = self.frame_interval
         results['num_clips'] = 1
         results['crop_quadruple'] = np.array([0, 0, 1, 1], dtype=np.float32)
@@ -1073,117 +1098,162 @@
 
 @TRANSFORMS.register_module()
 class DecordInit(BaseTransform):
     """Using decord to initialize the video_reader.
 
     Decord: https://github.com/dmlc/decord
 
-    Required keys are "filename",
-    added or modified keys are "video_reader" and "total_frames".
+    Required Keys:
+
+        - filename
+
+    Added Keys:
+
+        - video_reader
+        - total_frames
+        - fps
 
     Args:
         io_backend (str): io backend where frames are store.
-            Default: 'disk'.
-        num_threads (int): Number of thread to decode the video. Default: 1.
+            Defaults to ``'disk'``.
+        num_threads (int): Number of thread to decode the video. Defaults to 1.
         kwargs (dict): Args for file client.
     """
 
-    def __init__(self, io_backend='disk', num_threads=1, **kwargs):
+    def __init__(self,
+                 io_backend: str = 'disk',
+                 num_threads: int = 1,
+                 **kwargs) -> None:
         self.io_backend = io_backend
         self.num_threads = num_threads
         self.kwargs = kwargs
         self.file_client = None
 
-    def transform(self, results):
-        """Perform the Decord initialization.
-
-        Args:
-            results (dict): The resulting dict to be modified and passed
-                to the next transform in pipeline.
-        """
+    def _get_video_reader(self, filename: str) -> object:
+        if osp.splitext(filename)[0] == filename:
+            filename = filename + '.mp4'
         try:
             import decord
         except ImportError:
             raise ImportError(
                 'Please run "pip install decord" to install Decord first.')
 
         if self.file_client is None:
             self.file_client = FileClient(self.io_backend, **self.kwargs)
-
-        file_obj = io.BytesIO(self.file_client.get(results['filename']))
+        file_obj = io.BytesIO(self.file_client.get(filename))
         container = decord.VideoReader(file_obj, num_threads=self.num_threads)
-        results['fps'] = container.get_avg_fps()
-        results['video_reader'] = container
+        return container
+
+    def transform(self, results: Dict) -> Dict:
+        """Perform the Decord initialization.
+
+        Args:
+            results (dict): The result dict.
+
+        Returns:
+            dict: The result dict.
+        """
+        container = self._get_video_reader(results['filename'])
         results['total_frames'] = len(container)
+
+        results['video_reader'] = container
+        results['avg_fps'] = container.get_avg_fps()
         return results
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr_str = (f'{self.__class__.__name__}('
                     f'io_backend={self.io_backend}, '
                     f'num_threads={self.num_threads})')
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class DecordDecode(BaseTransform):
     """Using decord to decode the video.
 
     Decord: https://github.com/dmlc/decord
 
-    Required keys are "video_reader", "filename" and "frame_inds",
-    added or modified keys are "imgs" and "original_shape".
+    Required Keys:
+
+        - video_reader
+        - frame_inds
+
+    Added Keys:
+
+        - imgs
+        - original_shape
+        - img_shape
 
     Args:
         mode (str): Decoding mode. Options are 'accurate' and 'efficient'.
             If set to 'accurate', it will decode videos into accurate frames.
             If set to 'efficient', it will adopt fast seeking but only return
             key frames, which may be duplicated and inaccurate, and more
-            suitable for large scene-based video datasets. Default: 'accurate'.
+            suitable for large scene-based video datasets.
+            Defaults to ``'accurate'``.
     """
 
-    def __init__(self, mode='accurate'):
+    def __init__(self, mode: str = 'accurate') -> None:
         self.mode = mode
         assert mode in ['accurate', 'efficient']
 
-    def transform(self, results):
-        """Perform the Decord decoding.
-
-        Args:
-            results (dict): The resulting dict to be modified and passed
-                to the next transform in pipeline.
-        """
-        container = results['video_reader']
-
-        if results['frame_inds'].ndim != 1:
-            results['frame_inds'] = np.squeeze(results['frame_inds'])
-
-        frame_inds = results['frame_inds']
-
+    def _decord_load_frames(self, container: object,
+                            frame_inds: np.ndarray) -> List[np.ndarray]:
         if self.mode == 'accurate':
             imgs = container.get_batch(frame_inds).asnumpy()
             imgs = list(imgs)
         elif self.mode == 'efficient':
             # This mode is faster, however it always returns I-FRAME
             container.seek(0)
             imgs = list()
             for idx in frame_inds:
                 container.seek(idx)
                 frame = container.next()
                 imgs.append(frame.asnumpy())
+        return imgs
+
+    def transform(self, results: Dict) -> Dict:
+        """Perform the Decord decoding.
+
+        Args:
+            results (dict): The result dict.
+
+        Returns:
+            dict: The result dict.
+        """
+        container = results['video_reader']
+
+        if results['frame_inds'].ndim != 1:
+            results['frame_inds'] = np.squeeze(results['frame_inds'])
+
+        frame_inds = results['frame_inds']
+        imgs = self._decord_load_frames(container, frame_inds)
 
         results['video_reader'] = None
         del container
 
         results['imgs'] = imgs
         results['original_shape'] = imgs[0].shape[:2]
         results['img_shape'] = imgs[0].shape[:2]
 
+        # we resize the gt_bboxes and proposals to their real scale
+        if 'gt_bboxes' in results:
+            h, w = results['img_shape']
+            scale_factor = np.array([w, h, w, h])
+            gt_bboxes = results['gt_bboxes']
+            gt_bboxes = (gt_bboxes * scale_factor).astype(np.float32)
+            results['gt_bboxes'] = gt_bboxes
+            if 'proposals' in results and results['proposals'] is not None:
+                proposals = results['proposals']
+                proposals = (proposals * scale_factor).astype(np.float32)
+                results['proposals'] = proposals
+
         return results
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr_str = f'{self.__class__.__name__}(mode={self.mode})'
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class OpenCVInit(BaseTransform):
     """Using OpenCV to initialize the video_reader.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/transforms/pose_transforms.py` & `mmaction2-1.1.0/mmaction/datasets/transforms/pose_transforms.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import copy as cp
 import pickle
-from typing import Dict, List, Tuple
+from typing import Dict, List, Optional, Tuple, Union
 
 import numpy as np
 from mmcv.transforms import BaseTransform, KeyMapper
 from mmengine.dataset import Compose
 from mmengine.fileio import FileClient
 from scipy.stats import mode
 from torch.nn.modules.utils import _pair
 
 from mmaction.registry import TRANSFORMS
-from .processing import Flip, _combine_quadruple
+from .loading import DecordDecode, DecordInit
+from .processing import _combine_quadruple
 
 
 @TRANSFORMS.register_module()
 class LoadKineticsPose(BaseTransform):
     """Load Kinetics Pose given filename (The format should be pickle)
 
     Required keys are "filename", "total_frames", "img_shape", "frame_inds",
@@ -116,15 +117,18 @@
 
         h, w = results['img_shape']
         if self.source == 'openpose-18':
             kps[:, :, 0] *= w
             kps[:, :, 1] *= h
 
         num_kp = kps.shape[1]
-        num_person = mode(frame_inds)[-1][0]
+        num_person = mode(frame_inds)[-1]
+        # Ensure compatibility with lower version of scipy
+        if isinstance(num_person, np.ndarray):
+            num_person = num_person[0]
 
         new_kp = np.zeros([num_person, total_frames, num_kp, 2],
                           dtype=np.float16)
         new_kpscore = np.zeros([num_person, total_frames, num_kp],
                                dtype=np.float16)
         # 32768 is enough
         num_person_frame = np.zeros([total_frames], dtype=np.int16)
@@ -168,50 +172,73 @@
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class GeneratePoseTarget(BaseTransform):
     """Generate pseudo heatmaps based on joint coordinates and confidence.
 
-    Required keys are "keypoint", "img_shape", "keypoint_score" (optional),
-    added or modified keys are "imgs".
+    Required Keys:
+
+        - keypoint
+        - keypoint_score (optional)
+        - img_shape
+
+    Added Keys:
+
+        - imgs (optional)
+        - heatmap_imgs (optional)
 
     Args:
-        sigma (float): The sigma of the generated gaussian map. Default: 0.6.
+        sigma (float): The sigma of the generated gaussian map.
+            Defaults to 0.6.
         use_score (bool): Use the confidence score of keypoints as the maximum
-            of the gaussian maps. Default: True.
-        with_kp (bool): Generate pseudo heatmaps for keypoints. Default: True.
+            of the gaussian maps. Defaults to True.
+        with_kp (bool): Generate pseudo heatmaps for keypoints.
+            Defaults to True.
         with_limb (bool): Generate pseudo heatmaps for limbs. At least one of
-            'with_kp' and 'with_limb' should be True. Default: False.
+            'with_kp' and 'with_limb' should be True. Defaults to False.
         skeletons (tuple[tuple]): The definition of human skeletons.
-            Default: ((0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (5, 7), (7, 9),
-                      (0, 6), (6, 8), (8, 10), (5, 11), (11, 13), (13, 15),
-                      (6, 12), (12, 14), (14, 16), (11, 12)),
+            Defaults to ``((0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (5, 7),
+                         (7, 9), (0, 6), (6, 8), (8, 10), (5, 11), (11, 13),
+                         (13, 15), (6, 12), (12, 14), (14, 16), (11, 12))``,
             which is the definition of COCO-17p skeletons.
         double (bool): Output both original heatmaps and flipped heatmaps.
-            Default: False.
+            Defaults to False.
         left_kp (tuple[int]): Indexes of left keypoints, which is used when
-            flipping heatmaps. Default: (1, 3, 5, 7, 9, 11, 13, 15),
+            flipping heatmaps. Defaults to (1, 3, 5, 7, 9, 11, 13, 15),
             which is left keypoints in COCO-17p.
         right_kp (tuple[int]): Indexes of right keypoints, which is used when
-            flipping heatmaps. Default: (2, 4, 6, 8, 10, 12, 14, 16),
+            flipping heatmaps. Defaults to (2, 4, 6, 8, 10, 12, 14, 16),
             which is right keypoints in COCO-17p.
+        left_limb (tuple[int]): Indexes of left limbs, which is used when
+            flipping heatmaps. Defaults to (0, 2, 4, 5, 6, 10, 11, 12),
+            which is left limbs of skeletons we defined for COCO-17p.
+        right_limb (tuple[int]): Indexes of right limbs, which is used when
+            flipping heatmaps. Defaults to (1, 3, 7, 8, 9, 13, 14, 15),
+            which is right limbs of skeletons we defined for COCO-17p.
+        scaling (float): The ratio to scale the heatmaps. Defaults to 1.
     """
 
     def __init__(self,
-                 sigma=0.6,
-                 use_score=True,
-                 with_kp=True,
-                 with_limb=False,
-                 skeletons=((0, 1), (0, 2), (1, 3), (2, 4), (0, 5), (5, 7),
-                            (7, 9), (0, 6), (6, 8), (8, 10), (5, 11), (11, 13),
-                            (13, 15), (6, 12), (12, 14), (14, 16), (11, 12)),
-                 double=False,
-                 left_kp=(1, 3, 5, 7, 9, 11, 13, 15),
-                 right_kp=(2, 4, 6, 8, 10, 12, 14, 16)):
+                 sigma: float = 0.6,
+                 use_score: bool = True,
+                 with_kp: bool = True,
+                 with_limb: bool = False,
+                 skeletons: Tuple[Tuple[int]] = ((0, 1), (0, 2), (1, 3),
+                                                 (2, 4), (0, 5), (5, 7),
+                                                 (7, 9), (0, 6), (6, 8),
+                                                 (8, 10), (5, 11), (11, 13),
+                                                 (13, 15), (6, 12), (12, 14),
+                                                 (14, 16), (11, 12)),
+                 double: bool = False,
+                 left_kp: Tuple[int] = (1, 3, 5, 7, 9, 11, 13, 15),
+                 right_kp: Tuple[int] = (2, 4, 6, 8, 10, 12, 14, 16),
+                 left_limb: Tuple[int] = (0, 2, 4, 5, 6, 10, 11, 12),
+                 right_limb: Tuple[int] = (1, 3, 7, 8, 9, 13, 14, 15),
+                 scaling: float = 1.) -> None:
 
         self.sigma = sigma
         self.use_score = use_score
         self.with_kp = with_kp
         self.with_limb = with_limb
         self.double = double
 
@@ -220,79 +247,75 @@
 
         assert self.with_kp or self.with_limb, (
             'At least one of "with_limb" '
             'and "with_kp" should be set as True.')
         self.left_kp = left_kp
         self.right_kp = right_kp
         self.skeletons = skeletons
+        self.left_limb = left_limb
+        self.right_limb = right_limb
+        self.scaling = scaling
 
-    def generate_a_heatmap(self, img_h, img_w, centers, sigma, max_values):
+    def generate_a_heatmap(self, arr: np.ndarray, centers: np.ndarray,
+                           max_values: np.ndarray) -> None:
         """Generate pseudo heatmap for one keypoint in one frame.
 
         Args:
-            img_h (int): The height of the heatmap.
-            img_w (int): The width of the heatmap.
+            arr (np.ndarray): The array to store the generated heatmaps.
+                Shape: img_h * img_w.
             centers (np.ndarray): The coordinates of corresponding keypoints
-                (of multiple persons).
-            sigma (float): The sigma of generated gaussian.
-            max_values (np.ndarray): The max values of each keypoint.
-
-        Returns:
-            np.ndarray: The generated pseudo heatmap.
+                (of multiple persons). Shape: M * 2.
+            max_values (np.ndarray): The max values of each keypoint. Shape: M.
         """
 
-        heatmap = np.zeros([img_h, img_w], dtype=np.float32)
+        sigma = self.sigma
+        img_h, img_w = arr.shape
 
         for center, max_value in zip(centers, max_values):
-            mu_x, mu_y = center[0], center[1]
             if max_value < self.eps:
                 continue
 
+            mu_x, mu_y = center[0], center[1]
             st_x = max(int(mu_x - 3 * sigma), 0)
             ed_x = min(int(mu_x + 3 * sigma) + 1, img_w)
             st_y = max(int(mu_y - 3 * sigma), 0)
             ed_y = min(int(mu_y + 3 * sigma) + 1, img_h)
             x = np.arange(st_x, ed_x, 1, np.float32)
             y = np.arange(st_y, ed_y, 1, np.float32)
 
             # if the keypoint not in the heatmap coordinate system
             if not (len(x) and len(y)):
                 continue
             y = y[:, None]
 
             patch = np.exp(-((x - mu_x)**2 + (y - mu_y)**2) / 2 / sigma**2)
             patch = patch * max_value
-            heatmap[st_y:ed_y,
-                    st_x:ed_x] = np.maximum(heatmap[st_y:ed_y, st_x:ed_x],
-                                            patch)
-
-        return heatmap
+            arr[st_y:ed_y, st_x:ed_x] = \
+                np.maximum(arr[st_y:ed_y, st_x:ed_x], patch)
 
-    def generate_a_limb_heatmap(self, img_h, img_w, starts, ends, sigma,
-                                start_values, end_values):
+    def generate_a_limb_heatmap(self, arr: np.ndarray, starts: np.ndarray,
+                                ends: np.ndarray, start_values: np.ndarray,
+                                end_values: np.ndarray) -> None:
         """Generate pseudo heatmap for one limb in one frame.
 
         Args:
-            img_h (int): The height of the heatmap.
-            img_w (int): The width of the heatmap.
+            arr (np.ndarray): The array to store the generated heatmaps.
+                Shape: img_h * img_w.
             starts (np.ndarray): The coordinates of one keypoint in the
-                corresponding limbs (of multiple persons).
+                corresponding limbs. Shape: M * 2.
             ends (np.ndarray): The coordinates of the other keypoint in the
-                corresponding limbs (of multiple persons).
-            sigma (float): The sigma of generated gaussian.
+                corresponding limbs. Shape: M * 2.
             start_values (np.ndarray): The max values of one keypoint in the
-                corresponding limbs.
-            end_values (np.ndarray): The max values of the other keypoint in
-                the corresponding limbs.
-
-        Returns:
-            np.ndarray: The generated pseudo heatmap.
+                corresponding limbs. Shape: M.
+            end_values (np.ndarray): The max values of the other keypoint
+                in the corresponding limbs. Shape: M.
         """
 
-        heatmap = np.zeros([img_h, img_w], dtype=np.float32)
+        sigma = self.sigma
+        img_h, img_w = arr.shape
 
         for start, end, start_value, end_value in zip(starts, ends,
                                                       start_values,
                                                       end_values):
             value_coeff = min(start_value, end_value)
             if value_coeff < self.eps:
                 continue
@@ -321,17 +344,15 @@
             # distance to end keypoints
             d2_end = ((x - end[0])**2 + (y - end[1])**2)
 
             # the distance between start and end keypoints.
             d2_ab = ((start[0] - end[0])**2 + (start[1] - end[1])**2)
 
             if d2_ab < 1:
-                full_map = self.generate_a_heatmap(img_h, img_w, [start],
-                                                   sigma, [start_value])
-                heatmap = np.maximum(heatmap, full_map)
+                self.generate_a_heatmap(arr, start[None], start_value[None])
                 continue
 
             coeff = (d2_start - d2_end + d2_ab) / 2. / d2_ab
 
             a_dominate = coeff <= 0
             b_dominate = coeff >= 1
             seg_dominate = 1 - a_dominate - b_dominate
@@ -344,170 +365,181 @@
             d2_seg = (
                 a_dominate * d2_start + b_dominate * d2_end +
                 seg_dominate * d2_line)
 
             patch = np.exp(-d2_seg / 2. / sigma**2)
             patch = patch * value_coeff
 
-            heatmap[min_y:max_y, min_x:max_x] = np.maximum(
-                heatmap[min_y:max_y, min_x:max_x], patch)
-
-        return heatmap
+            arr[min_y:max_y, min_x:max_x] = \
+                np.maximum(arr[min_y:max_y, min_x:max_x], patch)
 
-    def generate_heatmap(self, img_h, img_w, kps, sigma, max_values):
+    def generate_heatmap(self, arr: np.ndarray, kps: np.ndarray,
+                         max_values: np.ndarray) -> None:
         """Generate pseudo heatmap for all keypoints and limbs in one frame (if
         needed).
 
         Args:
-            img_h (int): The height of the heatmap.
-            img_w (int): The width of the heatmap.
+            arr (np.ndarray): The array to store the generated heatmaps.
+                Shape: V * img_h * img_w.
             kps (np.ndarray): The coordinates of keypoints in this frame.
-            sigma (float): The sigma of generated gaussian.
+                Shape: M * V * 2.
             max_values (np.ndarray): The confidence score of each keypoint.
-
-        Returns:
-            np.ndarray: The generated pseudo heatmap.
+                Shape: M * V.
         """
 
-        heatmaps = []
         if self.with_kp:
             num_kp = kps.shape[1]
             for i in range(num_kp):
-                heatmap = self.generate_a_heatmap(img_h, img_w, kps[:, i],
-                                                  sigma, max_values[:, i])
-                heatmaps.append(heatmap)
+                self.generate_a_heatmap(arr[i], kps[:, i], max_values[:, i])
 
         if self.with_limb:
-            for limb in self.skeletons:
+            for i, limb in enumerate(self.skeletons):
                 start_idx, end_idx = limb
                 starts = kps[:, start_idx]
                 ends = kps[:, end_idx]
 
                 start_values = max_values[:, start_idx]
                 end_values = max_values[:, end_idx]
-                heatmap = self.generate_a_limb_heatmap(img_h, img_w, starts,
-                                                       ends, sigma,
-                                                       start_values,
-                                                       end_values)
-                heatmaps.append(heatmap)
-
-        return np.stack(heatmaps, axis=-1)
+                self.generate_a_limb_heatmap(arr[i], starts, ends,
+                                             start_values, end_values)
 
-    def gen_an_aug(self, results):
+    def gen_an_aug(self, results: Dict) -> np.ndarray:
         """Generate pseudo heatmaps for all frames.
 
         Args:
             results (dict): The dictionary that contains all info of a sample.
 
         Returns:
-            list[np.ndarray]: The generated pseudo heatmaps.
+            np.ndarray: The generated pseudo heatmaps.
         """
 
-        all_kps = results['keypoint']
+        all_kps = results['keypoint'].astype(np.float32)
         kp_shape = all_kps.shape
 
         if 'keypoint_score' in results:
             all_kpscores = results['keypoint_score']
         else:
             all_kpscores = np.ones(kp_shape[:-1], dtype=np.float32)
 
         img_h, img_w = results['img_shape']
+
+        # scale img_h, img_w and kps
+        img_h = int(img_h * self.scaling + 0.5)
+        img_w = int(img_w * self.scaling + 0.5)
+        all_kps[..., :2] *= self.scaling
+
         num_frame = kp_shape[1]
+        num_c = 0
+        if self.with_kp:
+            num_c += all_kps.shape[2]
+        if self.with_limb:
+            num_c += len(self.skeletons)
+
+        ret = np.zeros([num_frame, num_c, img_h, img_w], dtype=np.float32)
 
-        imgs = []
         for i in range(num_frame):
-            sigma = self.sigma
+            # M, V, C
             kps = all_kps[:, i]
-            kpscores = all_kpscores[:, i]
-
-            max_values = np.ones(kpscores.shape, dtype=np.float32)
-            if self.use_score:
-                max_values = kpscores
-
-            hmap = self.generate_heatmap(img_h, img_w, kps, sigma, max_values)
-            imgs.append(hmap)
+            # M, C
+            kpscores = all_kpscores[:, i] if self.use_score else \
+                np.ones_like(all_kpscores[:, i])
 
-        return imgs
+            self.generate_heatmap(ret[i], kps, kpscores)
+        return ret
 
-    def transform(self, results):
+    def transform(self, results: Dict) -> Dict:
         """Generate pseudo heatmaps based on joint coordinates and confidence.
 
         Args:
             results (dict): The resulting dict to be modified and passed
                 to the next transform in pipeline.
         """
-        if not self.double:
-            results['imgs'] = np.stack(self.gen_an_aug(results))
-        else:
-            results_ = cp.deepcopy(results)
-            flip = Flip(
-                flip_ratio=1, left_kp=self.left_kp, right_kp=self.right_kp)
-            results_ = flip(results_)
-            results['imgs'] = np.concatenate(
-                [self.gen_an_aug(results),
-                 self.gen_an_aug(results_)])
+        heatmap = self.gen_an_aug(results)
+        key = 'heatmap_imgs' if 'imgs' in results else 'imgs'
+
+        if self.double:
+            indices = np.arange(heatmap.shape[1], dtype=np.int64)
+            left, right = (self.left_kp, self.right_kp) if self.with_kp else (
+                self.left_limb, self.right_limb)
+            for l, r in zip(left, right):  # noqa: E741
+                indices[l] = r
+                indices[r] = l
+            heatmap_flip = heatmap[..., ::-1][:, indices]
+            heatmap = np.concatenate([heatmap, heatmap_flip])
+        results[key] = heatmap
         return results
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr_str = (f'{self.__class__.__name__}('
                     f'sigma={self.sigma}, '
                     f'use_score={self.use_score}, '
                     f'with_kp={self.with_kp}, '
                     f'with_limb={self.with_limb}, '
                     f'skeletons={self.skeletons}, '
                     f'double={self.double}, '
                     f'left_kp={self.left_kp}, '
-                    f'right_kp={self.right_kp})')
+                    f'right_kp={self.right_kp}, '
+                    f'left_limb={self.left_limb}, '
+                    f'right_limb={self.right_limb}, '
+                    f'scaling={self.scaling})')
         return repr_str
 
 
 @TRANSFORMS.register_module()
 class PoseCompact(BaseTransform):
     """Convert the coordinates of keypoints to make it more compact.
     Specifically, it first find a tight bounding box that surrounds all joints
     in each frame, then we expand the tight box by a given padding ratio. For
     example, if 'padding == 0.25', then the expanded box has unchanged center,
     and 1.25x width and height.
 
-    Required keys in results are "img_shape", "keypoint", add or modified keys
-    are "img_shape", "keypoint", "crop_quadruple".
+    Required Keys:
+
+        - keypoint
+        - img_shape
+
+    Modified Keys:
+
+        - img_shape
+        - keypoint
+
+    Added Keys:
+
+        - crop_quadruple
 
     Args:
-        padding (float): The padding size. Default: 0.25.
+        padding (float): The padding size. Defaults to 0.25.
         threshold (int): The threshold for the tight bounding box. If the width
             or height of the tight bounding box is smaller than the threshold,
-            we do not perform the compact operation. Default: 10.
+            we do not perform the compact operation. Defaults to 10.
         hw_ratio (float | tuple[float] | None): The hw_ratio of the expanded
             box. Float indicates the specific ratio and tuple indicates a
             ratio range. If set as None, it means there is no requirement on
-            hw_ratio. Default: None.
+            hw_ratio. Defaults to None.
         allow_imgpad (bool): Whether to allow expanding the box outside the
-            image to meet the hw_ratio requirement. Default: True.
-
-    Returns:
-        type: Description of returned object.
+            image to meet the hw_ratio requirement. Defaults to True.
     """
 
     def __init__(self,
-                 padding=0.25,
-                 threshold=10,
-                 hw_ratio=None,
-                 allow_imgpad=True):
+                 padding: float = 0.25,
+                 threshold: int = 10,
+                 hw_ratio: Optional[Union[float, Tuple[float]]] = None,
+                 allow_imgpad: bool = True) -> None:
 
         self.padding = padding
         self.threshold = threshold
         if hw_ratio is not None:
             hw_ratio = _pair(hw_ratio)
 
         self.hw_ratio = hw_ratio
 
         self.allow_imgpad = allow_imgpad
         assert self.padding >= 0
 
-    def transform(self, results):
+    def transform(self, results: Dict) -> Dict:
         """Convert the coordinates of keypoints to make it more compact.
 
         Args:
             results (dict): The resulting dict to be modified and passed
                 to the next transform in pipeline.
         """
         img_shape = results['img_shape']
@@ -557,15 +589,15 @@
         crop_quadruple = results.get('crop_quadruple', (0., 0., 1., 1.))
         new_crop_quadruple = (min_x / w, min_y / h, (max_x - min_x) / w,
                               (max_y - min_y) / h)
         crop_quadruple = _combine_quadruple(crop_quadruple, new_crop_quadruple)
         results['crop_quadruple'] = crop_quadruple
         return results
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr_str = (f'{self.__class__.__name__}(padding={self.padding}, '
                     f'threshold={self.threshold}, '
                     f'hw_ratio={self.hw_ratio}, '
                     f'allow_imgpad={self.allow_imgpad})')
         return repr_str
 
 
@@ -1153,25 +1185,25 @@
                 num_persons[i] = j + 1
             transitional = [False] * num_frames
             for i in range(1, num_frames - 1):
                 if num_persons[i] != num_persons[i - 1]:
                     transitional[i] = transitional[i - 1] = True
                 if num_persons[i] != num_persons[i + 1]:
                     transitional[i] = transitional[i + 1] = True
-            inds_int = inds.astype(np.int)
+            inds_int = inds.astype(np.int64)
             coeff = np.array([transitional[i] for i in inds_int])
             inds = (coeff * inds_int + (1 - coeff) * inds).astype(np.float32)
 
         results['frame_inds'] = inds.astype(np.int32)
         results['clip_len'] = self.clip_len
         results['frame_interval'] = None
         results['num_clips'] = self.num_clips
         return results
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         repr_str = (f'{self.__class__.__name__}('
                     f'clip_len={self.clip_len}, '
                     f'num_clips={self.num_clips}, '
                     f'test_mode={self.test_mode}, '
                     f'seed={self.seed})')
         return repr_str
 
@@ -1249,14 +1281,25 @@
 
     Modified Keys:
 
         - keypoint
         - keypoint_score (optional)
     """
 
+    @staticmethod
+    def _load_kp(kp: np.ndarray, frame_inds: np.ndarray) -> np.ndarray:
+        """Load keypoints according to sampled indexes."""
+        return kp[:, frame_inds].astype(np.float32)
+
+    @staticmethod
+    def _load_kpscore(kpscore: np.ndarray,
+                      frame_inds: np.ndarray) -> np.ndarray:
+        """Load keypoint scores according to sampled indexes."""
+        return kpscore[:, frame_inds].astype(np.float32)
+
     def transform(self, results: Dict) -> Dict:
         """The transform function of :class:`PoseDecode`.
 
         Args:
             results (dict): The result dict.
 
         Returns:
@@ -1270,20 +1313,260 @@
 
         if results['frame_inds'].ndim != 1:
             results['frame_inds'] = np.squeeze(results['frame_inds'])
 
         offset = results.get('offset', 0)
         frame_inds = results['frame_inds'] + offset
 
-        results['keypoint'] = results['keypoint'][:, frame_inds].astype(
-            np.float32)
-
         if 'keypoint_score' in results:
-            kpscore = results['keypoint_score']
-            results['keypoint_score'] = kpscore[:,
-                                                frame_inds].astype(np.float32)
+            results['keypoint_score'] = self._load_kpscore(
+                results['keypoint_score'], frame_inds)
+
+        results['keypoint'] = self._load_kp(results['keypoint'], frame_inds)
 
         return results
 
     def __repr__(self) -> str:
         repr_str = f'{self.__class__.__name__}()'
         return repr_str
+
+
+@TRANSFORMS.register_module()
+class MMUniformSampleFrames(UniformSampleFrames):
+    """Uniformly sample frames from the multi-modal data."""
+
+    def transform(self, results: Dict) -> Dict:
+        """The transform function of :class:`MMUniformSampleFrames`.
+
+        Args:
+            results (dict): The result dict.
+
+        Returns:
+            dict: The result dict.
+        """
+        num_frames = results['total_frames']
+        modalities = []
+        for modality, clip_len in self.clip_len.items():
+            if self.test_mode:
+                inds = self._get_test_clips(num_frames, clip_len)
+            else:
+                inds = self._get_train_clips(num_frames, clip_len)
+            inds = np.mod(inds, num_frames)
+            results[f'{modality}_inds'] = inds.astype(np.int32)
+            modalities.append(modality)
+        results['clip_len'] = self.clip_len
+        results['frame_interval'] = None
+        results['num_clips'] = self.num_clips
+        if not isinstance(results['modality'], list):
+            # should override
+            results['modality'] = modalities
+        return results
+
+
+@TRANSFORMS.register_module()
+class MMDecode(DecordInit, DecordDecode, PoseDecode):
+    """Decode RGB videos and skeletons."""
+
+    def __init__(self, io_backend: str = 'disk', **kwargs) -> None:
+        DecordInit.__init__(self, io_backend=io_backend, **kwargs)
+        DecordDecode.__init__(self)
+        self.io_backend = io_backend
+        self.kwargs = kwargs
+        self.file_client = None
+
+    def transform(self, results: Dict) -> Dict:
+        """The transform function of :class:`MMDecode`.
+
+        Args:
+            results (dict): The result dict.
+
+        Returns:
+            dict: The result dict.
+        """
+        for mod in results['modality']:
+            if results[f'{mod}_inds'].ndim != 1:
+                results[f'{mod}_inds'] = np.squeeze(results[f'{mod}_inds'])
+            frame_inds = results[f'{mod}_inds']
+            if mod == 'RGB':
+                if 'filename' not in results:
+                    results['filename'] = results['frame_dir'] + '.mp4'
+                video_reader = self._get_video_reader(results['filename'])
+                imgs = self._decord_load_frames(video_reader, frame_inds)
+                del video_reader
+                results['imgs'] = imgs
+            elif mod == 'Pose':
+                assert 'keypoint' in results
+                if 'keypoint_score' not in results:
+                    keypoint_score = [
+                        np.ones(keypoint.shape[:-1], dtype=np.float32)
+                        for keypoint in results['keypoint']
+                    ]
+                    results['keypoint_score'] = np.stack(keypoint_score)
+                results['keypoint'] = self._load_kp(results['keypoint'],
+                                                    frame_inds)
+                results['keypoint_score'] = self._load_kpscore(
+                    results['keypoint_score'], frame_inds)
+            else:
+                raise NotImplementedError(
+                    f'MMDecode: Modality {mod} not supported')
+
+        # We need to scale human keypoints to the new image size
+        if 'imgs' in results and 'keypoint' in results:
+            real_img_shape = results['imgs'][0].shape[:2]
+            if real_img_shape != results['img_shape']:
+                oh, ow = results['img_shape']
+                nh, nw = real_img_shape
+
+                assert results['keypoint'].shape[-1] in [2, 3]
+                results['keypoint'][..., 0] *= (nw / ow)
+                results['keypoint'][..., 1] *= (nh / oh)
+                results['img_shape'] = real_img_shape
+                results['original_shape'] = real_img_shape
+
+        return results
+
+    def __repr__(self) -> str:
+        repr_str = (f'{self.__class__.__name__}('
+                    f'io_backend={self.io_backend})')
+        return repr_str
+
+
+@TRANSFORMS.register_module()
+class MMCompact(BaseTransform):
+    """Convert the coordinates of keypoints and crop the images to make them
+    more compact.
+
+    Required Keys:
+
+        - imgs
+        - keypoint
+        - img_shape
+
+    Modified Keys:
+
+        - imgs
+        - keypoint
+        - img_shape
+
+    Args:
+        padding (float): The padding size. Defaults to 0.25.
+        threshold (int): The threshold for the tight bounding box. If the width
+            or height of the tight bounding box is smaller than the threshold,
+            we do not perform the compact operation. Defaults to 10.
+        hw_ratio (float | tuple[float]): The hw_ratio of the expanded
+            box. Float indicates the specific ratio and tuple indicates a
+            ratio range. If set as None, it means there is no requirement on
+            hw_ratio. Defaults to 1.
+        allow_imgpad (bool): Whether to allow expanding the box outside the
+            image to meet the hw_ratio requirement. Defaults to True.
+    """
+
+    def __init__(self,
+                 padding: float = 0.25,
+                 threshold: int = 10,
+                 hw_ratio: Union[float, Tuple[float]] = 1,
+                 allow_imgpad: bool = True) -> None:
+
+        self.padding = padding
+        self.threshold = threshold
+        if hw_ratio is not None:
+            hw_ratio = _pair(hw_ratio)
+        self.hw_ratio = hw_ratio
+        self.allow_imgpad = allow_imgpad
+        assert self.padding >= 0
+
+    def _get_box(self, keypoint: np.ndarray, img_shape: Tuple[int]) -> Tuple:
+        """Calculate the bounding box surrounding all joints in the frames."""
+        h, w = img_shape
+
+        kp_x = keypoint[..., 0]
+        kp_y = keypoint[..., 1]
+
+        min_x = np.min(kp_x[kp_x != 0], initial=np.Inf)
+        min_y = np.min(kp_y[kp_y != 0], initial=np.Inf)
+        max_x = np.max(kp_x[kp_x != 0], initial=-np.Inf)
+        max_y = np.max(kp_y[kp_y != 0], initial=-np.Inf)
+
+        # The compact area is too small
+        if max_x - min_x < self.threshold or max_y - min_y < self.threshold:
+            return 0, 0, w, h
+
+        center = ((max_x + min_x) / 2, (max_y + min_y) / 2)
+        half_width = (max_x - min_x) / 2 * (1 + self.padding)
+        half_height = (max_y - min_y) / 2 * (1 + self.padding)
+
+        if self.hw_ratio is not None:
+            half_height = max(self.hw_ratio[0] * half_width, half_height)
+            half_width = max(1 / self.hw_ratio[1] * half_height, half_width)
+
+        min_x, max_x = center[0] - half_width, center[0] + half_width
+        min_y, max_y = center[1] - half_height, center[1] + half_height
+
+        # hot update
+        if not self.allow_imgpad:
+            min_x, min_y = int(max(0, min_x)), int(max(0, min_y))
+            max_x, max_y = int(min(w, max_x)), int(min(h, max_y))
+        else:
+            min_x, min_y = int(min_x), int(min_y)
+            max_x, max_y = int(max_x), int(max_y)
+        return min_x, min_y, max_x, max_y
+
+    def _compact_images(self, imgs: List[np.ndarray], img_shape: Tuple[int],
+                        box: Tuple[int]) -> List:
+        """Crop the images acoordding the bounding box."""
+        h, w = img_shape
+        min_x, min_y, max_x, max_y = box
+        pad_l, pad_u, pad_r, pad_d = 0, 0, 0, 0
+        if min_x < 0:
+            pad_l = -min_x
+            min_x, max_x = 0, max_x + pad_l
+            w += pad_l
+        if min_y < 0:
+            pad_u = -min_y
+            min_y, max_y = 0, max_y + pad_u
+            h += pad_u
+        if max_x > w:
+            pad_r = max_x - w
+            w = max_x
+        if max_y > h:
+            pad_d = max_y - h
+            h = max_y
+
+        if pad_l > 0 or pad_r > 0 or pad_u > 0 or pad_d > 0:
+            imgs = [
+                np.pad(img, ((pad_u, pad_d), (pad_l, pad_r), (0, 0)))
+                for img in imgs
+            ]
+        imgs = [img[min_y:max_y, min_x:max_x] for img in imgs]
+        return imgs
+
+    def transform(self, results: Dict) -> Dict:
+        """The transform function of :class:`MMCompact`.
+
+        Args:
+            results (dict): The result dict.
+
+        Returns:
+            dict: The result dict.
+        """
+        img_shape = results['img_shape']
+        kp = results['keypoint']
+        # Make NaN zero
+        kp[np.isnan(kp)] = 0.
+        min_x, min_y, max_x, max_y = self._get_box(kp, img_shape)
+
+        kp_x, kp_y = kp[..., 0], kp[..., 1]
+        kp_x[kp_x != 0] -= min_x
+        kp_y[kp_y != 0] -= min_y
+
+        new_shape = (max_y - min_y, max_x - min_x)
+        results['img_shape'] = new_shape
+        results['imgs'] = self._compact_images(results['imgs'], img_shape,
+                                               (min_x, min_y, max_x, max_y))
+        return results
+
+    def __repr__(self) -> str:
+        repr_str = (f'{self.__class__.__name__}(padding={self.padding}, '
+                    f'threshold={self.threshold}, '
+                    f'hw_ratio={self.hw_ratio}, '
+                    f'allow_imgpad={self.allow_imgpad})')
+        return repr_str
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/transforms/processing.py` & `mmaction2-1.1.0/mmaction/datasets/transforms/processing.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/transforms/wrappers.py` & `mmaction2-1.1.0/mmaction/datasets/transforms/wrappers.py`

 * *Files 1% similar despite different names*

```diff
@@ -200,14 +200,17 @@
 
     Args:
         transforms (str | list[dict] | :obj:`iaa.Augmenter`): Three different
             ways to create imgaug augmenter.
     """
 
     def __init__(self, transforms):
+        # Hack to fix incompatibility of ImgAug and latest Numpy
+        if digit_version(np.__version__) >= digit_version('1.24.0'):
+            np.bool = bool
         import imgaug.augmenters as iaa
 
         if transforms == 'default':
             self.transforms = self.default_transforms()
         elif isinstance(transforms, list):
             assert all(isinstance(trans, dict) for trans in transforms)
             self.transforms = transforms
```

### Comparing `mmaction2-1.0.0rc3/mmaction/datasets/video_dataset.py` & `mmaction2-1.1.0/mmaction/datasets/video_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -82,14 +82,17 @@
         fin = list_from_file(self.ann_file)
         for line in fin:
             line_split = line.strip().split(self.delimiter)
             if self.multi_class:
                 assert self.num_classes is not None
                 filename, label = line_split[0], line_split[1:]
                 label = list(map(int, label))
+            # add fake label for inference datalist without label
+            elif len(line_split) == 1:
+                filename, label = line_split[0], -1
             else:
                 filename, label = line_split
                 label = int(label)
             if self.data_prefix['video'] is not None:
                 filename = osp.join(self.data_prefix['video'], filename)
             data_list.append(dict(filename=filename, label=label))
         return data_list
```

### Comparing `mmaction2-1.0.0rc3/mmaction/engine/hooks/output.py` & `mmaction2-1.1.0/mmaction/engine/hooks/output.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/engine/hooks/visualization_hook.py` & `mmaction2-1.1.0/mmaction/engine/hooks/visualization_hook.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/engine/model/weight_init.py` & `mmaction2-1.1.0/mmaction/engine/model/weight_init.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/engine/optimizers/layer_decay_optim_wrapper_constructor.py` & `mmaction2-1.1.0/mmaction/engine/optimizers/layer_decay_optim_wrapper_constructor.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,14 +6,36 @@
 from mmengine.dist import get_dist_info
 from mmengine.logging import MMLogger
 from mmengine.optim import DefaultOptimWrapperConstructor
 
 from mmaction.registry import OPTIM_WRAPPER_CONSTRUCTORS
 
 
+def get_layer_id_for_vit(var_name: str, max_layer_id: int) -> int:
+    """Get the layer id to set the different learning rates for ViT.
+
+    Args:
+        var_name (str): The key of the model.
+        num_max_layer (int): Maximum number of backbone layers.
+    Returns:
+        int: Returns the layer id of the key.
+    """
+
+    if var_name in ('backbone.cls_token', 'backbone.mask_token',
+                    'backbone.pos_embed'):
+        return 0
+    elif var_name.startswith('backbone.patch_embed'):
+        return 0
+    elif var_name.startswith('backbone.blocks'):
+        layer_id = int(var_name.split('.')[2])
+        return layer_id + 1
+    else:
+        return max_layer_id + 1
+
+
 def get_layer_id_for_mvit(var_name, max_layer_id):
     """Get the layer id to set the different learning rates in ``layer_wise``
     decay_type.
 
     Args:
         var_name (str): The key of the model.
         max_layer_id (int): Maximum layer id.
@@ -83,14 +105,17 @@
                 group_name = 'decay'
                 this_weight_decay = weight_decay
             if 'layer_wise' in decay_type:
                 if 'MViT' in module.backbone.__class__.__name__:
                     layer_id = get_layer_id_for_mvit(
                         name, self.paramwise_cfg.get('num_layers'))
                     logger.info(f'set param {name} as id {layer_id}')
+                elif 'VisionTransformer' in module.backbone.__class__.__name__:
+                    layer_id = get_layer_id_for_vit(name, num_layers)
+                    logger.info(f'set param {name} as id {layer_id}')
                 else:
                     raise NotImplementedError()
             else:
                 raise NotImplementedError(f'Only support layer wise decay,'
                                           f'but got {decay_type}')
 
             group_name = f'layer_{layer_id}_{group_name}'
```

### Comparing `mmaction2-1.0.0rc3/mmaction/engine/optimizers/swin_optim_wrapper_constructor.py` & `mmaction2-1.1.0/mmaction/engine/optimizers/swin_optim_wrapper_constructor.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/engine/optimizers/tsm_optim_wrapper_constructor.py` & `mmaction2-1.1.0/mmaction/engine/optimizers/tsm_optim_wrapper_constructor.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/engine/runner/multi_loop.py` & `mmaction2-1.1.0/mmaction/engine/runner/multi_loop.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/functional/__init__.py` & `mmaction2-1.1.0/mmaction/evaluation/functional/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -3,16 +3,18 @@
                        average_recall_at_avg_proposals, confusion_matrix,
                        get_weighted_score, interpolated_precision_recall,
                        mean_average_precision, mean_class_accuracy,
                        mmit_mean_average_precision, pairwise_temporal_iou,
                        softmax, top_k_accuracy, top_k_classes)
 from .ava_utils import ava_eval, read_labelmap, results2csv
 from .eval_detection import ActivityNetLocalization
+from .multisports_utils import frameAP, link_tubes, videoAP, videoAP_all
 
 __all__ = [
     'top_k_accuracy', 'mean_class_accuracy', 'confusion_matrix',
     'mean_average_precision', 'get_weighted_score',
     'average_recall_at_avg_proposals', 'pairwise_temporal_iou',
     'average_precision_at_temporal_iou', 'ActivityNetLocalization', 'softmax',
     'interpolated_precision_recall', 'mmit_mean_average_precision',
-    'top_k_classes', 'read_labelmap', 'ava_eval', 'results2csv'
+    'top_k_classes', 'read_labelmap', 'ava_eval', 'results2csv', 'frameAP',
+    'videoAP', 'link_tubes', 'videoAP_all'
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/functional/accuracy.py` & `mmaction2-1.1.0/mmaction/evaluation/functional/accuracy.py`

 * *Files 0% similar despite different names*

```diff
@@ -162,15 +162,15 @@
     Args:
         scores (list[np.ndarray]): Prediction scores of different classes for
             each sample.
         labels (list[np.ndarray]): Ground truth many-hot vector for each
             sample.
 
     Returns:
-        np.float: The MMIT style mean average precision.
+        np.float64: The MMIT style mean average precision.
     """
     results = []
     for score, label in zip(scores, labels):
         precision, recall, _ = binary_precision_recall_curve(score, label)
         ap = -np.sum(np.diff(recall) * np.array(precision)[:-1])
         results.append(ap)
     return np.mean(results)
@@ -182,15 +182,15 @@
     Args:
         scores (list[np.ndarray]): Prediction scores of different classes for
             each sample.
         labels (list[np.ndarray]): Ground truth many-hot vector for each
             sample.
 
     Returns:
-        np.float: The mean average precision.
+        np.float64: The mean average precision.
     """
     results = []
     scores = np.stack(scores).T
     labels = np.stack(labels).T
 
     for score, label in zip(scores, labels):
         precision, recall, _ = binary_precision_recall_curve(score, label)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/metrics.py` & `mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/metrics.py`

 * *Files 1% similar despite different names*

```diff
@@ -31,15 +31,15 @@
     Returns:
         precision: Fraction of positive instances over detected ones. This
             value is None if no ground truth labels are present.
         recall: Fraction of detected positive instance over all positive
             instances. This value is None if no ground truth labels are
             present.
     """
-    if (not isinstance(labels, np.ndarray) or labels.dtype != np.bool
+    if (not isinstance(labels, np.ndarray) or labels.dtype != bool
             or len(labels.shape) != 1):
         raise ValueError('labels must be single dimension bool numpy array')
 
     if not isinstance(scores, np.ndarray) or len(scores.shape) != 1:
         raise ValueError('scores must be single dimension numpy array')
 
     if num_gt < np.sum(labels):
@@ -86,15 +86,15 @@
         if recall is not None:
             raise ValueError('If precision is None, recall must also be None')
         return np.NAN
 
     if not isinstance(precision, np.ndarray) or not isinstance(
             recall, np.ndarray):
         raise ValueError('precision and recall must be numpy array')
-    if precision.dtype != np.float or recall.dtype != np.float:
+    if precision.dtype != np.float64 or recall.dtype != np.float64:
         raise ValueError('input must be float numpy array.')
     if len(precision) != len(recall):
         raise ValueError('precision and recall must be of the same size.')
     if not precision.size:
         return 0.0
     if np.amin(precision) < 0 or np.amax(precision) > 1:
         raise ValueError('Precision must be in the range of [0, 1].')
```

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/np_box_list.py` & `mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/np_box_list.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_evaluation/np_box_ops.py` & `mmaction2-1.1.0/mmaction/evaluation/functional/ava_evaluation/np_box_ops.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/functional/ava_utils.py` & `mmaction2-1.1.0/mmaction/evaluation/functional/ava_utils.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 # This piece of code is directly adapted from ActivityNet official repo
 # https://github.com/activitynet/ActivityNet/blob/master/
 # Evaluation/get_ava_performance.py. Some unused codes are removed.
 import csv
-import logging
+import multiprocessing
 import time
 from collections import defaultdict
 
 import numpy as np
 
-from .ava_evaluation import object_detection_evaluation as det_eval
-from .ava_evaluation import standard_fields
+from .ava_evaluation import metrics, np_box_list, np_box_ops
 
 
 def det2csv(results, custom_classes):
     """Convert detection results to csv file."""
     csv_results = []
     for idx in range(len(results)):
         video_id = results[idx]['video_id']
@@ -38,15 +37,15 @@
 def results2csv(results, out_file, custom_classes=None):
     """Convert detection results to csv file."""
     csv_results = det2csv(results, custom_classes)
 
     # save space for float
     def to_str(item):
         if isinstance(item, float):
-            return f'{item:.3f}'
+            return f'{item:.4f}'
         return str(item)
 
     with open(out_file, 'w') as f:
         for csv_result in csv_results:
             f.write(','.join(map(to_str, csv_result)))
             f.write('\n')
 
@@ -76,15 +75,14 @@
         boxes, given as coordinates [y1, x1, y2, x2].
         labels: A dictionary mapping each unique image key (string) to a list
         of integer class labels, matching the corresponding box in `boxes`.
         scores: A dictionary mapping each unique image key (string) to a list
         of score values labels, matching the corresponding label in `labels`.
         If scores are not provided in the csv, then they will default to 1.0.
     """
-    start = time.time()
     entries = defaultdict(list)
     boxes = defaultdict(list)
     labels = defaultdict(list)
     scores = defaultdict(list)
     reader = csv.reader(csv_file)
     for row in reader:
         assert len(row) in [7, 8], 'Wrong number of columns: ' + row
@@ -103,15 +101,14 @@
     for image_key in entries:
         # Evaluation API assumes boxes with descending scores
         entry = sorted(entries[image_key], key=lambda tup: -tup[0])
         boxes[image_key] = [x[2:] for x in entry]
         labels[image_key] = [x[1] for x in entry]
         scores[image_key] = [x[0] for x in entry]
 
-    print_time('read file ' + csv_file.name, start)
     return boxes, labels, scores
 
 
 def read_exclusions(exclusions_file):
     """Reads a CSV file of excluded timestamps.
 
     Args:
@@ -153,90 +150,151 @@
         elif line.startswith('  id:') or line.startswith('  label_id:'):
             class_id = int(line.strip().split(' ')[-1])
             labelmap.append({'id': class_id, 'name': name})
             class_ids.add(class_id)
     return labelmap, class_ids
 
 
+def get_overlaps_and_scores_box_mode(detected_boxes, detected_scores,
+                                     groundtruth_boxes):
+
+    detected_boxlist = np_box_list.BoxList(detected_boxes)
+    detected_boxlist.add_field('scores', detected_scores)
+    gt_non_group_of_boxlist = np_box_list.BoxList(groundtruth_boxes)
+
+    iou = np_box_ops.iou(detected_boxlist.get(), gt_non_group_of_boxlist.get())
+    scores = detected_boxlist.get_field('scores')
+    num_boxes = detected_boxlist.num_boxes()
+    return iou, scores, num_boxes
+
+
+def tpfp_single(tup, threshold=0.5):
+    gt_bboxes, gt_labels, bboxes, labels, scores = tup
+    ret_scores, ret_tp_fp_labels = dict(), dict()
+    all_labels = list(set(labels))
+    for label in all_labels:
+        gt_bbox = np.array(
+            [x for x, y in zip(gt_bboxes, gt_labels) if y == label],
+            dtype=np.float32).reshape(-1, 4)
+        bbox = np.array([x for x, y in zip(bboxes, labels) if y == label],
+                        dtype=np.float32).reshape(-1, 4)
+        score = np.array([x for x, y in zip(scores, labels) if y == label],
+                         dtype=np.float32).reshape(-1)
+        iou, score, num_boxes = get_overlaps_and_scores_box_mode(
+            bbox, score, gt_bbox)
+        if gt_bbox.size == 0:
+            ret_scores[label] = score
+            ret_tp_fp_labels[label] = np.zeros(num_boxes, dtype=bool)
+            continue
+        tp_fp_labels = np.zeros(num_boxes, dtype=bool)
+        if iou.shape[1] > 0:
+            max_overlap_gt_ids = np.argmax(iou, axis=1)
+            is_gt_box_detected = np.zeros(iou.shape[1], dtype=bool)
+            for i in range(num_boxes):
+                gt_id = max_overlap_gt_ids[i]
+                if iou[i, gt_id] >= threshold:
+                    if not is_gt_box_detected[gt_id]:
+                        tp_fp_labels[i] = True
+                        is_gt_box_detected[gt_id] = True
+        ret_scores[label], ret_tp_fp_labels[label] = score, tp_fp_labels
+    return ret_scores, ret_tp_fp_labels
+
+
 # Seems there is at most 100 detections for each image
 def ava_eval(result_file,
              result_type,
              label_file,
              ann_file,
              exclude_file,
              verbose=True,
+             ignore_empty_frames=True,
              custom_classes=None):
     """Perform ava evaluation."""
-    assert result_type in ['mAP']
 
+    assert result_type in ['mAP']
     start = time.time()
     categories, class_whitelist = read_labelmap(open(label_file))
     if custom_classes is not None:
         custom_classes = custom_classes[1:]
         assert set(custom_classes).issubset(set(class_whitelist))
         class_whitelist = custom_classes
         categories = [cat for cat in categories if cat['id'] in custom_classes]
 
     # loading gt, do not need gt score
-    gt_boxes, gt_labels, _ = read_csv(open(ann_file), class_whitelist)
+    gt_bboxes, gt_labels, _ = read_csv(open(ann_file), class_whitelist)
     if verbose:
-        print_time('Reading detection results', start)
+        print_time('Reading GT results', start)
 
     if exclude_file is not None:
         excluded_keys = read_exclusions(open(exclude_file))
     else:
         excluded_keys = list()
 
     start = time.time()
     boxes, labels, scores = read_csv(open(result_file), class_whitelist)
     if verbose:
-        print_time('Reading detection results', start)
-
-    # Evaluation for mAP
-    pascal_evaluator = det_eval.PascalDetectionEvaluator(categories)
+        print_time('Reading Detection results', start)
 
     start = time.time()
-    for image_key in gt_boxes:
-        if verbose and image_key in excluded_keys:
-            logging.info(
-                'Found excluded timestamp in detections: %s.'
-                'It will be ignored.', image_key)
-            continue
-        pascal_evaluator.add_single_ground_truth_image_info(
-            image_key, {
-                standard_fields.InputDataFields.groundtruth_boxes:
-                np.array(gt_boxes[image_key], dtype=float),
-                standard_fields.InputDataFields.groundtruth_classes:
-                np.array(gt_labels[image_key], dtype=int)
-            })
+    all_gt_labels = np.concatenate(list(gt_labels.values()))
+    gt_count = {k: np.sum(all_gt_labels == k) for k in class_whitelist}
+
+    pool = multiprocessing.Pool(32)
+    if ignore_empty_frames:
+        tups = [(gt_bboxes[k], gt_labels[k], boxes[k], labels[k], scores[k])
+                for k in gt_bboxes if k not in excluded_keys]
+    else:
+        tups = [(gt_bboxes.get(k, np.zeros((0, 4), dtype=np.float32)),
+                 gt_labels.get(k, []), boxes[k], labels[k], scores[k])
+                for k in boxes if k not in excluded_keys]
+    rets = pool.map(tpfp_single, tups)
+
     if verbose:
-        print_time('Convert groundtruth', start)
+        print_time('Calculating TP/FP', start)
 
     start = time.time()
-    for image_key in boxes:
-        if verbose and image_key in excluded_keys:
-            logging.info(
-                'Found excluded timestamp in detections: %s.'
-                'It will be ignored.', image_key)
-            continue
-        pascal_evaluator.add_single_detected_image_info(
-            image_key, {
-                standard_fields.DetectionResultFields.detection_boxes:
-                np.array(boxes[image_key], dtype=float),
-                standard_fields.DetectionResultFields.detection_classes:
-                np.array(labels[image_key], dtype=int),
-                standard_fields.DetectionResultFields.detection_scores:
-                np.array(scores[image_key], dtype=float)
-            })
+    scores, tpfps = defaultdict(list), defaultdict(list)
+    for score, tpfp in rets:
+        for k in score:
+            scores[k].append(score[k])
+            tpfps[k].append(tpfp[k])
+
+    cls_AP = []
+    for k in scores:
+        scores[k] = np.concatenate(scores[k])
+        tpfps[k] = np.concatenate(tpfps[k])
+        precision, recall = metrics.compute_precision_recall(
+            scores[k], tpfps[k], gt_count[k])
+        ap = metrics.compute_average_precision(precision, recall)
+        class_name = [x['name'] for x in categories if x['id'] == k]
+        assert len(class_name) == 1
+        class_name = class_name[0]
+        cls_AP.append((k, class_name, ap))
     if verbose:
-        print_time('convert detections', start)
+        print_time('Run Evaluator', start)
+
+    print('Per-class results: ', flush=True)
+    for k, class_name, ap in cls_AP:
+        print(f'Index: {k}, Action: {class_name}: AP: {ap:.4f};', flush=True)
+
+    overall = np.nanmean([x[2] for x in cls_AP])
+    person_movement = np.nanmean([x[2] for x in cls_AP if x[0] <= 14])
+    object_manipulation = np.nanmean([x[2] for x in cls_AP if 14 < x[0] < 64])
+    person_interaction = np.nanmean([x[2] for x in cls_AP if 64 <= x[0]])
+
+    print('Overall Results: ', flush=True)
+    print(f'Overall mAP: {overall:.4f}', flush=True)
+    print(f'Person Movement mAP: {person_movement:.4f}', flush=True)
+    print(f'Object Manipulation mAP: {object_manipulation:.4f}', flush=True)
+    print(f'Person Interaction mAP: {person_interaction:.4f}', flush=True)
+
+    results = {}
+    results['overall'] = overall
+    results['person_movement'] = person_movement
+    results['object_manipulation'] = object_manipulation
+    results['person_interaction'] = person_interaction
 
-    start = time.time()
-    metrics = pascal_evaluator.evaluate()
     if verbose:
-        print_time('run_evaluator', start)
-    for display_name in metrics:
-        print(f'{display_name}=\t{metrics[display_name]}')
-    return {
-        display_name: metrics[display_name]
-        for display_name in metrics if 'ByCategory' not in display_name
-    }
+        for k, class_name, ap in cls_AP:
+            print(f'Class {class_name} AP: {ap:.4f}', flush=True)
+
+    return results
```

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/functional/eval_detection.py` & `mmaction2-1.1.0/mmaction/evaluation/functional/eval_detection.py`

 * *Files 1% similar despite different names*

```diff
@@ -216,16 +216,16 @@
                 tp[t_idx, idx] = 1
                 lock_gt[t_idx, gts[j_idx]['index']] = idx
                 break
 
             if fp[t_idx, idx] == 0 and tp[t_idx, idx] == 0:
                 fp[t_idx, idx] = 1
 
-    tp_cumsum = np.cumsum(tp, axis=1).astype(np.float)
-    fp_cumsum = np.cumsum(fp, axis=1).astype(np.float)
+    tp_cumsum = np.cumsum(tp, axis=1).astype(np.float64)
+    fp_cumsum = np.cumsum(fp, axis=1).astype(np.float64)
     recall_cumsum = tp_cumsum / num_positive
 
     precision_cumsum = tp_cumsum / (tp_cumsum + fp_cumsum)
 
     for t_idx in range(len(tiou_thresholds)):
         ap[t_idx] = interpolated_precision_recall(precision_cumsum[t_idx, :],
                                                   recall_cumsum[t_idx, :])
```

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/metrics/anet_metric.py` & `mmaction2-1.1.0/mmaction/evaluation/metrics/anet_metric.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/evaluation/metrics/ava_metric.py` & `mmaction2-1.1.0/mmaction/evaluation/metrics/ava_metric.py`

 * *Files 2% similar despite different names*

```diff
@@ -77,12 +77,13 @@
 
         eval_results = ava_eval(
             temp_file,
             self.options[0],
             self.label_file,
             self.ann_file,
             self.exclude_file,
+            ignore_empty_frames=True,
             custom_classes=self.custom_classes)
 
         os.remove(temp_file)
 
         return eval_results
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/__init__.py` & `mmaction2-1.1.0/mmaction/models/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,9 +4,10 @@
 from .data_preprocessors import *  # noqa: F401,F403
 from .heads import *  # noqa: F401,F403
 from .localizers import *  # noqa: F401,F403
 from .losses import *  # noqa: F401,F403
 from .necks import *  # noqa: F401,F403
 from .recognizers import *  # noqa: F401,F403
 from .roi_heads import *  # noqa: F401,F403
+from .similarity import *  # noqa: F401,F403
 from .task_modules import *  # noqa: F401,F403
 from .utils import *  # noqa: F401,F403
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/__init__.py` & `mmaction2-1.1.0/mmaction/models/backbones/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,23 +11,25 @@
 from .resnet3d_csn import ResNet3dCSN
 from .resnet3d_slowfast import ResNet3dSlowFast
 from .resnet3d_slowonly import ResNet3dSlowOnly
 from .resnet_audio import ResNetAudio
 from .resnet_omni import OmniResNet
 from .resnet_tin import ResNetTIN
 from .resnet_tsm import ResNetTSM
+from .rgbposeconv3d import RGBPoseConv3D
 from .stgcn import STGCN
 from .swin import SwinTransformer3D
 from .tanet import TANet
 from .timesformer import TimeSformer
 from .uniformer import UniFormer
 from .uniformerv2 import UniFormerV2
 from .vit_mae import VisionTransformer
 from .x3d import X3D
 
 __all__ = [
     'AAGCN', 'C2D', 'C3D', 'MViT', 'MobileNetV2', 'MobileNetV2TSM',
     'OmniResNet', 'ResNet', 'ResNet2Plus1d', 'ResNet3d', 'ResNet3dCSN',
     'ResNet3dLayer', 'ResNet3dSlowFast', 'ResNet3dSlowOnly', 'ResNetAudio',
     'ResNetTIN', 'ResNetTSM', 'STGCN', 'SwinTransformer3D', 'TANet',
-    'TimeSformer', 'UniFormer', 'UniFormerV2', 'VisionTransformer', 'X3D'
+    'TimeSformer', 'UniFormer', 'UniFormerV2', 'VisionTransformer', 'X3D',
+    'RGBPoseConv3D'
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/aagcn.py` & `mmaction2-1.1.0/mmaction/models/backbones/aagcn.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/c2d.py` & `mmaction2-1.1.0/mmaction/models/backbones/c2d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/c3d.py` & `mmaction2-1.1.0/mmaction/models/backbones/c3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/mobilenet_v2.py` & `mmaction2-1.1.0/mmaction/models/backbones/mobilenet_v2.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,32 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Dict, List, Optional, Union
+
 import torch.nn as nn
 import torch.utils.checkpoint as cp
 from mmcv.cnn import ConvModule
-from mmengine.logging import MMLogger
-from mmengine.model.weight_init import constant_init, kaiming_init
-from mmengine.runner import load_checkpoint
+from mmengine.model import BaseModule
 from mmengine.utils.dl_utils.parrots_wrapper import _BatchNorm
 
 from mmaction.registry import MODELS
 
 
 def make_divisible(value, divisor, min_value=None, min_ratio=0.9):
     """Make divisible function.
 
     This function rounds the channel number down to the nearest value that can
     be divisible by the divisor.
     Args:
         value (int): The original channel number.
         divisor (int): The divisor to fully divide the channel number.
         min_value (int, optional): The minimum value of the output channel.
-            Default: None, means that the minimum value equal to the divisor.
+            Defaults to None, means that the minimum value equal to the
+            divisor.
         min_ratio (float, optional): The minimum ratio of the rounded channel
-            number to the original channel number. Default: 0.9.
+            number to the original channel number. Defaults to 0.9.
     Returns:
         int: The modified output channel number
     """
 
     if min_value is None:
         min_value = divisor
     new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)
@@ -41,21 +42,21 @@
     Args:
         in_channels (int): The input channels of the InvertedResidual block.
         out_channels (int): The output channels of the InvertedResidual block.
         stride (int): Stride of the middle (first) 3x3 convolution.
         expand_ratio (int): adjusts number of channels of the hidden layer
             in InvertedResidual by this amount.
         conv_cfg (dict): Config dict for convolution layer.
-            Default: None, which means using conv2d.
+            Defaults to None, which means using conv2d.
         norm_cfg (dict): Config dict for normalization layer.
-            Default: dict(type='BN').
+            Defaults to dict(type='BN').
         act_cfg (dict): Config dict for activation layer.
-            Default: dict(type='ReLU6').
+            Defaults to dict(type='ReLU6').
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
-            memory while slowing down the training speed. Default: False.
+            memory while slowing down the training speed. Defaults to False.
     Returns:
         Tensor: The output tensor
     """
 
     def __init__(self,
                  in_channels,
                  out_channels,
@@ -125,37 +126,42 @@
         else:
             out = _inner_forward(x)
 
         return out
 
 
 @MODELS.register_module()
-class MobileNetV2(nn.Module):
+class MobileNetV2(BaseModule):
     """MobileNetV2 backbone.
 
     Args:
-        pretrained (str | None): Name of pretrained model. Default: None.
+        pretrained (str | None): Name of pretrained model. Defaults to None.
         widen_factor (float): Width multiplier, multiply number of
-            channels in each layer by this amount. Default: 1.0.
+            channels in each layer by this amount. Defaults to 1.0.
         out_indices (None or Sequence[int]): Output from which stages.
-            Default: (7, ).
+            Defaults to (7, ).
         frozen_stages (int): Stages to be frozen (all param fixed). Note that
-            the last stage in ``MobileNetV2`` is ``conv2``. Default: -1,
+            the last stage in ``MobileNetV2`` is ``conv2``. Defaults to -1,
             which means not freezing any parameters.
         conv_cfg (dict): Config dict for convolution layer.
-            Default: None, which means using conv2d.
+            Defaults to None, which means using conv2d.
         norm_cfg (dict): Config dict for normalization layer.
-            Default: dict(type='BN').
+            Defaults to dict(type='BN').
         act_cfg (dict): Config dict for activation layer.
-            Default: dict(type='ReLU6').
+            Defaults to dict(type='ReLU6').
         norm_eval (bool): Whether to set norm layers to eval mode, namely,
             freeze running stats (mean and var). Note: Effect on Batch Norm
-            and its variants only. Default: False.
+            and its variants only. Defaults to False.
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
-            memory while slowing down the training speed. Default: False.
+            memory while slowing down the training speed. Defaults to False.
+        init_cfg (dict or list[dict]): Initialization config dict. Defaults to
+            ``[
+            dict(type='Kaiming', layer='Conv2d',),
+            dict(type='Constant', layer=['GroupNorm', '_BatchNorm'], val=1.)
+            ]``.
     """
 
     # Parameters to build layers. 4 parameters are needed to construct a
     # layer, from left to right: expand_ratio, channel, num_blocks, stride.
     arch_settings = [[1, 16, 1, 1], [6, 24, 2, 2], [6, 32, 3, 2],
                      [6, 64, 4, 2], [6, 96, 3, 1], [6, 160, 3, 2],
                      [6, 320, 1, 1]]
@@ -165,16 +171,25 @@
                  widen_factor=1.,
                  out_indices=(7, ),
                  frozen_stages=-1,
                  conv_cfg=dict(type='Conv'),
                  norm_cfg=dict(type='BN2d', requires_grad=True),
                  act_cfg=dict(type='ReLU6', inplace=True),
                  norm_eval=False,
-                 with_cp=False):
-        super().__init__()
+                 with_cp=False,
+                 init_cfg: Optional[Union[Dict, List[Dict]]] = [
+                     dict(type='Kaiming', layer='Conv2d'),
+                     dict(
+                         type='Constant',
+                         layer=['GroupNorm', '_BatchNorm'],
+                         val=1.)
+                 ]):
+        if pretrained is not None:
+            init_cfg = dict(type='Pretrained', checkpoint=pretrained)
+        super().__init__(init_cfg=init_cfg)
         self.pretrained = pretrained
         self.widen_factor = widen_factor
         self.out_indices = out_indices
         for index in out_indices:
             if index not in range(0, 8):
                 raise ValueError('the item in out_indices must in '
                                  f'range(0, 8). But received {index}')
@@ -235,17 +250,17 @@
 
     def make_layer(self, out_channels, num_blocks, stride, expand_ratio):
         """Stack InvertedResidual blocks to build a layer for MobileNetV2.
 
         Args:
             out_channels (int): out_channels of block.
             num_blocks (int): number of blocks.
-            stride (int): stride of the first block. Default: 1
+            stride (int): stride of the first block. Defaults to 1
             expand_ratio (int): Expand the number of channels of the
-                hidden layer in InvertedResidual by this ratio. Default: 6.
+                hidden layer in InvertedResidual by this ratio. Defaults to 6.
         """
         layers = []
         for i in range(num_blocks):
             if i >= 1:
                 stride = 1
             layers.append(
                 InvertedResidual(
@@ -257,29 +272,14 @@
                     norm_cfg=self.norm_cfg,
                     act_cfg=self.act_cfg,
                     with_cp=self.with_cp))
             self.in_channels = out_channels
 
         return nn.Sequential(*layers)
 
-    def init_weights(self):
-        """Initiate the parameters either from existing checkpoint or from
-        scratch."""
-        if isinstance(self.pretrained, str):
-            logger = MMLogger.get_current_instance()
-            load_checkpoint(self, self.pretrained, strict=False, logger=logger)
-        elif self.pretrained is None:
-            for m in self.modules():
-                if isinstance(m, nn.Conv2d):
-                    kaiming_init(m)
-                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):
-                    constant_init(m, 1)
-        else:
-            raise TypeError('pretrained must be a str or None')
-
     def forward(self, x):
         """Defines the computation performed at every call.
 
         Args:
             x (Tensor): The input data.
 
         Returns:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/mvit.py` & `mmaction2-1.1.0/mmaction/models/backbones/mvit.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,14 +10,15 @@
 from mmengine.logging import MMLogger
 from mmengine.model import BaseModule, ModuleList
 from mmengine.model.weight_init import trunc_normal_
 from mmengine.runner.checkpoint import _load_checkpoint_with_prefix
 from mmengine.utils import to_3tuple
 
 from mmaction.registry import MODELS
+from mmaction.utils import get_str_type
 from ..utils.embed import PatchEmbed3D
 
 
 def resize_pos_embed(pos_embed: torch.Tensor,
                      src_shape: Tuple[int],
                      dst_shape: Tuple[int],
                      mode: str = 'trilinear',
@@ -324,15 +325,15 @@
                 torch.zeros(2 * input_size[0] - 1, head_dim))
 
     def init_weights(self) -> None:
         """Weight initialization."""
         super().init_weights()
 
         if (isinstance(self.init_cfg, dict)
-                and self.init_cfg['type'] == 'Pretrained'):
+                and get_str_type(self.init_cfg['type']) == 'Pretrained'):
             # Suppress rel_pos_zero_init if use pretrained model.
             return
 
         if not self.rel_pos_zero_init:
             trunc_normal_(self.rel_pos_h, std=0.02)
             trunc_normal_(self.rel_pos_w, std=0.02)
             trunc_normal_(self.rel_pos_t, std=0.02)
@@ -463,15 +464,16 @@
             norm_cfg=norm_cfg,
             pool_kernel=qkv_pool_kernel,
             stride_q=stride_q,
             stride_kv=stride_kv,
             rel_pos_embed=rel_pos_embed,
             residual_pooling=residual_pooling,
             input_size=input_size,
-            rel_pos_zero_init=rel_pos_zero_init)
+            rel_pos_zero_init=rel_pos_zero_init,
+            with_cls_token=with_cls_token)
         self.drop_path = DropPath(
             drop_path) if drop_path > 0.0 else nn.Identity()
 
         self.norm2 = build_norm_layer(norm_cfg, attn_dims)[1]
 
         self.mlp = MLP(
             in_channels=attn_dims,
@@ -798,14 +800,15 @@
                 drop_path=dpr[i],
                 norm_cfg=norm_cfg,
                 qkv_pool_kernel=pool_kernel,
                 stride_q=stride_q,
                 stride_kv=stride_kv,
                 rel_pos_embed=rel_pos_embed,
                 residual_pooling=residual_pooling,
+                with_cls_token=with_cls_token,
                 dim_mul_in_attention=dim_mul_in_attention,
                 input_size=input_size,
                 rel_pos_zero_init=rel_pos_zero_init)
             self.blocks.append(attention_block)
 
             input_size = attention_block.init_out_size
             out_dims_list.append(out_dims)
@@ -848,15 +851,15 @@
             msg = self.load_state_dict(state_dict, strict=False)
             logger.info(msg)
 
         elif self.pretrained_type is None:
             super().init_weights()
 
             if (isinstance(self.init_cfg, dict)
-                    and self.init_cfg['type'] == 'Pretrained'):
+                    and get_str_type(self.init_cfg['type']) == 'Pretrained'):
                 # Suppress default init if use pretrained model.
                 return
 
         if self.use_abs_pos_embed:
             trunc_normal_(self.pos_embed, std=0.02)
 
     def forward(self, x: torch.Tensor) ->\
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from collections import OrderedDict
-from typing import List, Optional, Sequence, Tuple, Union
+from typing import Dict, List, Optional, Sequence, Tuple, Union
 
 import mmengine
 import torch
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmengine.logging import MMLogger
-from mmengine.model.weight_init import constant_init, kaiming_init
-from mmengine.runner.checkpoint import _load_checkpoint, load_checkpoint
+from mmengine.model import BaseModule
+from mmengine.runner.checkpoint import _load_checkpoint
 from mmengine.utils.dl_utils.parrots_wrapper import _BatchNorm
 from torch.utils import checkpoint as cp
 
 from mmaction.registry import MODELS
 from mmaction.utils import ConfigType
 
 
@@ -302,15 +302,15 @@
                 act_cfg=act_cfg,
                 with_cp=with_cp))
 
     return nn.Sequential(*layers)
 
 
 @MODELS.register_module()
-class ResNet(nn.Module):
+class ResNet(BaseModule):
     """ResNet backbone.
 
     Args:
         depth (int): Depth of resnet, from ``{18, 34, 50, 101, 152}``.
         pretrained (str, optional): Name of pretrained model. Defaults to None.
         torchvision_pretrain (bool): Whether to load pretrained model from
             torchvision. Defaults to True.
@@ -335,42 +335,53 @@
         act_cfg (Union[dict, ConfigDict]): Config for activate layers.
             Defaults to ``dict(type='ReLU', inplace=True)``.
         norm_eval (bool): Whether to set BN layers to eval mode, namely, freeze
             running stats (mean and var). Defaults to False.
         partial_bn (bool): Whether to use partial bn. Defaults to False.
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
             memory while slowing down the training speed. Defaults to False.
+        init_cfg (dict or list[dict]): Initialization config dict. Defaults to
+            ``[
+            dict(type='Kaiming', layer='Conv2d',),
+            dict(type='Constant', layer='BatchNorm', val=1.)
+            ]``.
     """
 
     arch_settings = {
         18: (BasicBlock, (2, 2, 2, 2)),
         34: (BasicBlock, (3, 4, 6, 3)),
         50: (Bottleneck, (3, 4, 6, 3)),
         101: (Bottleneck, (3, 4, 23, 3)),
         152: (Bottleneck, (3, 8, 36, 3))
     }
 
-    def __init__(self,
-                 depth: int,
-                 pretrained: Optional[str] = None,
-                 torchvision_pretrain: bool = True,
-                 in_channels: int = 3,
-                 num_stages: int = 4,
-                 out_indices: Sequence[int] = (3, ),
-                 strides: Sequence[int] = (1, 2, 2, 2),
-                 dilations: Sequence[int] = (1, 1, 1, 1),
-                 style: str = 'pytorch',
-                 frozen_stages: int = -1,
-                 conv_cfg: ConfigType = dict(type='Conv'),
-                 norm_cfg: ConfigType = dict(type='BN2d', requires_grad=True),
-                 act_cfg: ConfigType = dict(type='ReLU', inplace=True),
-                 norm_eval: bool = False,
-                 partial_bn: bool = False,
-                 with_cp: bool = False) -> None:
-        super().__init__()
+    def __init__(
+        self,
+        depth: int,
+        pretrained: Optional[str] = None,
+        torchvision_pretrain: bool = True,
+        in_channels: int = 3,
+        num_stages: int = 4,
+        out_indices: Sequence[int] = (3, ),
+        strides: Sequence[int] = (1, 2, 2, 2),
+        dilations: Sequence[int] = (1, 1, 1, 1),
+        style: str = 'pytorch',
+        frozen_stages: int = -1,
+        conv_cfg: ConfigType = dict(type='Conv'),
+        norm_cfg: ConfigType = dict(type='BN2d', requires_grad=True),
+        act_cfg: ConfigType = dict(type='ReLU', inplace=True),
+        norm_eval: bool = False,
+        partial_bn: bool = False,
+        with_cp: bool = False,
+        init_cfg: Optional[Union[Dict, List[Dict]]] = [
+            dict(type='Kaiming', layer='Conv2d'),
+            dict(type='Constant', layer='BatchNorm2d', val=1.)
+        ]
+    ) -> None:
+        super().__init__(init_cfg=init_cfg)
         if depth not in self.arch_settings:
             raise KeyError(f'invalid depth {depth} for resnet')
         self.depth = depth
         self.in_channels = in_channels
         self.pretrained = pretrained
         self.torchvision_pretrain = torchvision_pretrain
         self.num_stages = num_stages
@@ -493,15 +504,16 @@
                 if param.data.shape == param_tv.shape:
                     param.data.copy_(param_tv)
                     loaded_param_names.append(param_tv_name)
 
     def _load_torchvision_checkpoint(self,
                                      logger: mmengine.MMLogger = None) -> None:
         """Initiate the parameters from torchvision pretrained checkpoint."""
-        state_dict_torchvision = _load_checkpoint(self.pretrained)
+        state_dict_torchvision = _load_checkpoint(
+            self.pretrained, map_location='cpu')
         if 'state_dict' in state_dict_torchvision:
             state_dict_torchvision = state_dict_torchvision['state_dict']
 
         loaded_param_names = []
         for name, module in self.named_modules():
             if isinstance(module, ConvModule):
                 # we use a ConvModule to wrap conv+bn+relu layers, thus the
@@ -535,22 +547,20 @@
         if isinstance(self.pretrained, str):
             logger = MMLogger.get_current_instance()
             if self.torchvision_pretrain:
                 # torchvision's
                 self._load_torchvision_checkpoint(logger)
             else:
                 # ours
-                load_checkpoint(
-                    self, self.pretrained, strict=False, logger=logger)
+                if self.pretrained:
+                    self.init_cfg = dict(
+                        type='Pretrained', checkpoint=self.pretrained)
+                    super().init_weights()
         elif self.pretrained is None:
-            for m in self.modules():
-                if isinstance(m, nn.Conv2d):
-                    kaiming_init(m)
-                elif isinstance(m, nn.BatchNorm2d):
-                    constant_init(m, 1)
+            super().init_weights()
         else:
             raise TypeError('pretrained must be a str or None')
 
     def forward(self, x: torch.Tensor) \
             -> Union[torch.Tensor, Tuple[torch.Tensor]]:
         """Defines the computation performed at every call.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet2plus1d.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet2plus1d.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from mmaction.registry import MODELS
+from mmaction.utils import get_str_type
 from .resnet3d import ResNet3d
 
 
 @MODELS.register_module()
 class ResNet2Plus1d(ResNet3d):
     """ResNet (2+1)d backbone.
 
     This model is proposed in `A Closer Look at Spatiotemporal Convolutions for
     Action Recognition <https://arxiv.org/abs/1711.11248>`_
     """
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         assert self.pretrained2d is False
-        assert self.conv_cfg['type'] == 'Conv2plus1d'
+        assert get_str_type(self.conv_cfg['type']) == 'Conv2plus1d'
 
     def _freeze_stages(self):
         """Prevent all the parameters from being optimized before
         ``self.frozen_stages``."""
         if self.frozen_stages >= 0:
             self.conv1.eval()
             for param in self.conv1.parameters():
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet3d.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet3d.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,73 +1,77 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 from collections import OrderedDict
-from typing import List, Optional, Sequence, Tuple, Union
+from typing import Dict, List, Optional, Sequence, Tuple, Union
 
+import torch
+import torch.nn as nn
 import torch.utils.checkpoint as cp
 from mmcv.cnn import ConvModule, NonLocal3d, build_activation_layer
 from mmengine.logging import MMLogger
+from mmengine.model import BaseModule, Sequential
 from mmengine.model.weight_init import constant_init, kaiming_init
 from mmengine.runner.checkpoint import _load_checkpoint, load_checkpoint
 from mmengine.utils.dl_utils.parrots_wrapper import _BatchNorm
-from torch import Tensor, nn
 from torch.nn.modules.utils import _ntuple, _triple
 
 from mmaction.registry import MODELS
-from mmaction.utils import ConfigType, OptConfigType
 
 
-class BasicBlock3d(nn.Module):
+class BasicBlock3d(BaseModule):
     """BasicBlock 3d block for ResNet3D.
 
     Args:
         inplanes (int): Number of channels for the input in first conv3d layer.
         planes (int): Number of channels produced by some norm/conv3d layers.
         spatial_stride (int): Spatial stride in the conv3d layer.
             Defaults to 1.
         temporal_stride (int): Temporal stride in the conv3d layer.
             Defaults to 1.
         dilation (int): Spacing between kernel elements. Defaults to 1.
         downsample (nn.Module or None): Downsample layer. Defaults to None.
-        style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``, the
+        style (str): 'pytorch' or 'caffe'. If set to 'pytorch', the
             stride-two layer is the 3x3 conv layer, otherwise the stride-two
-            layer is the first 1x1 conv layer. Defaults to ``pytorch``.
+            layer is the first 1x1 conv layer. Defaults to ``'pytorch'``.
         inflate (bool): Whether to inflate kernel. Defaults to True.
         non_local (bool): Determine whether to apply non-local module in this
             block. Defaults to False.
-        non_local_cfg (dict or ConfigDict): Config for non-local module.
+        non_local_cfg (dict): Config for non-local module.
             Defaults to ``dict()``.
-        conv_cfg (dict or ConfigDict): Config dict for convolution layer.
+        conv_cfg (dict): Config dict for convolution layer.
             Defaults to ``dict(type='Conv3d')``.
-        norm_cfg (dict or ConfigDict): Config for norm layers.
+        norm_cfg (dict): Config for norm layers.
             Required keys are ``type``. Defaults to ``dict(type='BN3d')``.
-        act_cfg (dict or ConfigDict): Config dict for activation layer.
+        act_cfg (dict): Config dict for activation layer.
             Defaults to ``dict(type='ReLU')``.
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
             memory while slowing down the training speed. Defaults to False.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+            Defaults to None.
     """
     expansion = 1
 
     def __init__(self,
                  inplanes: int,
                  planes: int,
                  spatial_stride: int = 1,
                  temporal_stride: int = 1,
                  dilation: int = 1,
                  downsample: Optional[nn.Module] = None,
                  style: str = 'pytorch',
                  inflate: bool = True,
                  non_local: bool = False,
-                 non_local_cfg: ConfigType = dict(),
-                 conv_cfg: ConfigType = dict(type='Conv3d'),
-                 norm_cfg: ConfigType = dict(type='BN3d'),
-                 act_cfg: ConfigType = dict(type='ReLU'),
+                 non_local_cfg: Dict = dict(),
+                 conv_cfg: Dict = dict(type='Conv3d'),
+                 norm_cfg: Dict = dict(type='BN3d'),
+                 act_cfg: Dict = dict(type='ReLU'),
                  with_cp: bool = False,
+                 init_cfg: Optional[Union[Dict, List[Dict]]] = None,
                  **kwargs) -> None:
-        super().__init__()
+        super().__init__(init_cfg=init_cfg)
         assert style in ['pytorch', 'caffe']
         # make sure that only ``inflate_style`` is passed into kwargs
         assert set(kwargs).issubset(['inflate_style'])
 
         self.inplanes = inplanes
         self.planes = planes
         self.spatial_stride = spatial_stride
@@ -126,15 +130,15 @@
         self.downsample = downsample
         self.relu = build_activation_layer(self.act_cfg)
 
         if self.non_local:
             self.non_local_block = NonLocal3d(self.conv2.norm.num_features,
                                               **self.non_local_cfg)
 
-    def forward(self, x: Tensor) -> Tensor:
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         """Defines the computation performed at every call."""
 
         def _inner_forward(x):
             """Forward wrapper for utilizing checkpoint."""
             identity = x
 
             out = self.conv1(x)
@@ -154,65 +158,68 @@
 
         if self.non_local:
             out = self.non_local_block(out)
 
         return out
 
 
-class Bottleneck3d(nn.Module):
+class Bottleneck3d(BaseModule):
     """Bottleneck 3d block for ResNet3D.
 
     Args:
         inplanes (int): Number of channels for the input in first conv3d layer.
         planes (int): Number of channels produced by some norm/conv3d layers.
         spatial_stride (int): Spatial stride in the conv3d layer.
             Defaults to 1.
         temporal_stride (int): Temporal stride in the conv3d layer.
             Defaults to 1.
         dilation (int): Spacing between kernel elements. Defaults to 1.
         downsample (nn.Module, optional): Downsample layer. Defaults to None.
-        style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``, the
+        style (str): 'pytorch' or 'caffe'. If set to 'pytorch', the
             stride-two layer is the 3x3 conv layer, otherwise the stride-two
-            layer is the first 1x1 conv layer. Defaults to ``pytorch``.
+            layer is the first 1x1 conv layer. Defaults to ``'pytorch'``.
         inflate (bool): Whether to inflate kernel. Defaults to True.
-        inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines the
+        inflate_style (str): '3x1x1' or '3x3x3'. which determines the
             kernel sizes and padding strides for conv1 and conv2 in each block.
-            Defaults to ``3x1x1``.
+            Defaults to ``'3x1x1'``.
         non_local (bool): Determine whether to apply non-local module in this
             block. Defaults to False.
-        non_local_cfg (dict or ConfigDict): Config for non-local module.
+        non_local_cfg (dict): Config for non-local module.
             Defaults to ``dict()``.
-        conv_cfg (dict or ConfigDict): Config dict for convolution layer.
+        conv_cfg (dict): Config dict for convolution layer.
             Defaults to ``dict(type='Conv3d')``.
-        norm_cfg (dict or ConfigDict): Config for norm layers. required
+        norm_cfg (dict): Config for norm layers. required
             keys are ``type``. Defaults to ``dict(type='BN3d')``.
-        act_cfg (dict or ConfigDict): Config dict for activation layer.
+        act_cfg (dict): Config dict for activation layer.
             Defaults to ``dict(type='ReLU')``.
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
             memory while slowing down the training speed. Defaults to False.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+            Defaults to None.
     """
     expansion = 4
 
     def __init__(self,
                  inplanes: int,
                  planes: int,
                  spatial_stride: int = 1,
                  temporal_stride: int = 1,
                  dilation: int = 1,
                  downsample: Optional[nn.Module] = None,
                  style: str = 'pytorch',
                  inflate: bool = True,
                  inflate_style: str = '3x1x1',
                  non_local: bool = False,
-                 non_local_cfg: ConfigType = dict(),
-                 conv_cfg: ConfigType = dict(type='Conv3d'),
-                 norm_cfg: ConfigType = dict(type='BN3d'),
-                 act_cfg: ConfigType = dict(type='ReLU'),
-                 with_cp: bool = False) -> None:
-        super().__init__()
+                 non_local_cfg: Dict = dict(),
+                 conv_cfg: Dict = dict(type='Conv3d'),
+                 norm_cfg: Dict = dict(type='BN3d'),
+                 act_cfg: Dict = dict(type='ReLU'),
+                 with_cp: bool = False,
+                 init_cfg: Optional[Union[Dict, List[Dict]]] = None) -> None:
+        super().__init__(init_cfg=init_cfg)
         assert style in ['pytorch', 'caffe']
         assert inflate_style in ['3x1x1', '3x3x3']
 
         self.inplanes = inplanes
         self.planes = planes
         self.spatial_stride = spatial_stride
         self.temporal_stride = temporal_stride
@@ -293,15 +300,15 @@
         self.downsample = downsample
         self.relu = build_activation_layer(self.act_cfg)
 
         if self.non_local:
             self.non_local_block = NonLocal3d(self.conv3.norm.num_features,
                                               **self.non_local_cfg)
 
-    def forward(self, x: Tensor) -> Tensor:
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         """Defines the computation performed at every call."""
 
         def _inner_forward(x):
             """Forward wrapper for utilizing checkpoint."""
             identity = x
 
             out = self.conv1(x)
@@ -323,31 +330,31 @@
         if self.non_local:
             out = self.non_local_block(out)
 
         return out
 
 
 @MODELS.register_module()
-class ResNet3d(nn.Module):
+class ResNet3d(BaseModule):
     """ResNet 3d backbone.
 
     Args:
-        depth (int): Depth of resnet, from
-            {``18``, ``34``, ``50``, ``101``, ``152``}.
+        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.
+            Defaults to 50.
         pretrained (str, optional): Name of pretrained model. Defaults to None.
         stage_blocks (tuple, optional): Set number of stages for each res
             layer. Defaults to None.
         pretrained2d (bool): Whether to load pretrained 2D model.
             Defaults to True.
         in_channels (int): Channel num of input features. Defaults to 3.
+        num_stages (int): Resnet stages. Defaults to 4.
         base_channels (int): Channel num of stem output features.
             Defaults to 64.
         out_indices (Sequence[int]): Indices of output feature.
-            Defaults to ```(3, )``.
-        num_stages (int): Resnet stages. Defaults to 4.
+            Defaults to ``(3, )``.
         spatial_strides (Sequence[int]):
             Spatial strides of residual blocks of each stage.
             Defaults to ``(1, 2, 2, 2)``.
         temporal_strides (Sequence[int]):
             Temporal strides of residual blocks of each stage.
             Defaults to ``(1, 1, 1, 1)``.
         dilations (Sequence[int]): Dilation of each stage.
@@ -359,55 +366,57 @@
         conv1_stride_t (int): Temporal stride of the first conv layer.
             Defaults to 1.
         pool1_stride_s (int): Spatial stride of the first pooling layer.
             Defaults to 2.
         pool1_stride_t (int): Temporal stride of the first pooling layer.
             Defaults to 1.
         with_pool2 (bool): Whether to use pool2. Defaults to True.
-        style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``, the
+        style (str): 'pytorch' or 'caffe'. If set to 'pytorch', the
             stride-two layer is the 3x3 conv layer, otherwise the stride-two
-            layer is the first 1x1 conv layer. Defaults to ``pytorch``.
+            layer is the first 1x1 conv layer. Defaults to ``'pytorch'``.
         frozen_stages (int): Stages to be frozen (all param fixed). -1 means
             not freezing any parameters. Defaults to -1.
         inflate (Sequence[int]): Inflate Dims of each block.
             Defaults to ``(1, 1, 1, 1)``.
         inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines the
             kernel sizes and padding strides for conv1 and conv2 in each block.
             Defaults to ``3x1x1``.
-        conv_cfg (dict or ConfigDict): Config for conv layers.
+        conv_cfg (dict): Config for conv layers.
             Required keys are ``type``. Defaults to ``dict(type='Conv3d')``.
-        norm_cfg (dict or ConfigDict): Config for norm layers.
+        norm_cfg (dict): Config for norm layers.
             Required keys are ``type`` and ``requires_grad``.
             Defaults to ``dict(type='BN3d', requires_grad=True)``.
-        act_cfg (dict or ConfigDict): Config dict for activation layer.
+        act_cfg (dict): Config dict for activation layer.
             Defaults to ``dict(type='ReLU', inplace=True)``.
         norm_eval (bool): Whether to set BN layers to eval mode, namely, freeze
             running stats (``mean`` and ``var``). Defaults to False.
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
             memory while slowing down the training speed. Defaults to False.
         non_local (Sequence[int]): Determine whether to apply non-local module
             in the corresponding block of each stages.
             Defaults to ``(0, 0, 0, 0)``.
-        non_local_cfg (dict or ConfigDict): Config for non-local module.
+        non_local_cfg (dict): Config for non-local module.
             Defaults to ``dict()``.
         zero_init_residual (bool):
             Whether to use zero initialization for residual block,
             Defaults to True.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+            Defaults to None.
     """
 
     arch_settings = {
         18: (BasicBlock3d, (2, 2, 2, 2)),
         34: (BasicBlock3d, (3, 4, 6, 3)),
         50: (Bottleneck3d, (3, 4, 6, 3)),
         101: (Bottleneck3d, (3, 4, 23, 3)),
         152: (Bottleneck3d, (3, 8, 36, 3))
     }
 
     def __init__(self,
-                 depth: int,
+                 depth: int = 50,
                  pretrained: Optional[str] = None,
                  stage_blocks: Optional[Tuple] = None,
                  pretrained2d: bool = True,
                  in_channels: int = 3,
                  num_stages: int = 4,
                  base_channels: int = 64,
                  out_indices: Sequence[int] = (3, ),
@@ -421,24 +430,25 @@
                  pool1_stride_t: int = 1,
                  with_pool1: bool = True,
                  with_pool2: bool = True,
                  style: str = 'pytorch',
                  frozen_stages: int = -1,
                  inflate: Sequence[int] = (1, 1, 1, 1),
                  inflate_style: str = '3x1x1',
-                 conv_cfg: ConfigType = dict(type='Conv3d'),
-                 norm_cfg: ConfigType = dict(type='BN3d', requires_grad=True),
-                 act_cfg: ConfigType = dict(type='ReLU', inplace=True),
+                 conv_cfg: Dict = dict(type='Conv3d'),
+                 norm_cfg: Dict = dict(type='BN3d', requires_grad=True),
+                 act_cfg: Dict = dict(type='ReLU', inplace=True),
                  norm_eval: bool = False,
                  with_cp: bool = False,
                  non_local: Sequence[int] = (0, 0, 0, 0),
-                 non_local_cfg: ConfigType = dict(),
+                 non_local_cfg: Dict = dict(),
                  zero_init_residual: bool = True,
+                 init_cfg: Optional[Union[Dict, List[Dict]]] = None,
                  **kwargs) -> None:
-        super().__init__()
+        super().__init__(init_cfg=init_cfg)
         if depth not in self.arch_settings:
             raise KeyError(f'invalid depth {depth} for resnet')
         self.depth = depth
         self.pretrained = pretrained
         self.pretrained2d = pretrained2d
         self.in_channels = in_channels
         self.base_channels = base_channels
@@ -482,22 +492,24 @@
         self.inplanes = self.base_channels
 
         self.non_local_cfg = non_local_cfg
 
         self._make_stem_layer()
 
         self.res_layers = []
+        lateral_inplanes = getattr(self, 'lateral_inplanes', [0, 0, 0, 0])
+
         for i, num_blocks in enumerate(self.stage_blocks):
             spatial_stride = spatial_strides[i]
             temporal_stride = temporal_strides[i]
             dilation = dilations[i]
             planes = self.base_channels * 2**i
             res_layer = self.make_res_layer(
                 self.block,
-                self.inplanes,
+                self.inplanes + lateral_inplanes[i],
                 planes,
                 num_blocks,
                 spatial_stride=spatial_stride,
                 temporal_stride=temporal_stride,
                 dilation=dilation,
                 style=self.style,
                 norm_cfg=self.norm_cfg,
@@ -510,34 +522,34 @@
                 with_cp=with_cp,
                 **kwargs)
             self.inplanes = planes * self.block.expansion
             layer_name = f'layer{i + 1}'
             self.add_module(layer_name, res_layer)
             self.res_layers.append(layer_name)
 
-        self.feat_dim = self.block.expansion * self.base_channels * 2**(
-            len(self.stage_blocks) - 1)
+        self.feat_dim = self.block.expansion * \
+            self.base_channels * 2 ** (len(self.stage_blocks) - 1)
 
     @staticmethod
     def make_res_layer(block: nn.Module,
                        inplanes: int,
                        planes: int,
                        blocks: int,
                        spatial_stride: Union[int, Sequence[int]] = 1,
                        temporal_stride: Union[int, Sequence[int]] = 1,
                        dilation: int = 1,
                        style: str = 'pytorch',
                        inflate: Union[int, Sequence[int]] = 1,
                        inflate_style: str = '3x1x1',
                        non_local: Union[int, Sequence[int]] = 0,
-                       non_local_cfg: ConfigType = dict(),
-                       norm_cfg: OptConfigType = None,
-                       act_cfg: OptConfigType = None,
-                       conv_cfg: OptConfigType = None,
-                       with_cp: Optional[bool] = False,
+                       non_local_cfg: Dict = dict(),
+                       norm_cfg: Optional[Dict] = None,
+                       act_cfg: Optional[Dict] = None,
+                       conv_cfg: Optional[Dict] = None,
+                       with_cp: bool = False,
                        **kwargs) -> nn.Module:
         """Build residual layer for ResNet3D.
 
         Args:
             block (nn.Module): Residual module to be built.
             inplanes (int): Number of channels for the input feature
                 in each block.
@@ -545,45 +557,45 @@
                 in each block.
             blocks (int): Number of residual blocks.
             spatial_stride (int | Sequence[int]): Spatial strides in
                 residual and conv layers. Defaults to 1.
             temporal_stride (int | Sequence[int]): Temporal strides in
                 residual and conv layers. Defaults to 1.
             dilation (int): Spacing between kernel elements. Defaults to 1.
-            style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``,
-                the stride-two layer is the 3x3 conv layer, otherwise
-                the stride-two layer is the first 1x1 conv layer.
-                Default: ``pytorch``.
+            style (str): 'pytorch' or 'caffe'. If set to 'pytorch', the
+                stride-two layer is the 3x3 conv layer,otherwise the
+                stride-two layer is the first 1x1 conv layer.
+                Defaults to ``'pytorch'``.
             inflate (int | Sequence[int]): Determine whether to inflate
                 for each block. Defaults to 1.
             inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines
                 the kernel sizes and padding strides for conv1 and conv2
-                in each block. Default: ``3x1x1``.
+                in each block. Default: ``'3x1x1'``.
             non_local (int | Sequence[int]): Determine whether to apply
                 non-local module in the corresponding block of each stages.
                 Defaults to 0.
             non_local_cfg (dict): Config for non-local module.
                 Defaults to ``dict()``.
-            conv_cfg (dict or ConfigDict, optional): Config for conv layers.
+            conv_cfg (dict, optional): Config for conv layers.
                 Defaults to None.
-            norm_cfg (dict or ConfigDict, optional): Config for norm layers.
+            norm_cfg (dict, optional): Config for norm layers.
                 Defaults to None.
-            act_cfg (dict or ConfigDict, optional): Config for activate layers.
+            act_cfg (dict, optional): Config for activate layers.
                 Defaults to None.
             with_cp (bool, optional): Use checkpoint or not. Using checkpoint
                 will save some memory while slowing down the training speed.
                 Defaults to False.
 
         Returns:
             nn.Module: A residual layer for the given config.
         """
-        inflate = inflate if not isinstance(inflate,
-                                            int) else (inflate, ) * blocks
-        non_local = non_local if not isinstance(
-            non_local, int) else (non_local, ) * blocks
+        inflate = inflate if not isinstance(inflate, int) \
+            else (inflate,) * blocks
+        non_local = non_local if not isinstance(non_local, int) \
+            else (non_local,) * blocks
         assert len(inflate) == blocks and len(non_local) == blocks
         downsample = None
         if spatial_stride != 1 or inplanes != planes * block.expansion:
             downsample = ConvModule(
                 inplanes,
                 planes * block.expansion,
                 kernel_size=1,
@@ -628,28 +640,28 @@
                     non_local_cfg=non_local_cfg,
                     norm_cfg=norm_cfg,
                     conv_cfg=conv_cfg,
                     act_cfg=act_cfg,
                     with_cp=with_cp,
                     **kwargs))
 
-        return nn.Sequential(*layers)
+        return Sequential(*layers)
 
     @staticmethod
     def _inflate_conv_params(conv3d: nn.Module, state_dict_2d: OrderedDict,
                              module_name_2d: str,
                              inflated_param_names: List[str]) -> None:
         """Inflate a conv module from 2d to 3d.
 
         Args:
             conv3d (nn.Module): The destination conv3d module.
             state_dict_2d (OrderedDict): The state dict of pretrained 2d model.
             module_name_2d (str): The name of corresponding conv module in the
                 2d model.
-            inflated_param_names (List[str]): List of parameters that have been
+            inflated_param_names (list[str]): List of parameters that have been
                 inflated.
         """
         weight_2d_name = module_name_2d + '.weight'
 
         conv2d_weight = state_dict_2d[weight_2d_name]
         kernel_t = conv3d.weight.data.shape[2]
 
@@ -670,15 +682,15 @@
         """Inflate a norm module from 2d to 3d.
 
         Args:
             bn3d (nn.Module): The destination bn3d module.
             state_dict_2d (OrderedDict): The state dict of pretrained 2d model.
             module_name_2d (str): The name of corresponding bn module in the
                 2d model.
-            inflated_param_names (List[str]): List of parameters that have been
+            inflated_param_names (list[str]): List of parameters that have been
                 inflated.
         """
         for param_name, param in bn3d.named_parameters():
             param_2d_name = f'{module_name_2d}.{param_name}'
             param_2d = state_dict_2d[param_2d_name]
             if param.data.shape != param_2d.shape:
                 warnings.warn(f'The parameter of {module_name_2d} is not'
@@ -707,15 +719,15 @@
         the 3d counterpart.
 
         Args:
             logger (MMLogger): The logger used to print
                 debugging information.
         """
 
-        state_dict_r2d = _load_checkpoint(self.pretrained)
+        state_dict_r2d = _load_checkpoint(self.pretrained, map_location='cpu')
         if 'state_dict' in state_dict_r2d:
             state_dict_r2d = state_dict_r2d['state_dict']
 
         inflated_param_names = []
         for name, module in self.named_modules():
             if isinstance(module, ConvModule):
                 # we use a ConvModule to wrap conv+bn+relu layers, thus the
@@ -807,26 +819,25 @@
     def _init_weights(self, pretrained: Optional[str] = None) -> None:
         """Initiate the parameters either from existing checkpoint or from
         scratch.
 
         Args:
             pretrained (str | None): The path of the pretrained weight. Will
                 override the original `pretrained` if set. The arg is added to
-                be compatible with mmdet. Default: None.
+                be compatible with mmdet. Defaults to None.
         """
         if pretrained:
             self.pretrained = pretrained
         if isinstance(self.pretrained, str):
             logger = MMLogger.get_current_instance()
             logger.info(f'load model from: {self.pretrained}')
 
             if self.pretrained2d:
                 # Inflate 2D model into 3D model.
                 self.inflate_weights(logger)
-
             else:
                 # Directly load 3D model.
                 load_checkpoint(
                     self, self.pretrained, strict=False, logger=logger)
 
         elif self.pretrained is None:
             for m in self.modules():
@@ -844,23 +855,24 @@
         else:
             raise TypeError('pretrained must be a str or None')
 
     def init_weights(self, pretrained: Optional[str] = None) -> None:
         """Initialize weights."""
         self._init_weights(self, pretrained)
 
-    def forward(self, x: Tensor) -> Union[Tensor, Tuple[Tensor]]:
+    def forward(self, x: torch.Tensor) \
+            -> Union[torch.Tensor, Tuple[torch.Tensor]]:
         """Defines the computation performed at every call.
 
         Args:
-            x (Tensor): The input data.
+            x (torch.Tensor): The input data.
 
         Returns:
-            Tensor or Tuple[Tensor]: The feature of the input
-                samples extracted by the backbone.
+            torch.Tensor or tuple[torch.Tensor]: The feature of the input
+            samples extracted by the backbone.
         """
         x = self.conv1(x)
         if self.with_pool1:
             x = self.maxpool(x)
         outs = []
         for i, layer_name in enumerate(self.res_layers):
             res_layer = getattr(self, layer_name)
@@ -881,53 +893,54 @@
         if mode and self.norm_eval:
             for m in self.modules():
                 if isinstance(m, _BatchNorm):
                     m.eval()
 
 
 @MODELS.register_module()
-class ResNet3dLayer(nn.Module):
+class ResNet3dLayer(BaseModule):
     """ResNet 3d Layer.
 
     Args:
-        depth (int): Depth of resnet,
-            from {``18``, ``34``, ``50``, ``101``, ``152``}.
+        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.
         pretrained (str, optional): Name of pretrained model. Defaults to None.
         pretrained2d (bool): Whether to load pretrained 2D model.
             Defaults to True.
         stage (int): The index of Resnet stage. Defaults to 3.
         base_channels (int): Channel num of stem output features.
             Defaults to 64.
         spatial_stride (int): The 1st res block's spatial stride.
             Defaults to 2.
         temporal_stride (int): The 1st res block's temporal stride.
             Defaults to 1.
         dilation (int): The dilation. Defaults to 1.
-        style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``, the
+        style (str): 'pytorch' or 'caffe'. If set to 'pytorch', the
             stride-two layer is the 3x3 conv layer, otherwise the stride-two
-            layer is the first 1x1 conv layer. Defaults to ``pytorch``.
+            layer is the first 1x1 conv layer. Defaults to ``'pytorch'``.
         all_frozen (bool): Frozen all modules in the layer. Defaults to False.
         inflate (int): Inflate dims of each block. Defaults to 1.
         inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines the
             kernel sizes and padding strides for conv1 and conv2 in each block.
-            Defaults to ``3x1x1``.
-        conv_cfg (dict or ConfigDict): Config for conv layers.
+            Defaults to ``'3x1x1'``.
+        conv_cfg (dict): Config for conv layers.
             Required keys are ``type``. Defaults to ``dict(type='Conv3d')``.
-        norm_cfg (dict or ConfigDict): Config for norm layers.
+        norm_cfg (dict): Config for norm layers.
             Required keys are ``type`` and ``requires_grad``.
             Defaults to ``dict(type='BN3d', requires_grad=True)``.
-        act_cfg (dict or ConfigDict): Config dict for activation layer.
+        act_cfg (dict): Config dict for activation layer.
             Defaults to ``dict(type='ReLU', inplace=True)``.
         norm_eval (bool): Whether to set BN layers to eval mode, namely, freeze
             running stats (``mean`` and ``var``). Defaults to False.
         with_cp (bool): Use checkpoint or not. Using checkpoint will save some
             memory while slowing down the training speed. Defaults to False.
         zero_init_residual (bool):
             Whether to use zero initialization for residual block,
             Defaults to True.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+            Defaults to None.
     """
 
     def __init__(self,
                  depth: int,
                  pretrained: Optional[str] = None,
                  pretrained2d: bool = True,
                  stage: int = 3,
@@ -935,22 +948,23 @@
                  spatial_stride: int = 2,
                  temporal_stride: int = 1,
                  dilation: int = 1,
                  style: str = 'pytorch',
                  all_frozen: bool = False,
                  inflate: int = 1,
                  inflate_style: str = '3x1x1',
-                 conv_cfg: ConfigType = dict(type='Conv3d'),
-                 norm_cfg: ConfigType = dict(type='BN3d', requires_grad=True),
-                 act_cfg: ConfigType = dict(type='ReLU', inplace=True),
+                 conv_cfg: Dict = dict(type='Conv3d'),
+                 norm_cfg: Dict = dict(type='BN3d', requires_grad=True),
+                 act_cfg: Dict = dict(type='ReLU', inplace=True),
                  norm_eval: bool = False,
                  with_cp: bool = False,
                  zero_init_residual: bool = True,
+                 init_cfg: Optional[Union[Dict, List[Dict]]] = None,
                  **kwargs) -> None:
-        super().__init__()
+        super().__init__(init_cfg=init_cfg)
         self.arch_settings = ResNet3d.arch_settings
         assert depth in self.arch_settings
 
         self.make_res_layer = ResNet3d.make_res_layer
         self._inflate_conv_params = ResNet3d._inflate_conv_params
         self._inflate_bn_params = ResNet3d._inflate_bn_params
         self._inflate_weights = ResNet3d._inflate_weights
@@ -1018,23 +1032,23 @@
             for param in layer.parameters():
                 param.requires_grad = False
 
     def init_weights(self, pretrained: Optional[str] = None) -> None:
         """Initialize weights."""
         self._init_weights(self, pretrained)
 
-    def forward(self, x: Tensor) -> Tensor:
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         """Defines the computation performed at every call.
 
         Args:
-            x (Tensor): The input data.
+            x (torch.Tensor): The input data.
 
         Returns:
-            Tensor: The feature of the input
-                samples extracted by the resisual layer.
+            torch.Tensor: The feature of the input
+                samples extracted by the residual layer.
         """
         res_layer = getattr(self, self.layer_name)
         out = res_layer(x)
         return out
 
     def train(self, mode: bool = True) -> None:
         """Set the optimization status when training."""
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet3d_csn.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet3d_csn.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet3d_slowfast.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet3d_slowfast.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,216 +1,208 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
 from collections import OrderedDict
-from typing import List, Optional, Sequence, Union
+from typing import Dict, List, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
 from mmcv.cnn import ConvModule
 from mmengine.logging import MMLogger, print_log
+from mmengine.model import BaseModule
 from mmengine.model.weight_init import kaiming_init
 from mmengine.runner.checkpoint import _load_checkpoint, load_checkpoint
-from torch import Tensor
 
 from mmaction.registry import MODELS
-from mmaction.utils import ConfigType, OptConfigType
 from .resnet3d import ResNet3d
 
 
+class DeConvModule(BaseModule):
+    """A deconv module that bundles deconv/norm/activation layers.
+
+    Args:
+        in_channels (int): Number of channels in the input feature map.
+        out_channels (int): Number of channels produced by the convolution.
+        kernel_size (int | tuple[int]): Size of the convolving kernel.
+        stride (int | tuple[int]): Stride of the convolution.
+        padding (int | tuple[int]): Zero-padding added to both sides of
+            the input.
+        bias (bool): Whether to add a learnable bias to the output.
+            Defaults to False.
+        with_bn (bool): Whether to add a BN layer. Defaults to True.
+        with_relu (bool): Whether to add a ReLU layer. Defaults to True.
+    """
+
+    def __init__(self,
+                 in_channels: int,
+                 out_channels: int,
+                 kernel_size: int,
+                 stride: Union[int, Tuple[int]] = (1, 1, 1),
+                 padding: Union[int, Tuple[int]] = 0,
+                 bias: bool = False,
+                 with_bn: bool = True,
+                 with_relu: bool = True) -> None:
+        super().__init__()
+        self.in_channels = in_channels
+        self.out_channels = out_channels
+        self.kernel_size = kernel_size
+        self.stride = stride
+        self.padding = padding
+        self.bias = bias
+        self.with_bn = with_bn
+        self.with_relu = with_relu
+
+        self.conv = nn.ConvTranspose3d(
+            in_channels,
+            out_channels,
+            kernel_size,
+            stride=stride,
+            padding=padding,
+            bias=bias)
+        self.bn = nn.BatchNorm3d(out_channels)
+        self.relu = nn.ReLU()
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        """Defines the computation performed at every call."""
+        # x should be a 5-d tensor
+        assert len(x.shape) == 5
+        N, C, T, H, W = x.shape
+        out_shape = (N, self.out_channels, self.stride[0] * T,
+                     self.stride[1] * H, self.stride[2] * W)
+        x = self.conv(x, output_size=out_shape)
+        if self.with_bn:
+            x = self.bn(x)
+        if self.with_relu:
+            x = self.relu(x)
+        return x
+
+
 class ResNet3dPathway(ResNet3d):
     """A pathway of Slowfast based on ResNet3d.
 
     Args:
         lateral (bool): Determines whether to enable the lateral connection
             from another pathway. Defaults to False.
+        lateral_inv (bool): Whether to use deconv to upscale the time
+            dimension of features from another pathway. Defaults to False.
         lateral_norm (bool): Determines whether to enable the lateral norm
             in lateral layers. Defaults to False.
         speed_ratio (int): Speed ratio indicating the ratio between time
             dimension of the fast and slow pathway, corresponding to the
             ``alpha`` in the paper. Defaults to 8.
         channel_ratio (int): Reduce the channel number of fast pathway
             by ``channel_ratio``, corresponding to ``beta`` in the paper.
             Defaults to 8.
         fusion_kernel (int): The kernel size of lateral fusion.
             Defaults to 5.
+        lateral_infl (int): The ratio of the inflated channels.
+            Defaults to 2.
+        lateral_activate (list[int]): Flags for activating the lateral
+            connection. Defaults to ``[1, 1, 1, 1]``.
     """
 
     def __init__(self,
-                 *args,
                  lateral: bool = False,
+                 lateral_inv: bool = False,
                  lateral_norm: bool = False,
                  speed_ratio: int = 8,
                  channel_ratio: int = 8,
                  fusion_kernel: int = 5,
+                 lateral_infl: int = 2,
+                 lateral_activate: List[int] = [1, 1, 1, 1],
                  **kwargs) -> None:
         self.lateral = lateral
+        self.lateral_inv = lateral_inv
         self.lateral_norm = lateral_norm
         self.speed_ratio = speed_ratio
         self.channel_ratio = channel_ratio
         self.fusion_kernel = fusion_kernel
-        super().__init__(*args, **kwargs)
+        self.lateral_infl = lateral_infl
+        self.lateral_activate = lateral_activate
+        self._calculate_lateral_inplanes(kwargs)
+
+        super().__init__(**kwargs)
         self.inplanes = self.base_channels
-        if self.lateral:
-            self.conv1_lateral = ConvModule(
-                self.inplanes // self.channel_ratio,
-                # https://arxiv.org/abs/1812.03982, the
-                # third type of lateral connection has out_channel:
-                # 2 * \beta * C
-                self.inplanes * 2 // self.channel_ratio,
-                kernel_size=(fusion_kernel, 1, 1),
-                stride=(self.speed_ratio, 1, 1),
-                padding=((fusion_kernel - 1) // 2, 0, 0),
-                bias=False,
-                conv_cfg=self.conv_cfg,
-                norm_cfg=self.norm_cfg if self.lateral_norm else None,
-                act_cfg=self.act_cfg if self.lateral_norm else None)
+        if self.lateral and self.lateral_activate[0] == 1:
+            if self.lateral_inv:
+                self.conv1_lateral = DeConvModule(
+                    self.inplanes * self.channel_ratio,
+                    self.inplanes * self.channel_ratio // lateral_infl,
+                    kernel_size=(fusion_kernel, 1, 1),
+                    stride=(self.speed_ratio, 1, 1),
+                    padding=((fusion_kernel - 1) // 2, 0, 0),
+                    with_bn=True,
+                    with_relu=True)
+            else:
+                self.conv1_lateral = ConvModule(
+                    self.inplanes // self.channel_ratio,
+                    self.inplanes * lateral_infl // self.channel_ratio,
+                    kernel_size=(fusion_kernel, 1, 1),
+                    stride=(self.speed_ratio, 1, 1),
+                    padding=((fusion_kernel - 1) // 2, 0, 0),
+                    bias=False,
+                    conv_cfg=self.conv_cfg,
+                    norm_cfg=self.norm_cfg if self.lateral_norm else None,
+                    act_cfg=self.act_cfg if self.lateral_norm else None)
 
         self.lateral_connections = []
         for i in range(len(self.stage_blocks)):
             planes = self.base_channels * 2**i
             self.inplanes = planes * self.block.expansion
 
-            if lateral and i != self.num_stages - 1:
+            if lateral and i != self.num_stages - 1 \
+                    and self.lateral_activate[i + 1]:
                 # no lateral connection needed in final stage
                 lateral_name = f'layer{(i + 1)}_lateral'
-                setattr(
-                    self, lateral_name,
-                    ConvModule(
+                if self.lateral_inv:
+                    conv_module = DeConvModule(
+                        self.inplanes * self.channel_ratio,
+                        self.inplanes * self.channel_ratio // lateral_infl,
+                        kernel_size=(fusion_kernel, 1, 1),
+                        stride=(self.speed_ratio, 1, 1),
+                        padding=((fusion_kernel - 1) // 2, 0, 0),
+                        bias=False,
+                        with_bn=True,
+                        with_relu=True)
+                else:
+                    conv_module = ConvModule(
                         self.inplanes // self.channel_ratio,
-                        self.inplanes * 2 // self.channel_ratio,
+                        self.inplanes * lateral_infl // self.channel_ratio,
                         kernel_size=(fusion_kernel, 1, 1),
                         stride=(self.speed_ratio, 1, 1),
                         padding=((fusion_kernel - 1) // 2, 0, 0),
                         bias=False,
                         conv_cfg=self.conv_cfg,
                         norm_cfg=self.norm_cfg if self.lateral_norm else None,
-                        act_cfg=self.act_cfg if self.lateral_norm else None))
+                        act_cfg=self.act_cfg if self.lateral_norm else None)
+                setattr(self, lateral_name, conv_module)
                 self.lateral_connections.append(lateral_name)
 
-    def make_res_layer(self,
-                       block: nn.Module,
-                       inplanes: int,
-                       planes: int,
-                       blocks: int,
-                       spatial_stride: Union[int, Sequence[int]] = 1,
-                       temporal_stride: Union[int, Sequence[int]] = 1,
-                       dilation: int = 1,
-                       style: str = 'pytorch',
-                       inflate: Union[int, Sequence[int]] = 1,
-                       inflate_style: str = '3x1x1',
-                       non_local: Union[int, Sequence[int]] = 0,
-                       non_local_cfg: ConfigType = dict(),
-                       norm_cfg: OptConfigType = None,
-                       act_cfg: OptConfigType = None,
-                       conv_cfg: OptConfigType = None,
-                       with_cp: Optional[bool] = False,
-                       **kwargs) -> nn.Module:
-        """Build residual layer for SlowFast.
-
-        Args:
-            block (nn.Module): Residual module to be built.
-            inplanes (int): Number of channels for the input feature
-                in each block.
-            planes (int): Number of channels for the output feature
-                in each block.
-            blocks (int): Number of residual blocks.
-            spatial_stride (int | Sequence[int]): Spatial strides in
-                residual and conv layers. Defaults to 1.
-            temporal_stride (int | Sequence[int]): Temporal strides in
-                residual and conv layers. Defaults to 1.
-            dilation (int): Spacing between kernel elements. Defaults to 1.
-            style (str): ``pytorch`` or ``caffe``. If set to ``pytorch``,
-                the stride-two layer is the 3x3 conv layer, otherwise
-                the stride-two layer is the first 1x1 conv layer.
-                Default: ``pytorch``.
-            inflate (int | Sequence[int]): Determine whether to inflate
-                for each block. Defaults to 1.
-            inflate_style (str): ``3x1x1`` or ``3x3x3``. which determines
-                the kernel sizes and padding strides for conv1 and conv2
-                in each block. Default: ``3x1x1``.
-            non_local (int | Sequence[int]): Determine whether to apply
-                non-local module in the corresponding block of each stages.
-                Defaults to 0.
-            non_local_cfg (dict): Config for non-local module.
-                Defaults to ``dict()``.
-            conv_cfg (dict or ConfigDict, optional): Config for conv layers.
-                Defaults to None.
-            norm_cfg (dict or ConfigDict, optional): Config for norm layers.
-                Defaults to None.
-            act_cfg (dict or ConfigDict, optional): Config for activate layers.
-                Defaults to None.
-            with_cp (bool, optional): Use checkpoint or not. Using checkpoint
-                will save some memory while slowing down the training speed.
-                Defaults to False.
-
-        Returns:
-            nn.Module: A residual layer for the given config.
-        """
-        inflate = inflate if not isinstance(inflate,
-                                            int) else (inflate, ) * blocks
-        non_local = non_local if not isinstance(
-            non_local, int) else (non_local, ) * blocks
-        assert len(inflate) == blocks and len(non_local) == blocks
-        if self.lateral:
-            lateral_inplanes = inplanes * 2 // self.channel_ratio
-        else:
-            lateral_inplanes = 0
-        if (spatial_stride != 1
-                or (inplanes + lateral_inplanes) != planes * block.expansion):
-            downsample = ConvModule(
-                inplanes + lateral_inplanes,
-                planes * block.expansion,
-                kernel_size=1,
-                stride=(temporal_stride, spatial_stride, spatial_stride),
-                bias=False,
-                conv_cfg=conv_cfg,
-                norm_cfg=norm_cfg,
-                act_cfg=None)
-        else:
-            downsample = None
-
-        layers = []
-        layers.append(
-            block(
-                inplanes + lateral_inplanes,
-                planes,
-                spatial_stride,
-                temporal_stride,
-                dilation,
-                downsample,
-                style=style,
-                inflate=(inflate[0] == 1),
-                inflate_style=inflate_style,
-                non_local=(non_local[0] == 1),
-                non_local_cfg=non_local_cfg,
-                conv_cfg=conv_cfg,
-                norm_cfg=norm_cfg,
-                act_cfg=act_cfg,
-                with_cp=with_cp))
-        inplanes = planes * block.expansion
-
-        for i in range(1, blocks):
-            layers.append(
-                block(
-                    inplanes,
-                    planes,
-                    1,
-                    1,
-                    dilation,
-                    style=style,
-                    inflate=(inflate[i] == 1),
-                    inflate_style=inflate_style,
-                    non_local=(non_local[i] == 1),
-                    non_local_cfg=non_local_cfg,
-                    conv_cfg=conv_cfg,
-                    norm_cfg=norm_cfg,
-                    act_cfg=act_cfg,
-                    with_cp=with_cp))
-
-        return nn.Sequential(*layers)
+    def _calculate_lateral_inplanes(self, kwargs):
+        """Calculate inplanes for lateral connection."""
+        depth = kwargs.get('depth', 50)
+        expansion = 1 if depth < 50 else 4
+        base_channels = kwargs.get('base_channels', 64)
+        lateral_inplanes = []
+        for i in range(kwargs.get('num_stages', 4)):
+            if expansion % 2 == 0:
+                planes = base_channels * (2 ** i) * \
+                         ((expansion // 2) ** (i > 0))
+            else:
+                planes = base_channels * (2**i) // (2**(i > 0))
+            if self.lateral and self.lateral_activate[i]:
+                if self.lateral_inv:
+                    lateral_inplane = planes * \
+                                      self.channel_ratio // self.lateral_infl
+                else:
+                    lateral_inplane = planes * \
+                                      self.lateral_infl // self.channel_ratio
+            else:
+                lateral_inplane = 0
+            lateral_inplanes.append(lateral_inplane)
+        self.lateral_inplanes = lateral_inplanes
 
     def inflate_weights(self, logger: MMLogger) -> None:
         """Inflate the resnet2d parameters to resnet3d pathway.
 
         The differences between resnet3d and resnet2d mainly lie in an extra
         axis of conv kernel. To utilize the pretrained parameters in 2d model,
         the weight of conv2d models should be inflated to fit in the shapes of
@@ -218,15 +210,15 @@
         not be inflated from 2d weights.
 
         Args:
             logger (MMLogger): The logger used to print
                 debugging information.
         """
 
-        state_dict_r2d = _load_checkpoint(self.pretrained)
+        state_dict_r2d = _load_checkpoint(self.pretrained, map_location='cpu')
         if 'state_dict' in state_dict_r2d:
             state_dict_r2d = state_dict_r2d['state_dict']
 
         inflated_param_names = []
         for name, module in self.named_modules():
             if 'lateral' in name:
                 continue
@@ -276,15 +268,15 @@
         parameters by concatting conv2d parameters and extra zero paddings.
 
         Args:
             conv3d (nn.Module): The destination conv3d module.
             state_dict_2d (OrderedDict): The state dict of pretrained 2d model.
             module_name_2d (str): The name of corresponding conv module in the
                 2d model.
-            inflated_param_names (List[str]): List of parameters that have been
+            inflated_param_names (list[str]): List of parameters that have been
                 inflated.
         """
         weight_2d_name = module_name_2d + '.weight'
         conv2d_weight = state_dict_2d[weight_2d_name]
         old_shape = conv2d_weight.shape
         new_shape = conv3d.weight.data.shape
         kernel_t = new_shape[2]
@@ -354,19 +346,19 @@
 
 pathway_cfg = {
     'resnet3d': ResNet3dPathway,
     # TODO: BNInceptionPathway
 }
 
 
-def build_pathway(cfg: ConfigType, *args, **kwargs) -> nn.Module:
+def build_pathway(cfg: Dict, *args, **kwargs) -> nn.Module:
     """Build pathway.
 
     Args:
-        cfg (dict or ConfigDict): cfg should contain:
+        cfg (dict): cfg should contain:
             - type (str): identify backbone type.
 
     Returns:
         nn.Module: Created pathway.
     """
     if not (isinstance(cfg, dict) and 'type' in cfg):
         raise TypeError('cfg must be a dict containing the key "type"')
@@ -379,15 +371,15 @@
     pathway_cls = pathway_cfg[pathway_type]
     pathway = pathway_cls(*args, **kwargs, **cfg_)
 
     return pathway
 
 
 @MODELS.register_module()
-class ResNet3dSlowFast(nn.Module):
+class ResNet3dSlowFast(BaseModule):
     """Slowfast backbone.
 
     This module is proposed in `SlowFast Networks for Video Recognition
     <https://arxiv.org/abs/1812.03982>`_
 
     Args:
         pretrained (str): The file path to a pretrained model.
@@ -399,65 +391,51 @@
             ``resample_rate * interval`` frames. Defaults to 8.
         speed_ratio (int): Speed ratio indicating the ratio between time
             dimension of the fast and slow pathway, corresponding to the
             :math:`\\alpha` in the paper. Defaults to 8.
         channel_ratio (int): Reduce the channel number of fast pathway
             by ``channel_ratio``, corresponding to :math:`\\beta` in the paper.
             Defaults to 8.
-        slow_pathway (dict or ConfigDict): Configuration of slow branch, should
-            contain necessary arguments for building the specific type of
-            pathway and:
-                type (str): type of backbone the pathway bases on.
-                lateral (bool): determine whether to build lateral connection
-            for the pathway. Defaults to
-
-            .. code-block:: Python
-
-                dict(type='ResNetPathway',
-                lateral=True, depth=50, pretrained=None,
-                conv1_kernel=(1, 7, 7), dilations=(1, 1, 1, 1),
-                conv1_stride_t=1, pool1_stride_t=1, inflate=(0, 0, 1, 1))
-
-        fast_pathway (dict or ConfigDict): Configuration of fast branch,
-            similar to ``slow_pathway``. Defaults to
-
-            .. code-block:: Python
-
-                dict(type='ResNetPathway',
-                lateral=False, depth=50, pretrained=None, base_channels=8,
-                conv1_kernel=(5, 7, 7), conv1_stride_t=1, pool1_stride_t=1)
+        slow_pathway (dict): Configuration of slow branch. Defaults to
+            ``dict(type='resnet3d', depth=50, pretrained=None, lateral=True,
+            conv1_kernel=(1, 7, 7), conv1_stride_t=1, pool1_stride_t=1,
+            inflate=(0, 0, 1, 1))``.
+        fast_pathway (dict): Configuration of fast branch. Defaults to
+            ``dict(type='resnet3d', depth=50, pretrained=None, lateral=False,
+            base_channels=8, conv1_kernel=(5, 7, 7), conv1_stride_t=1,
+            pool1_stride_t=1)``.
+        init_cfg (dict or list[dict], optional): Initialization config dict.
+            Defaults to None.
     """
 
-    def __init__(
-        self,
-        pretrained,
-        resample_rate: int = 8,
-        speed_ratio: int = 8,
-        channel_ratio: int = 8,
-        slow_pathway: ConfigType = dict(
-            type='resnet3d',
-            depth=50,
-            pretrained=None,
-            lateral=True,
-            conv1_kernel=(1, 7, 7),
-            dilations=(1, 1, 1, 1),
-            conv1_stride_t=1,
-            pool1_stride_t=1,
-            inflate=(0, 0, 1, 1)),
-        fast_pathway: ConfigType = dict(
-            type='resnet3d',
-            depth=50,
-            pretrained=None,
-            lateral=False,
-            base_channels=8,
-            conv1_kernel=(5, 7, 7),
-            conv1_stride_t=1,
-            pool1_stride_t=1)
-    ) -> None:
-        super().__init__()
+    def __init__(self,
+                 pretrained: Optional[str] = None,
+                 resample_rate: int = 8,
+                 speed_ratio: int = 8,
+                 channel_ratio: int = 8,
+                 slow_pathway: Dict = dict(
+                     type='resnet3d',
+                     depth=50,
+                     pretrained=None,
+                     lateral=True,
+                     conv1_kernel=(1, 7, 7),
+                     conv1_stride_t=1,
+                     pool1_stride_t=1,
+                     inflate=(0, 0, 1, 1)),
+                 fast_pathway: Dict = dict(
+                     type='resnet3d',
+                     depth=50,
+                     pretrained=None,
+                     lateral=False,
+                     base_channels=8,
+                     conv1_kernel=(5, 7, 7),
+                     conv1_stride_t=1,
+                     pool1_stride_t=1),
+                 init_cfg: Optional[Union[Dict, List[Dict]]] = None) -> None:
+        super().__init__(init_cfg=init_cfg)
         self.pretrained = pretrained
         self.resample_rate = resample_rate
         self.speed_ratio = speed_ratio
         self.channel_ratio = channel_ratio
 
         if slow_pathway['lateral']:
             slow_pathway['speed_ratio'] = speed_ratio
@@ -481,23 +459,23 @@
         elif self.pretrained is None:
             # Init two branch separately.
             self.fast_path.init_weights()
             self.slow_path.init_weights()
         else:
             raise TypeError('pretrained must be a str or None')
 
-    def forward(self, x: Tensor) -> tuple:
+    def forward(self, x: torch.Tensor) -> tuple:
         """Defines the computation performed at every call.
 
         Args:
-            x (Tensor): The input data.
+            x (torch.Tensor): The input data.
 
         Returns:
-            Tuple[Tensor]: The feature of the input samples extracted
-                by the backbone.
+            tuple[torch.Tensor]: The feature of the input samples
+                extracted by the backbone.
         """
         x_slow = nn.functional.interpolate(
             x,
             mode='nearest',
             scale_factor=(1.0 / self.resample_rate, 1.0, 1.0))
         x_slow = self.slow_path.conv1(x_slow)
         x_slow = self.slow_path.maxpool(x_slow)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet_audio.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet_audio.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet_omni.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet_omni.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/resnet_tin.py` & `mmaction2-1.1.0/mmaction/models/backbones/resnet_tin.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/stgcn.py` & `mmaction2-1.1.0/mmaction/models/backbones/stgcn.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/swin.py` & `mmaction2-1.1.0/mmaction/models/backbones/swin.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/tanet.py` & `mmaction2-1.1.0/mmaction/models/backbones/tanet.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/timesformer.py` & `mmaction2-1.1.0/mmaction/models/backbones/timesformer.py`

 * *Files 2% similar despite different names*

```diff
@@ -231,15 +231,15 @@
 
         if pretrained:
             self.pretrained = pretrained
         if isinstance(self.pretrained, str):
             logger = MMLogger.get_current_instance()
             logger.info(f'load model from: {self.pretrained}')
 
-            state_dict = _load_checkpoint(self.pretrained)
+            state_dict = _load_checkpoint(self.pretrained, map_location='cpu')
             if 'state_dict' in state_dict:
                 state_dict = state_dict['state_dict']
 
             if self.attention_type == 'divided_space_time':
                 # modify the key names of norm layers
                 old_state_dict_keys = list(state_dict.keys())
                 for old_key in old_state_dict_keys:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/uniformer.py` & `mmaction2-1.1.0/mmaction/models/backbones/uniformer.py`

 * *Files 0% similar despite different names*

```diff
@@ -491,15 +491,15 @@
             Defaults to True.
         qk_scale (float, optional): Override default qk scale of
             ``head_dim ** -0.5`` if set. Defaults to None.
         drop_rate (float): Dropout rate. Defaults to 0.0.
         attn_drop_rate (float): Attention dropout rate. Defaults to 0.0.
         drop_path_rate (float): Stochastic depth rates.
             Defaults to 0.0.
-        clip_pretrained (bool): Whether to load pretrained CLIP visual encoder.
+        pretrained2d (bool): Whether to load pretrained from 2D model.
             Defaults to True.
         pretrained (str): Name of pretrained model.
             Defaults to None.
         init_cfg (dict or list[dict]): Initialization config dict. Defaults to
             ``[
             dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.),
             dict(type='Constant', layer='LayerNorm', val=1., bias=0.)
@@ -515,25 +515,25 @@
         head_dim: int = 64,
         mlp_ratio: float = 4.,
         qkv_bias: bool = True,
         qk_scale: float = None,
         drop_rate: float = 0.,
         attn_drop_rate: float = 0.,
         drop_path_rate: float = 0.,
-        clip_pretrained: bool = True,
+        pretrained2d: bool = True,
         pretrained: Optional[str] = None,
         init_cfg: Optional[Union[Dict, List[Dict]]] = [
             dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.),
             dict(type='Constant', layer='LayerNorm', val=1., bias=0.)
         ]
     ) -> None:
         super().__init__(init_cfg=init_cfg)
 
         self.pretrained = pretrained
-        self.clip_pretrained = clip_pretrained
+        self.pretrained2d = pretrained2d
         self.patch_embed1 = SpeicalPatchEmbed(
             img_size=img_size,
             patch_size=4,
             in_chans=in_chans,
             embed_dim=embed_dim[0])
         self.patch_embed2 = PatchEmbed(
             img_size=img_size // 4,
@@ -637,15 +637,15 @@
                     time_dim = state_dict_3d[k].shape[2]
                     state_dict[k] = self._inflate_weight(
                         state_dict[k], time_dim)
             self.load_state_dict(state_dict, strict=False)
 
     def init_weights(self):
         """Initialize the weights in backbone."""
-        if self.clip_pretrained:
+        if self.pretrained2d:
             logger = MMLogger.get_current_instance()
             logger.info(f'load model from: {self.pretrained}')
             self._load_pretrained(self.pretrained)
         else:
             if self.pretrained:
                 self.init_cfg = dict(
                     type='Pretrained', checkpoint=self.pretrained)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/uniformerv2.py` & `mmaction2-1.1.0/mmaction/models/backbones/uniformerv2.py`

 * *Files 1% similar despite different names*

```diff
@@ -544,31 +544,32 @@
         The visual encoder is extracted from CLIP.
         https://github.com/openai/CLIP
 
         Args:
             pretrained (str): Model name of pretrained CLIP visual encoder.
                 Defaults to None.
         """
-        if pretrained is not None:
-            model_path = _MODELS[pretrained]
-            logger.info(f'Load CLIP pretrained model from {model_path}')
-            state_dict = _load_checkpoint(model_path, map_location='cpu')
-            state_dict_3d = self.state_dict()
-            for k in state_dict.keys():
-                if k in state_dict_3d.keys(
-                ) and state_dict[k].shape != state_dict_3d[k].shape:
-                    if len(state_dict_3d[k].shape) <= 2:
-                        logger.info(f'Ignore: {k}')
-                        continue
-                    logger.info(f'Inflate: {k}, {state_dict[k].shape}' +
-                                f' => {state_dict_3d[k].shape}')
-                    time_dim = state_dict_3d[k].shape[2]
-                    state_dict[k] = self._inflate_weight(
-                        state_dict[k], time_dim)
-            self.load_state_dict(state_dict, strict=False)
+        assert pretrained is not None, \
+            'please specify clip pretraied checkpoint'
+
+        model_path = _MODELS[pretrained]
+        logger.info(f'Load CLIP pretrained model from {model_path}')
+        state_dict = _load_checkpoint(model_path, map_location='cpu')
+        state_dict_3d = self.state_dict()
+        for k in state_dict.keys():
+            if k in state_dict_3d.keys(
+            ) and state_dict[k].shape != state_dict_3d[k].shape:
+                if len(state_dict_3d[k].shape) <= 2:
+                    logger.info(f'Ignore: {k}')
+                    continue
+                logger.info(f'Inflate: {k}, {state_dict[k].shape}' +
+                            f' => {state_dict_3d[k].shape}')
+                time_dim = state_dict_3d[k].shape[2]
+                state_dict[k] = self._inflate_weight(state_dict[k], time_dim)
+        self.load_state_dict(state_dict, strict=False)
 
     def init_weights(self):
         """Initialize the weights in backbone."""
         if self.clip_pretrained:
             logger = MMLogger.get_current_instance()
             logger.info(f'load model from: {self.pretrained}')
             self._load_pretrained(self.pretrained)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/vit_mae.py` & `mmaction2-1.1.0/mmaction/models/backbones/vit_mae.py`

 * *Files 13% similar despite different names*

```diff
@@ -3,20 +3,25 @@
 
 import torch
 import torch.nn.functional as F
 from mmcv.cnn import build_norm_layer
 from mmcv.cnn.bricks import DropPath
 from mmcv.cnn.bricks.transformer import FFN, PatchEmbed
 from mmengine.model import BaseModule, ModuleList
-from mmengine.utils import to_2tuple
 from torch import Tensor, nn
 
 from mmaction.registry import MODELS
 from mmaction.utils import ConfigType, OptConfigType
 
+try:
+    from mmdet.registry import MODELS as MMDET_MODELS
+    mmdet_imported = True
+except (ImportError, ModuleNotFoundError):
+    mmdet_imported = False
+
 
 class Attention(BaseModule):
     """Multi-head Self-attention.
 
     Args:
         embed_dims (int): Dimensions of embedding.
         num_heads (int): Number of parallel attention heads.
@@ -242,14 +247,16 @@
         use_learnable_pos_emb (bool): If True, use learnable positional
             embedding, othersize use sinusoid encoding. Defaults to False.
         num_frames (int): Number of frames in the video. Defaults to 16.
         tubelet_size (int): Temporal size of one patch. Defaults to 2.
         use_mean_pooling (bool): If True, take the mean pooling over all
             positions. Defaults to True.
         pretrained (str, optional): Name of pretrained model. Default: None.
+        return_feat_map (bool): If True, return the feature in the shape of
+            `[B, C, T, H, W]`. Defaults to False.
         init_cfg (dict or list[dict]): Initialization config dict. Defaults to
             ``[
             dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.),
             dict(type='Constant', layer='LayerNorm', val=1., bias=0.)
             ]``.
     """
 
@@ -269,41 +276,42 @@
                  norm_cfg: ConfigType = dict(type='LN', eps=1e-6),
                  init_values: int = 0.,
                  use_learnable_pos_emb: bool = False,
                  num_frames: int = 16,
                  tubelet_size: int = 2,
                  use_mean_pooling: int = True,
                  pretrained: Optional[str] = None,
+                 return_feat_map: bool = False,
                  init_cfg: Optional[Union[Dict, List[Dict]]] = [
                      dict(
                          type='TruncNormal', layer='Linear', std=0.02,
                          bias=0.),
                      dict(type='Constant', layer='LayerNorm', val=1., bias=0.)
                  ],
                  **kwargs) -> None:
 
         if pretrained:
             self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)
         super().__init__(init_cfg=init_cfg)
 
-        patch_size = to_2tuple(patch_size)
-        img_size = to_2tuple(img_size)
+        self.embed_dims = embed_dims
+        self.patch_size = patch_size
 
         self.patch_embed = PatchEmbed(
             in_channels=in_channels,
             embed_dims=embed_dims,
             conv_type='Conv3d',
-            kernel_size=(tubelet_size, ) + patch_size,
-            stride=(tubelet_size, ) + patch_size,
+            kernel_size=(tubelet_size, patch_size, patch_size),
+            stride=(tubelet_size, patch_size, patch_size),
             padding=(0, 0, 0),
             dilation=(1, 1, 1))
 
-        num_patches = (img_size[1] // patch_size[1]) * \
-                      (img_size[0] // patch_size[0]) * \
-                      (num_frames // tubelet_size)
+        grid_size = img_size // patch_size
+        num_patches = grid_size**2 * (num_frames // tubelet_size)
+        self.grid_size = (grid_size, grid_size)
 
         if use_learnable_pos_emb:
             self.pos_embed = nn.Parameter(
                 torch.zeros(1, num_patches, embed_dims))
             nn.init.trunc_normal_(self.pos_embed, std=.02)
         else:
             # sine-cosine positional embeddings is on the way
@@ -332,30 +340,54 @@
         if use_mean_pooling:
             self.norm = nn.Identity()
             self.fc_norm = build_norm_layer(norm_cfg, embed_dims)[1]
         else:
             self.norm = build_norm_layer(norm_cfg, embed_dims)[1]
             self.fc_norm = None
 
+        self.return_feat_map = return_feat_map
+
     def forward(self, x: Tensor) -> Tensor:
         """Defines the computation performed at every call.
 
         Args:
             x (Tensor): The input data.
         Returns:
             Tensor: The feature of the input
                 samples extracted by the backbone.
         """
+        b, _, _, h, w = x.shape
+        h //= self.patch_size
+        w //= self.patch_size
         x = self.patch_embed(x)[0]
-        B, _, _ = x.size()
+        if (h, w) != self.grid_size:
+            pos_embed = self.pos_embed.reshape(-1, *self.grid_size,
+                                               self.embed_dims)
+            pos_embed = pos_embed.permute(0, 3, 1, 2)
+            pos_embed = F.interpolate(
+                pos_embed, size=(h, w), mode='bicubic', align_corners=False)
+            pos_embed = pos_embed.permute(0, 2, 3, 1).flatten(1, 2)
+            pos_embed = pos_embed.reshape(1, -1, self.embed_dims)
+        else:
+            pos_embed = self.pos_embed
 
-        x = x + self.pos_embed
+        x = x + pos_embed
         x = self.pos_drop(x)
 
         for blk in self.blocks:
             x = blk(x)
 
         x = self.norm(x)
+
+        if self.return_feat_map:
+            x = x.reshape(b, -1, h, w, self.embed_dims)
+            x = x.permute(0, 4, 1, 2, 3)
+            return x
+
         if self.fc_norm is not None:
             return self.fc_norm(x.mean(1))
-        else:
-            return x[:, 0]
+
+        return x[:, 0]
+
+
+if mmdet_imported:
+    MMDET_MODELS.register_module()(VisionTransformer)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/backbones/x3d.py` & `mmaction2-1.1.0/mmaction/models/backbones/x3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/common/conv2plus1d.py` & `mmaction2-1.1.0/mmaction/models/common/conv2plus1d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/common/conv_audio.py` & `mmaction2-1.1.0/mmaction/models/common/conv_audio.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/common/sub_batchnorm3d.py` & `mmaction2-1.1.0/mmaction/models/common/sub_batchnorm3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/common/tam.py` & `mmaction2-1.1.0/mmaction/models/common/tam.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/common/transformer.py` & `mmaction2-1.1.0/mmaction/models/common/transformer.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/data_preprocessors/data_preprocessor.py` & `mmaction2-1.1.0/mmaction/models/data_preprocessors/data_preprocessor.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,59 +1,57 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Optional, Sequence, Tuple, Union
+from typing import List, Optional, Sequence, Tuple, Union
 
 import torch
 from mmengine.model import BaseDataPreprocessor, stack_batch
 
 from mmaction.registry import MODELS
+from mmaction.utils import SampleList
 
 
 @MODELS.register_module()
 class ActionDataPreprocessor(BaseDataPreprocessor):
     """Data pre-processor for action recognition tasks.
 
     Args:
-        mean (Sequence[float or int, optional): The pixel mean of channels
+        mean (Sequence[float or int], optional): The pixel mean of channels
             of images or stacked optical flow. Defaults to None.
         std (Sequence[float or int], optional): The pixel standard deviation
             of channels of images or stacked optical flow. Defaults to None.
-        pad_size_divisor (int): The size of padded image should be
-            divisible by ``pad_size_divisor``. Defaults to 1.
-        pad_value (float or int): The padded pixel value. Defaults to 0.
         to_rgb (bool): Whether to convert image from BGR to RGB.
             Defaults to False.
+        to_float32 (bool): Whether to convert data to float32.
+            Defaults to True.
         blending (dict, optional): Config for batch blending.
             Defaults to None.
         format_shape (str): Format shape of input data.
             Defaults to ``'NCHW'``.
     """
 
     def __init__(self,
                  mean: Optional[Sequence[Union[float, int]]] = None,
                  std: Optional[Sequence[Union[float, int]]] = None,
-                 pad_size_divisor: int = 1,
-                 pad_value: Union[float, int] = 0,
                  to_rgb: bool = False,
+                 to_float32: bool = True,
                  blending: Optional[dict] = None,
                  format_shape: str = 'NCHW') -> None:
         super().__init__()
-        self.pad_size_divisor = pad_size_divisor
-        self.pad_value = pad_value
         self.to_rgb = to_rgb
+        self.to_float32 = to_float32
         self.format_shape = format_shape
 
         if mean is not None:
             assert std is not None, 'To enable the normalization in ' \
                                     'preprocessing, please specify both ' \
                                     '`mean` and `std`.'
             # Enable the normalization in preprocessing.
             self._enable_normalize = True
             if self.format_shape == 'NCHW':
                 normalizer_shape = (-1, 1, 1)
-            elif self.format_shape in ['NCTHW', 'NCTVM', 'MIX2d3d']:
+            elif self.format_shape in ['NCTHW', 'MIX2d3d']:
                 normalizer_shape = (-1, 1, 1, 1)
             else:
                 raise ValueError(f'Invalid format shape: {format_shape}')
 
             self.register_buffer(
                 'mean',
                 torch.tensor(mean, dtype=torch.float32).view(normalizer_shape),
@@ -77,46 +75,51 @@
         augmentation based on ``BaseDataPreprocessor``.
 
         Args:
             data (dict or Tuple[dict]): data sampled from dataloader.
             training (bool): Whether to enable training time augmentation.
 
         Returns:
-            dict or Tuple[dict]: Data in the same format as the model
-                input.
+            dict or Tuple[dict]: Data in the same format as the model input.
         """
+        data = self.cast_data(data)
         if isinstance(data, dict):
-            return self.forward_onesample(data, training)
+            return self.forward_onesample(data, training=training)
         elif isinstance(data, tuple):
             outputs = []
             for data_sample in data:
-                output = self.forward_onesample(data_sample, training)
+                output = self.forward_onesample(data_sample, training=training)
                 outputs.append(output)
             return tuple(outputs)
         else:
-            raise TypeError('Unsupported data type for `data`!')
+            raise TypeError(f'Unsupported data type: {type(data)}!')
 
-    def forward_onesample(self, data: dict, training: bool = False) -> dict:
+    def forward_onesample(self, data, training: bool = False) -> dict:
         """Perform normalization, padding, bgr2rgb conversion and batch
         augmentation on one data sample.
 
         Args:
             data (dict): data sampled from dataloader.
             training (bool): Whether to enable training time augmentation.
 
         Returns:
-            dict: Data in the same format as the model
-                input.
+            dict: Data in the same format as the model input.
         """
-        data = self.cast_data(data)
         inputs, data_samples = data['inputs'], data['data_samples']
+        inputs, data_samples = self.preprocess(inputs, data_samples, training)
+        data['inputs'] = inputs
+        data['data_samples'] = data_samples
+        return data
 
+    def preprocess(self,
+                   inputs: List[torch.Tensor],
+                   data_samples: SampleList,
+                   training: bool = False) -> Tuple:
         # --- Pad and stack --
-        batch_inputs = stack_batch(inputs, self.pad_size_divisor,
-                                   self.pad_value)
+        batch_inputs = stack_batch(inputs)
 
         if self.format_shape == 'MIX2d3d':
             if batch_inputs.ndim == 4:
                 format_shape, view_shape = 'NCHW', (-1, 1, 1)
             else:
                 format_shape, view_shape = 'NCTHW', None
         else:
@@ -135,17 +138,16 @@
         if self._enable_normalize:
             if view_shape is None:
                 batch_inputs = (batch_inputs - self.mean) / self.std
             else:
                 mean = self.mean.view(view_shape)
                 std = self.std.view(view_shape)
                 batch_inputs = (batch_inputs - mean) / std
-        else:
+        elif self.to_float32:
             batch_inputs = batch_inputs.to(torch.float32)
 
         # ----- Blending -----
         if training and self.blending is not None:
             batch_inputs, data_samples = self.blending(batch_inputs,
                                                        data_samples)
 
-        data['inputs'] = batch_inputs
-        return data
+        return batch_inputs, data_samples
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/__init__.py` & `mmaction2-1.1.0/mmaction/models/heads/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .base import BaseHead
+from .feature_head import FeatureHead
 from .gcn_head import GCNHead
 from .i3d_head import I3DHead
 from .mvit_head import MViTHead
 from .omni_head import OmniHead
+from .rgbpose_head import RGBPoseHead
 from .slowfast_head import SlowFastHead
 from .timesformer_head import TimeSformerHead
 from .tpn_head import TPNHead
 from .trn_head import TRNHead
 from .tsm_head import TSMHead
 from .tsn_audio_head import TSNAudioHead
 from .tsn_head import TSNHead
+from .uniformer_head import UniFormerHead
 from .x3d_head import X3DHead
 
 __all__ = [
     'BaseHead', 'GCNHead', 'I3DHead', 'MViTHead', 'OmniHead', 'SlowFastHead',
     'TPNHead', 'TRNHead', 'TSMHead', 'TSNAudioHead', 'TSNHead',
-    'TimeSformerHead', 'X3DHead'
+    'TimeSformerHead', 'UniFormerHead', 'RGBPoseHead', 'X3DHead', 'FeatureHead'
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/base.py` & `mmaction2-1.1.0/mmaction/models/heads/base.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,75 +1,72 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
-from typing import Tuple, Union
+from typing import Dict, Optional, Tuple, Union
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from mmengine.model import BaseModule
 from mmengine.structures import LabelData
-from torch import Tensor
 
 from mmaction.evaluation import top_k_accuracy
 from mmaction.registry import MODELS
-from mmaction.utils import (ConfigType, LabelList, OptConfigType,
-                            OptMultiConfig, SampleList)
+from mmaction.utils import ForwardResults, SampleList
 
 
 class AvgConsensus(nn.Module):
     """Average consensus module.
 
     Args:
         dim (int): Decide which dim consensus function to apply.
-            Default: 1.
+            Defaults to 1.
     """
 
     def __init__(self, dim: int = 1) -> None:
         super().__init__()
         self.dim = dim
 
-    def forward(self, x: Tensor) -> Tensor:
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         """Defines the computation performed at every call."""
         return x.mean(dim=self.dim, keepdim=True)
 
 
 class BaseHead(BaseModule, metaclass=ABCMeta):
     """Base class for head.
 
     All Head should subclass it.
     All subclass should overwrite:
-    - :meth:`init_weights`, initializing weights in some modules.
     - :meth:`forward`, supporting to forward both for training and testing.
 
     Args:
         num_classes (int): Number of classes to be classified.
         in_channels (int): Number of channels in input feature.
-        loss_cls (dict or ConfigDict): Config for building loss.
-            Default: dict(type='CrossEntropyLoss', loss_weight=1.0).
+        loss_cls (dict): Config for building loss.
+            Defaults to ``dict(type='CrossEntropyLoss', loss_weight=1.0)``.
         multi_class (bool): Determines whether it is a multi-class
-            recognition task. Default: False.
+            recognition task. Defaults to False.
         label_smooth_eps (float): Epsilon used in label smooth.
-            Reference: arxiv.org/abs/1906.02629. Default: 0.
-        topk (int or tuple): Top-k accuracy. Default: (1, 5).
-        average_clips (dict or ConfigDict, optional): Config for
-            averaging class scores over multiple clips. Default: None.
-        init_cfg (dict or ConfigDict, optional): Config to control the
-           initialization. Defaults to None.
+            Reference: arxiv.org/abs/1906.02629. Defaults to 0.
+        topk (int or tuple): Top-k accuracy. Defaults to ``(1, 5)``.
+        average_clips (dict, optional): Config for averaging class
+            scores over multiple clips. Defaults to None.
+        init_cfg (dict, optional): Config to control the initialization.
+            Defaults to None.
     """
 
     def __init__(self,
                  num_classes: int,
                  in_channels: int,
-                 loss_cls: ConfigType = dict(
+                 loss_cls: Dict = dict(
                      type='CrossEntropyLoss', loss_weight=1.0),
                  multi_class: bool = False,
                  label_smooth_eps: float = 0.0,
                  topk: Union[int, Tuple[int]] = (1, 5),
-                 average_clips: OptConfigType = None,
-                 init_cfg: OptMultiConfig = None) -> None:
+                 average_clips: Optional[Dict] = None,
+                 init_cfg: Optional[Dict] = None) -> None:
         super(BaseHead, self).__init__(init_cfg=init_cfg)
         self.num_classes = num_classes
         self.in_channels = in_channels
         self.loss_cls = MODELS.build(loss_cls)
         self.multi_class = multi_class
         self.label_smooth_eps = label_smooth_eps
         self.average_clips = average_clips
@@ -77,42 +74,43 @@
         if isinstance(topk, int):
             topk = (topk, )
         for _topk in topk:
             assert _topk > 0, 'Top-k should be larger than 0'
         self.topk = topk
 
     @abstractmethod
-    def forward(self, x, **kwargs) -> Tensor:
+    def forward(self, x, **kwargs) -> ForwardResults:
         """Defines the computation performed at every call."""
         raise NotImplementedError
 
-    def loss(self, feats: Union[Tensor, Tuple[Tensor]],
-             data_samples: SampleList, **kwargs) -> dict:
+    def loss(self, feats: Union[torch.Tensor, Tuple[torch.Tensor]],
+             data_samples: SampleList, **kwargs) -> Dict:
         """Perform forward propagation of head and loss calculation on the
         features of the upstream network.
 
         Args:
-            feats (Tensor or Tuple[Tensor]): Features from upstream network.
-            data_samples (List[:obj:`ActionDataSample`]): The batch
+            feats (torch.Tensor | tuple[torch.Tensor]): Features from
+                upstream network.
+            data_samples (list[:obj:`ActionDataSample`]): The batch
                 data samples.
 
         Returns:
             dict: A dictionary of loss components.
         """
         cls_scores = self(feats, **kwargs)
         return self.loss_by_feat(cls_scores, data_samples)
 
-    def loss_by_feat(self, cls_scores: Union[Tensor, Tuple[Tensor]],
-                     data_samples: SampleList) -> dict:
+    def loss_by_feat(self, cls_scores: torch.Tensor,
+                     data_samples: SampleList) -> Dict:
         """Calculate the loss based on the features extracted by the head.
 
         Args:
-            cls_scores (Tensor): Classification prediction results of
+            cls_scores (torch.Tensor): Classification prediction results of
                 all class, has shape (batch_size, num_classes).
-            data_samples (List[:obj:`ActionDataSample`]): The batch
+            data_samples (list[:obj:`ActionDataSample`]): The batch
                 data samples.
 
         Returns:
             dict: A dictionary of loss components.
         """
         labels = [x.gt_labels.item for x in data_samples]
         labels = torch.stack(labels).to(cls_scores.device)
@@ -120,15 +118,15 @@
 
         losses = dict()
         if labels.shape == torch.Size([]):
             labels = labels.unsqueeze(0)
         elif labels.dim() == 1 and labels.size()[0] == self.num_classes \
                 and cls_scores.size()[0] == 1:
             # Fix a bug when training with soft labels and batch size is 1.
-            # When using soft labels, `labels` and `cls_socre` share the same
+            # When using soft labels, `labels` and `cls_score` share the same
             # shape.
             labels = labels.unsqueeze(0)
 
         if cls_scores.size() != labels.size():
             top_k_acc = top_k_accuracy(cls_scores.detach().cpu().numpy(),
                                        labels.detach().cpu().numpy(),
                                        self.topk)
@@ -145,81 +143,85 @@
         # loss_cls may be dictionary or single tensor
         if isinstance(loss_cls, dict):
             losses.update(loss_cls)
         else:
             losses['loss_cls'] = loss_cls
         return losses
 
-    def predict(self, feats: Union[Tensor, Tuple[Tensor]],
-                data_samples: SampleList, **kwargs) -> LabelList:
+    def predict(self, feats: Union[torch.Tensor, Tuple[torch.Tensor]],
+                data_samples: SampleList, **kwargs) -> SampleList:
         """Perform forward propagation of head and predict recognition results
         on the features of the upstream network.
 
         Args:
-            feats (Tensor or Tuple[Tensor]): Features from upstream network.
-            data_samples (List[:obj:`ActionDataSample`]): The batch
+            feats (torch.Tensor | tuple[torch.Tensor]): Features from
+                upstream network.
+            data_samples (list[:obj:`ActionDataSample`]): The batch
                 data samples.
 
         Returns:
-             List[:obj:`ActionDataSample`]: Recognition results wrapped
+             list[:obj:`ActionDataSample`]: Recognition results wrapped
                 by :obj:`ActionDataSample`.
         """
         cls_scores = self(feats, **kwargs)
         return self.predict_by_feat(cls_scores, data_samples)
 
-    def predict_by_feat(self, cls_scores: Tensor,
-                        data_samples: SampleList) -> LabelList:
+    def predict_by_feat(self, cls_scores: torch.Tensor,
+                        data_samples: SampleList) -> SampleList:
         """Transform a batch of output features extracted from the head into
         prediction results.
 
         Args:
-            cls_scores (Tensor): Classification scores, has a shape
-                    (num_classes, )
-            data_samples (List[:obj:`ActionDataSample`]): The
+            cls_scores (torch.Tensor): Classification scores, has a shape
+                (B*num_segs, num_classes)
+            data_samples (list[:obj:`ActionDataSample`]): The
                 annotation data of every samples. It usually includes
                 information such as `gt_labels`.
 
         Returns:
             List[:obj:`ActionDataSample`]: Recognition results wrapped
                 by :obj:`ActionDataSample`.
         """
         num_segs = cls_scores.shape[0] // len(data_samples)
         cls_scores = self.average_clip(cls_scores, num_segs=num_segs)
         pred_labels = cls_scores.argmax(dim=-1, keepdim=True).detach()
 
-        for data_sample, score, pred_lable in zip(data_samples, cls_scores,
+        for data_sample, score, pred_label in zip(data_samples, cls_scores,
                                                   pred_labels):
             prediction = LabelData(item=score)
-            pred_label = LabelData(item=pred_lable)
+            pred_label = LabelData(item=pred_label)
             data_sample.pred_scores = prediction
             data_sample.pred_labels = pred_label
         return data_samples
 
-    def average_clip(self, cls_scores: Tensor, num_segs: int = 1) -> Tensor:
+    def average_clip(self,
+                     cls_scores: torch.Tensor,
+                     num_segs: int = 1) -> torch.Tensor:
         """Averaging class scores over multiple clips.
 
         Using different averaging types ('score' or 'prob' or None,
         which defined in test_cfg) to computed the final averaged
         class score. Only called in test mode.
 
         Args:
-            cls_scores (Tensor): Class scores to be averaged.
+            cls_scores (torch.Tensor): Class scores to be averaged.
             num_segs (int): Number of clips for each input sample.
 
         Returns:
-            Tensor: Averaged class scores.
+            torch.Tensor: Averaged class scores.
         """
 
         if self.average_clips not in ['score', 'prob', None]:
             raise ValueError(f'{self.average_clips} is not supported. '
                              f'Currently supported ones are '
                              f'["score", "prob", None]')
 
         batch_size = cls_scores.shape[0]
-        cls_scores = cls_scores.view(batch_size // num_segs, num_segs, -1)
+        cls_scores = cls_scores.view((batch_size // num_segs, num_segs) +
+                                     cls_scores.shape[1:])
 
         if self.average_clips is None:
             return cls_scores
         elif self.average_clips == 'prob':
             cls_scores = F.softmax(cls_scores, dim=2).mean(dim=1)
         elif self.average_clips == 'score':
             cls_scores = cls_scores.mean(dim=1)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/gcn_head.py` & `mmaction2-1.1.0/mmaction/models/heads/gcn_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/i3d_head.py` & `mmaction2-1.1.0/mmaction/models/heads/i3d_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/mvit_head.py` & `mmaction2-1.1.0/mmaction/models/heads/mvit_head.py`

 * *Files 9% similar despite different names*

```diff
@@ -17,33 +17,38 @@
     for Classification and Detection <https://arxiv.org/abs/2112.01526>`_
 
     Args:
         num_classes (int): Number of classes to be classified.
         in_channels (int): Number of channels in input feature.
         loss_cls (dict or ConfigDict): Config for building loss.
             Defaults to `dict(type='CrossEntropyLoss')`.
-        dropout_ratio (float): Probability of dropout layer. Default: 0.5.
+        dropout_ratio (float): Probability of dropout layer. Defaults to 0.5.
         init_std (float): Std value for Initiation. Defaults to 0.02.
-        init_scale (float): Scale factor for Initiation parameters. Default: 1.
+        init_scale (float): Scale factor for Initiation parameters.
+            Defaults to 1.
+        with_cls_token (bool): Whether the backbone output feature with
+            cls_token. Defaults to True.
         kwargs (dict, optional): Any keyword argument to be used to initialize
             the head.
     """
 
     def __init__(self,
                  num_classes: int,
                  in_channels: int,
                  loss_cls: ConfigType = dict(type='CrossEntropyLoss'),
                  dropout_ratio: float = 0.5,
                  init_std: float = 0.02,
                  init_scale: float = 1.0,
+                 with_cls_token: bool = True,
                  **kwargs) -> None:
         super().__init__(num_classes, in_channels, loss_cls, **kwargs)
         self.init_std = init_std
         self.init_scale = init_scale
         self.dropout_ratio = dropout_ratio
+        self.with_cls_token = with_cls_token
         if self.dropout_ratio != 0:
             self.dropout = nn.Dropout(p=self.dropout_ratio)
         else:
             self.dropout = None
         self.fc_cls = nn.Linear(self.in_channels, self.num_classes)
 
     def init_weights(self) -> None:
@@ -55,16 +60,20 @@
 
     def pre_logits(self, feats: Tuple[List[Tensor]]) -> Tensor:
         """The process before the final classification head.
 
         The input ``feats`` is a tuple of list of tensor, and each tensor is
         the feature of a backbone stage.
         """
-        _, cls_token = feats[-1]
-        return cls_token
+        if self.with_cls_token:
+            _, cls_token = feats[-1]
+            return cls_token
+        else:
+            patch_token = feats[-1]
+            return patch_token.mean(dim=(2, 3, 4))
 
     def forward(self, x: Tuple[List[Tensor]], **kwargs) -> Tensor:
         """Defines the computation performed at every call.
 
         Args:
             x (Tuple[List[Tensor]]): The input data.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/omni_head.py` & `mmaction2-1.1.0/mmaction/models/heads/omni_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/slowfast_head.py` & `mmaction2-1.1.0/mmaction/models/heads/slowfast_head.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import Tuple
+
 import torch
 from mmengine.model.weight_init import normal_init
 from torch import Tensor, nn
 
 from mmaction.registry import MODELS
 from mmaction.utils import ConfigType
 from .base import BaseHead
@@ -49,30 +51,30 @@
         else:
             self.avg_pool = None
 
     def init_weights(self) -> None:
         """Initiate the parameters from scratch."""
         normal_init(self.fc_cls, std=self.init_std)
 
-    def forward(self, x: Tensor, **kwargs) -> None:
+    def forward(self, x: Tuple[Tensor], **kwargs) -> None:
         """Defines the computation performed at every call.
 
         Args:
-            x (Tensor): The input data.
+            x (tuple[torch.Tensor]): The input data.
 
         Returns:
             Tensor: The classification scores for input samples.
         """
-        # ([N, channel_fast, T, H, W], [(N, channel_slow, T, H, W)])
-        x_fast, x_slow = x
-        # ([N, channel_fast, 1, 1, 1], [N, channel_slow, 1, 1, 1])
-        x_fast = self.avg_pool(x_fast)
+        # ([N, channel_slow, T1, H, W], [(N, channel_fast, T2, H, W)])
+        x_slow, x_fast = x
+        # ([N, channel_slow, 1, 1, 1], [N, channel_fast, 1, 1, 1])
         x_slow = self.avg_pool(x_slow)
+        x_fast = self.avg_pool(x_fast)
         # [N, channel_fast + channel_slow, 1, 1, 1]
-        x = torch.cat((x_slow, x_fast), dim=1)
+        x = torch.cat((x_fast, x_slow), dim=1)
 
         if self.dropout is not None:
             x = self.dropout(x)
 
         # [N x C]
         x = x.view(x.size(0), -1)
         # [N x num_classes]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/timesformer_head.py` & `mmaction2-1.1.0/mmaction/models/heads/timesformer_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/tpn_head.py` & `mmaction2-1.1.0/mmaction/models/heads/tpn_head.py`

 * *Files 11% similar despite different names*

```diff
@@ -49,15 +49,16 @@
             Tensor: The classification scores for input samples.
         """
         if fcn_test:
             if self.avg_pool3d:
                 x = self.avg_pool3d(x)
             if self.new_cls is None:
                 self._init_new_cls()
-            cls_score_feat_map = self.new_cls(x)
+            x = self.new_cls(x)
+            cls_score_feat_map = x.view(x.size(0), -1)
             return cls_score_feat_map
 
         if self.avg_pool2d is None:
             kernel_size = (1, x.shape[-2], x.shape[-1])
             self.avg_pool2d = nn.AvgPool3d(kernel_size, stride=1, padding=0)
 
         if num_segs is None:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/trn_head.py` & `mmaction2-1.1.0/mmaction/models/heads/trn_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/tsm_head.py` & `mmaction2-1.1.0/mmaction/models/heads/tsm_head.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import torch
 from mmengine.model.weight_init import normal_init
 from torch import Tensor, nn
 
 from mmaction.registry import MODELS
-from mmaction.utils import ConfigType
+from mmaction.utils import ConfigType, get_str_type
 from .base import AvgConsensus, BaseHead
 
 
 @MODELS.register_module()
 class TSMHead(BaseHead):
     """Class head for TSM.
 
@@ -50,15 +50,15 @@
         self.init_std = init_std
         self.is_shift = is_shift
         self.temporal_pool = temporal_pool
 
         consensus_ = consensus.copy()
 
         consensus_type = consensus_.pop('type')
-        if consensus_type == 'AvgConsensus':
+        if get_str_type(consensus_type) == 'AvgConsensus':
             self.consensus = AvgConsensus(**consensus_)
         else:
             self.consensus = None
 
         if self.dropout_ratio != 0:
             self.dropout = nn.Dropout(p=self.dropout_ratio)
         else:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/tsn_audio_head.py` & `mmaction2-1.1.0/mmaction/models/heads/tsn_audio_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/tsn_head.py` & `mmaction2-1.1.0/mmaction/models/heads/tsn_head.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from mmengine.model.weight_init import normal_init
 from torch import Tensor, nn
 
 from mmaction.registry import MODELS
-from mmaction.utils import ConfigType
+from mmaction.utils import ConfigType, get_str_type
 from .base import AvgConsensus, BaseHead
 
 
 @MODELS.register_module()
 class TSNHead(BaseHead):
     """Class head for TSN.
 
@@ -39,15 +39,15 @@
         self.spatial_type = spatial_type
         self.dropout_ratio = dropout_ratio
         self.init_std = init_std
 
         consensus_ = consensus.copy()
 
         consensus_type = consensus_.pop('type')
-        if consensus_type == 'AvgConsensus':
+        if get_str_type(consensus_type) == 'AvgConsensus':
             self.consensus = AvgConsensus(**consensus_)
         else:
             self.consensus = None
 
         if self.spatial_type == 'avg':
             # use `nn.AdaptiveAvgPool2d` to adaptively match the in_channels.
             self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/heads/x3d_head.py` & `mmaction2-1.1.0/mmaction/models/heads/x3d_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/localizers/bmn.py` & `mmaction2-1.1.0/mmaction/models/localizers/bmn.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/localizers/bsn.py` & `mmaction2-1.1.0/mmaction/models/localizers/bsn.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,14 +15,15 @@
     """Temporal Evaluation Model for Boundary Sensitive Network.
 
     Please refer `BSN: Boundary Sensitive Network for Temporal Action
     Proposal Generation <http://arxiv.org/abs/1806.02964>`_.
     Code reference
     https://github.com/wzmsltw/BSN-boundary-sensitive-network
     Args:
+        temporal_dim (int): Total frames selected for each video.
         tem_feat_dim (int): Feature dimension.
         tem_hidden_dim (int): Hidden layer dimension.
         tem_match_threshold (float): Temporal evaluation match threshold.
         loss_cls (dict): Config for building loss.
             Default: ``dict(type='BinaryLogisticRegressionLoss')``.
         loss_weight (float): Weight term for action_loss. Default: 2.
         output_dim (int): Output dimension. Default: 3.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/localizers/utils/bsn_utils.py` & `mmaction2-1.1.0/mmaction/models/localizers/utils/bsn_utils.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/localizers/utils/proposal_utils.py` & `mmaction2-1.1.0/mmaction/models/localizers/utils/proposal_utils.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/__init__.py` & `mmaction2-1.1.0/mmaction/models/losses/__init__.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/base.py` & `mmaction2-1.1.0/mmaction/models/losses/base.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/binary_logistic_regression_loss.py` & `mmaction2-1.1.0/mmaction/models/losses/binary_logistic_regression_loss.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/bmn_loss.py` & `mmaction2-1.1.0/mmaction/models/losses/bmn_loss.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/cross_entropy_loss.py` & `mmaction2-1.1.0/mmaction/models/losses/cross_entropy_loss.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,10 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from typing import List, Optional
+
 import numpy as np
 import torch
 import torch.nn.functional as F
 
 from mmaction.registry import MODELS
 from .base import BaseWeightedLoss
 
@@ -13,35 +15,38 @@
 
     Support two kinds of labels and their corresponding loss type. It's worth
     mentioning that loss type will be detected by the shape of ``cls_score``
     and ``label``.
     1) Hard label: This label is an integer array and all of the elements are
         in the range [0, num_classes - 1]. This label's shape should be
         ``cls_score``'s shape with the `num_classes` dimension removed.
-    2) Soft label(probablity distribution over classes): This label is a
+    2) Soft label(probability distribution over classes): This label is a
         probability distribution and all of the elements are in the range
         [0, 1]. This label's shape must be the same as ``cls_score``. For now,
         only 2-dim soft label is supported.
 
     Args:
         loss_weight (float): Factor scalar multiplied on the loss.
-            Default: 1.0.
+            Defaults to 1.0.
         class_weight (list[float] | None): Loss weight for each class. If set
             as None, use the same weight 1 for all classes. Only applies
             to CrossEntropyLoss and BCELossWithLogits (should not be set when
-            using other losses). Default: None.
+            using other losses). Defaults to None.
     """
 
-    def __init__(self, loss_weight=1.0, class_weight=None):
+    def __init__(self,
+                 loss_weight: float = 1.0,
+                 class_weight: Optional[List[float]] = None) -> None:
         super().__init__(loss_weight=loss_weight)
         self.class_weight = None
         if class_weight is not None:
             self.class_weight = torch.Tensor(class_weight)
 
-    def _forward(self, cls_score, label, **kwargs):
+    def _forward(self, cls_score: torch.Tensor, label: torch.Tensor,
+                 **kwargs) -> torch.Tensor:
         """Forward function.
 
         Args:
             cls_score (torch.Tensor): The class score.
             label (torch.Tensor): The ground truth label.
             kwargs: Any keyword argument to be used to calculate
                 CrossEntropy loss.
@@ -85,28 +90,31 @@
 
 @MODELS.register_module()
 class BCELossWithLogits(BaseWeightedLoss):
     """Binary Cross Entropy Loss with logits.
 
     Args:
         loss_weight (float): Factor scalar multiplied on the loss.
-            Default: 1.0.
+            Defaults to 1.0.
         class_weight (list[float] | None): Loss weight for each class. If set
             as None, use the same weight 1 for all classes. Only applies
             to CrossEntropyLoss and BCELossWithLogits (should not be set when
-            using other losses). Default: None.
+            using other losses). Defaults to None.
     """
 
-    def __init__(self, loss_weight=1.0, class_weight=None):
+    def __init__(self,
+                 loss_weight: float = 1.0,
+                 class_weight: Optional[List[float]] = None) -> None:
         super().__init__(loss_weight=loss_weight)
         self.class_weight = None
         if class_weight is not None:
             self.class_weight = torch.Tensor(class_weight)
 
-    def _forward(self, cls_score, label, **kwargs):
+    def _forward(self, cls_score: torch.Tensor, label: torch.Tensor,
+                 **kwargs) -> torch.Tensor:
         """Forward function.
 
         Args:
             cls_score (torch.Tensor): The class score.
             label (torch.Tensor): The ground truth label.
             kwargs: Any keyword argument to be used to calculate
                 bce loss with logits.
@@ -126,38 +134,39 @@
 class CBFocalLoss(BaseWeightedLoss):
     """Class Balanced Focal Loss. Adapted from https://github.com/abhinanda-
     punnakkal/BABEL/. This loss is used in the skeleton-based action
     recognition baseline for BABEL.
 
     Args:
         loss_weight (float): Factor scalar multiplied on the loss.
-            Default: 1.0.
+            Defaults to 1.0.
         samples_per_cls (list[int]): The number of samples per class.
-            Default: [].
+            Defaults to [].
         beta (float): Hyperparameter that controls the per class loss weight.
-            Default: 0.9999.
-        gamma (float): Hyperparameter of the focal loss. Default: 2.0.
+            Defaults to 0.9999.
+        gamma (float): Hyperparameter of the focal loss. Defaults to 2.0.
     """
 
     def __init__(self,
-                 loss_weight=1.0,
-                 samples_per_cls=[],
-                 beta=0.9999,
-                 gamma=2.):
+                 loss_weight: float = 1.0,
+                 samples_per_cls: List[int] = [],
+                 beta: float = 0.9999,
+                 gamma: float = 2.) -> None:
         super().__init__(loss_weight=loss_weight)
         self.samples_per_cls = samples_per_cls
         self.beta = beta
         self.gamma = gamma
         effective_num = 1.0 - np.power(beta, samples_per_cls)
         weights = (1.0 - beta) / np.array(effective_num)
         weights = weights / np.sum(weights) * len(weights)
         self.weights = weights
         self.num_classes = len(weights)
 
-    def _forward(self, cls_score, label, **kwargs):
+    def _forward(self, cls_score: torch.Tensor, label: torch.Tensor,
+                 **kwargs) -> torch.Tensor:
         """Forward function.
 
         Args:
             cls_score (torch.Tensor): The class score.
             label (torch.Tensor): The ground truth label.
             kwargs: Any keyword argument to be used to calculate
                 bce loss with logits.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/hvu_loss.py` & `mmaction2-1.1.0/mmaction/models/losses/hvu_loss.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/nll_loss.py` & `mmaction2-1.1.0/mmaction/models/losses/nll_loss.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/ohem_hinge_loss.py` & `mmaction2-1.1.0/mmaction/models/losses/ohem_hinge_loss.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/losses/ssn_loss.py` & `mmaction2-1.1.0/mmaction/models/losses/ssn_loss.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/necks/tpn.py` & `mmaction2-1.1.0/mmaction/models/necks/tpn.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/recognizers/base.py` & `mmaction2-1.1.0/mmaction/models/recognizers/base.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+import inspect
 import warnings
 from abc import ABCMeta, abstractmethod
 
 import torch
 import torch.nn as nn
 from mmengine.model import BaseModel, merge_dict
 
@@ -40,49 +41,81 @@
         if data_preprocessor is None:
             # This preprocessor will only stack batch data samples.
             data_preprocessor = dict(type='ActionDataPreprocessor')
 
         super(BaseRecognizer,
               self).__init__(data_preprocessor=data_preprocessor)
 
+        def is_from(module, pkg_name):
+            # check whether the backbone is from pkg
+            model_type = module['type']
+            if isinstance(model_type, str):
+                return model_type.startswith(pkg_name)
+            elif inspect.isclass(model_type) or inspect.isfunction(model_type):
+                module_name = model_type.__module__
+                return pkg_name in module_name
+            else:
+                raise TypeError(
+                    f'Unsupported type of module {type(module["type"])}')
+
         # Record the source of the backbone.
         self.backbone_from = 'mmaction2'
-
-        if backbone['type'].startswith('mmcls.'):
+        if is_from(backbone, 'mmcls.'):
             try:
                 # Register all mmcls models.
                 import mmcls.models  # noqa: F401
             except (ImportError, ModuleNotFoundError):
                 raise ImportError('Please install mmcls to use this backbone.')
             self.backbone = MODELS.build(backbone)
             self.backbone_from = 'mmcls'
-        elif backbone['type'].startswith('torchvision.'):
+        elif is_from(backbone, 'mmpretrain.'):
+            try:
+                # Register all mmpretrain models.
+                import mmpretrain.models  # noqa: F401
+            except (ImportError, ModuleNotFoundError):
+                raise ImportError(
+                    'Please install mmpretrain to use this backbone.')
+            self.backbone = MODELS.build(backbone)
+            self.backbone_from = 'mmpretrain'
+        elif is_from(backbone, 'torchvision.'):
             try:
                 import torchvision.models
             except (ImportError, ModuleNotFoundError):
                 raise ImportError('Please install torchvision to use this '
                                   'backbone.')
-            backbone_type = backbone.pop('type')[12:]
-            self.backbone = torchvision.models.__dict__[backbone_type](
-                **backbone)
+            self.backbone_from = 'torchvision'
+            self.feature_shape = backbone.pop('feature_shape', None)
+            backbone_type = backbone.pop('type')
+            if isinstance(backbone_type, str):
+                backbone_type = backbone_type[12:]
+                self.backbone = torchvision.models.__dict__[backbone_type](
+                    **backbone)
+            else:
+                self.backbone = backbone_type(**backbone)
             # disable the classifier
             self.backbone.classifier = nn.Identity()
             self.backbone.fc = nn.Identity()
-            self.backbone_from = 'torchvision'
-        elif backbone['type'].startswith('timm.'):
+        elif is_from(backbone, 'timm.'):
+            # currently, only support use `str` as backbone type
             try:
                 import timm
             except (ImportError, ModuleNotFoundError):
-                raise ImportError('Please install timm to use this '
+                raise ImportError('Please install timm>=0.9.0 to use this '
                                   'backbone.')
-            backbone_type = backbone.pop('type')[5:]
+            self.backbone_from = 'timm'
+            self.feature_shape = backbone.pop('feature_shape', None)
             # disable the classifier
             backbone['num_classes'] = 0
-            self.backbone = timm.create_model(backbone_type, **backbone)
-            self.backbone_from = 'timm'
+            backbone_type = backbone.pop('type')
+            if isinstance(backbone_type, str):
+                backbone_type = backbone_type[5:]
+                self.backbone = timm.create_model(backbone_type, **backbone)
+            else:
+                raise TypeError(
+                    f'Unsupported timm backbone type: {type(backbone_type)}')
         else:
             self.backbone = MODELS.build(backbone)
 
         if neck is not None:
             self.neck = MODELS.build(neck)
 
         if cls_head is not None:
@@ -103,21 +136,27 @@
     @property
     def with_cls_head(self) -> bool:
         """bool: whether the recognizer has a cls_head"""
         return hasattr(self, 'cls_head') and self.cls_head is not None
 
     def init_weights(self) -> None:
         """Initialize the model network weights."""
-        super().init_weights()
         if self.backbone_from in ['torchvision', 'timm']:
             warnings.warn('We do not initialize weights for backbones in '
                           f'{self.backbone_from}, since the weights for '
                           f'backbones in {self.backbone_from} are initialized '
                           'in their __init__ functions.')
 
+            def fake_init():
+                pass
+
+            # avoid repeated initialization
+            self.backbone.init_weights = fake_init
+        super().init_weights()
+
     def loss(self, inputs: torch.Tensor, data_samples: SampleList,
              **kwargs) -> dict:
         """Calculate losses from a batch of inputs and data samples.
 
         Args:
             inputs (torch.Tensor): Raw Inputs of the recognizer.
                 These should usually be mean centered and std scaled.
@@ -200,15 +239,15 @@
 
         Note that this method doesn't handle neither back propagation nor
         optimizer updating, which are done in the :meth:`train_step`.
 
         Args:
             inputs (torch.Tensor): The input tensor with shape
                 (N, C, ...) in general.
-            data_samples (List[``ActionDataSample`1], optional): The
+            data_samples (List[``ActionDataSample], optional): The
                 annotation data of every samples. Defaults to None.
             mode (str): Return what kind of value. Defaults to ``tensor``.
 
         Returns:
             The return type depends on ``mode``.
 
             - If ``mode="tensor"``, return a tensor or a tuple of tensor.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer2d.py` & `mmaction2-1.1.0/mmaction/models/recognizers/recognizer3d.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,118 +1,115 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import torch.nn as nn
+import torch
 from torch import Tensor
 
 from mmaction.registry import MODELS
-from mmaction.utils import SampleList
+from mmaction.utils import OptSampleList
 from .base import BaseRecognizer
 
 
 @MODELS.register_module()
-class Recognizer2D(BaseRecognizer):
-    """2D recognizer model framework."""
+class Recognizer3D(BaseRecognizer):
+    """3D recognizer model framework."""
 
     def extract_feat(self,
                      inputs: Tensor,
                      stage: str = 'neck',
-                     data_samples: SampleList = None,
+                     data_samples: OptSampleList = None,
                      test_mode: bool = False) -> tuple:
         """Extract features of different stages.
 
         Args:
-            inputs (Tensor): The input data.
+            inputs (torch.Tensor): The input data.
             stage (str): Which stage to output the feature.
-                Defaults to ``neck``.
-            data_samples (List[:obj:`ActionDataSample`]): Action data
+                Defaults to ``'neck'``.
+            data_samples (list[:obj:`ActionDataSample`], optional): Action data
                 samples, which are only needed in training. Defaults to None.
-            test_mode: (bool): Whether in test mode. Defaults to False.
+            test_mode (bool): Whether in test mode. Defaults to False.
 
         Returns:
-                Tensor: The extracted features.
+                torch.Tensor: The extracted features.
                 dict: A dict recording the kwargs for downstream
                     pipeline. These keys are usually included:
-                    ``num_segs``, ``fcn_test``, ``loss_aux``.
+                    ``loss_aux``.
         """
 
-        # Record the kwargs required by `loss` and `predict`.
+        # Record the kwargs required by `loss` and `predict`
         loss_predict_kwargs = dict()
 
         num_segs = inputs.shape[1]
-        loss_predict_kwargs['num_segs'] = num_segs
-
-        # [N, num_crops * num_segs, C, H, W] ->
-        # [N * num_crops * num_segs, C, H, W]
+        # [N, num_crops, C, T, H, W] ->
+        # [N * num_crops, C, T, H, W]
         # `num_crops` is calculated by:
         #   1) `twice_sample` in `SampleFrames`
         #   2) `num_sample_positions` in `DenseSampleFrames`
         #   3) `ThreeCrop/TenCrop` in `test_pipeline`
         #   4) `num_clips` in `SampleFrames` or its subclass if `clip_len != 1`
         inputs = inputs.view((-1, ) + inputs.shape[2:])
 
-        # Check settings of `fcn_test`.
-        fcn_test = False
+        # Check settings of test
         if test_mode:
+            if self.test_cfg is not None:
+                loss_predict_kwargs['fcn_test'] = self.test_cfg.get(
+                    'fcn_test', False)
             if self.test_cfg is not None and self.test_cfg.get(
-                    'fcn_test', False):
-                fcn_test = True
-                num_segs = self.test_cfg.get('num_segs',
-                                             self.backbone.num_segments)
-            loss_predict_kwargs['fcn_test'] = fcn_test
-
-        # Extract features through backbone.
-        if (hasattr(self.backbone, 'features')
-                and self.backbone_from == 'torchvision'):
-            x = self.backbone.features(inputs)
-        elif self.backbone_from == 'timm':
-            x = self.backbone.forward_features(inputs)
-        elif self.backbone_from == 'mmcls':
-            x = self.backbone(inputs)
-            if isinstance(x, tuple):
-                assert len(x) == 1
-                x = x[0]
-        else:
-            x = self.backbone(inputs)
-
-        if self.backbone_from in ['torchvision', 'timm']:
-            # Transformer-based feature shape: B x L x C.
-            if len(x.shape) == 3 and x.shape[2] > 1:
-                x = nn.AdaptiveAvgPool1d(1)(x.transpose(1, 2))  # B x C x 1
-            # Resnet-based feature shape: B x C x Hs x Ws。
-            if len(x.shape) == 4 and (x.shape[2] > 1 or x.shape[3] > 1):
-                x = nn.AdaptiveAvgPool2d(1)(x)  # B x C x 1 x 1
-            x = x.reshape((x.shape[0], -1))  # B x C
-            x = x.reshape(x.shape + (1, 1))  # B x C x 1 x 1
-
-        # Return features extracted through backbone.
-        if stage == 'backbone':
-            return x, loss_predict_kwargs
-
-        loss_aux = dict()
-        if self.with_neck:
-            # x is a tuple with multiple feature maps.
-            x = [
-                each.reshape((-1, num_segs) +
-                             each.shape[1:]).transpose(1, 2).contiguous()
-                for each in x
-            ]
-            x, loss_aux = self.neck(x, data_samples=data_samples)
-            if not fcn_test:
-                x = x.squeeze(2)
-                loss_predict_kwargs['num_segs'] = 1
-        elif fcn_test:
-            # full convolution (fcn) testing when no neck
-            # [N * num_crops * num_segs, C', H', W'] ->
-            # [N * num_crops, C', num_segs, H', W']
-            x = x.reshape((-1, num_segs) +
-                          x.shape[1:]).transpose(1, 2).contiguous()
-
-        loss_predict_kwargs['loss_aux'] = loss_aux
+                    'max_testing_views', False):
+                max_testing_views = self.test_cfg.get('max_testing_views')
+                assert isinstance(max_testing_views, int)
+
+                total_views = inputs.shape[0]
+                assert num_segs == total_views, (
+                    'max_testing_views is only compatible '
+                    'with batch_size == 1')
+                view_ptr = 0
+                feats = []
+                while view_ptr < total_views:
+                    batch_imgs = inputs[view_ptr:view_ptr + max_testing_views]
+                    feat = self.backbone(batch_imgs)
+                    if self.with_neck:
+                        feat, _ = self.neck(feat)
+                    feats.append(feat)
+                    view_ptr += max_testing_views
+
+                def recursively_cat(feats):
+                    # recursively traverse feats until it's a tensor,
+                    # then concat
+                    out_feats = []
+                    for e_idx, elem in enumerate(feats[0]):
+                        batch_elem = [feat[e_idx] for feat in feats]
+                        if not isinstance(elem, torch.Tensor):
+                            batch_elem = recursively_cat(batch_elem)
+                        else:
+                            batch_elem = torch.cat(batch_elem)
+                        out_feats.append(batch_elem)
+
+                    return tuple(out_feats)
+
+                if isinstance(feats[0], tuple):
+                    x = recursively_cat(feats)
+                else:
+                    x = torch.cat(feats)
+            else:
+                x = self.backbone(inputs)
+                if self.with_neck:
+                    x, _ = self.neck(x)
 
-        # Return features extracted through neck.
-        if stage == 'neck':
             return x, loss_predict_kwargs
+        else:
+            # Return features extracted through backbone
+            x = self.backbone(inputs)
+            if stage == 'backbone':
+                return x, loss_predict_kwargs
 
-        # Return raw logits through head.
-        if self.with_cls_head and stage == 'head':
-            # [N * num_crops, num_classes]
-            x = self.cls_head(x, **loss_predict_kwargs)
-            return x, loss_predict_kwargs
+            loss_aux = dict()
+            if self.with_neck:
+                x, loss_aux = self.neck(x, data_samples=data_samples)
+
+            # Return features extracted through neck
+            loss_predict_kwargs['loss_aux'] = loss_aux
+            if stage == 'neck':
+                return x, loss_predict_kwargs
+
+            # Return raw logits through head.
+            if self.with_cls_head and stage == 'head':
+                x = self.cls_head(x, **loss_predict_kwargs)
+                return x, loss_predict_kwargs
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer_audio.py` & `mmaction2-1.1.0/mmaction/models/recognizers/recognizer_audio.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer_gcn.py` & `mmaction2-1.1.0/mmaction/models/recognizers/recognizer_gcn.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/recognizers/recognizer_omni.py` & `mmaction2-1.1.0/mmaction/models/recognizers/recognizer_omni.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/roi_heads/bbox_heads/bbox_head.py` & `mmaction2-1.1.0/mmaction/models/roi_heads/bbox_heads/bbox_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/roi_heads/roi_extractors/single_straight3d.py` & `mmaction2-1.1.0/mmaction/models/roi_heads/roi_extractors/single_straight3d.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/roi_heads/roi_head.py` & `mmaction2-1.1.0/mmaction/models/roi_heads/roi_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/acrn_head.py` & `mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/acrn_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/fbo_head.py` & `mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/fbo_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/lfb.py` & `mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/lfb.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import io
 import os
 import os.path as osp
 import warnings
 
-import numpy as np
 import torch
 import torch.distributed as dist
 from mmengine.dist import get_dist_info
 
 try:
     import lmdb
     lmdb_imported = True
@@ -126,14 +125,21 @@
     def load_lfb(self, map_location):
         self.lfb = {}
         for dataset_mode in self.dataset_modes:
             lfb_path = osp.normpath(
                 osp.join(self.lfb_prefix_path, f'lfb_{dataset_mode}.pkl'))
             print(f'Loading LFB from {lfb_path}...')
             self.lfb.update(torch.load(lfb_path, map_location=map_location))
+
+        for video_id in self.lfb:
+            video_features = self.lfb[video_id]
+            for sec in video_features:
+                if isinstance(video_features[sec], (list, tuple)):
+                    video_features[sec] = torch.stack(video_features[sec])
+            self.lfb[video_id] = video_features
         print(f'LFB has been loaded on {map_location}.')
 
     def load_lfb_on_lmdb(self):
         lfb = {}
         for dataset_mode in self.dataset_modes:
             lfb_path = osp.normpath(
                 osp.join(self.lfb_prefix_path, f'lfb_{dataset_mode}.pkl'))
@@ -158,30 +164,28 @@
                 video_features = torch.load(io.BytesIO(buf))
         else:
             video_features = self.lfb[video_id]
 
         # Sample long term features.
         window_size, K = self.window_size, self.max_num_sampled_feat
         start = timestamp - (window_size // 2)
-        lt_feats = torch.zeros(window_size * K, self.lfb_channels)
+        lt_feats = torch.zeros(window_size, K, self.lfb_channels)
 
         for idx, sec in enumerate(range(start, start + window_size)):
             if sec in video_features:
                 # `num_feat` is the number of roi features in this second.
-                num_feat = len(video_features[sec])
-                num_feat_sampled = min(num_feat, K)
-                # Sample some roi features randomly.
-                random_lfb_indices = np.random.choice(
-                    range(num_feat), num_feat_sampled, replace=False)
+                feat = video_features[sec]
+                num_feat = feat.shape[0]
 
-                for k, rand_idx in enumerate(random_lfb_indices):
-                    lt_feats[idx * K + k] = video_features[sec][rand_idx]
+                # Sample some roi features randomly.
+                random_lfb_indices = torch.randperm(num_feat)[:K]
+                lt_feats[idx, :num_feat] = feat[random_lfb_indices]
 
         # [window_size * max_num_sampled_feat, lfb_channels]
-        return lt_feats
+        return lt_feats.reshape(-1, self.lfb_channels)
 
     def __getitem__(self, img_key):
         """Sample long term features like `lfb['0f39OWEqJ24,0902']` where `lfb`
         is a instance of class LFB."""
         video_id, timestamp = img_key.split(',')
         return self.sample_long_term_features(video_id, int(timestamp))
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/roi_heads/shared_heads/lfb_infer_head.py` & `mmaction2-1.1.0/mmaction/models/roi_heads/shared_heads/lfb_infer_head.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/task_modules/assigners/max_iou_assigner_ava.py` & `mmaction2-1.1.0/mmaction/models/task_modules/assigners/max_iou_assigner_ava.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/utils/blending_utils.py` & `mmaction2-1.1.0/mmaction/models/utils/blending_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,72 +1,83 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from abc import ABCMeta, abstractmethod
-from typing import Union
+from functools import partial
+from typing import List, Optional, Tuple, Union
 
 import numpy as np
 import torch
 import torch.nn.functional as F
-from torch import Tensor
+from mmengine.utils import digit_version
 from torch.distributions.beta import Beta
 
 from mmaction.registry import MODELS
 from mmaction.utils import SampleList
 
+if digit_version(torch.__version__) < digit_version('1.8.0'):
+    floor_div = torch.floor_divide
+else:
+    floor_div = partial(torch.div, rounding_mode='floor')
+
 __all__ = ['BaseMiniBatchBlending', 'MixupBlending', 'CutmixBlending']
 
 
 class BaseMiniBatchBlending(metaclass=ABCMeta):
     """Base class for Image Aliasing.
 
     Args:
         num_classes (int): Number of classes.
     """
 
     def __init__(self, num_classes: int) -> None:
         self.num_classes = num_classes
 
     @abstractmethod
-    def do_blending(self, imgs: Tensor, label: Tensor, **kwargs) -> tuple:
+    def do_blending(self, imgs: torch.Tensor, label: torch.Tensor,
+                    **kwargs) -> Tuple:
         """Blending images process."""
         raise NotImplementedError
 
-    def __call__(self, imgs: Tensor, batch_data_samples: SampleList,
-                 **kwargs) -> tuple:
+    def __call__(self, imgs: torch.Tensor, batch_data_samples: SampleList,
+                 **kwargs) -> Tuple:
         """Blending data in a mini-batch.
 
         Images are float tensors with the shape of (B, N, C, H, W) for 2D
         recognizers or (B, N, C, T, H, W) for 3D recognizers.
 
         Besides, labels are converted from hard labels to soft labels.
-        Hard labels are integer tensors with the shape of (B, 1) and all of the
+        Hard labels are integer tensors with the shape of (B, ) and all of the
         elements are in the range [0, num_classes - 1].
-        Soft labels (probablity distribution over classes) are float tensors
-        with the shape of (B, 1, num_classes) and all of the elements are in
+        Soft labels (probability distribution over classes) are float tensors
+        with the shape of (B, num_classes) and all of the elements are in
         the range [0, 1].
 
         Args:
-            imgs (Tensor): Model input images, float tensor with the
+            imgs (torch.Tensor): Model input images, float tensor with the
                 shape of (B, N, C, H, W) or (B, N, C, T, H, W).
             batch_data_samples (List[:obj:`ActionDataSample`]): The batch
                 data samples. It usually includes information such
                 as `gt_labels`.
 
         Returns:
-            mixed_imgs (Tensor): Blending images, float tensor with the
+            mixed_imgs (torch.Tensor): Blending images, float tensor with the
                 same shape of the input imgs.
             batch_data_samples (List[:obj:`ActionDataSample`]): The modified
                 batch data samples. ``gt_labels`` in each data sample are
                 converted from a hard label to a blended soft label, float
-                tensor with the shape of (1, num_classes) and all elements are
+                tensor with the shape of (num_classes, ) and all elements are
                 in range [0, 1].
         """
         label = [x.gt_labels.item for x in batch_data_samples]
-        label = torch.tensor(label, dtype=torch.long).to(imgs.device)
-
-        one_hot_label = F.one_hot(label, num_classes=self.num_classes)
+        # single-label classification
+        if label[0].size(0) == 1:
+            label = torch.tensor(label, dtype=torch.long).to(imgs.device)
+            one_hot_label = F.one_hot(label, num_classes=self.num_classes)
+        # multi-label classification
+        else:
+            one_hot_label = torch.stack(label)
 
         mixed_imgs, mixed_label = self.do_blending(imgs, one_hot_label,
                                                    **kwargs)
 
         for label_item, sample in zip(mixed_label, batch_data_samples):
             sample.gt_labels.item = label_item
 
@@ -86,21 +97,22 @@
         alpha (float): Parameters for Beta distribution.
     """
 
     def __init__(self, num_classes: int, alpha: float = .2) -> None:
         super().__init__(num_classes=num_classes)
         self.beta = Beta(alpha, alpha)
 
-    def do_blending(self, imgs: Tensor, label: Tensor, **kwargs) -> tuple:
+    def do_blending(self, imgs: torch.Tensor, label: torch.Tensor,
+                    **kwargs) -> Tuple:
         """Blending images with mixup.
 
         Args:
-            imgs (Tensor): Model input images, float tensor with the
+            imgs (torch.Tensor): Model input images, float tensor with the
                 shape of (B, N, C, H, W) or (B, N, C, T, H, W).
-            label (Tensor): One hot labels, integer tensor with the shape
+            label (torch.Tensor): One hot labels, integer tensor with the shape
                 of (B, num_classes).
 
         Returns:
             tuple: A tuple of blended images and labels.
         """
         assert len(kwargs) == 0, f'unexpected kwargs for mixup {kwargs}'
 
@@ -128,40 +140,41 @@
     """
 
     def __init__(self, num_classes: int, alpha: float = .2) -> None:
         super().__init__(num_classes=num_classes)
         self.beta = Beta(alpha, alpha)
 
     @staticmethod
-    def rand_bbox(img_size: torch.Size, lam: Tensor) -> tuple:
+    def rand_bbox(img_size: torch.Size, lam: torch.Tensor) -> Tuple:
         """Generate a random boudning box."""
         w = img_size[-1]
         h = img_size[-2]
         cut_rat = torch.sqrt(1. - lam)
         cut_w = torch.tensor(int(w * cut_rat))
         cut_h = torch.tensor(int(h * cut_rat))
 
         # uniform
         cx = torch.randint(w, (1, ))[0]
         cy = torch.randint(h, (1, ))[0]
 
-        bbx1 = torch.clamp(cx - cut_w // 2, 0, w)
-        bby1 = torch.clamp(cy - cut_h // 2, 0, h)
-        bbx2 = torch.clamp(cx + cut_w // 2, 0, w)
-        bby2 = torch.clamp(cy + cut_h // 2, 0, h)
+        bbx1 = torch.clamp(cx - floor_div(cut_w, 2), 0, w)
+        bby1 = torch.clamp(cy - floor_div(cut_h, 2), 0, h)
+        bbx2 = torch.clamp(cx + floor_div(cut_w, 2), 0, w)
+        bby2 = torch.clamp(cy + floor_div(cut_h, 2), 0, h)
 
         return bbx1, bby1, bbx2, bby2
 
-    def do_blending(self, imgs: Tensor, label: Tensor, **kwargs) -> tuple:
+    def do_blending(self, imgs: torch.Tensor, label: torch.Tensor,
+                    **kwargs) -> Tuple:
         """Blending images with cutmix.
 
         Args:
-            imgs (Tensor): Model input images, float tensor with the
+            imgs (torch.Tensor): Model input images, float tensor with the
                 shape of (B, N, C, H, W) or (B, N, C, T, H, W).
-            label (Tensor): One hot labels, integer tensor with the shape
+            label (torch.Tensor): One hot labels, integer tensor with the shape
                 of (B, num_classes).
 
         Returns:
             tuple: A tuple of blended images and labels.
         """
 
         assert len(kwargs) == 0, f'unexpected kwargs for cutmix {kwargs}'
@@ -205,15 +218,17 @@
 
         To decide which batch augmentation will be used, it picks one of
         ``augments`` based on the probabilities. In the example above, the
         probability to use CutmixBlending is 0.5, to use MixupBlending is 0.3,
         and to do nothing is 0.2.
     """
 
-    def __init__(self, augments: Union[dict, list], probs=None):
+    def __init__(self,
+                 augments: Union[dict, list],
+                 probs: Optional[Union[float, List[float]]] = None) -> None:
         if not isinstance(augments, (tuple, list)):
             augments = [augments]
 
         self.augments = []
         for aug in augments:
             assert isinstance(aug, dict), \
                 f'blending augment config must be a dict. Got {type(aug)}'
@@ -231,15 +246,16 @@
             assert sum(probs) <= 1, \
                 'The total probability of batch augments exceeds 1.'
             self.augments.append(None)
             probs.append(1 - sum(probs))
 
         self.probs = probs
 
-    def do_blending(self, imgs: Tensor, label: Tensor, **kwargs) -> tuple:
+    def do_blending(self, imgs: torch.Tensor, label: torch.Tensor,
+                    **kwargs) -> Tuple:
         """Randomly apply batch augmentations to the batch inputs and batch
         data samples."""
         aug_index = np.random.choice(len(self.augments), p=self.probs)
         aug = self.augments[aug_index]
 
         if aug is not None:
             return aug.do_blending(imgs, label, **kwargs)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/models/utils/embed.py` & `mmaction2-1.1.0/mmaction/models/utils/embed.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/utils/gcn_utils.py` & `mmaction2-1.1.0/mmaction/models/utils/gcn_utils.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/models/utils/graph.py` & `mmaction2-1.1.0/mmaction/models/utils/graph.py`

 * *Files 7% similar despite different names*

```diff
@@ -107,32 +107,39 @@
     return hop_dis
 
 
 class Graph:
     """The Graph to model the skeletons.
 
     Args:
-        layout (str): must be one of the following candidates:
-            'openpose', 'nturgb+d', 'coco'. Defaults to ``'coco'``.
+        layout (str or dict): must be one of the following candidates:
+            'openpose', 'nturgb+d', 'coco', or a dict with the following
+            keys: 'num_node', 'inward', and 'center'.
+            Defaults to ``'coco'``.
         mode (str): must be one of the following candidates:
             'stgcn_spatial', 'spatial'. Defaults to ``'spatial'``.
         max_hop (int): the maximal distance between two connected
             nodes. Defaults to 1.
     """
 
     def __init__(self,
-                 layout: str = 'coco',
+                 layout: Union[str, dict] = 'coco',
                  mode: str = 'spatial',
                  max_hop: int = 1) -> None:
 
         self.max_hop = max_hop
         self.layout = layout
         self.mode = mode
 
-        assert layout in ['openpose', 'nturgb+d', 'coco']
+        if isinstance(layout, dict):
+            assert 'num_node' in layout
+            assert 'inward' in layout
+            assert 'center' in layout
+        else:
+            assert layout in ['openpose', 'nturgb+d', 'coco']
 
         self.set_layout(layout)
         self.hop_dis = get_hop_distance(self.num_node, self.inward, max_hop)
 
         assert hasattr(self, mode), f'Do Not Exist This Mode: {mode}'
         self.A = getattr(self, mode)()
 
@@ -159,14 +166,18 @@
             self.center = 21 - 1
         elif layout == 'coco':
             self.num_node = 17
             self.inward = [(15, 13), (13, 11), (16, 14), (14, 12), (11, 5),
                            (12, 6), (9, 7), (7, 5), (10, 8), (8, 6), (5, 0),
                            (6, 0), (1, 0), (3, 1), (2, 0), (4, 2)]
             self.center = 0
+        elif isinstance(layout, dict):
+            self.num_node = layout['num_node']
+            self.inward = layout['inward']
+            self.center = layout['center']
         else:
             raise ValueError(f'Do Not Exist This Layout: {layout}')
         self.self_link = [(i, i) for i in range(self.num_node)]
         self.outward = [(j, i) for (i, j) in self.inward]
         self.neighbor = self.inward + self.outward
 
     def stgcn_spatial(self) -> np.ndarray:
```

### Comparing `mmaction2-1.0.0rc3/mmaction/registry.py` & `mmaction2-1.1.0/mmaction/registry.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 More details can be found at
 https://mmengine.readthedocs.io/en/latest/tutorials/registry.html.
 """
 
 from mmengine.registry import DATA_SAMPLERS as MMENGINE_DATA_SAMPLERS
 from mmengine.registry import DATASETS as MMENGINE_DATASETS
 from mmengine.registry import EVALUATOR as MMENGINE_EVALUATOR
+from mmengine.registry import FUNCTIONS as MMENGINE_FUNCTION
 from mmengine.registry import HOOKS as MMENGINE_HOOKS
 from mmengine.registry import INFERENCERS as MMENGINE_INFERENCERS
 from mmengine.registry import LOG_PROCESSORS as MMENGINE_LOG_PROCESSORS
 from mmengine.registry import LOOPS as MMENGINE_LOOPS
 from mmengine.registry import METRICS as MMENGINE_METRICS
 from mmengine.registry import MODEL_WRAPPERS as MMENGINE_MODEL_WRAPPERS
 from mmengine.registry import MODELS as MMENGINE_MODELS
@@ -123,7 +124,11 @@
     locations=['mmaction.engine'])
 
 # manage inferencer
 INFERENCERS = Registry(
     'inferencer',
     parent=MMENGINE_INFERENCERS,
     locations=['mmaction.apis.inferencers'])
+
+# manage function
+FUNCTION = Registry(
+    'function', parent=MMENGINE_FUNCTION, locations=['mmaction.mmengine'])
```

### Comparing `mmaction2-1.0.0rc3/mmaction/structures/bbox/bbox_target.py` & `mmaction2-1.1.0/mmaction/structures/bbox/bbox_target.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/structures/bbox/transforms.py` & `mmaction2-1.1.0/mmaction/structures/bbox/transforms.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/testing/__init__.py` & `mmaction2-1.1.0/mmaction/testing/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from ._utils import (check_norm_state, generate_backbone_demo_inputs,
-                     generate_detector_demo_inputs,
-                     generate_recognizer_demo_inputs, get_audio_recognizer_cfg,
+                     generate_detector_demo_inputs, get_audio_recognizer_cfg,
                      get_cfg, get_detector_cfg, get_localizer_cfg,
-                     get_recognizer_cfg, get_skeletongcn_cfg)
+                     get_recognizer_cfg, get_similarity_cfg,
+                     get_skeletongcn_cfg)
 
 __all__ = [
-    'check_norm_state', 'generate_backbone_demo_inputs',
-    'generate_recognizer_demo_inputs', 'get_cfg', 'get_recognizer_cfg',
-    'get_audio_recognizer_cfg', 'get_localizer_cfg', 'get_detector_cfg',
-    'generate_detector_demo_inputs', 'get_skeletongcn_cfg'
+    'check_norm_state', 'generate_backbone_demo_inputs', 'get_cfg',
+    'get_recognizer_cfg', 'get_audio_recognizer_cfg', 'get_localizer_cfg',
+    'get_detector_cfg', 'generate_detector_demo_inputs', 'get_skeletongcn_cfg',
+    'get_similarity_cfg'
 ]
```

### Comparing `mmaction2-1.0.0rc3/mmaction/testing/_utils.py` & `mmaction2-1.1.0/mmaction/testing/_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -107,15 +107,15 @@
 def get_cfg(config_type, fname):
     """Grab configs necessary to create a recognizer.
 
     These are deep copied to allow for safe modification of parameters without
     influencing other tests.
     """
     config_types = ('recognition', 'recognition_audio', 'localization',
-                    'detection', 'skeleton')
+                    'detection', 'skeleton', 'retrieval')
     assert config_type in config_types
 
     repo_dpath = osp.dirname(osp.dirname(osp.dirname(__file__)))
     config_dpath = osp.join(repo_dpath, 'configs/' + config_type)
     config_fpath = osp.join(config_dpath, fname)
     if not osp.exists(config_dpath):
         raise Exception('Cannot find config path')
@@ -137,7 +137,11 @@
 
 def get_detector_cfg(fname):
     return get_cfg('detection', fname)
 
 
 def get_skeletongcn_cfg(fname):
     return get_cfg('skeleton', fname)
+
+
+def get_similarity_cfg(fname):
+    return get_cfg('retrieval', fname)
```

### Comparing `mmaction2-1.0.0rc3/mmaction/utils/gradcam_utils.py` & `mmaction2-1.1.0/mmaction/utils/gradcam_utils.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/utils/setup_env.py` & `mmaction2-1.1.0/mmaction/utils/setup_env.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/utils/typing.py` & `mmaction2-1.1.0/mmaction/utils/typing_utils.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction/version.py` & `mmaction2-1.1.0/mmaction/version.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # Copyright (c) Open-MMLab. All rights reserved.
 
-__version__ = '1.0.0rc3'
+__version__ = '1.1.0'
 
 
 def parse_version_info(version_str: str):
     """Parse a version string into a tuple.
 
     Args:
         version_str (str): The version string.
```

### Comparing `mmaction2-1.0.0rc3/mmaction/visualization/action_visualizer.py` & `mmaction2-1.1.0/mmaction/visualization/action_visualizer.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,15 +52,15 @@
         fig_show_cfg (dict): Keyword parameters of figure for showing.
             Defaults to empty dict.
 
     Examples:
         >>> import torch
         >>> import decord
         >>> from pathlib import Path
-        >>> from mmaction.core import ActionDataSample, ActionVisualizer
+        >>> from mmaction.structures import ActionDataSample, ActionVisualizer
         >>> from mmengine.structures import LabelData
         >>> # Example frame
         >>> video = decord.VideoReader('./demo/demo.mp4')
         >>> video = video.get_batch(range(32)).asnumpy()
         >>> # Example annotation
         >>> data_sample = ActionDataSample()
         >>> data_sample.gt_labels = LabelData(item=torch.tensor([2]))
@@ -264,15 +264,18 @@
             frame_wait_time = 1. / fps
             for frame_idx, drawn_img in enumerate(resulted_video):
                 frame_name = 'frame %d of %s' % (frame_idx + 1, name)
                 if frame_idx < len(resulted_video) - 1:
                     wait_time = frame_wait_time
                 else:
                     wait_time = wait_time
-                self.show(drawn_img, win_name=frame_name, wait_time=wait_time)
+                self.show(
+                    drawn_img[:, :, ::-1],
+                    win_name=frame_name,
+                    wait_time=wait_time)
 
         resulted_video = np.array(resulted_video)
         if out_path is not None:
             save_dir, save_name = osp.split(out_path)
             vis_backend_cfg = dict(type='LocalVisBackend', save_dir=save_dir)
             tmp_local_vis_backend = VISBACKENDS.build(vis_backend_cfg)
             tmp_local_vis_backend.add_video(
```

### Comparing `mmaction2-1.0.0rc3/mmaction/visualization/video_backend.py` & `mmaction2-1.1.0/mmaction/visualization/video_backend.py`

 * *Files identical despite different names*

### Comparing `mmaction2-1.0.0rc3/mmaction2.egg-info/PKG-INFO` & `mmaction2-1.1.0/PKG-INFO`

 * *Files 24% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 Metadata-Version: 2.1
 Name: mmaction2
-Version: 1.0.0rc3
+Version: 1.1.0
 Summary: OpenMMLab Video Understanding Toolbox and Benchmark
 Home-page: https://github.com/open-mmlab/mmaction2
 Author: MMAction2 Contributors
 Author-email: openmmlab@gmail.com
 Maintainer: MMAction2 Contributors
 Maintainer-email: openmmlab@gmail.com
 License: Apache License 2.0
 Description: <div align="center">
-          <img src="https://github.com/open-mmlab/mmaction2/raw/1.x/resources/mmaction2_logo.png" width="600"/>
+          <img src="https://github.com/open-mmlab/mmaction2/raw/main/resources/mmaction2_logo.png" width="600"/>
           <div>&nbsp;</div>
           <div align="center">
             <b><font size="5">OpenMMLab website</font></b>
             <sup>
               <a href="https://openmmlab.com">
                 <i><font size="4">HOT</font></i>
               </a>
@@ -23,284 +23,383 @@
             <sup>
               <a href="https://platform.openmmlab.com">
                 <i><font size="4">TRY IT OUT</font></i>
               </a>
             </sup>
           </div>
         
-        [![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/1.x/)
+        [![Documentation](https://readthedocs.org/projects/mmaction2/badge/?version=latest)](https://mmaction2.readthedocs.io/en/latest/)
         [![actions](https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://github.com/open-mmlab/mmaction2/actions)
-        [![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2)
+        [![codecov](https://codecov.io/gh/open-mmlab/mmaction2/branch/main/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmaction2)
         [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://pypi.org/project/mmaction2/)
-        [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/master/LICENSE)
+        [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/main/LICENSE)
         [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
         [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
         
-        [📘Documentation](https://mmaction2.readthedocs.io/en/1.x/) |
-        [🛠️Installation](https://mmaction2.readthedocs.io/en/1.x/get_started.html) |
-        [👀Model Zoo](https://mmaction2.readthedocs.io/en/1.x/modelzoo.html) |
-        [🆕Update News](https://mmaction2.readthedocs.io/en/1.x/notes/changelog.html) |
+        [📘Documentation](https://mmaction2.readthedocs.io/en/latest/) |
+        [🛠️Installation](https://mmaction2.readthedocs.io/en/latest/get_started/installation.html) |
+        [👀Model Zoo](https://mmaction2.readthedocs.io/en/latest/modelzoo_statistics.html) |
+        [🆕Update News](https://mmaction2.readthedocs.io/en/latest/notes/changelog.html) |
         [🚀Ongoing Projects](https://github.com/open-mmlab/mmaction2/projects) |
         [🤔Reporting Issues](https://github.com/open-mmlab/mmaction2/issues/new/choose)
         
         </div>
         
-        ## Introduction
+        <div align="center">
+          <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
+        </div>
         
-        MMAction2 is an open-source toolbox for video understanding based on PyTorch.
-        It is a part of the [OpenMMLab](http://openmmlab.org/) project.
+        English | [简体中文](/README_zh-CN.md)
+        
+        ## 📄 Table of Contents
         
-        The 1.x branch works with **PyTorch 1.6+**.
+        - [📄 Table of Contents](#-table-of-contents)
+        - [🥳 🚀 What's New](#--whats-new-)
+        - [📖 Introduction](#-introduction-)
+        - [🎁 Major Features](#-major-features-)
+        - [🛠️ Installation](#️-installation-)
+        - [👀 Model Zoo](#-model-zoo-)
+        - [👨‍🏫 Get Started](#-get-started-)
+        - [🎫 License](#-license-)
+        - [🖊️ Citation](#️-citation-)
+        - [🙌 Contributing](#-contributing-)
+        - [🤝 Acknowledgement](#-acknowledgement-)
+        - [🏗️ Projects in OpenMMLab](#️-projects-in-openmmlab-)
+        
+        ## 🥳 🚀 What's New [🔝](#-table-of-contents)
+        
+        **The default branch has been switched to `main`(previous `1.x`) from `master`(current `0.x`), and we encourage users to migrate to the latest version with more supported models, stronger pre-training checkpoints and simpler coding. Please refer to [Migration Guide](https://mmaction2.readthedocs.io/en/latest/migration.html) for more details.**
+        
+        **Release (2023.07.04)**: v1.1.0 with the following new features:
+        
+        - Support CLIP-based multi-modality models: ActionCLIP(Arxiv'2021) and CLIP4clip(ArXiv'2022)
+        - Support rich projects: gesture recognition, spatio-temporal action detection tutorial, and knowledge distillation
+        - Support HACS-segments dataset(ICCV'2019), MultiSports dataset(ICCV'2021), Kinetics-710 dataset(Arxiv'2022)
+        - Support VideoMAE V2(CVPR'2023), and VideoMAE(NeurIPS'2022) on action detection
+        - Support TCANet(CVPR'2021)
+        - Support [Pure Python style Configuration File](https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#a-pure-python-style-configuration-file-beta) and downloading datasets by MIM with one command
+        
+        ## 📖 Introduction [🔝](#-table-of-contents)
+        
+        MMAction2 is an open-source toolbox for video understanding based on PyTorch.
+        It is a part of the [OpenMMLab](http://openmmlab.com/) project.
         
         <div align="center">
-          <div style="float:left;margin-right:10px;">
-          <img src="https://github.com/open-mmlab/mmaction2/raw/1.x/resources/mmaction2_overview.gif" width="380px"><br>
-            <p style="font-size:1.5vw;">Action Recognition Results on Kinetics-400</p>
-          </div>
-          <div style="float:right;margin-right:0px;">
-          <img src="https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif" width="380px"><br>
-            <p style="font-size:1.5vw;">Skeleton-based Action Recognition Results on NTU-RGB+D-120</p>
-          </div>
+          <img src="https://github.com/open-mmlab/mmaction2/raw/main/resources/mmaction2_overview.gif" width="380px">
+          <img src="https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif" width="380px">
+          <p style="font-size:1.5vw;"> Action Recognition on Kinetics-400 (left) and Skeleton-based Action Recognition on NTU-RGB+D-120 (right)</p>
         </div>
+        
         <div align="center">
           <img src="https://user-images.githubusercontent.com/30782254/155710881-bb26863e-fcb4-458e-b0c4-33cd79f96901.gif" width="580px"/><br>
             <p style="font-size:1.5vw;">Skeleton-based Spatio-Temporal Action Detection and Action Recognition Results on Kinetics-400</p>
         </div>
         <div align="center">
-          <img src="https://github.com/open-mmlab/mmaction2/raw/1.x/resources/spatio-temporal-det.gif" width="800px"/><br>
+          <img src="https://github.com/open-mmlab/mmaction2/raw/main/resources/spatio-temporal-det.gif" width="800px"/><br>
             <p style="font-size:1.5vw;">Spatio-Temporal Action Detection Results on AVA-2.1</p>
         </div>
         
-        ## Major Features
+        ## 🎁 Major Features [🔝](#-table-of-contents)
         
         - **Modular design**: We decompose a video understanding framework into different components. One can easily construct a customized video understanding framework by combining different modules.
         
-        - **Support four major video understanding tasks**: MMAction2 implements various algorithms for multiple video understanding tasks, including action recognition, action localization, spatio-temporal action detection, and skeleton-based action detection.
+        - **Support five major video understanding tasks**: MMAction2 implements various algorithms for multiple video understanding tasks, including action recognition, action localization, spatio-temporal action detection, skeleton-based action detection and video retrieval.
         
         - **Well tested and documented**: We provide detailed documentation and API reference, as well as unit tests.
         
-        ## What's New
+        ## 🛠️ Installation [🔝](#-table-of-contents)
+        
+        MMAction2 depends on [PyTorch](https://pytorch.org/), [MMCV](https://github.com/open-mmlab/mmcv), [MMEngine](https://github.com/open-mmlab/mmengine), [MMDetection](https://github.com/open-mmlab/mmdetection) (optional) and [MMPose](https://github.com/open-mmlab/mmpose) (optional).
         
-        **Release (2022.02.10)**: v1.0.0rc3 with the following new features:
+        Please refer to [install.md](https://mmaction2.readthedocs.io/en/latest/get_started/installation.html) for detailed instructions.
+        
+        <details close>
+        <summary>Quick instructions</summary>
+        
+        ```shell
+        conda create --name openmmlab python=3.8 -y
+        conda activate open-mmlab
+        conda install pytorch torchvision -c pytorch  # This command will automatically install the latest version PyTorch and cudatoolkit, please check whether they match your environment.
+        pip install -U openmim
+        mim install mmengine
+        mim install mmcv
+        mim install mmdet  # optional
+        mim install mmpose  # optional
+        git clone https://github.com/open-mmlab/mmaction2.git
+        cd mmaction2
+        pip install -v -e .
+        ```
         
-        - Support Action Recognition model UniFormer V1(ICLR'2022), UniFormer V2(Arxiv'2022).
-        - Support training MViT V2(CVPR'2022), and MaskFeat(CVPR'2022) fine-tuning.
-        - Add a new handy interface for inference MMAction2 models ([demo](https://github.com/open-mmlab/mmaction2/blob/dev-1.x/demo/README.md#inferencer))
+        </details>
         
-        ## Installation
+        ## 👀 Model Zoo [🔝](#-table-of-contents)
         
-        Please refer to [install.md](https://mmaction2.readthedocs.io/en/1.x/get_started.html) for more detailed instructions.
+        Results and models are available in the [model zoo](https://mmaction2.readthedocs.io/en/latest/model_zoo/modelzoo.html).
         
-        ## Supported Methods
+        <details close>
+        
+        <summary>Supported model</summary>
         
         <table style="margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;">
           <tr>
             <td colspan="5" style="font-weight:bold;">Action Recognition</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/c3d/README.md">C3D</a> (CVPR'2014)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tsn/README.md">TSN</a> (ECCV'2016)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/i3d/README.md">I3D</a> (CVPR'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/c2d/README.md">C2D</a> (CVPR'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/i3d/README.md">I3D Non-Local</a> (CVPR'2018)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/r2plus1d/README.md">R(2+1)D</a> (CVPR'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/trn/README.md">TRN</a> (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tsm/README.md">TSM</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tsm/README.md">TSM Non-Local</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/slowonly/README.md">SlowOnly</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/c3d/README.md">C3D</a> (CVPR'2014)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tsn/README.md">TSN</a> (ECCV'2016)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/i3d/README.md">I3D</a> (CVPR'2017)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/c2d/README.md">C2D</a> (CVPR'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/i3d/README.md">I3D Non-Local</a> (CVPR'2018)</td>
+          </tr>
+          <tr>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/r2plus1d/README.md">R(2+1)D</a> (CVPR'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/trn/README.md">TRN</a> (ECCV'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tsm/README.md">TSM</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tsm/README.md">TSM Non-Local</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/slowonly/README.md">SlowOnly</a> (ICCV'2019)</td>
+          </tr>
+          <tr>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/slowfast/README.md">SlowFast</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/csn/README.md">CSN</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tin/README.md">TIN</a> (AAAI'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tpn/README.md">TPN</a> (CVPR'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/x3d/README.md">X3D</a> (CVPR'2020)</td>
+          </tr>
+          <tr>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition_audio/resnet/README.md">MultiModality: Audio</a> (ArXiv'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/tanet/README.md">TANet</a> (ArXiv'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/timesformer/README.md">TimeSformer</a> (ICML'2021)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/projects/actionclip/README.md">ActionCLIP</a> (ArXiv'2021)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/swin/README.md">VideoSwin</a> (CVPR'2022)</td>
+          </tr>
+          <tr>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/videomae/README.md">VideoMAE</a> (NeurIPS'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/mvit/README.md">MViT V2</a> (CVPR'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/uniformer/README.md">UniFormer V1</a> (ICLR'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/uniformerv2/README.md">UniFormer V2</a> (Arxiv'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/videomaev2/README.md">VideoMAE V2</a> (CVPR'2023)</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/slowfast/README.md">SlowFast</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/csn/README.md">CSN</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tin/README.md">TIN</a> (AAAI'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tpn/README.md">TPN</a> (CVPR'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/x3d/README.md">X3D</a> (CVPR'2020)</td>
-          </tr>
-          <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition_audio/resnet/README.md">MultiModality: Audio</a> (ArXiv'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/tanet/README.md">TANet</a> (ArXiv'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/timesformer/README.md">TimeSformer</a> (ICML'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/swin/README.md">VideoSwin</a> (CVPR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/videomae/README.md">VideoMAE</a> (NeurIPS'2022)</td>
+            <td colspan="5" style="font-weight:bold;">Action Localization</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/mvit/README.md">MViT V2</a> (CVPR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/uniformer/README.md">UniFormer V1</a> (ICLR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/recognition/uniformerv2/README.md">UniFormer V2</a> (Arxiv'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/localization/bsn/README.md">BSN</a> (ECCV'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/localization/bmn/README.md">BMN</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/localization/tcanet/README.md">TCANet</a> (CVPR'2021)</td>
             <td></td>
             <td></td>
           </tr>
           <tr>
-            <td colspan="5" style="font-weight:bold;">Action Localization</td>
+            <td colspan="5" style="font-weight:bold;">Spatio-Temporal Action Detection</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/localization/ssn/README.md">SSN</a> (ICCV'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/localization/bsn/README.md">BSN</a> (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/localization/bmn/README.md">BMN</a> (ICCV'2019)</td>
-            <td></td>
-            <td></td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/acrn/README.md">ACRN</a> (ECCV'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/slowonly/README.md">SlowOnly+Fast R-CNN</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/slowfast/README.md">SlowFast+Fast R-CNN</a> (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/detection/lfb/README.md">LFB</a> (CVPR'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/recognition/videomae/README.md">VideoMAE</a> (NeurIPS'2022)</td>
           </tr>
           <tr>
-            <td colspan="5" style="font-weight:bold;">Spatio-Temporal Action Detection</td>
+            <td colspan="5" style="font-weight:bold;">Skeleton-based Action Recognition</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/acrn/README.md">ACRN</a> (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/ava/README.md">SlowOnly+Fast R-CNN</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/ava/README.md">SlowFast+Fast R-CNN</a> (ICCV'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/detection/lfb/README.md">LFB</a> (CVPR'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/stgcn/README.md">ST-GCN</a> (AAAI'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/2s-agcn/README.md">2s-AGCN</a> (CVPR'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/posec3d/README.md">PoseC3D</a> (CVPR'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/skeleton/stgcnpp/README.md">STGCN++</a> (ArXiv'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/projects/ctrgcn/README.md">CTRGCN</a> (CVPR'2021)</td>
+          </tr>
+          <tr>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/projects/msg3d/README.md">MSG3D</a> (CVPR'2020)</td>
+            <td></td>
+            <td></td>
+            <td></td>
             <td></td>
           </tr>
           <tr>
-            <td colspan="5" style="font-weight:bold;">Skeleton-based Action Recognition</td>
+            <td colspan="5" style="font-weight:bold;">Video Retrieval</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/stgcn/README.md">ST-GCN</a> (AAAI'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/2s-agcn/README.md">2s-AGCN</a> (CVPR'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/posec3d/README.md">PoseC3D</a> (CVPR'2022)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/configs/skeleton/stgcnpp/README.md">STGCN++</a> (ArXiv'2022)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/configs/retrieval/clip4clip/README.md">CLIP4Clip</a> (ArXiv'2022)</td>
+            <td></td>
+            <td></td>
+            <td></td>
             <td></td>
           </tr>
+        
         </table>
         
-        Results and models are available in the *README.md* of each method's config directory.
-        A summary can be found on the [**model zoo**](https://mmaction2.readthedocs.io/en/1.x/modelzoo.html) page.
+        </details>
         
-        We will keep up with the latest progress of the community and support more popular algorithms and frameworks.
-        If you have any feature requests, please feel free to leave a comment in [Issues](https://github.com/open-mmlab/mmaction2/issues/19).
+        <details close>
         
-        ## Supported Datasets
+        <summary>Supported dataset</summary>
         
         <table style="margin-left:auto;margin-right:auto;font-size:1.3vw;padding:3px 5px;text-align:center;vertical-align:center;">
           <tr>
             <td colspan="4" style="font-weight:bold;">Action Recognition</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/hmdb51/README.md">HMDB51</a> (<a href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">Homepage</a>) (ICCV'2011)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ucf101/README.md">UCF101</a> (<a href="https://www.crcv.ucf.edu/research/data-sets/ucf101/">Homepage</a>) (CRCV-IR-12-01)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/kinetics/README.md">Kinetics-[400/600/700]</a> (<a href="https://deepmind.com/research/open-source/kinetics/">Homepage</a>) (CVPR'2017)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/hmdb51/README.md">HMDB51</a> (<a href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/">Homepage</a>) (ICCV'2011)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ucf101/README.md">UCF101</a> (<a href="https://www.crcv.ucf.edu/research/data-sets/ucf101/">Homepage</a>) (CRCV-IR-12-01)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/kinetics/README.md">Kinetics-[400/600/700]</a> (<a href="https://deepmind.com/research/open-source/kinetics/">Homepage</a>) (CVPR'2017)</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/sthv1/README.md">SthV1</a>  (ICCV'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/sthv2/README.md">SthV2</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/something-something">Homepage</a>) (ICCV'2017)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/diving48/README.md">Diving48</a> (<a href="http://www.svcl.ucsd.edu/projects/resound/dataset.html">Homepage</a>) (ECCV'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/jester/README.md">Jester</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/jester">Homepage</a>) (ICCV'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/sthv1/README.md">SthV1</a>  (ICCV'2017)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/sthv2/README.md">SthV2</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/something-something">Homepage</a>) (ICCV'2017)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/diving48/README.md">Diving48</a> (<a href="http://www.svcl.ucsd.edu/projects/resound/dataset.html">Homepage</a>) (ECCV'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/jester/README.md">Jester</a> (<a href="https://developer.qualcomm.com/software/ai-datasets/jester">Homepage</a>) (ICCV'2019)</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/mit/README.md">Moments in Time</a> (<a href="http://moments.csail.mit.edu/">Homepage</a>) (TPAMI'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/mmit/README.md">Multi-Moments in Time</a> (<a href="http://moments.csail.mit.edu/challenge_iccv_2019.html">Homepage</a>) (ArXiv'2019)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/hvu/README.md">HVU</a> (<a href="https://github.com/holistic-video-understanding/HVU-Dataset">Homepage</a>) (ECCV'2020)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/omnisource/README.md">OmniSource</a> (<a href="https://kennymckormick.github.io/omnisource/">Homepage</a>) (ECCV'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/mit/README.md">Moments in Time</a> (<a href="http://moments.csail.mit.edu/">Homepage</a>) (TPAMI'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/mmit/README.md">Multi-Moments in Time</a> (<a href="http://moments.csail.mit.edu/challenge_iccv_2019.html">Homepage</a>) (ArXiv'2019)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/hvu/README.md">HVU</a> (<a href="https://github.com/holistic-video-understanding/HVU-Dataset">Homepage</a>) (ECCV'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/omnisource/README.md">OmniSource</a> (<a href="https://kennymckormick.github.io/omnisource/">Homepage</a>) (ECCV'2020)</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/gym/README.md">FineGYM</a> (<a href="https://sdolivia.github.io/FineGym/">Homepage</a>) (CVPR'2020)</td>
-            <td></td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/gym/README.md">FineGYM</a> (<a href="https://sdolivia.github.io/FineGym/">Homepage</a>) (CVPR'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/kinetics710/README.md">Kinetics-710</a> (<a href="https://arxiv.org/pdf/2211.09552.pdf">Homepage</a>) (Arxiv'2022)</td>
             <td></td>
             <td></td>
           </tr>
           <tr>
             <td colspan="4" style="font-weight:bold;">Action Localization</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/thumos14/README.md">THUMOS14</a> (<a href="https://www.crcv.ucf.edu/THUMOS14/download.html">Homepage</a>) (THUMOS Challenge 2014)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
-            <td></td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/thumos14/README.md">THUMOS14</a> (<a href="https://www.crcv.ucf.edu/THUMOS14/download.html">Homepage</a>) (THUMOS Challenge 2014)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/activitynet/README.md">ActivityNet</a> (<a href="http://activity-net.org/">Homepage</a>) (CVPR'2015)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/hacs/README.md">HACS</a> (<a href="https://github.com/hangzhaomit/HACS-dataset">Homepage</a>) (ICCV'2019)</td>
             <td></td>
           </tr>
           <tr>
             <td colspan="4" style="font-weight:bold;">Spatio-Temporal Action Detection</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ucf101_24/README.md">UCF101-24*</a> (<a href="http://www.thumos.info/download.html">Homepage</a>) (CRCV-IR-12-01)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/jhmdb/README.md">JHMDB*</a> (<a href="http://jhmdb.is.tue.mpg.de/">Homepage</a>) (ICCV'2015)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ava/README.md">AVA</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (CVPR'2018)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/ava_kinetics/README.md">AVA-Kinetics</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (Arxiv'2020)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ucf101_24/README.md">UCF101-24*</a> (<a href="http://www.thumos.info/download.html">Homepage</a>) (CRCV-IR-12-01)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/jhmdb/README.md">JHMDB*</a> (<a href="http://jhmdb.is.tue.mpg.de/">Homepage</a>) (ICCV'2015)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ava/README.md">AVA</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (CVPR'2018)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/ava_kinetics/README.md">AVA-Kinetics</a> (<a href="https://research.google.com/ava/index.html">Homepage</a>) (Arxiv'2020)</td>
+          </tr>
+          <tr>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/multisports/README.md">MultiSports</a> (<a href="https://deeperaction.github.io/datasets/multisports.html">Homepage</a>) (ICCV'2021)</td>
+            <td></td>
+            <td></td>
+            <td></td>
           </tr>
           <tr>
             <td colspan="4" style="font-weight:bold;">Skeleton-based Action Recognition</td>
           </tr>
           <tr>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-FineGYM</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-NTURGB+D</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-UCF101</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
-            <td><a href="https://github.com/open-mmlab/mmaction2/blob/1.x/tools/data/skeleton/README.md">PoseC3D-HMDB51</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-FineGYM</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-NTURGB+D</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-UCF101</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/skeleton/README.md">PoseC3D-HMDB51</a> (<a href="https://kennymckormick.github.io/posec3d/">Homepage</a>) (ArXiv'2021)</td>
+          </tr>
+          <tr>
+            <td colspan="4" style="font-weight:bold;">Video Retrieval</td>
+          </tr>
+          <tr>
+            <td><a href="https://github.com/open-mmlab/mmaction2/blob/main/tools/data/video_retrieval/README.md">MSRVTT</a> (<a href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/">Homepage</a>) (CVPR'2016)</td>
+            <td></td>
+            <td></td>
+            <td></td>
           </tr>
-        </table>
-        
-        Datasets marked with * are not fully supported yet, but related dataset preparation steps are provided. A summary can be found on the [**Supported Datasets**](https://mmaction2.readthedocs.io/en/latest/supported_datasets.html) page.
         
-        ## Data Preparation
+        </table>
         
-        Please refer to [data_preparation.md](docs/en/user_guides/2_data_prepare.md) for a general knowledge of data preparation.
+        </details>
         
-        ## FAQ
+        ## 👨‍🏫 Get Started [🔝](#-table-of-contents)
         
-        Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked questions.
+        For tutorials, we provide the following user guides for basic usage:
         
-        ## Projects built on MMAction2
+        - [Migration from MMAction2 0.X](https://mmaction2.readthedocs.io/en/latest/migration.html)
+        - [Learn about Configs](https://mmaction2.readthedocs.io/en/latest/user_guides/config.html)
+        - [Prepare Datasets](https://mmaction2.readthedocs.io/en/latest/user_guides/prepare_dataset.html)
+        - [Inference with Existing Models](https://mmaction2.readthedocs.io/en/latest/user_guides/inference.html)
+        - [Training and Testing](https://mmaction2.readthedocs.io/en/latest/user_guides/train_test.html)
         
-        Currently, there are many research works and projects built on MMAction2 by users from community, such as:
+        <details close>
+        <summary>Research works built on MMAction2 by users from community</summary>
         
         - Video Swin Transformer. [\[paper\]](https://arxiv.org/abs/2106.13230)[\[github\]](https://github.com/SwinTransformer/Video-Swin-Transformer)
         - Evidential Deep Learning for Open Set Action Recognition, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/2107.10161)[\[github\]](https://github.com/Cogito2012/DEAR)
         - Rethinking Self-supervised Correspondence Learning: A Video Frame-level Similarity Perspective, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/2103.17263)[\[github\]](https://github.com/xvjiarui/VFS)
         
-        etc., check [projects.md](docs/en/notes/projects.md) to see all related projects.
+        </details>
         
-        ## License
+        ## 🎫 License [🔝](#-table-of-contents)
         
         This project is released under the [Apache 2.0 license](LICENSE).
         
-        ## Citation
+        ## 🖊️ Citation [🔝](#-table-of-contents)
         
         If you find this project useful in your research, please consider cite:
         
         ```BibTeX
         @misc{2020mmaction2,
             title={OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark},
             author={MMAction2 Contributors},
             howpublished = {\url{https://github.com/open-mmlab/mmaction2}},
             year={2020}
         }
         ```
         
-        ## Contributing
+        ## 🙌 Contributing [🔝](#-table-of-contents)
         
-        We appreciate all contributions to improve MMAction2. Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/1.x/CONTRIBUTING.md) in MMCV for more details about the contributing guideline.
+        We appreciate all contributions to improve MMAction2. Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/2.x/CONTRIBUTING.md) in MMCV for more details about the contributing guideline.
         
-        ## Acknowledgement
+        ## 🤝 Acknowledgement [🔝](#-table-of-contents)
         
         MMAction2 is an open-source project that is contributed by researchers and engineers from various colleges and companies.
         We appreciate all the contributors who implement their methods or add new features and users who give valuable feedback.
         We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their new models.
         
-        ## Projects in OpenMMLab
+        ## 🏗️ Projects in OpenMMLab [🔝](#-table-of-contents)
         
         - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
         - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
         - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-        - [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
+        - [MMEval](https://github.com/open-mmlab/mmeval): A unified evaluation library for multiple machine learning libraries.
+        - [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
         - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
         - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
         - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
+        - [MMYOLO](https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and benchmark.
         - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
         - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
         - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
         - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
         - [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
         - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
         - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
         - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
         - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
         - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-        - [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
+        - [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
         - [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
         - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
+        - [Playground](https://github.com/open-mmlab/playground): A central hub for gathering and showcasing amazing projects built upon OpenMMLab.
         
 Keywords: computer vision,video understanding
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
```

#### html2text {}

```diff
@@ -1,175 +1,220 @@
-Metadata-Version: 2.1 Name: mmaction2 Version: 1.0.0rc3 Summary: OpenMMLab
-Video Understanding Toolbox and Benchmark Home-page: https://github.com/open-
-mmlab/mmaction2 Author: MMAction2 Contributors Author-email:
-openmmlab@gmail.com Maintainer: MMAction2 Contributors Maintainer-email:
-openmmlab@gmail.com License: Apache License 2.0 Description:
-[https://github.com/open-mmlab/mmaction2/raw/1.x/resources/mmaction2_logo.png]
+Metadata-Version: 2.1 Name: mmaction2 Version: 1.1.0 Summary: OpenMMLab Video
+Understanding Toolbox and Benchmark Home-page: https://github.com/open-mmlab/
+mmaction2 Author: MMAction2 Contributors Author-email: openmmlab@gmail.com
+Maintainer: MMAction2 Contributors Maintainer-email: openmmlab@gmail.com
+License: Apache License 2.0 Description:
+[https://github.com/open-mmlab/mmaction2/raw/main/resources/mmaction2_logo.png]
                                         
            OpenMMLab website HOT      OpenMMLab platform TRY_IT_OUT
       [![Documentation](https://readthedocs.org/projects/mmaction2/badge/
-?version=latest)](https://mmaction2.readthedocs.io/en/1.x/) [![actions](https:/
-     /github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://
+  ?version=latest)](https://mmaction2.readthedocs.io/en/latest/) [![actions]
+ (https://github.com/open-mmlab/mmaction2/workflows/build/badge.svg)](https://
   github.com/open-mmlab/mmaction2/actions) [![codecov](https://codecov.io/gh/
-  open-mmlab/mmaction2/branch/master/graph/badge.svg)](https://codecov.io/gh/
-open-mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https:
-   //pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/
-  license/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/
-      blob/master/LICENSE) [![Average time to resolve an issue](https://
-    isitmaintained.com/badge/resolution/open-mmlab/mmaction2.svg)](https://
-  github.com/open-mmlab/mmaction2/issues) [![Percentage of issues still open]
-  (https://isitmaintained.com/badge/open/open-mmlab/mmaction2.svg)](https://
-     github.com/open-mmlab/mmaction2/issues) [ðDocumentation](https://
-      mmaction2.readthedocs.io/en/1.x/) | [ð ï¸Installation](https://
- mmaction2.readthedocs.io/en/1.x/get_started.html) | [ðModel Zoo](https://
-  mmaction2.readthedocs.io/en/1.x/modelzoo.html) | [ðUpdate News](https://
-mmaction2.readthedocs.io/en/1.x/notes/changelog.html) | [ðOngoing Projects]
-  (https://github.com/open-mmlab/mmaction2/projects) | [ð¤Reporting Issues]
-          (https://github.com/open-mmlab/mmaction2/issues/new/choose)
-## Introduction MMAction2 is an open-source toolbox for video understanding
-based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.org/
-) project. The 1.x branch works with **PyTorch 1.6+**.
-          [https://github.com/open-mmlab/mmaction2/raw/1.x/resources/
-                            mmaction2_overview.gif]
-                  Action Recognition Results on Kinetics-400
- [https://user-images.githubusercontent.com/34324155/123989146-2ecae680-d9fb-
-                          11eb-916b-b9db5563a9e5.gif]
-          Skeleton-based Action Recognition Results on NTU-RGB+D-120
+open-mmlab/mmaction2/branch/main/graph/badge.svg)](https://codecov.io/gh/open-
+ mmlab/mmaction2) [![PyPI](https://img.shields.io/pypi/v/mmaction2)](https://
+pypi.org/project/mmaction2/) [![LICENSE](https://img.shields.io/github/license/
+ open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/blob/main/
+LICENSE) [![Average time to resolve an issue](https://isitmaintained.com/badge/
+resolution/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/
+ issues) [![Percentage of issues still open](https://isitmaintained.com/badge/
+open/open-mmlab/mmaction2.svg)](https://github.com/open-mmlab/mmaction2/issues)
+      [ðDocumentation](https://mmaction2.readthedocs.io/en/latest/) |
+ [ð ï¸Installation](https://mmaction2.readthedocs.io/en/latest/get_started/
+   installation.html) | [ðModel Zoo](https://mmaction2.readthedocs.io/en/
+         latest/modelzoo_statistics.html) | [ðUpdate News](https://
+    mmaction2.readthedocs.io/en/latest/notes/changelog.html) | [ðOngoing
+ Projects](https://github.com/open-mmlab/mmaction2/projects) | [ð¤Reporting
+      Issues](https://github.com/open-mmlab/mmaction2/issues/new/choose)
+
+English | [ç®ä½ä¸­æ](/README_zh-CN.md) ## ð Table of Contents - [ð
+Table of Contents](#-table-of-contents) - [ð¥³ ð What's New](#--whats-new-
+) - [ð Introduction](#-introduction-) - [ð Major Features](#-major-
+features-) - [ð ï¸ Installation](#ï¸-installation-) - [ð Model Zoo](#-
+model-zoo-) - [ð¨âð« Get Started](#-get-started-) - [ð« License](#-
+license-) - [ðï¸ Citation](#ï¸-citation-) - [ð Contributing](#-
+contributing-) - [ð¤ Acknowledgement](#-acknowledgement-) - [ðï¸ Projects
+in OpenMMLab](#ï¸-projects-in-openmmlab-) ## ð¥³ ð What's New [ð](#-
+table-of-contents) **The default branch has been switched to `main`(previous
+`1.x`) from `master`(current `0.x`), and we encourage users to migrate to the
+latest version with more supported models, stronger pre-training checkpoints
+and simpler coding. Please refer to [Migration Guide](https://
+mmaction2.readthedocs.io/en/latest/migration.html) for more details.**
+**Release (2023.07.04)**: v1.1.0 with the following new features: - Support
+CLIP-based multi-modality models: ActionCLIP(Arxiv'2021) and CLIP4clip
+(ArXiv'2022) - Support rich projects: gesture recognition, spatio-temporal
+action detection tutorial, and knowledge distillation - Support HACS-segments
+dataset(ICCV'2019), MultiSports dataset(ICCV'2021), Kinetics-710 dataset
+(Arxiv'2022) - Support VideoMAE V2(CVPR'2023), and VideoMAE(NeurIPS'2022) on
+action detection - Support TCANet(CVPR'2021) - Support [Pure Python style
+Configuration File](https://mmengine.readthedocs.io/en/latest/
+advanced_tutorials/config.html#a-pure-python-style-configuration-file-beta) and
+downloading datasets by MIM with one command ## ð Introduction [ð](#-
+table-of-contents) MMAction2 is an open-source toolbox for video understanding
+based on PyTorch. It is a part of the [OpenMMLab](http://openmmlab.com/
+) project.
+         [https://github.com/open-mmlab/mmaction2/raw/main/resources/
+ mmaction2_overview.gif] [https://user-images.githubusercontent.com/34324155/
+              123989146-2ecae680-d9fb-11eb-916b-b9db5563a9e5.gif]
+Action Recognition on Kinetics-400 (left) and Skeleton-based Action Recognition
+                           on NTU-RGB+D-120 (right)
  [https://user-images.githubusercontent.com/30782254/155710881-bb26863e-fcb4-
                           458e-b0c4-33cd79f96901.gif]
 Skeleton-based Spatio-Temporal Action Detection and Action Recognition Results
                                 on Kinetics-400
-  [https://github.com/open-mmlab/mmaction2/raw/1.x/resources/spatio-temporal-
+ [https://github.com/open-mmlab/mmaction2/raw/main/resources/spatio-temporal-
                                    det.gif]
               Spatio-Temporal Action Detection Results on AVA-2.1
-## Major Features - **Modular design**: We decompose a video understanding
-framework into different components. One can easily construct a customized
-video understanding framework by combining different modules. - **Support four
-major video understanding tasks**: MMAction2 implements various algorithms for
-multiple video understanding tasks, including action recognition, action
-localization, spatio-temporal action detection, and skeleton-based action
-detection. - **Well tested and documented**: We provide detailed documentation
-and API reference, as well as unit tests. ## What's New **Release
-(2022.02.10)**: v1.0.0rc3 with the following new features: - Support Action
-Recognition model UniFormer V1(ICLR'2022), UniFormer V2(Arxiv'2022). - Support
-training MViT V2(CVPR'2022), and MaskFeat(CVPR'2022) fine-tuning. - Add a new
-handy interface for inference MMAction2 models ([demo](https://github.com/open-
-mmlab/mmaction2/blob/dev-1.x/demo/README.md#inferencer)) ## Installation Please
-refer to [install.md](https://mmaction2.readthedocs.io/en/1.x/get_started.html)
-for more detailed instructions. ## Supported Methods
+## ð Major Features [ð](#-table-of-contents) - **Modular design**: We
+decompose a video understanding framework into different components. One can
+easily construct a customized video understanding framework by combining
+different modules. - **Support five major video understanding tasks**:
+MMAction2 implements various algorithms for multiple video understanding tasks,
+including action recognition, action localization, spatio-temporal action
+detection, skeleton-based action detection and video retrieval. - **Well tested
+and documented**: We provide detailed documentation and API reference, as well
+as unit tests. ## ð ï¸ Installation [ð](#-table-of-contents) MMAction2
+depends on [PyTorch](https://pytorch.org/), [MMCV](https://github.com/open-
+mmlab/mmcv), [MMEngine](https://github.com/open-mmlab/mmengine), [MMDetection]
+(https://github.com/open-mmlab/mmdetection) (optional) and [MMPose](https://
+github.com/open-mmlab/mmpose) (optional). Please refer to [install.md](https://
+mmaction2.readthedocs.io/en/latest/get_started/installation.html) for detailed
+instructions.  Quick instructions ```shell conda create --name openmmlab
+python=3.8 -y conda activate open-mmlab conda install pytorch torchvision -
+c pytorch # This command will automatically install the latest version PyTorch
+and cudatoolkit, please check whether they match your environment. pip install
+-U openmim mim install mmengine mim install mmcv mim install mmdet # optional
+mim install mmpose # optional git clone https://github.com/open-mmlab/
+mmaction2.git cd mmaction2 pip install -v -e . ```  ## ð Model Zoo [ð](#-
+table-of-contents) Results and models are available in the [model zoo](https://
+mmaction2.readthedocs.io/en/latest/model_zoo/modelzoo.html).  Supported model
 Action Recognition
 C3D (CVPR'2014) TSN (ECCV'2016) I3D (CVPR'2017) C2D (CVPR'2018) I3D_Non-Local
                                                                 (CVPR'2018)
 R(2+1)D         TRN (ECCV'2018) TSM (ICCV'2019) TSM_Non-Local   SlowOnly
 (CVPR'2018)                                     (ICCV'2019)     (ICCV'2019)
 SlowFast        CSN (ICCV'2019) TIN (AAAI'2020) TPN (CVPR'2020) X3D (CVPR'2020)
 (ICCV'2019)
-MultiModality:  TANet           TimeSformer     VideoSwin       VideoMAE
-Audio           (ArXiv'2020)    (ICML'2021)     (CVPR'2022)     (NeurIPS'2022)
+MultiModality:  TANet           TimeSformer     ActionCLIP      VideoSwin
+Audio           (ArXiv'2020)    (ICML'2021)     (ArXiv'2021)    (CVPR'2022)
 (ArXiv'2020)
-MViT_V2         UniFormer_V1    UniFormer_V2
-(CVPR'2022)     (ICLR'2022)     (Arxiv'2022)
+VideoMAE        MViT_V2         UniFormer_V1    UniFormer_V2    VideoMAE_V2
+(NeurIPS'2022)  (CVPR'2022)     (ICLR'2022)     (Arxiv'2022)    (CVPR'2023)
 Action Localization
-SSN (ICCV'2017) BSN (ECCV'2018) BMN (ICCV'2019)
+BSN (ECCV'2018) BMN (ICCV'2019) TCANet
+                                (CVPR'2021)
 Spatio-Temporal Action Detection
-ACRN            SlowOnly+Fast   SlowFast+Fast
-(ECCV'2018)     R-CNN           R-CNN           LFB (CVPR'2019)
+ACRN            SlowOnly+Fast   SlowFast+Fast                   VideoMAE
+(ECCV'2018)     R-CNN           R-CNN           LFB (CVPR'2019) (NeurIPS'2022)
                 (ICCV'2019)     (ICCV'2019)
 Skeleton-based Action Recognition
-ST-GCN          2s-AGCN         PoseC3D         STGCN++
-(AAAI'2018)     (CVPR'2019)     (CVPR'2022)     (ArXiv'2022)
-Results and models are available in the *README.md* of each method's config
-directory. A summary can be found on the [**model zoo**](https://
-mmaction2.readthedocs.io/en/1.x/modelzoo.html) page. We will keep up with the
-latest progress of the community and support more popular algorithms and
-frameworks. If you have any feature requests, please feel free to leave a
-comment in [Issues](https://github.com/open-mmlab/mmaction2/issues/19). ##
-Supported Datasets
+ST-GCN          2s-AGCN         PoseC3D         STGCN++         CTRGCN
+(AAAI'2018)     (CVPR'2019)     (CVPR'2022)     (ArXiv'2022)    (CVPR'2021)
+MSG3D
+(CVPR'2020)
+Video Retrieval
+CLIP4Clip
+(ArXiv'2022)
+  Supported dataset
 Action Recognition
 HMDB51 (Homepage)    UCF101 (Homepage)  ActivityNet          Kinetics-[400/600/
 (ICCV'2011)          (CRCV-IR-12-01)    (Homepage)           700] (Homepage)
                                         (CVPR'2015)          (CVPR'2017)
 SthV1 (ICCV'2017)    SthV2 (Homepage)   Diving48 (Homepage)  Jester (Homepage)
                      (ICCV'2017)        (ECCV'2018)          (ICCV'2019)
 Moments_in_Time      Multi-Moments_in   HVU (Homepage)       OmniSource
 (Homepage)           Time (Homepage)    (ECCV'2020)          (Homepage)
 (TPAMI'2019)         (ArXiv'2019)                            (ECCV'2020)
-FineGYM (Homepage)
-(CVPR'2020)
+FineGYM (Homepage)   Kinetics-710
+(CVPR'2020)          (Homepage)
+                     (Arxiv'2022)
 Action Localization
-THUMOS14 (Homepage)  ActivityNet
-(THUMOS Challenge    (Homepage)
+THUMOS14 (Homepage)  ActivityNet        HACS (Homepage)
+(THUMOS Challenge    (Homepage)         (ICCV'2019)
 2014)                (CVPR'2015)
 Spatio-Temporal Action Detection
 UCF101-24*           JHMDB* (Homepage)  AVA (Homepage)       AVA-Kinetics
 (Homepage) (CRCV-IR- (ICCV'2015)        (CVPR'2018)          (Homepage)
 12-01)                                                       (Arxiv'2020)
+MultiSports
+(Homepage)
+(ICCV'2021)
 Skeleton-based Action Recognition
 PoseC3D-FineGYM      PoseC3D-NTURGB+D   PoseC3D-UCF101       PoseC3D-HMDB51
 (Homepage)           (Homepage)         (Homepage)           (Homepage)
 (ArXiv'2021)         (ArXiv'2021)       (ArXiv'2021)         (ArXiv'2021)
-Datasets marked with * are not fully supported yet, but related dataset
-preparation steps are provided. A summary can be found on the [**Supported
-Datasets**](https://mmaction2.readthedocs.io/en/latest/supported_datasets.html)
-page. ## Data Preparation Please refer to [data_preparation.md](docs/en/
-user_guides/2_data_prepare.md) for a general knowledge of data preparation. ##
-FAQ Please refer to [FAQ](docs/en/notes/faq.md) for frequently asked questions.
-## Projects built on MMAction2 Currently, there are many research works and
-projects built on MMAction2 by users from community, such as: - Video Swin
-Transformer. [\[paper\]](https://arxiv.org/abs/2106.13230)[\[github\]](https://
-github.com/SwinTransformer/Video-Swin-Transformer) - Evidential Deep Learning
-for Open Set Action Recognition, ICCV 2021 **Oral**. [\[paper\]](https://
-arxiv.org/abs/2107.10161)[\[github\]](https://github.com/Cogito2012/DEAR) -
-Rethinking Self-supervised Correspondence Learning: A Video Frame-level
-Similarity Perspective, ICCV 2021 **Oral**. [\[paper\]](https://arxiv.org/abs/
-2103.17263)[\[github\]](https://github.com/xvjiarui/VFS) etc., check
-[projects.md](docs/en/notes/projects.md) to see all related projects. ##
-License This project is released under the [Apache 2.0 license](LICENSE). ##
-Citation If you find this project useful in your research, please consider
-cite: ```BibTeX @misc{2020mmaction2, title={OpenMMLab's Next Generation Video
-Understanding Toolbox and Benchmark}, author={MMAction2 Contributors},
-howpublished = {\url{https://github.com/open-mmlab/mmaction2}}, year={2020} }
-``` ## Contributing We appreciate all contributions to improve MMAction2.
-Please refer to [CONTRIBUTING.md](https://github.com/open-mmlab/mmcv/blob/1.x/
-CONTRIBUTING.md) in MMCV for more details about the contributing guideline. ##
-Acknowledgement MMAction2 is an open-source project that is contributed by
-researchers and engineers from various colleges and companies. We appreciate
-all the contributors who implement their methods or add new features and users
-who give valuable feedback. We wish that the toolbox and benchmark could serve
-the growing research community by providing a flexible toolkit to reimplement
-existing methods and develop their new models. ## Projects in OpenMMLab -
-[MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational
-library for training deep learning models. - [MMCV](https://github.com/open-
-mmlab/mmcv): OpenMMLab foundational library for computer vision. - [MIM](https:
-//github.com/open-mmlab/mim): MIM installs OpenMMLab packages. -
-[MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab
-image classification toolbox and benchmark. - [MMDetection](https://github.com/
-open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark. -
-[MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-
-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
+Video Retrieval
+MSRVTT (Homepage)
+(CVPR'2016)
+ ## ð¨âð« Get Started [ð](#-table-of-contents) For tutorials, we
+provide the following user guides for basic usage: - [Migration from MMAction2
+0.X](https://mmaction2.readthedocs.io/en/latest/migration.html) - [Learn about
+Configs](https://mmaction2.readthedocs.io/en/latest/user_guides/config.html) -
+[Prepare Datasets](https://mmaction2.readthedocs.io/en/latest/user_guides/
+prepare_dataset.html) - [Inference with Existing Models](https://
+mmaction2.readthedocs.io/en/latest/user_guides/inference.html) - [Training and
+Testing](https://mmaction2.readthedocs.io/en/latest/user_guides/
+train_test.html)  Research works built on MMAction2 by users from community -
+Video Swin Transformer. [\[paper\]](https://arxiv.org/abs/2106.13230)[\
+[github\]](https://github.com/SwinTransformer/Video-Swin-Transformer) -
+Evidential Deep Learning for Open Set Action Recognition, ICCV 2021 **Oral**.
+[\[paper\]](https://arxiv.org/abs/2107.10161)[\[github\]](https://github.com/
+Cogito2012/DEAR) - Rethinking Self-supervised Correspondence Learning: A Video
+Frame-level Similarity Perspective, ICCV 2021 **Oral**. [\[paper\]](https://
+arxiv.org/abs/2103.17263)[\[github\]](https://github.com/xvjiarui/VFS)  ## ð«
+License [ð](#-table-of-contents) This project is released under the [Apache
+2.0 license](LICENSE). ## ðï¸ Citation [ð](#-table-of-contents) If you
+find this project useful in your research, please consider cite: ```BibTeX
+@misc{2020mmaction2, title={OpenMMLab's Next Generation Video Understanding
+Toolbox and Benchmark}, author={MMAction2 Contributors}, howpublished = {\url
+{https://github.com/open-mmlab/mmaction2}}, year={2020} } ``` ## ð
+Contributing [ð](#-table-of-contents) We appreciate all contributions to
+improve MMAction2. Please refer to [CONTRIBUTING.md](https://github.com/open-
+mmlab/mmcv/blob/2.x/CONTRIBUTING.md) in MMCV for more details about the
+contributing guideline. ## ð¤ Acknowledgement [ð](#-table-of-contents)
+MMAction2 is an open-source project that is contributed by researchers and
+engineers from various colleges and companies. We appreciate all the
+contributors who implement their methods or add new features and users who give
+valuable feedback. We wish that the toolbox and benchmark could serve the
+growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their new models. ## ðï¸ Projects in OpenMMLab
+[ð](#-table-of-contents) - [MMEngine](https://github.com/open-mmlab/
+mmengine): OpenMMLab foundational library for training deep learning models. -
+[MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for
+computer vision. - [MIM](https://github.com/open-mmlab/mim): MIM installs
+OpenMMLab packages. - [MMEval](https://github.com/open-mmlab/mmeval): A unified
+evaluation library for multiple machine learning libraries. - [MMPreTrain]
+(https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and
+benchmark. - [MMDetection](https://github.com/open-mmlab/mmdetection):
+OpenMMLab detection toolbox and benchmark. - [MMDetection3D](https://
+github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for
+general 3D object detection. - [MMRotate](https://github.com/open-mmlab/
+mmrotate): OpenMMLab rotated object detection toolbox and benchmark. - [MMYOLO]
+(https://github.com/open-mmlab/mmyolo): OpenMMLab YOLO series toolbox and
 benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
 OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
 github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
 understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
 OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
 github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
 and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
 OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
 github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
 benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
 fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
 mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
 benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
 video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab model deployment framework.
+mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMagic](https:/
+/github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and
+**I**ntelligent **C**reation toolbox. - [MMGeneration](https://github.com/open-
+mmlab/mmgeneration): OpenMMLab image and video generative models toolbox. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab model deployment
+framework. - [Playground](https://github.com/open-mmlab/playground): A central
+hub for gathering and showcasing amazing projects built upon OpenMMLab.
 Keywords: computer vision,video understanding Platform: UNKNOWN Classifier:
 Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
 Software License Classifier: Operating System :: OS Independent Classifier:
 Programming Language :: Python :: 3 Classifier: Programming Language :: Python
 :: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
 Programming Language :: Python :: 3.9 Description-Content-Type: text/markdown
 Provides-Extra: all Provides-Extra: tests Provides-Extra: optional Provides-
```

### Comparing `mmaction2-1.0.0rc3/mmaction2.egg-info/SOURCES.txt` & `mmaction2-1.1.0/mmaction2.egg-info/SOURCES.txt`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 MANIFEST.in
 README.md
 setup.cfg
 setup.py
 mmaction/__init__.py
 mmaction/registry.py
 mmaction/version.py
+mmaction/.mim/dataset-index.yml
 mmaction/.mim/model-index.yml
 mmaction/.mim/configs/_base_/default_runtime.py
 mmaction/.mim/configs/_base_/models/audioonly_r50.py
 mmaction/.mim/configs/_base_/models/bmn_400x100.py
 mmaction/.mim/configs/_base_/models/bsn_pem.py
 mmaction/.mim/configs/_base_/models/bsn_tem.py
 mmaction/.mim/configs/_base_/models/c2d_r50.py
@@ -34,50 +35,57 @@
 mmaction/.mim/configs/_base_/schedules/sgd_100e.py
 mmaction/.mim/configs/_base_/schedules/sgd_150e_warmup.py
 mmaction/.mim/configs/_base_/schedules/sgd_50e.py
 mmaction/.mim/configs/_base_/schedules/sgd_tsm_100e.py
 mmaction/.mim/configs/_base_/schedules/sgd_tsm_50e.py
 mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_100e.py
 mmaction/.mim/configs/_base_/schedules/sgd_tsm_mobilenet_v2_50e.py
-mmaction/.mim/configs/detection/_base_/models/slowonly_r50.py
-mmaction/.mim/configs/detection/_base_/models/slowonly_r50_nl.py
 mmaction/.mim/configs/detection/acrn/metafile.yml
 mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava21-rgb.py
 mmaction/.mim/configs/detection/acrn/slowfast-acrn_kinetics400-pretrained-r50_8xb8-8x8x1-cosine-10e_ava22-rgb.py
-mmaction/.mim/configs/detection/ava/metafile.yml
-mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50-context_8xb16-4x16x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50-temporal-max_8xb6-8x8x1-cosine-10e_ava22-rgb.py
-mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb6-8x8x1-cosine-10e_ava22-rgb.py
-mmaction/.mim/configs/detection/ava/slowfast_kinetics400-pretrained-r50_8xb8-8x8x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava/slowfast_r50-k400-pre-temporal-max-focal-alpha3-gamma1_8xb6-8x8x1-cosine-10e_ava22-rgb.py
-mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50-nl_8xb16-4x16x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50-nl_8xb16-8x8x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava/slowonly_kinetics700-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k400-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k400-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-10e_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-focal-10e_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context-temporal-max_8xb8-8x8x1-10e_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50-context_8xb8-8x8x1-10e_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50_8xb8-16x4x1-10e-tricks_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
-mmaction/.mim/configs/detection/ava_kinetics/slowonly_k700-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
 mmaction/.mim/configs/detection/lfb/metafile.yml
+mmaction/.mim/configs/detection/lfb/slowonly-lfb-infer_r50_ava21-rgb.py
 mmaction/.mim/configs/detection/lfb/slowonly-lfb-max_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py
 mmaction/.mim/configs/detection/lfb/slowonly-lfb-nl_kinetics400-pretrained-r50_8xb12-4x16x1-20e_ava21-rgb.py
 mmaction/.mim/configs/detection/lfb/slowonly-lfb_ava-pretrained-r50_infer-4x16x1_ava21-rgb.py
+mmaction/.mim/configs/detection/slowfast/metafile.yml
+mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50-context_8xb16-4x16x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50-temporal-max_8xb6-8x8x1-cosine-10e_ava22-rgb.py
+mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb.py
+mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb6-8x8x1-cosine-10e_ava22-rgb.py
+mmaction/.mim/configs/detection/slowfast/slowfast_kinetics400-pretrained-r50_8xb8-8x8x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/slowfast/slowfast_r50-k400-pre-temporal-max-focal-alpha3-gamma1_8xb6-8x8x1-cosine-10e_ava22-rgb.py
+mmaction/.mim/configs/detection/slowonly/metafile.yml
+mmaction/.mim/configs/detection/slowonly/slowonly_k400-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k400-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context-temporal-max-nl-head_8xb8-8x8x1-focal-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context-temporal-max_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50-context_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50_8xb8-16x4x1-10e-tricks_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50_8xb8-4x16x1-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_k700-pre-r50_8xb8-8x8x1-10e_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r101_8xb16-8x8x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50-nl_8xb16-4x16x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50-nl_8xb16-8x8x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_kinetics400-pretrained-r50_8xb16-4x16x1-8e_multisports-rgb.py
+mmaction/.mim/configs/detection/slowonly/slowonly_kinetics700-pretrained-r50_8xb16-4x16x1-20e_ava21-rgb.py
+mmaction/.mim/configs/detection/videomae/metafile.yml
+mmaction/.mim/configs/detection/videomae/vit-base-p16_videomae-k400-pre_8xb8-16x4x1-20e-adamw_ava-kinetics-rgb.py
+mmaction/.mim/configs/detection/videomae/vit-large-p16_videomae-k400-pre_8xb8-16x4x1-20e-adamw_ava-kinetics-rgb.py
 mmaction/.mim/configs/localization/bmn/bmn_2xb8-400x100-9e_activitynet-feature.py
 mmaction/.mim/configs/localization/bmn/metafile.yml
 mmaction/.mim/configs/localization/bsn/bsn_pem_1xb16-400x100-20e_activitynet-feature.py
 mmaction/.mim/configs/localization/bsn/bsn_pgm_400x100_activitynet-feature.py
 mmaction/.mim/configs/localization/bsn/bsn_tem_1xb16-400x100-20e_activitynet-feature.py
 mmaction/.mim/configs/localization/bsn/metafile.yml
+mmaction/.mim/configs/localization/tcanet/metafile.yml
+mmaction/.mim/configs/localization/tcanet/tcanet_2xb8-700x100-9e_hacs-feature.py
 mmaction/.mim/configs/recognition/c2d/c2d_r101-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre-nopool_8xb32-8x8x1-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-16x4x1-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/c2d/c2d_r50-in1k-pre_8xb32-8x8x1-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/c2d/metafile.yml
 mmaction/.mim/configs/recognition/c3d/c3d_sports1m-pretrained_8xb30-16x1x1-45e_ucf101-rgb.py
 mmaction/.mim/configs/recognition/c3d/metafile.yml
@@ -115,25 +123,27 @@
 mmaction/.mim/configs/recognition/slowfast/slowfast_r101_8xb8-8x8x1-256e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-4x16x1-256e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-8x8x1-256e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowfast/slowfast_r50_8xb8-8x8x1-steplr-256e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowonly/metafile.yml
 mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-4x16x1-steplr-150e_kinetics700-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_16xb16-8x8x1-steplr-150e_kinetics700-rgb.py
+mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_32xb8-8x8x1-steplr-150e_kinetics710-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_imagenet-pretrained-r50_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_r101_8xb16-8x8x1-196e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-4x16x1-steplr-150e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_r50-in1k-pre-nl-embedded-gaussian_8xb16-8x8x1-steplr-150e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-4x16x1-256e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/slowonly/slowonly_r50_8xb16-8x8x1-256e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/swin/metafile.yml
 mmaction/.mim/configs/recognition/swin/swin-base-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb.py
 mmaction/.mim/configs/recognition/swin/swin-large-p244-w877_in22k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
+mmaction/.mim/configs/recognition/swin/swin-small-p244-w877_in1k-pre_32xb4-amp-32x2x1-30e_kinetics710-rgb.py
 mmaction/.mim/configs/recognition/swin/swin-small-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/swin/swin-tiny-p244-w877_in1k-pre_8xb8-amp-32x2x1-30e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tanet/metafile.yml
 mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb6-1x1x16-50e_sthv1-rgb.py
 mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-1x1x8-50e_sthv1-rgb.py
 mmaction/.mim/configs/recognition/tanet/tanet_imagenet-pretrained-r50_8xb8-dense-1x1x8-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/timesformer/metafile.yml
@@ -148,14 +158,15 @@
 mmaction/.mim/configs/recognition/tpn/tpn-slowonly_imagenet-pretrained-r50_8xb8-8x8x1-150e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tpn/tpn-slowonly_r50_8xb8-8x8x1-150e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tpn/tpn-tsm_imagenet-pretrained-r50_8xb8-1x1x8-150e_sthv1-rgb.py
 mmaction/.mim/configs/recognition/trn/metafile.yml
 mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv1-rgb.py
 mmaction/.mim/configs/recognition/trn/trn_imagenet-pretrained-r50_8xb16-1x1x8-50e_sthv2-rgb.py
 mmaction/.mim/configs/recognition/tsm/metafile.yml
+mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-mobilenetv2_8xb16-1x1x8-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r101_8xb16-1x1x8-50e_sthv2-rgb.py
 mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-dot-product_8xb16-1x1x8-50e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-embedded-gaussian_8xb16-1x1x8-50e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50-nl-gaussian_8xb16-1x1x8-50e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x16-50e_sthv2-rgb.py
 mmaction/.mim/configs/recognition/tsm/tsm_imagenet-pretrained-r50_8xb16-1x1x8-100e_kinetics400-rgb.py
@@ -168,25 +179,29 @@
 mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x3-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x5-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-1x1x8-50e_sthv2-rgb.py
 mmaction/.mim/configs/recognition/tsn/tsn_imagenet-pretrained-r50_8xb32-dense-1x1x5-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-dense161_8xb32-1x1x3-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-rn101-32x4d_8xb32-1x1x3-100e_kinetics400-rgb.py
+mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-swin-transformer_32xb8-1x1x8-50e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/tsn/custom_backbones/tsn_imagenet-pretrained-swin-transformer_8xb32-1x1x3-100e_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/uniformer/metafile.yml
 mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_16x4x1_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/uniformer/uniformer-base_imagenet1k-pre_32x4x1_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/uniformer/uniformer-small_imagenet1k-pre_16x4x1_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/metafile.yml
-mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
-mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py
-mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics600-rgb.py
-mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_u8_kinetics700-rgb.py
+mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-kinetics-k400-pre_16xb32-u8_mitv1-rgb.py
+mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics400-rgb.py
+mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics600-rgb.py
+mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-kinetics710-pre_8xb32-u8_kinetics700-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip-pre_u8_kinetics710-rgb.py
+mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics400-rgb.py
+mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_8xb32-u8_kinetics700-rgb.py
+mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-base-p16-res224_clip_u8_kinetics710-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics600-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u16_kinetics700-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics400-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics600-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u32_kinetics700-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res224_clip-kinetics710-pre_u8_kinetics400-rgb.py
@@ -198,21 +213,26 @@
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-kinetics710-pre_u32_kinetics700-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p14-res336_clip-pre_u8_kinetics710-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res224_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
 mmaction/.mim/configs/recognition/uniformerv2/uniformerv2-large-p16-res336_clip-kinetics710-kinetics-k400-pre_u8_mitv1-rgb.py
 mmaction/.mim/configs/recognition/videomae/metafile.yml
 mmaction/.mim/configs/recognition/videomae/vit-base-p16_videomae-k400-pre_16x4x1_kinetics-400.py
 mmaction/.mim/configs/recognition/videomae/vit-large-p16_videomae-k400-pre_16x4x1_kinetics-400.py
+mmaction/.mim/configs/recognition/videomaev2/metafile.yml
+mmaction/.mim/configs/recognition/videomaev2/vit-base-p16_videomaev2-vit-g-dist-k710-pre_16x4x1_kinetics-400.py
+mmaction/.mim/configs/recognition/videomaev2/vit-small-p16_videomaev2-vit-g-dist-k710-pre_16x4x1_kinetics-400.py
 mmaction/.mim/configs/recognition/x3d/metafile.yml
 mmaction/.mim/configs/recognition/x3d/x3d_m_16x5x1_facebook-kinetics400-rgb.py
 mmaction/.mim/configs/recognition/x3d/x3d_s_13x6x1_facebook-kinetics400-rgb.py
 mmaction/.mim/configs/recognition_audio/audioonly/audioonly_r50_8xb160-64x1x1-100e_kinetics400-audio-feature.py
 mmaction/.mim/configs/recognition_audio/resnet/metafile.yml
 mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio-feature.py
 mmaction/.mim/configs/recognition_audio/resnet/tsn_r18_8xb320-64x1x1-100e_kinetics400-audio.py
+mmaction/.mim/configs/retrieval/clip4clip/clip4clip_vit-base-p32-res224-clip-pre_8xb16-u12-5e_msrvtt-9k-rgb.py
+mmaction/.mim/configs/retrieval/clip4clip/metafile.yml
 mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
 mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
 mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-2d.py
 mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-bone-u100-80e_ntu60-xsub-keypoint-3d.py
 mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-2d.py
 mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-motion-u100-80e_ntu60-xsub-keypoint-3d.py
 mmaction/.mim/configs/skeleton/2s-agcn/2s-agcn_8xb16-joint-u100-80e_ntu60-xsub-keypoint-2d.py
@@ -221,14 +241,17 @@
 mmaction/.mim/configs/skeleton/posec3d/metafile.yml
 mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_hmdb51-split1-keypoint.py
 mmaction/.mim/configs/skeleton/posec3d/slowonly_kinetics400-pretrained-r50_8xb16-u48-120e_ucf101-split1-keypoint.py
 mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-keypoint.py
 mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_gym-limb.py
 mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py
 mmaction/.mim/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-limb.py
+mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/pose_only.py
+mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/rgb_only.py
+mmaction/.mim/configs/skeleton/posec3d/rgbpose_conv3d/rgbpose_conv3d.py
 mmaction/.mim/configs/skeleton/stgcn/metafile.yml
 mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-2d.py
 mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu120-xsub-keypoint-3d.py
 mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-2d.py
 mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-motion-u100-80e_ntu60-xsub-keypoint-3d.py
 mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-2d.py
 mmaction/.mim/configs/skeleton/stgcn/stgcn_8xb16-bone-u100-80e_ntu120-xsub-keypoint-3d.py
@@ -257,14 +280,15 @@
 mmaction/.mim/tools/slurm_train.sh
 mmaction/.mim/tools/test.py
 mmaction/.mim/tools/train.py
 mmaction/.mim/tools/analysis_tools/analyze_logs.py
 mmaction/.mim/tools/analysis_tools/bench_processing.py
 mmaction/.mim/tools/analysis_tools/benchmark.py
 mmaction/.mim/tools/analysis_tools/check_videos.py
+mmaction/.mim/tools/analysis_tools/confusion_matrix.py
 mmaction/.mim/tools/analysis_tools/eval_metric.py
 mmaction/.mim/tools/analysis_tools/get_flops.py
 mmaction/.mim/tools/analysis_tools/print_config.py
 mmaction/.mim/tools/analysis_tools/report_accuracy.py
 mmaction/.mim/tools/analysis_tools/report_map.py
 mmaction/.mim/tools/convert/convert_recognizer.py
 mmaction/.mim/tools/data/anno_txt2json.py
@@ -283,15 +307,17 @@
 mmaction/.mim/tools/data/activitynet/download_bsn_videos.sh
 mmaction/.mim/tools/data/activitynet/download_feature_annotations.sh
 mmaction/.mim/tools/data/activitynet/download_features.sh
 mmaction/.mim/tools/data/activitynet/download_videos.sh
 mmaction/.mim/tools/data/activitynet/extract_frames.sh
 mmaction/.mim/tools/data/activitynet/generate_rawframes_filelist.py
 mmaction/.mim/tools/data/activitynet/process_annotations.py
-mmaction/.mim/tools/data/activitynet/tsn_feature_extraction.py
+mmaction/.mim/tools/data/activitynet/tsn_extract_flow_feat_config.py
+mmaction/.mim/tools/data/activitynet/tsn_extract_rgb_feat_config.py
+mmaction/.mim/tools/data/activitynet/tsn_extract_video_feat_config.py
 mmaction/.mim/tools/data/ava/cut_videos.sh
 mmaction/.mim/tools/data/ava/download_annotations.sh
 mmaction/.mim/tools/data/ava/download_videos.sh
 mmaction/.mim/tools/data/ava/download_videos_gnu_parallel.sh
 mmaction/.mim/tools/data/ava/download_videos_parallel.py
 mmaction/.mim/tools/data/ava/download_videos_parallel.sh
 mmaction/.mim/tools/data/ava/extract_frames.sh
@@ -308,21 +334,26 @@
 mmaction/.mim/tools/data/diving48/download_annotations.sh
 mmaction/.mim/tools/data/diving48/download_videos.sh
 mmaction/.mim/tools/data/diving48/extract_frames.sh
 mmaction/.mim/tools/data/diving48/extract_rgb_frames.sh
 mmaction/.mim/tools/data/diving48/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/diving48/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/diving48/generate_videos_filelist.sh
+mmaction/.mim/tools/data/diving48/preprocess.sh
 mmaction/.mim/tools/data/gym/download.py
 mmaction/.mim/tools/data/gym/download_annotations.sh
 mmaction/.mim/tools/data/gym/download_videos.sh
 mmaction/.mim/tools/data/gym/extract_frames.sh
 mmaction/.mim/tools/data/gym/generate_file_list.py
 mmaction/.mim/tools/data/gym/trim_event.py
 mmaction/.mim/tools/data/gym/trim_subaction.py
+mmaction/.mim/tools/data/hacs/generate_anotations.py
+mmaction/.mim/tools/data/hacs/generate_list.py
+mmaction/.mim/tools/data/hacs/slowonly_feature_infer.py
+mmaction/.mim/tools/data/hacs/write_feature_csv.py
 mmaction/.mim/tools/data/hmdb51/download_annotations.sh
 mmaction/.mim/tools/data/hmdb51/download_videos.sh
 mmaction/.mim/tools/data/hmdb51/extract_frames.sh
 mmaction/.mim/tools/data/hmdb51/extract_rgb_frames.sh
 mmaction/.mim/tools/data/hmdb51/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/hmdb51/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/hmdb51/generate_videos_filelist.sh
@@ -344,55 +375,65 @@
 mmaction/.mim/tools/data/kinetics/download_backup_annotations.sh
 mmaction/.mim/tools/data/kinetics/download_videos.sh
 mmaction/.mim/tools/data/kinetics/extract_frames.sh
 mmaction/.mim/tools/data/kinetics/extract_rgb_frames.sh
 mmaction/.mim/tools/data/kinetics/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/kinetics/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/kinetics/generate_videos_filelist.sh
+mmaction/.mim/tools/data/kinetics/preprocess_k400.sh
+mmaction/.mim/tools/data/kinetics/preprocess_k600.sh
+mmaction/.mim/tools/data/kinetics/preprocess_k700.sh
 mmaction/.mim/tools/data/kinetics/rename_classnames.sh
 mmaction/.mim/tools/data/mit/extract_frames.sh
 mmaction/.mim/tools/data/mit/extract_rgb_frames.sh
 mmaction/.mim/tools/data/mit/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/mit/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/mit/generate_videos_filelist.sh
 mmaction/.mim/tools/data/mit/preprocess_data.sh
 mmaction/.mim/tools/data/mmit/extract_frames.sh
 mmaction/.mim/tools/data/mmit/extract_rgb_frames.sh
 mmaction/.mim/tools/data/mmit/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/mmit/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/mmit/generate_videos_filelist.sh
 mmaction/.mim/tools/data/mmit/preprocess_data.sh
+mmaction/.mim/tools/data/multisports/format_det_result.py
+mmaction/.mim/tools/data/multisports/parse_anno.py
 mmaction/.mim/tools/data/omnisource/trim_raw_video.py
 mmaction/.mim/tools/data/skeleton/babel2mma2.py
-mmaction/.mim/tools/data/skeleton/download_annotations.sh
+mmaction/.mim/tools/data/skeleton/compress_nturgbd.py
 mmaction/.mim/tools/data/skeleton/gen_ntu_rgbd_raw.py
 mmaction/.mim/tools/data/skeleton/ntu_pose_extraction.py
 mmaction/.mim/tools/data/sthv1/encode_videos.sh
 mmaction/.mim/tools/data/sthv1/extract_flow.sh
 mmaction/.mim/tools/data/sthv1/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/sthv1/generate_videos_filelist.sh
 mmaction/.mim/tools/data/sthv2/extract_frames.sh
 mmaction/.mim/tools/data/sthv2/extract_rgb_frames.sh
 mmaction/.mim/tools/data/sthv2/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/sthv2/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/sthv2/generate_videos_filelist.sh
+mmaction/.mim/tools/data/sthv2/preprocss.sh
 mmaction/.mim/tools/data/thumos14/denormalize_proposal_file.sh
 mmaction/.mim/tools/data/thumos14/download_annotations.sh
 mmaction/.mim/tools/data/thumos14/download_videos.sh
 mmaction/.mim/tools/data/thumos14/extract_frames.sh
 mmaction/.mim/tools/data/thumos14/extract_rgb_frames.sh
 mmaction/.mim/tools/data/thumos14/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/thumos14/fetch_tag_proposals.sh
 mmaction/.mim/tools/data/ucf101/download_annotations.sh
 mmaction/.mim/tools/data/ucf101/download_videos.sh
 mmaction/.mim/tools/data/ucf101/extract_frames.sh
 mmaction/.mim/tools/data/ucf101/extract_rgb_frames.sh
 mmaction/.mim/tools/data/ucf101/extract_rgb_frames_opencv.sh
 mmaction/.mim/tools/data/ucf101/generate_rawframes_filelist.sh
 mmaction/.mim/tools/data/ucf101/generate_videos_filelist.sh
+mmaction/.mim/tools/data/video_retrieval/prepare_msrvtt.py
+mmaction/.mim/tools/data/video_retrieval/prepare_msrvtt.sh
+mmaction/.mim/tools/deployment/export_onnx_gcn.py
+mmaction/.mim/tools/deployment/export_onnx_posec3d.py
 mmaction/.mim/tools/deployment/export_onnx_stdet.py
 mmaction/.mim/tools/deployment/mmaction2torchserve.py
 mmaction/.mim/tools/deployment/mmaction_handler.py
 mmaction/.mim/tools/deployment/publish_model.py
 mmaction/.mim/tools/misc/bsn_proposal_generation.py
 mmaction/.mim/tools/misc/clip_feature_extraction.py
 mmaction/.mim/tools/misc/dist_clip_feature_extraction.sh
@@ -410,19 +451,21 @@
 mmaction/datasets/audio_dataset.py
 mmaction/datasets/ava_dataset.py
 mmaction/datasets/base.py
 mmaction/datasets/pose_dataset.py
 mmaction/datasets/rawframe_dataset.py
 mmaction/datasets/repeat_aug_dataset.py
 mmaction/datasets/video_dataset.py
+mmaction/datasets/video_text_dataset.py
 mmaction/datasets/transforms/__init__.py
 mmaction/datasets/transforms/formatting.py
 mmaction/datasets/transforms/loading.py
 mmaction/datasets/transforms/pose_transforms.py
 mmaction/datasets/transforms/processing.py
+mmaction/datasets/transforms/text_transforms.py
 mmaction/datasets/transforms/wrappers.py
 mmaction/engine/__init__.py
 mmaction/engine/hooks/__init__.py
 mmaction/engine/hooks/output.py
 mmaction/engine/hooks/visualization_hook.py
 mmaction/engine/model/__init__.py
 mmaction/engine/model/weight_init.py
@@ -433,25 +476,25 @@
 mmaction/engine/runner/__init__.py
 mmaction/engine/runner/multi_loop.py
 mmaction/evaluation/__init__.py
 mmaction/evaluation/functional/__init__.py
 mmaction/evaluation/functional/accuracy.py
 mmaction/evaluation/functional/ava_utils.py
 mmaction/evaluation/functional/eval_detection.py
+mmaction/evaluation/functional/multisports_utils.py
 mmaction/evaluation/functional/ava_evaluation/__init__.py
 mmaction/evaluation/functional/ava_evaluation/metrics.py
 mmaction/evaluation/functional/ava_evaluation/np_box_list.py
 mmaction/evaluation/functional/ava_evaluation/np_box_ops.py
-mmaction/evaluation/functional/ava_evaluation/object_detection_evaluation.py
-mmaction/evaluation/functional/ava_evaluation/per_image_evaluation.py
-mmaction/evaluation/functional/ava_evaluation/standard_fields.py
 mmaction/evaluation/metrics/__init__.py
 mmaction/evaluation/metrics/acc_metric.py
 mmaction/evaluation/metrics/anet_metric.py
 mmaction/evaluation/metrics/ava_metric.py
+mmaction/evaluation/metrics/multisports_metric.py
+mmaction/evaluation/metrics/retrieval_metric.py
 mmaction/models/__init__.py
 mmaction/models/backbones/__init__.py
 mmaction/models/backbones/aagcn.py
 mmaction/models/backbones/c2d.py
 mmaction/models/backbones/c3d.py
 mmaction/models/backbones/mobilenet_v2.py
 mmaction/models/backbones/mobilenet_v2_tsm.py
@@ -462,14 +505,15 @@
 mmaction/models/backbones/resnet3d_csn.py
 mmaction/models/backbones/resnet3d_slowfast.py
 mmaction/models/backbones/resnet3d_slowonly.py
 mmaction/models/backbones/resnet_audio.py
 mmaction/models/backbones/resnet_omni.py
 mmaction/models/backbones/resnet_tin.py
 mmaction/models/backbones/resnet_tsm.py
+mmaction/models/backbones/rgbposeconv3d.py
 mmaction/models/backbones/stgcn.py
 mmaction/models/backbones/swin.py
 mmaction/models/backbones/tanet.py
 mmaction/models/backbones/timesformer.py
 mmaction/models/backbones/uniformer.py
 mmaction/models/backbones/uniformerv2.py
 mmaction/models/backbones/vit_mae.py
@@ -478,34 +522,40 @@
 mmaction/models/common/conv2plus1d.py
 mmaction/models/common/conv_audio.py
 mmaction/models/common/sub_batchnorm3d.py
 mmaction/models/common/tam.py
 mmaction/models/common/transformer.py
 mmaction/models/data_preprocessors/__init__.py
 mmaction/models/data_preprocessors/data_preprocessor.py
+mmaction/models/data_preprocessors/multimodal_data_preprocessor.py
 mmaction/models/heads/__init__.py
 mmaction/models/heads/base.py
+mmaction/models/heads/feature_head.py
 mmaction/models/heads/gcn_head.py
 mmaction/models/heads/i3d_head.py
 mmaction/models/heads/mvit_head.py
 mmaction/models/heads/omni_head.py
+mmaction/models/heads/rgbpose_head.py
 mmaction/models/heads/slowfast_head.py
 mmaction/models/heads/timesformer_head.py
 mmaction/models/heads/tpn_head.py
 mmaction/models/heads/trn_head.py
 mmaction/models/heads/tsm_head.py
 mmaction/models/heads/tsn_audio_head.py
 mmaction/models/heads/tsn_head.py
+mmaction/models/heads/uniformer_head.py
 mmaction/models/heads/x3d_head.py
 mmaction/models/localizers/__init__.py
 mmaction/models/localizers/bmn.py
 mmaction/models/localizers/bsn.py
+mmaction/models/localizers/tcanet.py
 mmaction/models/localizers/utils/__init__.py
 mmaction/models/localizers/utils/bsn_utils.py
 mmaction/models/localizers/utils/proposal_utils.py
+mmaction/models/localizers/utils/tcanet_utils.py
 mmaction/models/losses/__init__.py
 mmaction/models/losses/base.py
 mmaction/models/losses/binary_logistic_regression_loss.py
 mmaction/models/losses/bmn_loss.py
 mmaction/models/losses/cross_entropy_loss.py
 mmaction/models/losses/hvu_loss.py
 mmaction/models/losses/nll_loss.py
@@ -513,28 +563,32 @@
 mmaction/models/losses/ssn_loss.py
 mmaction/models/necks/__init__.py
 mmaction/models/necks/tpn.py
 mmaction/models/recognizers/__init__.py
 mmaction/models/recognizers/base.py
 mmaction/models/recognizers/recognizer2d.py
 mmaction/models/recognizers/recognizer3d.py
+mmaction/models/recognizers/recognizer3d_mm.py
 mmaction/models/recognizers/recognizer_audio.py
 mmaction/models/recognizers/recognizer_gcn.py
 mmaction/models/recognizers/recognizer_omni.py
 mmaction/models/roi_heads/__init__.py
 mmaction/models/roi_heads/roi_head.py
 mmaction/models/roi_heads/bbox_heads/__init__.py
 mmaction/models/roi_heads/bbox_heads/bbox_head.py
 mmaction/models/roi_heads/roi_extractors/__init__.py
 mmaction/models/roi_heads/roi_extractors/single_straight3d.py
 mmaction/models/roi_heads/shared_heads/__init__.py
 mmaction/models/roi_heads/shared_heads/acrn_head.py
 mmaction/models/roi_heads/shared_heads/fbo_head.py
 mmaction/models/roi_heads/shared_heads/lfb.py
 mmaction/models/roi_heads/shared_heads/lfb_infer_head.py
+mmaction/models/similarity/__init__.py
+mmaction/models/similarity/adapters.py
+mmaction/models/similarity/clip_similarity.py
 mmaction/models/task_modules/__init__.py
 mmaction/models/task_modules/assigners/__init__.py
 mmaction/models/task_modules/assigners/max_iou_assigner_ava.py
 mmaction/models/utils/__init__.py
 mmaction/models/utils/blending_utils.py
 mmaction/models/utils/embed.py
 mmaction/models/utils/gcn_utils.py
@@ -547,15 +601,15 @@
 mmaction/testing/__init__.py
 mmaction/testing/_utils.py
 mmaction/utils/__init__.py
 mmaction/utils/collect_env.py
 mmaction/utils/gradcam_utils.py
 mmaction/utils/misc.py
 mmaction/utils/setup_env.py
-mmaction/utils/typing.py
+mmaction/utils/typing_utils.py
 mmaction/visualization/__init__.py
 mmaction/visualization/action_visualizer.py
 mmaction/visualization/video_backend.py
 mmaction2.egg-info/PKG-INFO
 mmaction2.egg-info/SOURCES.txt
 mmaction2.egg-info/dependency_links.txt
 mmaction2.egg-info/not-zip-safe
```

### Comparing `mmaction2-1.0.0rc3/mmaction2.egg-info/requires.txt` & `mmaction2-1.1.0/mmaction2.egg-info/requires.txt`

 * *Files 22% similar despite different names*

```diff
@@ -14,19 +14,19 @@
 numpy
 opencv-contrib-python
 Pillow
 scipy
 torch>=1.3
 av>=9.0
 future
-fvcore
 imgaug
 librosa
 lmdb
 moviepy
+openai-clip
 packaging
 pims
 PyTurboJPEG
 soundfile
 tensorboard
 wandb
 coverage
@@ -42,19 +42,19 @@
 [mim]
 mmcv<2.1.0,>=2.0.0rc0
 mmengine<1.0.0,>=0.5.0
 
 [optional]
 av>=9.0
 future
-fvcore
 imgaug
 librosa
 lmdb
 moviepy
+openai-clip
 packaging
 pims
 PyTurboJPEG
 soundfile
 tensorboard
 wandb
```

### Comparing `mmaction2-1.0.0rc3/setup.py` & `mmaction2-1.1.0/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -115,15 +115,15 @@
     elif 'sdist' in sys.argv or 'bdist_wheel' in sys.argv:
         # installed by `pip install .`
         # or create source distribution by `python setup.py sdist`
         mode = 'copy'
     else:
         return
 
-    filenames = ['tools', 'configs', 'model-index.yml']
+    filenames = ['tools', 'configs', 'model-index.yml', 'dataset-index.yml']
     repo_path = osp.dirname(__file__)
     mim_path = osp.join(repo_path, 'mmaction', '.mim')
     os.makedirs(mim_path, exist_ok=True)
 
     for filename in filenames:
         if osp.exists(filename):
             src_path = osp.join(repo_path, filename)
```

