# Comparing `tmp/mmpose-1.0.0rc1.tar.gz` & `tmp/mmpose-1.1.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/mmpose-1.0.0rc1.tar", last modified: Wed Mar 15 09:51:10 2023, max compression
+gzip compressed data, was "dist/mmpose-1.1.0.tar", last modified: Tue Jul  4 13:35:38 2023, max compression
```

## Comparing `mmpose-1.0.0rc1.tar` & `mmpose-1.1.0.tar`

### file list

```diff
@@ -1,740 +1,951 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/
--rw-r--r--   0 runner    (1001) docker     (123)      198 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)    26601 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    23305 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/
--rw-r--r--   0 runner    (1001) docker     (123)     6588 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/300w.py
--rw-r--r--   0 runner    (1001) docker     (123)     2227 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/aflw.py
--rw-r--r--   0 runner    (1001) docker     (123)     4400 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/aic.py
--rw-r--r--   0 runner    (1001) docker     (123)     5415 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/animalpose.py
--rw-r--r--   0 runner    (1001) docker     (123)     4508 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/ap10k.py
--rw-r--r--   0 runner    (1001) docker     (123)     4273 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/atrw.py
--rw-r--r--   0 runner    (1001) docker     (123)     4417 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/campus.py
--rw-r--r--   0 runner    (1001) docker     (123)     5252 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco.py
--rw-r--r--   0 runner    (1001) docker     (123)     6151 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_aic.py
--rw-r--r--   0 runner    (1001) docker     (123)    30735 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_wholebody.py
--rw-r--r--   0 runner    (1001) docker     (123)     7566 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_wholebody_face.py
--rw-r--r--   0 runner    (1001) docker     (123)     4998 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_wholebody_hand.py
--rw-r--r--   0 runner    (1001) docker     (123)     2936 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/cofw.py
--rw-r--r--   0 runner    (1001) docker     (123)     4341 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/crowdpose.py
--rw-r--r--   0 runner    (1001) docker     (123)     1951 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/deepfashion_full.py
--rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/deepfashion_lower.py
--rw-r--r--   0 runner    (1001) docker     (123)     1610 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/deepfashion_upper.py
--rw-r--r--   0 runner    (1001) docker     (123)     7150 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/fly.py
--rw-r--r--   0 runner    (1001) docker     (123)     4879 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/freihand2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     4589 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/h36m.py
--rw-r--r--   0 runner    (1001) docker     (123)    31008 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/halpe.py
--rw-r--r--   0 runner    (1001) docker     (123)     5586 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/horse10.py
--rw-r--r--   0 runner    (1001) docker     (123)     4773 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/interhand2d.py
--rw-r--r--   0 runner    (1001) docker     (123)    13089 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/interhand3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     4056 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/jhmdb.py
--rw-r--r--   0 runner    (1001) docker     (123)     7793 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/locust.py
--rw-r--r--   0 runner    (1001) docker     (123)     5364 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/macaque.py
--rw-r--r--   0 runner    (1001) docker     (123)     4684 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mhp.py
--rw-r--r--   0 runner    (1001) docker     (123)     4307 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mpi_inf_3dhp.py
--rw-r--r--   0 runner    (1001) docker     (123)     4610 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mpii.py
--rw-r--r--   0 runner    (1001) docker     (123)    10582 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mpii_trb.py
--rw-r--r--   0 runner    (1001) docker     (123)     5316 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/ochuman.py
--rw-r--r--   0 runner    (1001) docker     (123)     4776 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/onehand10k.py
--rw-r--r--   0 runner    (1001) docker     (123)     5000 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/panoptic_body3d.py
--rw-r--r--   0 runner    (1001) docker     (123)     4817 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/panoptic_hand2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     5116 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/posetrack18.py
--rw-r--r--   0 runner    (1001) docker     (123)     5364 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/rhd2d.py
--rw-r--r--   0 runner    (1001) docker     (123)     4416 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/shelf.py
--rw-r--r--   0 runner    (1001) docker     (123)     9472 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/wflw.py
--rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/zebra.py
--rw-r--r--   0 runner    (1001) docker     (123)     1196 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/default_runtime.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/
--rw-r--r--   0 runner    (1001) docker     (123)     7016 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/rtmpose-m_8xb64-210e_ap10k-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/
--rw-r--r--   0 runner    (1001) docker     (123)     4175 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w32_8xb64-210e_animalpose-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4175 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w48_8xb64-210e_animalpose-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3224 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res101_8xb64-210e_animalpose-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3224 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res152_8xb32-210e_animalpose-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3222 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res50_8xb64-210e_animalpose-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/
--rw-r--r--   0 runner    (1001) docker     (123)     6590 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/cspnext-m_udp_8xb64-210e_ap10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     1399 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/resnet_ap10k.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4673 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w32_8xb64-210e_ap10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4673 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w48_8xb64-210e_ap10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3722 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res101_8xb64-210e_ap10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3720 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res50_8xb64-210e_ap10k-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/
--rw-r--r--   0 runner    (1001) docker     (123)     3310 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res101_8xb64-210e_locust-160x160.py
--rw-r--r--   0 runner    (1001) docker     (123)     3310 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res152_8xb32-210e_locust-160x160.py
--rw-r--r--   0 runner    (1001) docker     (123)     3308 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res50_8xb64-210e_locust-160x160.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/
--rw-r--r--   0 runner    (1001) docker     (123)     3305 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res101_8xb64-210e_zebra-160x160.py
--rw-r--r--   0 runner    (1001) docker     (123)     3305 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res152_8xb32-210e_zebra-160x160.py
--rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res50_8xb64-210e_zebra-160x160.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/coco/
--rw-r--r--   0 runner    (1001) docker     (123)     4457 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/coco/ae_hrnet-w32_8xb24-300e_coco-512x512.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/cid/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/cid/coco/
--rw-r--r--   0 runner    (1001) docker     (123)     4786 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w32_8xb20-140e_coco-512x512.py
--rw-r--r--   0 runner    (1001) docker     (123)     4786 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w48_8xb20-140e_coco-512x512.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/
--rw-r--r--   0 runner    (1001) docker     (123)     5365 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w32_8xb10-140e_coco-512x512.py
--rw-r--r--   0 runner    (1001) docker     (123)     5397 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w48_8xb10-140e_coco-640x640.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/
--rw-r--r--   0 runner    (1001) docker     (123)     5408 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w32_8xb10-300e_crowdpose-512x512.py
--rw-r--r--   0 runner    (1001) docker     (123)     5439 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w48_8xb5-300e_crowdpose-640x640.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/
--rw-r--r--   0 runner    (1001) docker     (123)     3750 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_8xb64-210e_coco-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3787 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_debias-8xb64-210e_coco-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3748 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_dsnt-8xb64-210e_coco-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/
--rw-r--r--   0 runner    (1001) docker     (123)     7705 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7705 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     6677 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7712 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7712 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     6680 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7710 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6677 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7772 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6744 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_coco-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/
--rw-r--r--   0 runner    (1001) docker     (123)     6753 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/rtmpose-m_8xb64-210e_crowdpose-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/
--rw-r--r--   0 runner    (1001) docker     (123)     6481 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/rtmpose-m_8xb64-210e_mpii-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/
--rw-r--r--   0 runner    (1001) docker     (123)      852 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/mobilenetv2_coco.yml
--rw-r--r--   0 runner    (1001) docker     (123)     1313 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/resnet_coco.yml
--rw-r--r--   0 runner    (1001) docker     (123)     3570 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3470 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb32-140e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3383 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3487 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)      818 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/vipnas_coco.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/mpii/
--rw-r--r--   0 runner    (1001) docker     (123)     3367 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/mpii/simcc_res50_wo-deconv-8xb64-210e_mpii-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/
--rw-r--r--   0 runner    (1001) docker     (123)     4368 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_hrnet-w32_8xb64-210e_aic-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3417 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_res101_8xb64-210e_aic-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/
--rw-r--r--   0 runner    (1001) docker     (123)     7654 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6249 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7657 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6252 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7645 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     7663 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_aic-coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6259 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     1328 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hourglass_coco.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2405 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_coco.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2333 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/litehrnet_coco.yml
--rw-r--r--   0 runner    (1001) docker     (123)     2239 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/mspn_coco.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4250 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xmspn50_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4214 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xrsn50_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4254 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xmspn50_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4218 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xrsn50_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4254 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_4xmspn50_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4392 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4345 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4394 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge-simple_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4347 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4394 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4347 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4531 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small-simple_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4484 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3277 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_alexnet_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3445 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_cpm_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3445 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_cpm_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3355 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3355 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-384x384.py
--rw-r--r--   0 runner    (1001) docker     (123)     5140 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-base_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     5140 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-base_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     5141 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     5141 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4326 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4326 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     5743 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-combine.py
--rw-r--r--   0 runner    (1001) docker     (123)     5237 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-merge.py
--rw-r--r--   0 runner    (1001) docker     (123)     4770 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_coarsedropout-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)      153 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_fp16-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4692 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_gridmask-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4477 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_photometric-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4354 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4354 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4449 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-regress-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4326 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4326 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4354 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4354 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4001 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4001 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4001 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4001 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3439 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3439 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4225 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mspn50_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3579 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvt-s_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3604 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvtv2-b2_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_dark-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_dark-8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3406 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_dark-8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3431 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_dark-8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3373 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3373 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3404 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3404 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)      149 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_fp16-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest101_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest101_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb16-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb16-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3369 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3369 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest50_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3376 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3376 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3376 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d152_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3376 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d152_8xb48-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d50_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3390 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb48-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4194 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn18_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4189 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn50_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3466 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet101_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3466 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet101_8xb48-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3464 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet50_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3464 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3297 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3297 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb48-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3372 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3372 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3377 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv1_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3377 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv1_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3386 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv2_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3386 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv2_8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3898 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-b-p4-w7_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3900 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-b-p4-w7_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4190 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4191 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3891 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-t-p4-w7_8xb32-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3397 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vgg16-bn_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-mbv3_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3346 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-res50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     5187 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/
--rw-r--r--   0 runner    (1001) docker     (123)     6282 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/cspnext-m_udp_8xb64-210e_crowpose-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_hrnet-w32_8xb64-210e_crowdpose-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-320x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res152_8xb64-210e_crowdpose-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3422 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res50_8xb64-210e_crowdpose-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/
--rw-r--r--   0 runner    (1001) docker     (123)     3311 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub1-368x368.py
--rw-r--r--   0 runner    (1001) docker     (123)     3311 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub2-368x368.py
--rw-r--r--   0 runner    (1001) docker     (123)     3311 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub3-368x368.py
--rw-r--r--   0 runner    (1001) docker     (123)     3336 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub1-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3336 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub2-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3336 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub3-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3259 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub1-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3259 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub2-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3259 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub3-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/
--rw-r--r--   0 runner    (1001) docker     (123)     6013 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cspnext-m_udp_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3345 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_cpm_8xb64-210e_mpii-368x368.py
--rw-r--r--   0 runner    (1001) docker     (123)     3177 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb32-210e_mpii-384x384.py
--rw-r--r--   0 runner    (1001) docker     (123)     3177 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4148 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4179 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_dark-8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4148 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4179 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3831 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-18_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3831 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-30_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3232 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_mobilenetv2_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3197 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res101_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3197 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res152_8xb32-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3195 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res50_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d101_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d152_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d50_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3212 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnext152_8xb32-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3288 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet101_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3286 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet50_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet101_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3119 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet152_8xb32-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3194 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet50_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3199 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv1_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3208 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv2_8xb64-210e_mpii-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/
--rw-r--r--   0 runner    (1001) docker     (123)     4518 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4518 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4518 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4518 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3470 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_res50_8xb64-20e_posetrack18-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/
--rw-r--r--   0 runner    (1001) docker     (123)     3598 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_mobilenetv2_rle-pretrained-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3376 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_rle-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3376 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3389 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-8xb64-210e_coco-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3553 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-pretrained-8xb64-210e_coco-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/
--rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res101_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res152_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_8xb64-210e_mpii-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3184 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_rle-8xb64-210e_mpii-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/rtmpose/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/
--rw-r--r--   0 runner    (1001) docker     (123)     6552 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/rtmpose-m_8xb32-60e_coco-wholebody-face-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/
--rw-r--r--   0 runner    (1001) docker     (123)     6526 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/rtmpose-m_8xb64-60e_wflw-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/
--rw-r--r--   0 runner    (1001) docker     (123)     4435 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/td-hm_hrnetv2-w18_8xb64-60e_300w-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/
--rw-r--r--   0 runner    (1001) docker     (123)     4429 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_8xb64-60e_aflw-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4460 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_dark-8xb64-60e_aflw-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/
--rw-r--r--   0 runner    (1001) docker     (123)     3253 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hourglass52_8xb32-60e_coco-wholebody-face-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4405 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_8xb32-60e_coco-wholebody-face-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4436 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_dark-8xb32-60e_coco-wholebody-face-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3302 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_mobilenetv2_8xb32-60e_coco-wholebody-face-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3265 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_res50_8xb32-60e_coco-wholebody-face-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3356 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_scnet50_8xb32-60e_coco-wholebody-face-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/
--rw-r--r--   0 runner    (1001) docker     (123)     4400 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/td-hm_hrnetv2-w18_8xb64-60e_cofw-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/
--rw-r--r--   0 runner    (1001) docker     (123)      862 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_wflw.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4416 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_8xb64-60e_wflw-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4417 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_awing-8xb64-60e_wflw-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4447 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_dark-8xb64-60e_wflw-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/
--rw-r--r--   0 runner    (1001) docker     (123)     6573 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose-m_8xb32-210e_coco-wholebody-hand-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/
--rw-r--r--   0 runner    (1001) docker     (123)     3263 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hourglass52_8xb32-210e_coco-wholebody-hand-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4404 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_8xb32-210e_coco-wholebody-hand-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4435 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_dark-8xb32-210e_coco-wholebody-hand-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3828 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_litehrnet-w18_8xb32-210e_coco-wholebody-hand-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_mobilenetv2_8xb32-210e_coco-wholebody-hand-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3264 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_res50_8xb32-210e_coco-wholebody-hand-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3355 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_scnet50_8xb32-210e_coco-wholebody-hand-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/
--rw-r--r--   0 runner    (1001) docker     (123)     3710 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/
--rw-r--r--   0 runner    (1001) docker     (123)      877 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/resnet_onehand10k.yml
--rw-r--r--   0 runner    (1001) docker     (123)     4405 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_8xb64-210e_onehand10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4436 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_dark-8xb64-210e_onehand10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4405 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_udp-8xb64-210e_onehand10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3315 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_mobilenetv2_8xb64-210e_onehand10k-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3278 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_res50_8xb32-210e_onehand10k-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/
--rw-r--r--   0 runner    (1001) docker     (123)     4379 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_8xb64-210e_rhd2d-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4410 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_dark-8xb64-210e_rhd2d-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     4379 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_udp-8xb64-210e_rhd2d-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3289 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_mobilenetv2_8xb64-210e_rhd2d-256x256.py
--rw-r--r--   0 runner    (1001) docker     (123)     3252 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_res50_8xb64-210e_rhd2d-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/
--rw-r--r--   0 runner    (1001) docker     (123)     3258 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/td-reg_res50_8xb64-210e_onehand10k-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/
--rw-r--r--   0 runner    (1001) docker     (123)     3232 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/td-reg_res50_8xb64-210e_rhd2d-256x256.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/
--rw-r--r--   0 runner    (1001) docker     (123)     6596 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     6596 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6599 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-m_8xb64-270e_coco-wholebody-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/
--rw-r--r--   0 runner    (1001) docker     (123)     6127 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-l_udp_8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     6130 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-m_udp_8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4344 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4344 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_dark-8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4344 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     4344 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     4375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3393 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3393 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3393 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3393 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3391 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-384x288.py
--rw-r--r--   0 runner    (1001) docker     (123)     3447 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3478 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_dark-8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3387 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_8xb64-210e_coco-wholebody-256x192.py
--rw-r--r--   0 runner    (1001) docker     (123)     3418 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_dark-8xb64-210e_coco-wholebody-256x192.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/
--rw-r--r--   0 runner    (1001) docker     (123)     5619 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/bottomup_demo.py
--rw-r--r--   0 runner    (1001) docker     (123)     2142 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/image_demo.py
--rw-r--r--   0 runner    (1001) docker     (123)     3026 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/inferencer_demo.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/
--rw-r--r--   0 runner    (1001) docker     (123)     9234 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_1class.py
--rw-r--r--   0 runner    (1001) docker     (123)     8608 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)     5869 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_1class.py
--rw-r--r--   0 runner    (1001) docker     (123)     6482 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)     8524 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/mask_rcnn_r50_fpn_2x_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)     4435 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/ssdlite_mobilenetv2-scratch_8xb24-600e_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)     4480 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/yolov3_d53_320_273e_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)    10552 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/yolox-s_8xb8-300e_coco-face.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmtracking_cfg/
--rw-r--r--   0 runner    (1001) docker     (123)    11566 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmtracking_cfg/deepsort_faster-rcnn_fpn_4e_mot17-private-half.py
--rw-r--r--   0 runner    (1001) docker     (123)    11604 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/mmtracking_cfg/tracktor_faster-rcnn_r50_fpn_4e_mot17-private.py
--rw-r--r--   0 runner    (1001) docker     (123)     7753 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/topdown_demo_with_mmdet.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/webcam_cfg/
--rw-r--r--   0 runner    (1001) docker     (123)     5669 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/webcam_cfg/pose_estimation.py
--rw-r--r--   0 runner    (1001) docker     (123)      634 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/webcam_cfg/test_camera.py
--rw-r--r--   0 runner    (1001) docker     (123)     1904 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/demo/webcam_demo.py
--rw-r--r--   0 runner    (1001) docker     (123)      708 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/model-index.yml
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/analysis_tools/
--rw-r--r--   0 runner    (1001) docker     (123)     5567 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/analysis_tools/analyze_logs.py
--rw-r--r--   0 runner    (1001) docker     (123)     3555 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/analysis_tools/get_flops.py
--rw-r--r--   0 runner    (1001) docker     (123)      643 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/analysis_tools/print_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/
--rw-r--r--   0 runner    (1001) docker     (123)     4794 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/h36m_to_coco.py
--rw-r--r--   0 runner    (1001) docker     (123)     1649 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/mat2json.py
--rw-r--r--   0 runner    (1001) docker     (123)    13482 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_animalpose_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     2817 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_cofw_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     6251 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_deepposekit_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     5595 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_macaquepose_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    15559 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/preprocess_h36m.py
--rw-r--r--   0 runner    (1001) docker     (123)    12309 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/preprocess_mpi_inf_3dhp.py
--rw-r--r--   0 runner    (1001) docker     (123)      527 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dist_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      490 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/dist_train.sh
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/misc/
--rw-r--r--   0 runner    (1001) docker     (123)     5799 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/misc/browse_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     4658 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/misc/keypoints2coco_without_mmdet.py
--rw-r--r--   0 runner    (1001) docker     (123)     2103 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/misc/publish_model.py
--rw-r--r--   0 runner    (1001) docker     (123)      614 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/slurm_test.sh
--rw-r--r--   0 runner    (1001) docker     (123)      622 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/slurm_train.sh
--rw-r--r--   0 runner    (1001) docker     (123)     4430 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/test.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/torchserve/
--rw-r--r--   0 runner    (1001) docker     (123)     4827 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/torchserve/mmpose2torchserve.py
--rw-r--r--   0 runner    (1001) docker     (123)     2709 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/torchserve/mmpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2688 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/torchserve/test_torchserver.py
--rw-r--r--   0 runner    (1001) docker     (123)     5280 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/.mim/tools/train.py
--rw-r--r--   0 runner    (1001) docker     (123)     1031 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/
--rw-r--r--   0 runner    (1001) docker     (123)      300 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8046 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/inference.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/
--rw-r--r--   0 runner    (1001) docker     (123)      196 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17449 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/base_mmpose_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (123)    11198 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/mmpose_inferencer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10005 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/pose2d_inferencer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1694 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/inferencers/utils/default_det_models.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/
--rw-r--r--   0 runner    (1001) docker     (123)      122 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/
--rw-r--r--   0 runner    (1001) docker     (123)      704 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2348 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/base_visualizer_node.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/helper_nodes/
--rw-r--r--   0 runner    (1001) docker     (123)      244 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/helper_nodes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5909 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/helper_nodes/monitor_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     5401 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/helper_nodes/object_assigner_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     4220 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/helper_nodes/recorder_node.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/model_nodes/
--rw-r--r--   0 runner    (1001) docker     (123)      202 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/model_nodes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5395 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/model_nodes/detector_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     5350 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/model_nodes/pose_estimator_node.py
--rw-r--r--   0 runner    (1001) docker     (123)    15620 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/node.py
--rw-r--r--   0 runner    (1001) docker     (123)      113 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/visualizer_nodes/
--rw-r--r--   0 runner    (1001) docker     (123)      367 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/visualizer_nodes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4636 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/visualizer_nodes/bigeye_effect_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     5023 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/visualizer_nodes/notice_board_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     6135 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/visualizer_nodes/object_visualizer_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     5755 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/nodes/visualizer_nodes/sunglasses_effect_node.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/
--rw-r--r--   0 runner    (1001) docker     (123)     1021 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7022 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/buffer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5178 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/event.py
--rw-r--r--   0 runner    (1001) docker     (123)     1005 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/image_capture.py
--rw-r--r--   0 runner    (1001) docker     (123)     6484 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/message.py
--rw-r--r--   0 runner    (1001) docker     (123)    11425 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (123)     6554 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/utils/pose.py
--rw-r--r--   0 runner    (1001) docker     (123)    12158 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/apis/webcam/webcam_executor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/codecs/
--rw-r--r--   0 runner    (1001) docker     (123)      617 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    20970 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/associative_embedding.py
--rw-r--r--   0 runner    (1001) docker     (123)     2505 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/base.py
--rw-r--r--   0 runner    (1001) docker     (123)    10341 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/decoupled_heatmap.py
--rw-r--r--   0 runner    (1001) docker     (123)     4279 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/integral_regression_label.py
--rw-r--r--   0 runner    (1001) docker     (123)     4815 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/megvii_heatmap.py
--rw-r--r--   0 runner    (1001) docker     (123)     5625 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/msra_heatmap.py
--rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/regression_label.py
--rw-r--r--   0 runner    (1001) docker     (123)    11162 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/simcc_label.py
--rw-r--r--   0 runner    (1001) docker     (123)    12184 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/spr.py
--rw-r--r--   0 runner    (1001) docker     (123)     7726 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/udp_heatmap.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/codecs/utils/
--rw-r--r--   0 runner    (1001) docker     (123)     1261 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7031 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/utils/gaussian_heatmap.py
--rw-r--r--   0 runner    (1001) docker     (123)     4017 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/utils/instance_property.py
--rw-r--r--   0 runner    (1001) docker     (123)     5352 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/utils/offset_heatmap.py
--rw-r--r--   0 runner    (1001) docker     (123)     6184 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/utils/post_processing.py
--rw-r--r--   0 runner    (1001) docker     (123)     7221 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/codecs/utils/refinement.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/
--rw-r--r--   0 runner    (1001) docker     (123)      306 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3039 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3685 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/dataset_wrappers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/
--rw-r--r--   0 runner    (1001) docker     (123)      338 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/
--rw-r--r--   0 runner    (1001) docker     (123)      539 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3323 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/animalpose_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3249 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/ap10k_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3237 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/atrw_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/fly_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3414 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/horse10_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     5368 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/locust_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3358 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/macaque_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     4768 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/zebra_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/base/
--rw-r--r--   0 runner    (1001) docker     (123)      142 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/base/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16987 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/base/base_coco_style_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/
--rw-r--r--   0 runner    (1001) docker     (123)      693 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3211 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/aic_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3249 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/coco_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3229 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/crowdpose_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     5484 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/jhmdb_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3306 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/mhp_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     8675 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/mpii_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     6616 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/mpii_trb_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3611 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/ochuman_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3302 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/posetrack18_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)    16527 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/body/posetrack18_video_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/face/
--rw-r--r--   0 runner    (1001) docker     (123)      389 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/face/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5343 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/face/aflw_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     4951 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/face/coco_wholebody_face_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     2879 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/face/cofw_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     4889 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/face/face_300w_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     4867 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/face/wflw_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/fashion/
--rw-r--r--   0 runner    (1001) docker     (123)      134 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/fashion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5843 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/fashion/deepfashion_dataset.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/
--rw-r--r--   0 runner    (1001) docker     (123)      440 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6108 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/coco_wholebody_hand_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     5140 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/freihand_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3470 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/onehand10k_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     5435 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/panoptic_hand2d_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     3421 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/rhd2d_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     7649 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/wholebody/
--rw-r--r--   0 runner    (1001) docker     (123)      197 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/wholebody/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5421 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/wholebody/coco_wholebody_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     2935 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/datasets/wholebody/halpe_dataset.py
--rw-r--r--   0 runner    (1001) docker     (123)     4552 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/samplers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/
--rw-r--r--   0 runner    (1001) docker     (123)      895 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    18636 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/bottomup_transforms.py
--rw-r--r--   0 runner    (1001) docker     (123)    36948 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/common_transforms.py
--rw-r--r--   0 runner    (1001) docker     (123)     1761 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/converting.py
--rw-r--r--   0 runner    (1001) docker     (123)     8000 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/formatting.py
--rw-r--r--   0 runner    (1001) docker     (123)     2114 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/loading.py
--rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/datasets/transforms/topdown_transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/engine/
--rw-r--r--   0 runner    (1001) docker     (123)       89 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/engine/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/engine/hooks/
--rw-r--r--   0 runner    (1001) docker     (123)      194 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/engine/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/engine/hooks/ema_hook.py
--rw-r--r--   0 runner    (1001) docker     (123)     7024 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/engine/hooks/visualization_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/engine/optim_wrappers/
--rw-r--r--   0 runner    (1001) docker     (123)      170 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/engine/optim_wrappers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2956 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/engine/optim_wrappers/layer_decay_optim_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/evaluation/
--rw-r--r--   0 runner    (1001) docker     (123)      135 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/evaluation/functional/
--rw-r--r--   0 runner    (1001) docker     (123)      558 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/functional/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11832 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/functional/keypoint_eval.py
--rw-r--r--   0 runner    (1001) docker     (123)    11122 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/functional/nms.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/evaluation/metrics/
--rw-r--r--   0 runner    (1001) docker     (123)      569 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/metrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    24250 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/metrics/coco_metric.py
--rw-r--r--   0 runner    (1001) docker     (123)    12760 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/metrics/coco_wholebody_metric.py
--rw-r--r--   0 runner    (1001) docker     (123)    39347 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/metrics/keypoint_2d_metrics.py
--rw-r--r--   0 runner    (1001) docker     (123)     9402 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/metrics/keypoint_partition_metric.py
--rw-r--r--   0 runner    (1001) docker     (123)     8875 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/evaluation/metrics/posetrack18_metric.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/
--rw-r--r--   0 runner    (1001) docker     (123)      602 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/backbones/
--rw-r--r--   0 runner    (1001) docker     (123)     1365 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/alexnet.py
--rw-r--r--   0 runner    (1001) docker     (123)      848 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/base_backbone.py
--rw-r--r--   0 runner    (1001) docker     (123)     6174 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/cpm.py
--rw-r--r--   0 runner    (1001) docker     (123)     6746 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/hourglass.py
--rw-r--r--   0 runner    (1001) docker     (123)     6848 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/hourglass_ae.py
--rw-r--r--   0 runner    (1001) docker     (123)    30414 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/hrformer.py
--rw-r--r--   0 runner    (1001) docker     (123)    22592 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/hrnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    36829 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/litehrnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    10150 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/mobilenet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)     7175 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/mobilenet_v3.py
--rw-r--r--   0 runner    (1001) docker     (123)    19753 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/mspn.py
--rw-r--r--   0 runner    (1001) docker     (123)    21991 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/pvt.py
--rw-r--r--   0 runner    (1001) docker     (123)    12565 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/regnet.py
--rw-r--r--   0 runner    (1001) docker     (123)    12745 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/resnest.py
--rw-r--r--   0 runner    (1001) docker     (123)    25333 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/resnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     6986 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/resnext.py
--rw-r--r--   0 runner    (1001) docker     (123)    22957 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/rsn.py
--rw-r--r--   0 runner    (1001) docker     (123)     8248 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/scnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     4929 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/seresnet.py
--rw-r--r--   0 runner    (1001) docker     (123)     7469 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/seresnext.py
--rw-r--r--   0 runner    (1001) docker     (123)    12144 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/shufflenet_v1.py
--rw-r--r--   0 runner    (1001) docker     (123)    10818 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/shufflenet_v2.py
--rw-r--r--   0 runner    (1001) docker     (123)    29134 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/swin.py
--rw-r--r--   0 runner    (1001) docker     (123)    10566 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/tcn.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      392 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      914 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/channel_shuffle.py
--rw-r--r--   0 runner    (1001) docker     (123)     1984 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/ckpt_convert.py
--rw-r--r--   0 runner    (1001) docker     (123)     4196 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/inverted_residual.py
--rw-r--r--   0 runner    (1001) docker     (123)     1056 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/make_divisible.py
--rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/se_layer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2947 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     8824 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/v2v_net.py
--rw-r--r--   0 runner    (1001) docker     (123)     7461 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/vgg.py
--rw-r--r--   0 runner    (1001) docker     (123)     6184 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/vipnas_mbv3.py
--rw-r--r--   0 runner    (1001) docker     (123)    22076 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/backbones/vipnas_resnet.py
--rw-r--r--   0 runner    (1001) docker     (123)      836 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/data_preprocessors/
--rw-r--r--   0 runner    (1001) docker     (123)      136 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/data_preprocessors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      265 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/data_preprocessors/data_preprocessor.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/heads/
--rw-r--r--   0 runner    (1001) docker     (123)      655 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5597 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/base_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/heads/coord_cls_heads/
--rw-r--r--   0 runner    (1001) docker     (123)      154 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/coord_cls_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11539 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/coord_cls_heads/rtmcc_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    16090 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/coord_cls_heads/simcc_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/
--rw-r--r--   0 runner    (1001) docker     (123)      373 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11155 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/ae_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    31170 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/cid_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    12403 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/cpm_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    16983 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/heatmap_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    16554 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/mix_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    16505 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/mspn_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     9341 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/vipnas_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/heads/hybrid_heads/
--rw-r--r--   0 runner    (1001) docker     (123)      111 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/hybrid_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    23744 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/hybrid_heads/dekr_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/
--rw-r--r--   0 runner    (1001) docker     (123)      313 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7144 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/dsnt_head.py
--rw-r--r--   0 runner    (1001) docker     (123)    15039 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/integral_regression_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     6579 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/regression_head.py
--rw-r--r--   0 runner    (1001) docker     (123)     8151 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/rle_head.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/losses/
--rw-r--r--   0 runner    (1001) docker     (123)      914 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/losses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4167 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/losses/ae_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     7458 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/losses/classification_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)    16723 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/losses/heatmap_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     2644 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/losses/loss_wrappers.py
--rw-r--r--   0 runner    (1001) docker     (123)    21225 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/losses/regression_loss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/necks/
--rw-r--r--   0 runner    (1001) docker     (123)      217 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/necks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8775 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/necks/fpn.py
--rw-r--r--   0 runner    (1001) docker     (123)     1267 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/necks/gap_neck.py
--rw-r--r--   0 runner    (1001) docker     (123)    12528 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/necks/posewarper_neck.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/pose_estimators/
--rw-r--r--   0 runner    (1001) docker     (123)      195 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/pose_estimators/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7560 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/pose_estimators/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     6959 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/pose_estimators/bottomup.py
--rw-r--r--   0 runner    (1001) docker     (123)     7626 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/pose_estimators/topdown.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/models/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      293 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/ckpt_convert.py
--rw-r--r--   0 runner    (1001) docker     (123)     2098 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/geometry.py
--rw-r--r--   0 runner    (1001) docker     (123)     1174 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/ops.py
--rw-r--r--   0 runner    (1001) docker     (123)     2657 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/realnvp.py
--rw-r--r--   0 runner    (1001) docker     (123)     2706 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/regularizations.py
--rw-r--r--   0 runner    (1001) docker     (123)     9365 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/rtmcc_block.py
--rw-r--r--   0 runner    (1001) docker     (123)    13244 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/transformer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6190 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/models/utils/tta.py
--rw-r--r--   0 runner    (1001) docker     (123)     5375 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/structures/
--rw-r--r--   0 runner    (1001) docker     (123)      753 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/structures/bbox/
--rw-r--r--   0 runner    (1001) docker     (123)      441 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/bbox/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11589 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/bbox/transforms.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/structures/keypoint/
--rw-r--r--   0 runner    (1001) docker     (123)      118 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/keypoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2395 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/keypoint/transforms.py
--rw-r--r--   0 runner    (1001) docker     (123)    10214 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/multilevel_pixel_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/pose_data_sample.py
--rw-r--r--   0 runner    (1001) docker     (123)     4606 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/structures/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/testing/
--rw-r--r--   0 runner    (1001) docker     (123)      304 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8666 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/testing/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/utils/
--rw-r--r--   0 runner    (1001) docker     (123)      499 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10679 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/camera.py
--rw-r--r--   0 runner    (1001) docker     (123)      428 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/collect_env.py
--rw-r--r--   0 runner    (1001) docker     (123)      772 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/config_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1836 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/hooks.py
--rw-r--r--   0 runner    (1001) docker     (123)      967 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/logger.py
--rw-r--r--   0 runner    (1001) docker     (123)     4079 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/setup_env.py
--rw-r--r--   0 runner    (1001) docker     (123)     2211 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/tensor_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3743 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/timer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1167 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/utils/typing.py
--rw-r--r--   0 runner    (1001) docker     (123)      983 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/version.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/visualization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    22648 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/visualization/local_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5370 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/mmpose/visualization/simcc_vis.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    26601 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    43717 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (123)      508 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)        7 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/mmpose.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/requirements/
--rw-r--r--   0 runner    (1001) docker     (123)       56 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/albu.txt
--rw-r--r--   0 runner    (1001) docker     (123)       66 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/build.txt
--rw-r--r--   0 runner    (1001) docker     (123)      181 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/docs.txt
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/mminstall.txt
--rw-r--r--   0 runner    (1001) docker     (123)        9 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/optional.txt
--rw-r--r--   0 runner    (1001) docker     (123)       69 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/poseval.txt
--rw-r--r--   0 runner    (1001) docker     (123)      103 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/readthedocs.txt
--rw-r--r--   0 runner    (1001) docker     (123)      101 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/runtime.txt
--rw-r--r--   0 runner    (1001) docker     (123)       99 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/requirements/tests.txt
--rw-r--r--   0 runner    (1001) docker     (123)      678 2023-03-15 09:51:10.000000 mmpose-1.0.0rc1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     6911 2023-03-15 09:51:07.000000 mmpose-1.0.0rc1/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/
+-rw-r--r--   0 runner    (1001) docker     (123)      236 2023-07-04 13:35:29.000000 mmpose-1.1.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)    30904 2023-07-04 13:35:38.000000 mmpose-1.1.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    27139 2023-07-04 13:35:29.000000 mmpose-1.1.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)     6588 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/300w.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2227 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/aflw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4400 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/aic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7327 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/ak.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5415 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/animalpose.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4508 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/ap10k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4273 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/atrw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4417 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/campus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5252 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6151 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_aic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4866 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_openpose.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30735 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_wholebody.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7566 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_wholebody_face.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4998 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_wholebody_hand.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2936 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/cofw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4341 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/crowdpose.py
+-rw-r--r--   0 runner    (1001) docker     (123)    75480 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/deepfashion2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1951 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/deepfashion_full.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/deepfashion_lower.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1610 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/deepfashion_upper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7150 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/fly.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4879 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/freihand2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4589 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)    31008 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/halpe.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7740 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/halpe26.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5586 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/horse10.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/humanart.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6100 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/humanart_aic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4773 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/interhand2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13089 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/interhand3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4056 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/jhmdb.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16750 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/lapa.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7793 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/locust.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5364 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/macaque.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4684 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mhp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4307 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mpi_inf_3dhp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4610 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mpii.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10582 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mpii_trb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5316 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/ochuman.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4776 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/onehand10k.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5000 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/panoptic_body3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4817 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/panoptic_hand2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5116 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/posetrack18.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5364 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/rhd2d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4416 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/shelf.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9472 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/wflw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/zebra.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1278 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/_base_/default_runtime.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/
+-rw-r--r--   0 runner    (1001) docker     (123)     6980 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/rtmpose-m_8xb64-210e_ap10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/rtmpose_ap10k.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/
+-rw-r--r--   0 runner    (1001) docker     (123)     3944 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/hrnet_animalkingdom.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4050 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P1-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4050 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P2-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4070 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_amphibian-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4060 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_bird-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4060 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_fish-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4064 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_mammal-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4066 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_reptile-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/
+-rw-r--r--   0 runner    (1001) docker     (123)     1104 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/hrnet_animalpose.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1676 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/resnet_animalpose.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4083 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w32_8xb64-210e_animalpose-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4083 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w48_8xb64-210e_animalpose-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3132 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res101_8xb64-210e_animalpose-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3132 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res152_8xb32-210e_animalpose-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3130 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res50_8xb64-210e_animalpose-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/
+-rw-r--r--   0 runner    (1001) docker     (123)     6227 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/cspnext-m_udp_8xb64-210e_ap10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      565 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/cspnext_udp_ap10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/hrnet_ap10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1079 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/resnet_ap10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4581 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w32_8xb64-210e_ap10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4581 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w48_8xb64-210e_ap10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3630 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res101_8xb64-210e_ap10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3628 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res50_8xb64-210e_ap10k-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/
+-rw-r--r--   0 runner    (1001) docker     (123)     1511 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/resnet_locust.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3218 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res101_8xb64-210e_locust-160x160.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3218 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res152_8xb32-210e_locust-160x160.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3216 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res50_8xb64-210e_locust-160x160.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/
+-rw-r--r--   0 runner    (1001) docker     (123)     1565 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/resnet_zebra.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3213 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res101_8xb64-210e_zebra-160x160.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3213 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res152_8xb32-210e_zebra-160x160.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3211 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res50_8xb64-210e_zebra-160x160.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)     4411 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/coco/ae_hrnet-w32_8xb24-300e_coco-512x512.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/cid/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/cid/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)     4683 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w32_8xb20-140e_coco-512x512.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4683 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w48_8xb20-140e_coco-512x512.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1419 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/cid/coco/hrnet_coco.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)     5262 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w32_8xb10-140e_coco-512x512.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5294 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w48_8xb10-140e_coco-640x640.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1318 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/hrnet_coco.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/
+-rw-r--r--   0 runner    (1001) docker     (123)     5305 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w32_8xb10-300e_crowdpose-512x512.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5336 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w48_8xb5-300e_crowdpose-640x640.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1183 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/hrnet_crowdpose.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)     3639 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_8xb64-210e_coco-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3676 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_debias-8xb64-210e_coco-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3637 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_dsnt-8xb64-210e_coco-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      922 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/resnet_debias_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      829 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/resnet_dsnt_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      771 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/resnet_ipr_coco.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/
+-rw-r--r--   0 runner    (1001) docker     (123)    13528 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb256-420e_body8-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13527 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb256-420e_body8-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13682 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb512-700e_body8-halpe26-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13683 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb512-700e_body8-halpe26-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13534 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb256-420e_body8-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13533 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb256-420e_body8-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13567 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb512-700e_body8-halpe26-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13890 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb512-700e_body8-halpe26-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13686 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-s_8xb1024-700e_body8-halpe26-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13530 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-s_8xb256-420e_body8-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13713 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-t_8xb1024-700e_body8-halpe26-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13548 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-t_8xb256-420e_body8-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13676 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-x_8xb256-700e_body8-halpe26-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3157 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose_body8-coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3801 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose_body8-halpe26.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)     7715 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7714 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6687 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7722 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7721 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6690 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7720 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6687 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7782 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6754 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5556 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose_coco.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/
+-rw-r--r--   0 runner    (1001) docker     (123)     6717 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/rtmpose-m_8xb64-210e_crowdpose-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)      579 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/rtmpose_crowdpose.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/
+-rw-r--r--   0 runner    (1001) docker     (123)     6656 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-l_8xb256-420e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6659 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-m_8xb256-420e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6656 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-s_8xb256-420e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6723 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-t_8xb256-420e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3646 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose_humanart.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/
+-rw-r--r--   0 runner    (1001) docker     (123)     6491 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/rtmpose-m_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      476 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/rtmpose_mpii.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)      598 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/mobilenetv2_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1314 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/resnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3498 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3397 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb32-140e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3311 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3415 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)      564 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/vipnas_coco.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/mpii/
+-rw-r--r--   0 runner    (1001) docker     (123)     3295 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/mpii/simcc_res50_wo-deconv-8xb64-210e_mpii-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/
+-rw-r--r--   0 runner    (1001) docker     (123)      529 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/hrnet_aic.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      556 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/resnet_aic.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4276 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_hrnet-w32_8xb64-210e_aic-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3325 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_res101_8xb64-210e_aic-256x192.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)      547 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/alexnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1345 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cpm_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     7623 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6218 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7626 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6221 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7614 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6210 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7632 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_aic-coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6228 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4301 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext_udp_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1329 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hourglass_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2422 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrformer_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_augmentation_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4331 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2659 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_dark_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3209 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_udp_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2334 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/litehrnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1173 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/mobilenetv2_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2240 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/mspn_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1052 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/pvt_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4177 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnest_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4204 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3373 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnet_dark_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3427 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnetv1d_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3141 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnext_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2427 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/rsn_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2052 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/scnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3146 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/seresnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1180 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/shufflenetv1_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1179 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/shufflenetv2_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3167 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/swin_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4158 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xmspn50_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4122 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xrsn50_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4162 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xmspn50_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4126 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xrsn50_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4162 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_4xmspn50_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4441 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4317 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4443 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge-simple_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4319 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4443 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4319 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4580 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small-simple_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4456 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3185 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_alexnet_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3348 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_cpm_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3348 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_cpm_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3263 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3263 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-384x384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5048 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-base_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5048 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-base_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5049 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5049 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4234 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4234 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5651 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-combine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5145 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-merge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4678 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_coarsedropout-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)      153 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_fp16-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4600 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_gridmask-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4385 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_photometric-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4262 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4262 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-regress-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4234 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4234 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4265 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4262 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4262 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3909 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3909 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3909 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3909 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3347 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3347 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4133 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mspn50_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3507 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvt-s_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3532 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvtv2-b2_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3314 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_dark-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3314 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_dark-8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3314 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_dark-8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3339 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_dark-8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3281 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3281 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3312 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3312 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)      149 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_fp16-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest101_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest101_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb16-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb16-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3277 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3277 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest50_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3284 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3284 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3284 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d152_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3284 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d152_8xb48-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3282 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3282 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d50_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb48-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4102 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn18_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4097 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn50_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet101_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3374 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet101_8xb48-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3372 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet50_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3372 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3282 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3282 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3205 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3205 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb48-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3280 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3285 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv1_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3285 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv1_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3294 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv2_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3294 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv2_8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3806 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-b-p4-w7_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3808 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-b-p4-w7_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4098 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4099 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3799 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-t-p4-w7_8xb32-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3305 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vgg16-bn_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3337 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-mbv3_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3254 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-res50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)      541 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vgg_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1383 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vipnas_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     5188 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/
+-rw-r--r--   0 runner    (1001) docker     (123)     6205 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/cspnext-m_udp_8xb64-210e_crowpose-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)      597 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/cspnext_udp_crowdpose.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/hrnet_crowdpose.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2275 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/resnet_crowdpose.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_hrnet-w32_8xb64-210e_crowdpose-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3332 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3332 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-320x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3332 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res152_8xb64-210e_crowdpose-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3330 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res50_8xb64-210e_crowdpose-256x192.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2182 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/hrnet_humanart.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4329 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-base_8xb64-210e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4330 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-huge_8xb64-210e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4330 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-large_8xb64-210e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4468 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-small_8xb64-210e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4246 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_hrnet-w32_8xb64-210e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4246 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_hrnet-w48_8xb32-210e_humanart-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4048 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/vitpose_humanart.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/
+-rw-r--r--   0 runner    (1001) docker     (123)     3292 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/cpm_jhmdb.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     6896 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/resnet_jhmdb.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3214 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub1-368x368.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3214 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub2-368x368.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3214 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub3-368x368.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3244 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub1-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3244 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub2-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3244 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub3-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3167 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub1-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3167 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub2-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3167 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub3-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/
+-rw-r--r--   0 runner    (1001) docker     (123)      438 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cpm_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     5982 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cspnext-m_udp_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      492 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cspnext_udp_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      950 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/hourglass_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/hrnet_dark_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      916 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/hrnet_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      953 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/litehrnet_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      514 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/mobilenetv2_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1396 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/resnet_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1460 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/resnetv1d_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/resnext_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      953 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/scnet_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1450 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/seresnet_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      519 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/shufflenetv1_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      519 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/shufflenetv2_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3248 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_cpm_8xb64-210e_mpii-368x368.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3085 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb32-210e_mpii-384x384.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3085 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4056 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_dark-8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4056 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4087 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3739 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-18_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3739 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-30_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3140 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_mobilenetv2_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3105 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res101_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3105 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res152_8xb32-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3103 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res50_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d101_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3106 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d152_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3104 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d50_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnext152_8xb32-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet101_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3194 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet50_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3104 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet101_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3027 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet152_8xb32-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3102 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet50_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3107 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv1_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3116 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv2_8xb64-210e_mpii-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/
+-rw-r--r--   0 runner    (1001) docker     (123)     4674 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/hrnet_posetrack18.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      627 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/resnet_posetrack18.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4426 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4426 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4426 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4426 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3378 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_res50_8xb64-20e_posetrack18-256x192.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/
+-rw-r--r--   0 runner    (1001) docker     (123)      651 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/mobilenetv2_rle_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1918 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/resnet_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2982 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/resnet_rle_coco.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3487 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_mobilenetv2_rle-pretrained-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3284 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3299 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_rle-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3284 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3299 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3299 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3282 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3297 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-8xb64-210e_coco-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3442 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-pretrained-8xb64-210e_coco-256x192.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/
+-rw-r--r--   0 runner    (1001) docker     (123)     1409 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/resnet_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      503 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/resnet_rle_mpii.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3087 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res101_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res152_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3085 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_8xb64-210e_mpii-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3073 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_rle-8xb64-210e_mpii-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/
+-rw-r--r--   0 runner    (1001) docker     (123)     6232 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_simplebaseline3d_8xb64-200e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3392 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-1frm-supv-cpn-ft_8xb128-80e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3396 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-243frm-supv-cpn-ft_8xb128-200e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3168 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-243frm-supv_8xb128-80e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2899 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-semi-supv-cpn-ft_8xb64-200e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2786 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-semi-supv_8xb64-200e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3160 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-supv_8xb128-80e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3163 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-81frm-supv_8xb128-80e_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)      832 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/simplebaseline3d_h36m.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4069 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/videopose3d_h36m.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/
+-rw-r--r--   0 runner    (1001) docker     (123)     6516 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/rtmpose-m_8xb32-60e_coco-wholebody-face-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      541 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/rtmpose_coco_wholebody_face.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/
+-rw-r--r--   0 runner    (1001) docker     (123)    14494 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose-m_8xb256-120e_face6-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14512 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose-s_8xb256-120e_face6-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14487 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose-t_8xb256-120e_face6-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1671 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose_face6.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/lapa/
+-rw-r--r--   0 runner    (1001) docker     (123)     6883 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/lapa/rtmpose-m_8xb64-120e_lapa-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      466 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/lapa/rtmpose_lapa.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/
+-rw-r--r--   0 runner    (1001) docker     (123)     6490 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/rtmpose-m_8xb64-60e_wflw-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      463 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/rtmpose_wflw.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/
+-rw-r--r--   0 runner    (1001) docker     (123)      793 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/hrnetv2_300w.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4333 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/td-hm_hrnetv2-w18_8xb64-60e_300w-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/
+-rw-r--r--   0 runner    (1001) docker     (123)      473 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/hrnetv2_aflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      505 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/hrnetv2_dark_aflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4327 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_8xb64-60e_aflw-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4358 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_dark-8xb64-60e_aflw-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/
+-rw-r--r--   0 runner    (1001) docker     (123)      542 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/hourglass_coco_wholebody_face.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      536 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/hrnetv2_coco_wholebody_face.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      568 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/hrnetv2_dark_coco_wholebody_face.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      576 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/mobilenetv2_coco_wholebody_face.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      548 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/resnet_coco_wholebody_face.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      552 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/scnet_coco_wholebody_face.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3161 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hourglass52_8xb32-60e_coco-wholebody-face-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4303 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_8xb32-60e_coco-wholebody-face-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4334 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_dark-8xb32-60e_coco-wholebody-face-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3210 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_mobilenetv2_8xb32-60e_coco-wholebody-face-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3173 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_res50_8xb32-60e_coco-wholebody-face-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3264 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_scnet50_8xb32-60e_coco-wholebody-face-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/
+-rw-r--r--   0 runner    (1001) docker     (123)      444 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/hrnetv2_cofw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4298 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/td-hm_hrnetv2-w18_8xb64-60e_cofw-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/
+-rw-r--r--   0 runner    (1001) docker     (123)      637 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_awing_wflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      628 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_dark_wflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      596 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_wflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4314 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_8xb64-60e_wflw-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4315 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_awing-8xb64-60e_wflw-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4345 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_dark-8xb64-60e_wflw-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/
+-rw-r--r--   0 runner    (1001) docker     (123)      514 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/resnet_softwingloss_wflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      456 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/resnet_wflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/resnet_wingloss_wflw.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3155 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_8xb64-210e_wflw-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3155 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_softwingloss_8xb64-210e_wflw-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3151 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_wingloss_8xb64-210e_wflw-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/
+-rw-r--r--   0 runner    (1001) docker     (123)     7252 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/res50_deepfasion2.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3223 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-long-sleeved-dress-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3197 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-skirt-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3207 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-vest-dress-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_2xb64-210e_deepfasion2-trousers-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3200 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_3xb64-210e_deepfasion2-shorts-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3226 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-short-sleeved-dress-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-sling-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3210 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-sling-dress-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3196 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-vest-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3226 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_6xb64-210e_deepfasion2-short-sleeved-shirt-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3239 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-long-sleeved-outwear-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3224 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-long-sleeved-shirt-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3241 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-short-sleeved-outwear-256x192.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/
+-rw-r--r--   0 runner    (1001) docker     (123)     6537 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose-m_8xb32-210e_coco-wholebody-hand-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      594 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose_coco_wholebody_hand.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/hand5/
+-rw-r--r--   0 runner    (1001) docker     (123)    10083 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/hand5/rtmpose-m_8xb256-210e_hand5-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)      830 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/hand5/rtmpose_hand5.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/
+-rw-r--r--   0 runner    (1001) docker     (123)      580 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/hourglass_coco_wholebody_hand.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      573 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/hrnetv2_coco_wholebody_hand.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      601 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/hrnetv2_dark_coco_wholebody_hand.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      585 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/litehrnet_coco_wholebody_hand.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/mobilenetv2_coco_wholebody_hand.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      584 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/resnet_coco_wholebody_hand.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      567 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/scnet_coco_wholebody_hand.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3171 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hourglass52_8xb32-210e_coco-wholebody-hand-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4302 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_8xb32-210e_coco-wholebody-hand-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4333 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_dark-8xb32-210e_coco-wholebody-hand-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3736 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_litehrnet-w18_8xb32-210e_coco-wholebody-hand-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3209 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_mobilenetv2_8xb32-210e_coco-wholebody-hand-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3172 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_res50_8xb32-210e_coco-wholebody-hand-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3263 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_scnet50_8xb32-210e_coco-wholebody-hand-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/
+-rw-r--r--   0 runner    (1001) docker     (123)      526 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/resnet_freihand2d.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3618 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/
+-rw-r--r--   0 runner    (1001) docker     (123)      548 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/hrnetv2_dark_onehand10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      520 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/hrnetv2_onehand10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      534 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/hrnetv2_udp_onehand10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      561 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/mobilenetv2_onehand10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      533 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/resnet_onehand10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4303 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_8xb64-210e_onehand10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4334 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_dark-8xb64-210e_onehand10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4303 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_udp-8xb64-210e_onehand10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3223 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_mobilenetv2_8xb64-210e_onehand10k-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3186 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_res50_8xb32-210e_onehand10k-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/
+-rw-r--r--   0 runner    (1001) docker     (123)      514 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/hrnetv2_dark_rhd2d.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      486 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/hrnetv2_rhd2d.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      501 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/hrnetv2_udp_rhd2d.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      526 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/mobilenetv2_rhd2d.yml
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/resnet_rhd2d.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4277 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_8xb64-210e_rhd2d-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4308 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_dark-8xb64-210e_rhd2d-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4277 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_udp-8xb64-210e_rhd2d-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3197 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_mobilenetv2_8xb64-210e_rhd2d-256x256.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3160 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_res50_8xb64-210e_rhd2d-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/
+-rw-r--r--   0 runner    (1001) docker     (123)      532 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/resnet_onehand10k.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3166 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/td-reg_res50_8xb64-210e_onehand10k-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/resnet_rhd2d.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     3140 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/td-reg_res50_8xb64-210e_rhd2d-256x256.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/
+-rw-r--r--   0 runner    (1001) docker     (123)     6670 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6671 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6674 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-m_8xb64-270e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2151 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose_coco-wholebody.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/
+-rw-r--r--   0 runner    (1001) docker     (123)     6050 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-l_udp_8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6053 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-m_udp_8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)      740 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext_udp_coco-wholebody.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     2784 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/hrnet_coco-wholebody.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1453 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/hrnet_dark_coco-wholebody.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4182 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/resnet_coco-wholebody.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     4252 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4252 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_dark-8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4252 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4252 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4283 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3299 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3299 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-384x288.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3355 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3386 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_dark-8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3295 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3326 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_dark-8xb64-210e_coco-wholebody-256x192.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/vipnas_coco-wholebody.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1457 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/vipnas_dark_coco-wholebody.yml
+-rw-r--r--   0 runner    (1001) docker     (123)     1648 2023-07-04 13:35:37.000000 mmpose-1.1.0/mmpose/.mim/dataset-index.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/demo/
+-rw-r--r--   0 runner    (1001) docker     (123)    18404 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/body3d_pose_lifter_demo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6863 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/bottomup_demo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3088 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/image_demo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6126 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/inferencer_demo.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/
+-rw-r--r--   0 runner    (1001) docker     (123)     9123 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_1class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8608 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5869 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_1class.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6371 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8524 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/mask_rcnn_r50_fpn_2x_coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4435 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/ssdlite_mobilenetv2-scratch_8xb24-600e_coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4927 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/ssdlite_mobilenetv2_scratch_600e_onehand.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4480 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/yolov3_d53_320_273e_coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10435 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/yolox-s_8xb8-300e_coco-face.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/demo/mmtracking_cfg/
+-rw-r--r--   0 runner    (1001) docker     (123)    11566 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmtracking_cfg/deepsort_faster-rcnn_fpn_4e_mot17-private-half.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11604 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/mmtracking_cfg/tracktor_faster-rcnn_r50_fpn_4e_mot17-private.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9476 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/demo/topdown_demo_with_mmdet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8686 2023-07-04 13:35:37.000000 mmpose-1.1.0/mmpose/.mim/model-index.yml
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/tools/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/tools/analysis_tools/
+-rw-r--r--   0 runner    (1001) docker     (123)     5567 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/analysis_tools/analyze_logs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4703 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/analysis_tools/get_flops.py
+-rw-r--r--   0 runner    (1001) docker     (123)      643 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/analysis_tools/print_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/
+-rw-r--r--   0 runner    (1001) docker     (123)     4794 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/h36m_to_coco.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     9689 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/labelstudio2coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3262 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/lapa2coco.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1649 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/mat2json.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13482 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_animalpose_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2817 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_cofw_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6251 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_deepposekit_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5595 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_macaquepose_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15559 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/preprocess_h36m.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12309 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/preprocess_mpi_inf_3dhp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_300w.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      163 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_aic.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      208 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_ap10k.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      307 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_coco2017.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      151 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_crowdpose.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      148 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_freihand.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      196 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_hagrid.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      203 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_halpe.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      136 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_lapa.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      169 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_mpii.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      228 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_onehand10k.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      198 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/scripts/preprocess_wflw.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      527 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dist_test.sh
+-rwxr-xr-x   0 runner    (1001) docker     (123)      490 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/dist_train.sh
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/tools/misc/
+-rw-r--r--   0 runner    (1001) docker     (123)     5837 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/misc/browse_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4658 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/misc/keypoints2coco_without_mmdet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2103 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/misc/publish_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)      614 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/slurm_test.sh
+-rw-r--r--   0 runner    (1001) docker     (123)      622 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/slurm_train.sh
+-rw-r--r--   0 runner    (1001) docker     (123)     4549 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/test.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/.mim/tools/torchserve/
+-rw-r--r--   0 runner    (1001) docker     (123)     4827 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/torchserve/mmpose2torchserve.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2709 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/torchserve/mmpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2688 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/torchserve/test_torchserver.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5526 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/.mim/tools/train.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1031 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/apis/
+-rw-r--r--   0 runner    (1001) docker     (123)      779 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9338 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inference.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14987 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inference_3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3074 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inference_tracking.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/apis/inferencers/
+-rw-r--r--   0 runner    (1001) docker     (123)      332 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18733 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/base_mmpose_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10007 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/mmpose_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13067 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/pose2d_inferencer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21775 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/pose3d_inferencer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/apis/inferencers/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      201 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1174 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/utils/default_det_models.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1384 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/apis/inferencers/utils/get_model_alias.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/codecs/
+-rw-r--r--   0 runner    (1001) docker     (123)      755 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20970 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/associative_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2505 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10341 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/decoupled_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8621 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/image_pose_lifting.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4279 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/integral_regression_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4815 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/megvii_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5625 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/msra_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3371 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/regression_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11162 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/simcc_label.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12184 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/spr.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7726 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/udp_heatmap.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/codecs/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7031 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/utils/gaussian_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4017 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/utils/instance_property.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5352 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/utils/offset_heatmap.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7054 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/utils/post_processing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7221 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/utils/refinement.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8336 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/codecs/video_pose_lifting.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)      306 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3039 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3685 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/dataset_wrappers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/
+-rw-r--r--   0 runner    (1001) docker     (123)      380 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/
+-rw-r--r--   0 runner    (1001) docker     (123)      623 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3836 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/animalkingdom_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3323 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/animalpose_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3249 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/ap10k_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3237 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/atrw_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3651 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/fly_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3414 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/horse10_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5368 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/locust_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3358 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/macaque_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4768 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/animal/zebra_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/base/
+-rw-r--r--   0 runner    (1001) docker     (123)      211 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/base/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17252 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/base/base_coco_style_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15382 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/base/base_mocap_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/
+-rw-r--r--   0 runner    (1001) docker     (123)      758 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3211 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/aic_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3249 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/coco_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3229 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/crowdpose_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3314 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/humanart_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5484 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/jhmdb_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3306 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/mhp_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8887 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/mpii_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6732 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/mpii_trb_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3611 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/ochuman_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3302 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/posetrack18_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16662 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body/posetrack18_video_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/body3d/
+-rw-r--r--   0 runner    (1001) docker     (123)      121 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body3d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11157 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/body3d/h36m_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/
+-rw-r--r--   0 runner    (1001) docker     (123)      442 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5343 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/aflw_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4951 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/coco_wholebody_face_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2879 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/cofw_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4889 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/face_300w_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2906 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/lapa_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4867 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/face/wflw_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/fashion/
+-rw-r--r--   0 runner    (1001) docker     (123)      211 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/fashion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      373 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/fashion/deepfashion2_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5843 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/fashion/deepfashion_dataset.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/hand/
+-rw-r--r--   0 runner    (1001) docker     (123)      440 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/hand/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6230 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/hand/coco_wholebody_hand_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5140 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/hand/freihand_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3470 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/hand/onehand10k_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5435 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/hand/panoptic_hand2d_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3421 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/hand/rhd2d_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7847 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/datasets/wholebody/
+-rw-r--r--   0 runner    (1001) docker     (123)      197 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/wholebody/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5421 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/wholebody/coco_wholebody_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2935 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/datasets/wholebody/halpe_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4552 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/samplers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/datasets/transforms/
+-rw-r--r--   0 runner    (1001) docker     (123)      971 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18636 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/bottomup_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)    37677 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/common_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4482 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/converting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10299 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/formatting.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2062 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/loading.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3884 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/pose3d_transforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4357 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/datasets/transforms/topdown_transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/engine/
+-rw-r--r--   0 runner    (1001) docker     (123)      139 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/engine/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/engine/hooks/
+-rw-r--r--   0 runner    (1001) docker     (123)      194 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/engine/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/engine/hooks/ema_hook.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6694 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/engine/hooks/visualization_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/engine/optim_wrappers/
+-rw-r--r--   0 runner    (1001) docker     (123)      170 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/engine/optim_wrappers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2956 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/engine/optim_wrappers/layer_decay_optim_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (123)      135 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/evaluation/functional/
+-rw-r--r--   0 runner    (1001) docker     (123)      592 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/functional/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13917 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/functional/keypoint_eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2146 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/functional/mesh_eval.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11118 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/functional/nms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/evaluation/metrics/
+-rw-r--r--   0 runner    (1001) docker     (123)      617 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/metrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24705 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/metrics/coco_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12760 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/metrics/coco_wholebody_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)    39449 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/metrics/keypoint_2d_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5162 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/metrics/keypoint_3d_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9402 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/metrics/keypoint_partition_metric.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8875 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/evaluation/metrics/posetrack18_metric.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/
+-rw-r--r--   0 runner    (1001) docker     (123)      602 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/backbones/
+-rw-r--r--   0 runner    (1001) docker     (123)     1365 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/alexnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)      848 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/base_backbone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6174 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/cpm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6746 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/hourglass.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6848 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/hourglass_ae.py
+-rw-r--r--   0 runner    (1001) docker     (123)    30304 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/hrformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22592 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/hrnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    36829 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/litehrnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10150 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/mobilenet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7175 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/mobilenet_v3.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19753 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/mspn.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21991 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/pvt.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12565 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/regnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12745 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/resnest.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25333 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6986 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/resnext.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22957 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/rsn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8248 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/scnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4929 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/seresnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7469 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/seresnext.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12144 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/shufflenet_v1.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10818 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/shufflenet_v2.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29134 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/swin.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10566 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/tcn.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/backbones/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      392 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      914 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/utils/channel_shuffle.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1984 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/utils/ckpt_convert.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4196 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/utils/inverted_residual.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1056 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/utils/make_divisible.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/utils/se_layer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2947 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8824 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/v2v_net.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7461 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/vgg.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6184 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/vipnas_mbv3.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22076 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/backbones/vipnas_resnet.py
+-rw-r--r--   0 runner    (1001) docker     (123)      836 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/data_preprocessors/
+-rw-r--r--   0 runner    (1001) docker     (123)      136 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/data_preprocessors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      265 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/data_preprocessors/data_preprocessor.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      832 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2922 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/base_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/heads/coord_cls_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      154 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/coord_cls_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10983 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/coord_cls_heads/rtmcc_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14272 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/coord_cls_heads/simcc_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      373 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10877 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/ae_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29581 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/cid_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12465 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/cpm_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14941 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/heatmap_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16505 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/mspn_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7948 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/heatmap_heads/vipnas_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/heads/hybrid_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      159 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/hybrid_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22514 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/hybrid_heads/dekr_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8407 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/hybrid_heads/vis_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/
+-rw-r--r--   0 runner    (1001) docker     (123)      501 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6103 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/dsnt_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13301 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/integral_regression_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5104 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/regression_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6676 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/rle_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5080 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/temporal_regression_head.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5074 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/heads/regression_heads/trajectory_regression_head.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/losses/
+-rw-r--r--   0 runner    (1001) docker     (123)      914 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/losses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4167 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/losses/ae_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7366 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/losses/classification_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16728 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/losses/heatmap_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2644 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/losses/loss_wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    21225 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/losses/regression_loss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/necks/
+-rw-r--r--   0 runner    (1001) docker     (123)      294 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/necks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3209 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/necks/fmap_proc_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8775 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/necks/fpn.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1267 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/necks/gap_neck.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12528 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/necks/posewarper_neck.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/pose_estimators/
+-rw-r--r--   0 runner    (1001) docker     (123)      245 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/pose_estimators/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8030 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/pose_estimators/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6959 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/pose_estimators/bottomup.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13186 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/pose_estimators/pose_lifter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7629 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/pose_estimators/topdown.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/models/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      381 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8883 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/check_and_update_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/ckpt_convert.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2098 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/geometry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2308 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/ops.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2657 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/realnvp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2706 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/regularizations.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9864 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/rtmcc_block.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13244 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/transformer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6602 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/models/utils/tta.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5338 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/structures/
+-rw-r--r--   0 runner    (1001) docker     (123)      753 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/structures/bbox/
+-rw-r--r--   0 runner    (1001) docker     (123)      441 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/bbox/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11597 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/bbox/transforms.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/structures/keypoint/
+-rw-r--r--   0 runner    (1001) docker     (123)      180 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/keypoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4685 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/keypoint/transforms.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10214 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/multilevel_pixel_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/pose_data_sample.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4606 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/structures/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/testing/
+-rw-r--r--   0 runner    (1001) docker     (123)      304 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8666 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/testing/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)      499 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10679 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/camera.py
+-rw-r--r--   0 runner    (1001) docker     (123)      428 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/collect_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)      772 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/config_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1836 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/hooks.py
+-rw-r--r--   0 runner    (1001) docker     (123)      967 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4079 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/setup_env.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2211 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/tensor_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3743 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/timer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1167 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/utils/typing.py
+-rw-r--r--   0 runner    (1001) docker     (123)      980 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/version.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)      275 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/visualization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2914 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/visualization/fast_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25086 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/visualization/local_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25110 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/visualization/local_visualizer_3d.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22533 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/visualization/opencv_backend_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5668 2023-07-04 13:35:29.000000 mmpose-1.1.0/mmpose/visualization/simcc_vis.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    30904 2023-07-04 13:35:37.000000 mmpose-1.1.0/mmpose.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    61406 2023-07-04 13:35:38.000000 mmpose-1.1.0/mmpose.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-04 13:35:37.000000 mmpose-1.1.0/mmpose.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-04 13:35:37.000000 mmpose-1.1.0/mmpose.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (123)      532 2023-07-04 13:35:37.000000 mmpose-1.1.0/mmpose.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        7 2023-07-04 13:35:37.000000 mmpose-1.1.0/mmpose.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 13:35:38.000000 mmpose-1.1.0/requirements/
+-rw-r--r--   0 runner    (1001) docker     (123)       56 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/albu.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       66 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/build.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      195 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/docs.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       62 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/mminstall.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        9 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/optional.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       69 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/poseval.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      108 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/readthedocs.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      101 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/runtime.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       99 2023-07-04 13:35:29.000000 mmpose-1.1.0/requirements/tests.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      780 2023-07-04 13:35:38.000000 mmpose-1.1.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     7496 2023-07-04 13:35:29.000000 mmpose-1.1.0/setup.py
```

### Comparing `mmpose-1.0.0rc1/PKG-INFO` & `mmpose-1.1.0/PKG-INFO`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mmpose
-Version: 1.0.0rc1
+Version: 1.1.0
 Summary: OpenMMLab Pose Estimation Toolbox and Benchmark.
 Home-page: https://github.com/open-mmlab/mmpose
 Author: MMPose Contributors
 Author-email: openmmlab@gmail.com
 License: Apache License 2.0
 Description: <div align="center">
           <img src="resources/mmpose-logo.png" width="450"/>
@@ -22,66 +22,72 @@
               <a href="https://platform.openmmlab.com">
                 <i>TRY IT OUT</i>
               </a>
             </sup>
           </div>
           <div>&nbsp;</div>
         
-        [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/1.x/?badge=latest)
+        [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest)
         [![actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)](https://github.com/open-mmlab/mmpose/actions)
-        [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/1.x/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose)
+        [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/latest/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose)
         [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/project/mmpose/)
-        [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE)
+        [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/main/LICENSE)
         [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
         [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
         
-        [Documentation](https://mmpose.readthedocs.io/en/1.x/) |
-        [Installation](https://mmpose.readthedocs.io/en/1.x/installation.html) |
-        [Model Zoo](https://mmpose.readthedocs.io/en/1.x/model_zoo.html) |
-        [Papers](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html) |
-        [Update News](https://mmpose.readthedocs.io/en/1.x/notes/changelog.html) |
+        [Documentation](https://mmpose.readthedocs.io/en/latest/) |
+        [Installation](https://mmpose.readthedocs.io/en/latest/installation.html) |
+        [Model Zoo](https://mmpose.readthedocs.io/en/latest/model_zoo.html) |
+        [Papers](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html) |
+        [Update News](https://mmpose.readthedocs.io/en/latest/notes/changelog.html) |
         [Reporting Issues](https://github.com/open-mmlab/mmpose/issues/new/choose) |
         [RTMPose](/projects/rtmpose/)
         
         </div>
         
         <div align="center">
           <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
-            <img src="https://user-images.githubusercontent.com/25839884/218352562-cdded397-b0f3-4ca1-b8dd-a60df8dca75b.png" width="3%" alt="" /></a>
+            <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
           <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
-          <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+          <a href="https://discord.com/channels/1037617289144569886/1072798105428299817" style="text-decoration:none;">
             <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
           <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
           <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
             <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
           <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
           <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
             <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
         </div>
         
         ## Introduction
         
         English | [](README_CN.md)
         
         MMPose is an open-source toolbox for pose estimation based on PyTorch.
         It is a part of the [OpenMMLab project](https://github.com/open-mmlab).
         
-        The master branch works with **PyTorch 1.6+**.
+        The main branch works with **PyTorch 1.8+**.
         
         https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-ded1-11eb-84f6-24eeddbf4d91.mp4
         
         <br/>
         
         <details close>
         <summary><b>Major Features</b></summary>
         
         - **Support diverse tasks**
         
           We support a wide spectrum of mainstream pose analysis tasks in current research community, including 2d multi-person human pose estimation, 2d hand pose estimation, 2d face landmark detection, 133 keypoint whole-body human pose estimation, 3d human mesh recovery, fashion landmark detection and animal pose estimation.
-          See [Demo](demo/docs/) for more information.
+          See [Demo](demo/docs/en) for more information.
         
         - **Higher efficiency and higher accuracy**
         
           MMPose implements multiple state-of-the-art (SOTA) deep learning models, including both top-down & bottom-up approaches. We achieve faster training speed and higher accuracy than other popular codebases, such as [HRNet](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch).
           See [benchmark.md](docs/en/notes/benchmark.md) for more information.
         
         - **Support for various datasets**
@@ -95,184 +101,238 @@
           pose estimation framework by combining different modules.
           We provide detailed documentation and API reference, as well as unittests.
         
         </details>
         
         ## What's New
         
-        - We are excited to release **RTMPose**, a real-time pose estimation framework including:
-        
-          - A family of lightweight pose estimation models with state-of-the-art performance
-          - Inference APIs for Python, C++, C#, Java, etc. Easy to integrate into your applications and empower real-time stable pose estimation
-          - Cross-platform deployment with various backends
-          - A step-by-step guide to training and deploying your own models
-        
-          Checkout our [project page](/projects/rtmpose/) and [technical report](https://arxiv.org/abs/2303.07399) for more information!
+        - We are glad to support 3 new datasets:
+          - (CVPR 2023) [Human-Art](https://github.com/IDEA-Research/HumanArt)
+          - (CVPR 2022) [Animal Kingdom](https://github.com/sutdcv/Animal-Kingdom)
+          - (AAAI 2020) [LaPa](https://github.com/JDAI-CV/lapa-dataset/)
         
-        ![rtmpose_intro](https://user-images.githubusercontent.com/13503330/219269619-935499e5-bdd9-49ea-8104-3c7796dbd862.png)
+        ![image](https://github.com/open-mmlab/mmpose/assets/13503330/c9171dbb-7e7a-4c39-98e3-c92932182efb)
         
         - Welcome to [*projects of MMPose*](/projects/README.md), where you can access to the latest features of MMPose, and share your ideas and codes with the community at once. Contribution to MMPose will be simple and smooth:
         
           - Provide an easy and agile way to integrate algorithms, features and applications into MMPose
           - Allow flexible code structure and style; only need a short code review process
           - Build individual projects with full power of MMPose but not bound up with heavy frameworks
           - Checkout new projects:
             - [RTMPose](/projects/rtmpose/)
-            - [YOLOX-Pose (coming soon)](<>)
-            - [MMPose4AIGC (coming soon)](<>)
+            - [YOLOX-Pose](/projects/yolox_pose/)
+            - [MMPose4AIGC](/projects/mmpose4aigc/)
+            - [Simple Keypoints](/projects/skps/)
           - Become a contributors and make MMPose greater. Start your journey from the [example project](/projects/example_project/)
         
         <br/>
         
-        - 2022-03-15: MMPose [v1.0.0rc1](https://github.com/open-mmlab/mmpose/releases/tag/v1.0.0rc1) is released. Major updates include:
+        - 2023-07-04: MMPose [v1.1.0](https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) is officially released, with the main updates including:
         
-          - Release [RTMPose](/projects/rtmpose/), a high-performance real-time pose estimation framework based on MMPose
-          - Support [ViTPose](/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.md) (NeurIPS'22), [CID](/configs/body_2d_keypoint/cid/coco/hrnet_coco.md) (CVPR'22) and [DEKR](/configs/body_2d_keypoint/dekr/) (CVPR'21)
-          - Add [*Inferencer*](/docs/en/user_guides/inference.md#out-of-the-box-inferencer), a convenient interface for inference and visualization
+          - Support new datasets: Human-Art, Animal Kingdom and LaPa.
+          - Support new config type that is more user-friendly and flexible.
+          - Improve RTMPose with better performance.
+          - Migrate 3D pose estimation models on h36m.
+          - Inference speedup and webcam inference with all demo scripts.
         
-          See the full [release note](https://github.com/open-mmlab/mmpose/releases/tag/v1.0.0rc1) for more exciting updates brought by MMPose v1.0.0rc1!
+          Please refer to the [release notes](https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) for more updates brought by MMPose v1.1.0!
         
-        ## Installation
+        ## 0.x / 1.x Migration
         
-        Below are quick steps for installation:
+        MMPose v1.0.0 is a major update, including many API and config file changes. Currently, a part of the algorithms have been migrated to v1.0.0, and the remaining algorithms will be completed in subsequent versions. We will show the migration progress in the following list.
         
-        ```shell
-        conda create -n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y
-        conda activate open-mmlab
-        pip install openmim
-        git clone -b 1.x https://github.com/open-mmlab/mmpose.git
-        cd mmpose
-        mim install -e .
-        ```
+        <details close>
+        <summary><b>Migration Progress</b></summary>
         
-        Please refer to [installation.md](https://mmpose.readthedocs.io/en/1.x/installation.html) for more detailed installation and dataset preparation.
+        | Algorithm                         |   Status    |
+        | :-------------------------------- | :---------: |
+        | MTUT (CVPR 2019)                  |             |
+        | MSPN (ArXiv 2019)                 |    done     |
+        | InterNet (ECCV 2020)              |             |
+        | DEKR (CVPR 2021)                  |    done     |
+        | HigherHRNet (CVPR 2020)           |             |
+        | DeepPose (CVPR 2014)              |    done     |
+        | RLE (ICCV 2021)                   |    done     |
+        | SoftWingloss (TIP 2021)           |    done     |
+        | VideoPose3D (CVPR 2019)           |    done     |
+        | Hourglass (ECCV 2016)             |    done     |
+        | LiteHRNet (CVPR 2021)             |    done     |
+        | AdaptiveWingloss (ICCV 2019)      |    done     |
+        | SimpleBaseline2D (ECCV 2018)      |    done     |
+        | PoseWarper (NeurIPS 2019)         |             |
+        | SimpleBaseline3D (ICCV 2017)      |    done     |
+        | HMR (CVPR 2018)                   |             |
+        | UDP (CVPR 2020)                   |    done     |
+        | VIPNAS (CVPR 2021)                |    done     |
+        | Wingloss (CVPR 2018)              |    done     |
+        | DarkPose (CVPR 2020)              |    done     |
+        | Associative Embedding (NIPS 2017) | in progress |
+        | VoxelPose (ECCV 2020)             |             |
+        | RSN (ECCV 2020)                   |    done     |
+        | CID (CVPR 2022)                   |    done     |
+        | CPM (CVPR 2016)                   |    done     |
+        | HRNet (CVPR 2019)                 |    done     |
+        | HRNetv2 (TPAMI 2019)              |    done     |
+        | SCNet (CVPR 2020)                 |    done     |
+        
+        </details>
+        
+        If your algorithm has not been migrated, you can continue to use the [0.x branch](https://github.com/open-mmlab/mmpose/tree/0.x) and [old documentation](https://mmpose.readthedocs.io/en/0.x/).
+        
+        ## Installation
+        
+        Please refer to [installation.md](https://mmpose.readthedocs.io/en/latest/installation.html) for more detailed installation and dataset preparation.
         
         ## Getting Started
         
         We provided a series of tutorials about the basic usage of MMPose for new users:
         
-        - [About Configs](https://mmpose.readthedocs.io/en/1.x/user_guides/configs.html)
-        - [Add New Dataset](https://mmpose.readthedocs.io/en/1.x/user_guides/prepare_datasets.html)
-        - [Keypoint Encoding & Decoding](https://mmpose.readthedocs.io/en/1.x/user_guides/codecs.html)
-        - [Inference with Existing Models](https://mmpose.readthedocs.io/en/1.x/user_guides/inference.html)
-        - [Train and Test](https://mmpose.readthedocs.io/en/1.x/user_guides/train_and_test.html)
-        - [Visualization Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/visualization.html)
-        - [Other Useful Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/useful_tools.html)
+        1. For the basic usage of MMPose:
+        
+           - [A 20-minute Tour to MMPose](https://mmpose.readthedocs.io/en/latest/guide_to_framework.html)
+           - [Demos](https://mmpose.readthedocs.io/en/latest/demos.html)
+           - [Inference](https://mmpose.readthedocs.io/en/latest/user_guides/inference.html)
+           - [Configs](https://mmpose.readthedocs.io/en/latest/user_guides/configs.html)
+           - [Prepare Datasets](https://mmpose.readthedocs.io/en/latest/user_guides/prepare_datasets.html)
+           - [Train and Test](https://mmpose.readthedocs.io/en/latest/user_guides/train_and_test.html)
+        
+        2. For developers who wish to develop based on MMPose:
+        
+           - [Learn about Codecs](https://mmpose.readthedocs.io/en/latest/advanced_guides/codecs.html)
+           - [Dataflow in MMPose](https://mmpose.readthedocs.io/en/latest/advanced_guides/dataflow.html)
+           - [Implement New Models](https://mmpose.readthedocs.io/en/latest/advanced_guides/implement_new_models.html)
+           - [Customize Datasets](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_datasets.html)
+           - [Customize Data Transforms](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_transforms.html)
+           - [Customize Optimizer](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_optimizer.html)
+           - [Customize Logging](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_logging.html)
+           - [How to Deploy](https://mmpose.readthedocs.io/en/latest/advanced_guides/how_to_deploy.html)
+           - [Model Analysis](https://mmpose.readthedocs.io/en/latest/advanced_guides/model_analysis.html)
+           - [Migration Guide](https://mmpose.readthedocs.io/en/latest/migration.html)
+        
+        3. For researchers and developers who are willing to contribute to MMPose:
+        
+           - [Contribution Guide](https://mmpose.readthedocs.io/en/latest/contribution_guide.html)
+        
+        4. For some common issues, we provide a FAQ list:
+        
+           - [FAQ](https://mmpose.readthedocs.io/en/latest/faq.html)
         
         ## Model Zoo
         
         Results and models are available in the **README.md** of each method's config directory.
-        A summary can be found in the [Model Zoo](https://mmpose.readthedocs.io/en/1.x/modelzoo.html) page.
+        A summary can be found in the [Model Zoo](https://mmpose.readthedocs.io/en/latest/model_zoo.html) page.
         
-        <details open>
+        <details close>
         <summary><b>Supported algorithms:</b></summary>
         
-        - [x] [DeepPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#deeppose-cvpr-2014) (CVPR'2014)
-        - [x] [CPM](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#cpm-cvpr-2016) (CVPR'2016)
-        - [x] [Hourglass](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016)
-        - [ ] [SimpleBaseline3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017)
-        - [ ] [Associative Embedding](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017)
-        - [x] [SimpleBaseline2D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018)
-        - [x] [DSNT](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#dsnt-2018) (ArXiv'2021)
-        - [x] [HRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019)
-        - [x] [IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#ipr-eccv-2018) (ECCV'2018)
-        - [ ] [VideoPose3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019)
-        - [x] [HRNetv2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrnetv2-tpami-2019) (TPAMI'2019)
-        - [x] [MSPN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mspn-arxiv-2019) (ArXiv'2019)
-        - [x] [SCNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020)
-        - [ ] [HigherHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#higherhrnet-cvpr-2020) (CVPR'2020)
-        - [x] [RSN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#rsn-eccv-2020) (ECCV'2020)
-        - [ ] [InterNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020)
-        - [ ] [VoxelPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#voxelpose-eccv-2020) (ECCV'2020)
-        - [x] [LiteHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#litehrnet-cvpr-2021) (CVPR'2021)
-        - [x] [ViPNAS](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021)
-        - [x] [Debias-IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#debias-ipr-iccv-2021) (ICCV'2021)
-        - [x] [SimCC](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simcc-eccv-2022) (ECCV'2022)
+        - [x] [DeepPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#deeppose-cvpr-2014) (CVPR'2014)
+        - [x] [CPM](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#cpm-cvpr-2016) (CVPR'2016)
+        - [x] [Hourglass](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016)
+        - [x] [SimpleBaseline3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017)
+        - [ ] [Associative Embedding](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017)
+        - [x] [SimpleBaseline2D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018)
+        - [x] [DSNT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#dsnt-2018) (ArXiv'2021)
+        - [x] [HRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019)
+        - [x] [IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#ipr-eccv-2018) (ECCV'2018)
+        - [x] [VideoPose3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019)
+        - [x] [HRNetv2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrnetv2-tpami-2019) (TPAMI'2019)
+        - [x] [MSPN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mspn-arxiv-2019) (ArXiv'2019)
+        - [x] [SCNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020)
+        - [ ] [HigherHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#higherhrnet-cvpr-2020) (CVPR'2020)
+        - [x] [RSN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#rsn-eccv-2020) (ECCV'2020)
+        - [ ] [InterNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020)
+        - [ ] [VoxelPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#voxelpose-eccv-2020) (ECCV'2020)
+        - [x] [LiteHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#litehrnet-cvpr-2021) (CVPR'2021)
+        - [x] [ViPNAS](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021)
+        - [x] [Debias-IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#debias-ipr-iccv-2021) (ICCV'2021)
+        - [x] [SimCC](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simcc-eccv-2022) (ECCV'2022)
         
         </details>
         
-        <details open>
+        <details close>
         <summary><b>Supported techniques:</b></summary>
         
-        - [ ] [FPN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017)
-        - [ ] [FP16](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017)
-        - [ ] [Wingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018)
-        - [ ] [AdaptiveWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019)
-        - [x] [DarkPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#darkpose-cvpr-2020) (CVPR'2020)
-        - [x] [UDP](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020)
-        - [ ] [Albumentations](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#albumentations-information-2020) (Information'2020)
-        - [ ] [SoftWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#softwingloss-tip-2021) (TIP'2021)
-        - [x] [RLE](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#rle-iccv-2021) (ICCV'2021)
+        - [x] [FPN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017)
+        - [x] [FP16](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017)
+        - [x] [Wingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018)
+        - [x] [AdaptiveWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019)
+        - [x] [DarkPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#darkpose-cvpr-2020) (CVPR'2020)
+        - [x] [UDP](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020)
+        - [x] [Albumentations](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#albumentations-information-2020) (Information'2020)
+        - [x] [SoftWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#softwingloss-tip-2021) (TIP'2021)
+        - [x] [RLE](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#rle-iccv-2021) (ICCV'2021)
         
         </details>
         
-        <details open>
-        <summary><b>Supported <a href="https://mmpose.readthedocs.io/en/1.x/dataset_zoo.html">datasets</a>:</b></summary>
+        <details close>
+        <summary><b>Supported datasets:</b></summary>
         
-        - [x] [AFLW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\] (ICCVW'2011)
-        - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013)
-        - [x] [COFW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cofw-iccv-2013) \[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013)
-        - [x] [MPII](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\] (CVPR'2014)
-        - [x] [Human3.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014)
-        - [x] [COCO](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-eccv-2014) \[[homepage](http://cocodataset.org/)\] (ECCV'2014)
-        - [x] [CMU Panoptic](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015)
-        - [x] [DeepFashion](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016)
-        - [x] [300W](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#300w-imavis-2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
-        - [x] [RHD](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017)
-        - [x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017)
-        - [x] [AI Challenger](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/AI_Challenger_2017)\] (ArXiv'2017)
-        - [x] [MHP](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018)
-        - [x] [WFLW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018)
-        - [x] [PoseTrack18](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/download.php)\] (CVPR'2018)
-        - [x] [OCHuman](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
-        - [x] [CrowdPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019)
-        - [x] [MPII-TRB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019)
-        - [x] [FreiHand](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-freiburg.de/projects/freihand/)\] (ICCV'2019)
-        - [x] [Animal-Pose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#animal-pose-iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019)
-        - [x] [OneHand10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019)
-        - [x] [Vinegar Fly](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Nature Methods'2019)
-        - [x] [Desert Locust](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
-        - [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
-        - [x] [ATRW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020)
-        - [x] [Halpe](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/Halpe-FullBody/)\] (CVPR'2020)
-        - [x] [COCO-WholeBody](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-wholebody-eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\] (ECCV'2020)
-        - [x] [MacaquePose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020)
-        - [x] [InterHand2.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/InterHand2.6M/)\] (ECCV'2020)
-        - [x] [AP-10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021)
-        - [x] [Horse-10](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#horse-10-wacv-2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
+        - [x] [AFLW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\] (ICCVW'2011)
+        - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013)
+        - [x] [COFW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cofw-iccv-2013) \[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013)
+        - [x] [MPII](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\] (CVPR'2014)
+        - [x] [Human3.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014)
+        - [x] [COCO](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-eccv-2014) \[[homepage](http://cocodataset.org/)\] (ECCV'2014)
+        - [x] [CMU Panoptic](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015)
+        - [x] [DeepFashion](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016)
+        - [x] [300W](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#300w-imavis-2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
+        - [x] [RHD](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017)
+        - [x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017)
+        - [x] [AI Challenger](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/AI_Challenger_2017)\] (ArXiv'2017)
+        - [x] [MHP](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018)
+        - [x] [WFLW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018)
+        - [x] [PoseTrack18](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/download.php)\] (CVPR'2018)
+        - [x] [OCHuman](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
+        - [x] [CrowdPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019)
+        - [x] [MPII-TRB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019)
+        - [x] [FreiHand](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-freiburg.de/projects/freihand/)\] (ICCV'2019)
+        - [x] [Animal-Pose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#animal-pose-iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019)
+        - [x] [OneHand10K](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019)
+        - [x] [Vinegar Fly](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Nature Methods'2019)
+        - [x] [Desert Locust](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
+        - [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
+        - [x] [ATRW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020)
+        - [x] [Halpe](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/Halpe-FullBody/)\] (CVPR'2020)
+        - [x] [COCO-WholeBody](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-wholebody-eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\] (ECCV'2020)
+        - [x] [MacaquePose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020)
+        - [x] [InterHand2.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/InterHand2.6M/)\] (ECCV'2020)
+        - [x] [AP-10K](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021)
+        - [x] [Horse-10](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#horse-10-wacv-2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
+        - [x] [Human-Art](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#human-art-cvpr-2023) \[[homepage](https://idea-research.github.io/HumanArt/)\] (CVPR'2023)
+        - [x] [LaPa](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#lapa-aaai-2020) \[[homepage](https://github.com/JDAI-CV/lapa-dataset)\] (AAAI'2020)
         
         </details>
         
-        <details open>
+        <details close>
         <summary><b>Supported backbones:</b></summary>
         
-        - [x] [AlexNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012)
-        - [x] [VGG](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vgg-iclr-2015) (ICLR'2015)
-        - [x] [ResNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016)
-        - [x] [ResNext](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017)
-        - [x] [SEResNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018)
-        - [x] [ShufflenetV1](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018)
-        - [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018)
-        - [x] [MobilenetV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018)
-        - [x] [ResNetV1D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnetv1d-cvpr-2019) (CVPR'2019)
-        - [x] [ResNeSt](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020)
-        - [x] [Swin](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021)
-        - [x] [HRFormer](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021)
-        - [x] [PVT](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021)
-        - [x] [PVTV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvtv2-cvmj-2022) (CVMJ'2022)
+        - [x] [AlexNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012)
+        - [x] [VGG](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#vgg-iclr-2015) (ICLR'2015)
+        - [x] [ResNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016)
+        - [x] [ResNext](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017)
+        - [x] [SEResNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018)
+        - [x] [ShufflenetV1](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018)
+        - [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018)
+        - [x] [MobilenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018)
+        - [x] [ResNetV1D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnetv1d-cvpr-2019) (CVPR'2019)
+        - [x] [ResNeSt](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020)
+        - [x] [Swin](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021)
+        - [x] [HRFormer](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021)
+        - [x] [PVT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021)
+        - [x] [PVTV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvtv2-cvmj-2022) (CVMJ'2022)
         
         </details>
         
         ### Model Request
         
-        We will keep up with the latest progress of the community, and support more popular algorithms and frameworks. If you have any feature requests, please feel free to leave a comment in [MMPose Roadmap](https://github.com/open-mmlab/mmpose/issues/9).
+        We will keep up with the latest progress of the community, and support more popular algorithms and frameworks. If you have any feature requests, please feel free to leave a comment in [MMPose Roadmap](https://github.com/open-mmlab/mmpose/issues/2258).
         
         ## Contributing
         
-        We appreciate all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://mmpose.readthedocs.io/en/1.x/notes/contribution_guide.html) for the contributing guideline.
+        We appreciate all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://mmpose.readthedocs.io/en/latest/contribution_guide.html) for the contributing guideline.
         
         ## Acknowledgement
         
         MMPose is an open source project that is contributed by researchers and engineers from various colleges and companies.
         We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks.
         We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new models.
         
@@ -293,32 +353,31 @@
         
         This project is released under the [Apache 2.0 license](LICENSE).
         
         ## Projects in OpenMMLab
         
         - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
         - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
-        - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-        - [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
+        - [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
+        - [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
         - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
         - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
         - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
+        - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
         - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
         - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
         - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
         - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
-        - [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
-        - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
         - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
         - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
-        - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
         - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-        - [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
-        - [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
         - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment Framework.
+        - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
+        - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
+        - [Playground](https://github.com/open-mmlab/playground): A central hub for gathering and showcasing amazing projects built upon OpenMMLab.
         
 Keywords: computer vision,pose estimation
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
```

#### html2text {}

```diff
@@ -1,302 +1,334 @@
-Metadata-Version: 2.1 Name: mmpose Version: 1.0.0rc1 Summary: OpenMMLab Pose
+Metadata-Version: 2.1 Name: mmpose Version: 1.1.0 Summary: OpenMMLab Pose
 Estimation Toolbox and Benchmark. Home-page: https://github.com/open-mmlab/
 mmpose Author: MMPose Contributors Author-email: openmmlab@gmail.com License:
 Apache License 2.0 Description:
                           [resources/mmpose-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
        [![Documentation](https://readthedocs.org/projects/mmpose/badge/
-   ?version=latest)](https://mmpose.readthedocs.io/en/1.x/?badge=latest) [!
+  ?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest) [!
   [actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)]
 (https://github.com/open-mmlab/mmpose/actions) [![codecov](https://codecov.io/
- gh/open-mmlab/mmpose/branch/1.x/graph/badge.svg)](https://codecov.io/gh/open-
-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/
-project/mmpose/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/
-   mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE) [!
-[Average time to resolve an issue](https://isitmaintained.com/badge/resolution/
-    open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues) [!
- [Percentage of issues still open](https://isitmaintained.com/badge/open/open-
+  gh/open-mmlab/mmpose/branch/latest/graph/badge.svg)](https://codecov.io/gh/
+  open-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://
+ pypi.org/project/mmpose/) [![LICENSE](https://img.shields.io/github/license/
+open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/main/LICENSE)
+    [![Average time to resolve an issue](https://isitmaintained.com/badge/
+resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
+[![Percentage of issues still open](https://isitmaintained.com/badge/open/open-
        mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
-         [Documentation](https://mmpose.readthedocs.io/en/1.x/) |
-[Installation](https://mmpose.readthedocs.io/en/1.x/installation.html) |
-    [Model Zoo](https://mmpose.readthedocs.io/en/1.x/model_zoo.html) |
-      [Papers](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-  algorithms.html) | [Update News](https://mmpose.readthedocs.io/en/1.x/
- notes/changelog.html) | [Reporting Issues](https://github.com/open-mmlab/
-         mmpose/issues/new/choose) | [RTMPose](/projects/rtmpose/)
+       [Documentation](https://mmpose.readthedocs.io/en/latest/) |
+        [Installation](https://mmpose.readthedocs.io/en/latest/
+ installation.html) | [Model Zoo](https://mmpose.readthedocs.io/en/latest/
+    model_zoo.html) | [Papers](https://mmpose.readthedocs.io/en/latest/
+        model_zoo_papers/algorithms.html) | [Update News](https://
+mmpose.readthedocs.io/en/latest/notes/changelog.html) | [Reporting Issues]
+  (https://github.com/open-mmlab/mmpose/issues/new/choose) | [RTMPose](/
+                              projects/rtmpose/)
 
 ## Introduction English | [](README_CN.md) MMPose is an open-source
 toolbox for pose estimation based on PyTorch. It is a part of the [OpenMMLab
-project](https://github.com/open-mmlab). The master branch works with **PyTorch
-1.6+**. https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-
+project](https://github.com/open-mmlab). The main branch works with **PyTorch
+1.8+**. https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-
 ded1-11eb-84f6-24eeddbf4d91.mp4
  Major Features - **Support diverse tasks** We support a wide spectrum of
 mainstream pose analysis tasks in current research community, including 2d
 multi-person human pose estimation, 2d hand pose estimation, 2d face landmark
 detection, 133 keypoint whole-body human pose estimation, 3d human mesh
 recovery, fashion landmark detection and animal pose estimation. See [Demo]
-(demo/docs/) for more information. - **Higher efficiency and higher accuracy**
-MMPose implements multiple state-of-the-art (SOTA) deep learning models,
-including both top-down & bottom-up approaches. We achieve faster training
-speed and higher accuracy than other popular codebases, such as [HRNet](https:/
-/github.com/leoxiaobin/deep-high-resolution-net.pytorch). See [benchmark.md]
-(docs/en/notes/benchmark.md) for more information. - **Support for various
-datasets** The toolbox directly supports multiple popular and representative
-datasets, COCO, AIC, MPII, MPII-TRB, OCHuman etc. See [dataset_zoo](docs/en/
-dataset_zoo) for more information. - **Well designed, tested and documented**
-We decompose MMPose into different components and one can easily construct a
-customized pose estimation framework by combining different modules. We provide
-detailed documentation and API reference, as well as unittests.  ## What's New
-- We are excited to release **RTMPose**, a real-time pose estimation framework
-including: - A family of lightweight pose estimation models with state-of-the-
-art performance - Inference APIs for Python, C++, C#, Java, etc. Easy to
-integrate into your applications and empower real-time stable pose estimation -
-Cross-platform deployment with various backends - A step-by-step guide to
-training and deploying your own models Checkout our [project page](/projects/
-rtmpose/) and [technical report](https://arxiv.org/abs/2303.07399) for more
-information! ![rtmpose_intro](https://user-images.githubusercontent.com/
-13503330/219269619-935499e5-bdd9-49ea-8104-3c7796dbd862.png) - Welcome to
+(demo/docs/en) for more information. - **Higher efficiency and higher
+accuracy** MMPose implements multiple state-of-the-art (SOTA) deep learning
+models, including both top-down & bottom-up approaches. We achieve faster
+training speed and higher accuracy than other popular codebases, such as
+[HRNet](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch). See
+[benchmark.md](docs/en/notes/benchmark.md) for more information. - **Support
+for various datasets** The toolbox directly supports multiple popular and
+representative datasets, COCO, AIC, MPII, MPII-TRB, OCHuman etc. See
+[dataset_zoo](docs/en/dataset_zoo) for more information. - **Well designed,
+tested and documented** We decompose MMPose into different components and one
+can easily construct a customized pose estimation framework by combining
+different modules. We provide detailed documentation and API reference, as well
+as unittests.  ## What's New - We are glad to support 3 new datasets: - (CVPR
+2023) [Human-Art](https://github.com/IDEA-Research/HumanArt) - (CVPR 2022)
+[Animal Kingdom](https://github.com/sutdcv/Animal-Kingdom) - (AAAI 2020) [LaPa]
+(https://github.com/JDAI-CV/lapa-dataset/) ![image](https://github.com/open-
+mmlab/mmpose/assets/13503330/c9171dbb-7e7a-4c39-98e3-c92932182efb) - Welcome to
 [*projects of MMPose*](/projects/README.md), where you can access to the latest
 features of MMPose, and share your ideas and codes with the community at once.
 Contribution to MMPose will be simple and smooth: - Provide an easy and agile
 way to integrate algorithms, features and applications into MMPose - Allow
 flexible code structure and style; only need a short code review process -
 Build individual projects with full power of MMPose but not bound up with heavy
 frameworks - Checkout new projects: - [RTMPose](/projects/rtmpose/) - [YOLOX-
-Pose (coming soon)](<>) - [MMPose4AIGC (coming soon)](<>) - Become a
-contributors and make MMPose greater. Start your journey from the [example
-project](/projects/example_project/)
-- 2022-03-15: MMPose [v1.0.0rc1](https://github.com/open-mmlab/mmpose/releases/
-tag/v1.0.0rc1) is released. Major updates include: - Release [RTMPose](/
-projects/rtmpose/), a high-performance real-time pose estimation framework
-based on MMPose - Support [ViTPose](/configs/body_2d_keypoint/topdown_heatmap/
-coco/vitpose_coco.md) (NeurIPS'22), [CID](/configs/body_2d_keypoint/cid/coco/
-hrnet_coco.md) (CVPR'22) and [DEKR](/configs/body_2d_keypoint/dekr/) (CVPR'21)
-- Add [*Inferencer*](/docs/en/user_guides/inference.md#out-of-the-box-
-inferencer), a convenient interface for inference and visualization See the
-full [release note](https://github.com/open-mmlab/mmpose/releases/tag/
-v1.0.0rc1) for more exciting updates brought by MMPose v1.0.0rc1! ##
-Installation Below are quick steps for installation: ```shell conda create -
-n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -
-c pytorch -y conda activate open-mmlab pip install openmim git clone -b 1.x
-https://github.com/open-mmlab/mmpose.git cd mmpose mim install -e . ``` Please
-refer to [installation.md](https://mmpose.readthedocs.io/en/1.x/
-installation.html) for more detailed installation and dataset preparation. ##
-Getting Started We provided a series of tutorials about the basic usage of
-MMPose for new users: - [About Configs](https://mmpose.readthedocs.io/en/1.x/
-user_guides/configs.html) - [Add New Dataset](https://mmpose.readthedocs.io/en/
-1.x/user_guides/prepare_datasets.html) - [Keypoint Encoding & Decoding](https:/
-/mmpose.readthedocs.io/en/1.x/user_guides/codecs.html) - [Inference with
-Existing Models](https://mmpose.readthedocs.io/en/1.x/user_guides/
-inference.html) - [Train and Test](https://mmpose.readthedocs.io/en/1.x/
-user_guides/train_and_test.html) - [Visualization Tools](https://
-mmpose.readthedocs.io/en/1.x/user_guides/visualization.html) - [Other Useful
-Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/useful_tools.html) ##
-Model Zoo Results and models are available in the **README.md** of each
-method's config directory. A summary can be found in the [Model Zoo](https://
-mmpose.readthedocs.io/en/1.x/modelzoo.html) page.  Supported algorithms: - [x]
-[DeepPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+Pose](/projects/yolox_pose/) - [MMPose4AIGC](/projects/mmpose4aigc/) - [Simple
+Keypoints](/projects/skps/) - Become a contributors and make MMPose greater.
+Start your journey from the [example project](/projects/example_project/)
+- 2023-07-04: MMPose [v1.1.0](https://github.com/open-mmlab/mmpose/releases/
+tag/v1.1.0) is officially released, with the main updates including: - Support
+new datasets: Human-Art, Animal Kingdom and LaPa. - Support new config type
+that is more user-friendly and flexible. - Improve RTMPose with better
+performance. - Migrate 3D pose estimation models on h36m. - Inference speedup
+and webcam inference with all demo scripts. Please refer to the [release notes]
+(https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) for more updates
+brought by MMPose v1.1.0! ## 0.x / 1.x Migration MMPose v1.0.0 is a major
+update, including many API and config file changes. Currently, a part of the
+algorithms have been migrated to v1.0.0, and the remaining algorithms will be
+completed in subsequent versions. We will show the migration progress in the
+following list.  Migration Progress | Algorithm | Status | | :-----------------
+--------------- | :---------: | | MTUT (CVPR 2019) | | | MSPN (ArXiv 2019) |
+done | | InterNet (ECCV 2020) | | | DEKR (CVPR 2021) | done | | HigherHRNet
+(CVPR 2020) | | | DeepPose (CVPR 2014) | done | | RLE (ICCV 2021) | done | |
+SoftWingloss (TIP 2021) | done | | VideoPose3D (CVPR 2019) | done | | Hourglass
+(ECCV 2016) | done | | LiteHRNet (CVPR 2021) | done | | AdaptiveWingloss (ICCV
+2019) | done | | SimpleBaseline2D (ECCV 2018) | done | | PoseWarper (NeurIPS
+2019) | | | SimpleBaseline3D (ICCV 2017) | done | | HMR (CVPR 2018) | | | UDP
+(CVPR 2020) | done | | VIPNAS (CVPR 2021) | done | | Wingloss (CVPR 2018) |
+done | | DarkPose (CVPR 2020) | done | | Associative Embedding (NIPS 2017) | in
+progress | | VoxelPose (ECCV 2020) | | | RSN (ECCV 2020) | done | | CID (CVPR
+2022) | done | | CPM (CVPR 2016) | done | | HRNet (CVPR 2019) | done | |
+HRNetv2 (TPAMI 2019) | done | | SCNet (CVPR 2020) | done |  If your algorithm
+has not been migrated, you can continue to use the [0.x branch](https://
+github.com/open-mmlab/mmpose/tree/0.x) and [old documentation](https://
+mmpose.readthedocs.io/en/0.x/). ## Installation Please refer to
+[installation.md](https://mmpose.readthedocs.io/en/latest/installation.html)
+for more detailed installation and dataset preparation. ## Getting Started We
+provided a series of tutorials about the basic usage of MMPose for new users:
+1. For the basic usage of MMPose: - [A 20-minute Tour to MMPose](https://
+mmpose.readthedocs.io/en/latest/guide_to_framework.html) - [Demos](https://
+mmpose.readthedocs.io/en/latest/demos.html) - [Inference](https://
+mmpose.readthedocs.io/en/latest/user_guides/inference.html) - [Configs](https:/
+/mmpose.readthedocs.io/en/latest/user_guides/configs.html) - [Prepare Datasets]
+(https://mmpose.readthedocs.io/en/latest/user_guides/prepare_datasets.html) -
+[Train and Test](https://mmpose.readthedocs.io/en/latest/user_guides/
+train_and_test.html) 2. For developers who wish to develop based on MMPose: -
+[Learn about Codecs](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+codecs.html) - [Dataflow in MMPose](https://mmpose.readthedocs.io/en/latest/
+advanced_guides/dataflow.html) - [Implement New Models](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/implement_new_models.html) -
+[Customize Datasets](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+customize_datasets.html) - [Customize Data Transforms](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/customize_transforms.html) -
+[Customize Optimizer](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+customize_optimizer.html) - [Customize Logging](https://mmpose.readthedocs.io/
+en/latest/advanced_guides/customize_logging.html) - [How to Deploy](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/how_to_deploy.html) - [Model
+Analysis](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+model_analysis.html) - [Migration Guide](https://mmpose.readthedocs.io/en/
+latest/migration.html) 3. For researchers and developers who are willing to
+contribute to MMPose: - [Contribution Guide](https://mmpose.readthedocs.io/en/
+latest/contribution_guide.html) 4. For some common issues, we provide a FAQ
+list: - [FAQ](https://mmpose.readthedocs.io/en/latest/faq.html) ## Model Zoo
+Results and models are available in the **README.md** of each method's config
+directory. A summary can be found in the [Model Zoo](https://
+mmpose.readthedocs.io/en/latest/model_zoo.html) page.  Supported algorithms: -
+[x] [DeepPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#deeppose-cvpr-2014) (CVPR'2014) - [x] [CPM](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#cpm-cvpr-2016)
-(CVPR'2016) - [x] [Hourglass](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016) - [ ]
-[SimpleBaseline3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#cpm-cvpr-2016)
+(CVPR'2016) - [x] [Hourglass](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016) - [x]
+[SimpleBaseline3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017) - [ ] [Associative
-Embedding](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+Embedding](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017) - [x]
-[SimpleBaseline2D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[SimpleBaseline2D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018) - [x] [DSNT](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#dsnt-2018)
-(ArXiv'2021) - [x] [HRNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#dsnt-2018)
+(ArXiv'2021) - [x] [HRNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019) - [x] [IPR](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#ipr-eccv-2018)
-(ECCV'2018) - [ ] [VideoPose3D](https://mmpose.readthedocs.io/en/1.x/
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#ipr-eccv-
+2018) (ECCV'2018) - [x] [VideoPose3D](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019) - [x]
-[HRNetv2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[HRNetv2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 backbones.html#hrnetv2-tpami-2019) (TPAMI'2019) - [x] [MSPN](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mspn-arxiv-2019)
-(ArXiv'2019) - [x] [SCNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mspn-arxiv-
+2019) (ArXiv'2019) - [x] [SCNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020) - [ ]
-[HigherHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[HigherHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 backbones.html#higherhrnet-cvpr-2020) (CVPR'2020) - [x] [RSN](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#rsn-eccv-2020)
-(ECCV'2020) - [ ] [InterNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#rsn-eccv-2020)
+(ECCV'2020) - [ ] [InterNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020) - [ ]
-[VoxelPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[VoxelPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#voxelpose-eccv-2020) (ECCV'2020) - [x] [LiteHRNet](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#litehrnet-cvpr-
-2021) (CVPR'2021) - [x] [ViPNAS](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#litehrnet-cvpr-
+2021) (CVPR'2021) - [x] [ViPNAS](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021) - [x] [Debias-
-IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#debias-ipr-iccv-2021) (ICCV'2021) - [x] [SimCC](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simcc-eccv-2022)
-(ECCV'2022)   Supported techniques: - [ ] [FPN](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017) - [ ] [FP16]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fp16-
-arxiv-2017) (ArXiv'2017) - [ ] [Wingloss](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018) - [ ]
-[AdaptiveWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simcc-eccv-
+2022) (ECCV'2022)   Supported techniques: - [x] [FPN](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fpn-cvpr-2017)
+(CVPR'2017) - [x] [FP16](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017) - [x] [Wingloss]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+techniques.html#wingloss-cvpr-2018) (CVPR'2018) - [x] [AdaptiveWingloss](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/
 techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019) - [x] [DarkPose](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#darkpose-cvpr-
-2020) (CVPR'2020) - [x] [UDP](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020) - [ ]
-[Albumentations](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-techniques.html#albumentations-information-2020) (Information'2020) - [ ]
-[SoftWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#darkpose-
+cvpr-2020) (CVPR'2020) - [x] [UDP](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020) - [x]
+[Albumentations](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+techniques.html#albumentations-information-2020) (Information'2020) - [x]
+[SoftWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 techniques.html#softwingloss-tip-2021) (TIP'2021) - [x] [RLE](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#rle-iccv-2021)
+mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#rle-iccv-2021)
 (ICCV'2021)   Supported datasets: - [x] [AFLW](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://
+en/latest/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://
 www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\]
-(ICCVW'2011) - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/1.x/
+(ICCVW'2011) - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://
 jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013) - [x] [COFW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cofw-iccv-2013) \[
-[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013) - [x]
-[MPII](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cofw-iccv-2013)
+\[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013) -
+[x] [MPII](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\]
-(CVPR'2014) - [x] [Human3.6M](https://mmpose.readthedocs.io/en/1.x/
+(CVPR'2014) - [x] [Human3.6M](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://
 vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014) - [x] [COCO](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-eccv-2014) \[
-[homepage](http://cocodataset.org/)\] (ECCV'2014) - [x] [CMU Panoptic](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-iccv-
-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015) - [x]
-[DeepFashion](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-eccv-2014)
+\[[homepage](http://cocodataset.org/)\] (ECCV'2014) - [x] [CMU Panoptic](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-
+iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015) -
+[x] [DeepFashion](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/
 projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016) - [x] [300W](https:/
-/mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#300w-imavis-2016)
-\[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016) - [x]
-[RHD](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#rhd-
-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/
-datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017) - [x] [CMU Panoptic
-HandDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://
-domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017) - [x] [AI Challenger]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ai-
-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/
-AI_Challenger_2017)\] (ArXiv'2017) - [x] [MHP](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-
-mhp.github.io/dataset)\] (ACM MM'2018) - [x] [WFLW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[
-[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018) - [x]
-[PoseTrack18](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/
-download.php)\] (CVPR'2018) - [x] [OCHuman](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://
-github.com/liruilong940607/OCHumanApi)\] (CVPR'2019) - [x] [CrowdPose](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#crowdpose-cvpr-
-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019) - [x]
-[MPII-TRB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/
-kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019) - [x]
-[FreiHand](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+/mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#300w-imavis-
+2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
+- [x] [RHD](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-
+freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017) -
+[x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage]
+(http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017) - [x] [AI
+Challenger](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/
+AIChallenger/AI_Challenger_2017)\] (ArXiv'2017) - [x] [MHP](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mhp-acm-mm-2018)
+\[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018) - [x] [WFLW]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#wflw-
+cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\]
+(CVPR'2018) - [x] [PoseTrack18](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://
+posetrack.net/users/download.php)\] (CVPR'2018) - [x] [OCHuman](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ochuman-cvpr-
+2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
+- [x] [CrowdPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/
+CrowdPose)\] (CVPR'2019) - [x] [MPII-TRB](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://
+github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019) -
+[x] [FreiHand](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-
 freiburg.de/projects/freihand/)\] (ICCV'2019) - [x] [Animal-Pose](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#animal-pose-iccv-
-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019) -
-[x] [OneHand10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/
-papers/WANG-MCC-2018-10.html)\] (TCSVT'2019) - [x] [Vinegar Fly](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#vinegar-fly-nature-
-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\]
-(Nature Methods'2019) - [x] [Desert Locust](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage]
-(https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [Grvys
-Zebra](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/
-jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [ATRW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \
-[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020) - [x]
-[Halpe](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#animal-pose-
+iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\]
+(ICCV'2019) - [x] [OneHand10K](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://
+www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019) - [x] [Vinegar
+Fly](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/
+jgraving/DeepPoseKit-Data)\] (Nature Methods'2019) - [x] [Desert Locust](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#desert-locust-
+elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\]
+(Elife'2019) - [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://
+github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [ATRW](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#atrw-acm-mm-
+2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020) -
+[x] [Halpe](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/
 Halpe-FullBody/)\] (CVPR'2020) - [x] [COCO-WholeBody](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-wholebody-
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-wholebody-
 eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\]
-(ECCV'2020) - [x] [MacaquePose](https://mmpose.readthedocs.io/en/1.x/
+(ECCV'2020) - [x] [MacaquePose](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://
 www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020) - [x]
-[InterHand2.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[InterHand2.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/
 InterHand2.6M/)\] (ECCV'2020) - [x] [AP-10K](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://
-github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021) - [x] [Horse-10](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#horse-10-wacv-2021)
-\[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
-Supported backbones: - [x] [AlexNet](https://mmpose.readthedocs.io/en/1.x/
+latest/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https:/
+/github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021) - [x] [Horse-10](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#horse-10-wacv-
+2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021) -
+[x] [Human-Art](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#human-art-cvpr-2023) \[[homepage](https://idea-
+research.github.io/HumanArt/)\] (CVPR'2023) - [x] [LaPa](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#lapa-aaai-2020)
+\[[homepage](https://github.com/JDAI-CV/lapa-dataset)\] (AAAI'2020)   Supported
+backbones: - [x] [AlexNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012) - [x]
-[VGG](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vgg-
-iclr-2015) (ICLR'2015) - [x] [ResNet](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016) - [x] [ResNext]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnext-
-cvpr-2017) (CVPR'2017) - [x] [SEResNet](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018) - [x]
-[ShufflenetV1](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018) - [x] [ShufflenetV2](https:/
-/mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv2-
-eccv-2018) (ECCV'2018) - [x] [MobilenetV2](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018) - [x]
-[ResNetV1D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-backbones.html#resnetv1d-cvpr-2019) (CVPR'2019) - [x] [ResNeSt](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnest-arxiv-
-2020) (ArXiv'2020) - [x] [Swin](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021) - [x] [HRFormer]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrformer-
-nips-2021) (NIPS'2021) - [x] [PVT](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021) - [x] [PVTV2](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvtv2-cvmj-2022)
-(CVMJ'2022)  ### Model Request We will keep up with the latest progress of the
-community, and support more popular algorithms and frameworks. If you have any
-feature requests, please feel free to leave a comment in [MMPose Roadmap]
-(https://github.com/open-mmlab/mmpose/issues/9). ## Contributing We appreciate
-all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://
-mmpose.readthedocs.io/en/1.x/notes/contribution_guide.html) for the
-contributing guideline. ## Acknowledgement MMPose is an open source project
-that is contributed by researchers and engineers from various colleges and
-companies. We appreciate all the contributors who implement their methods or
-add new features, as well as users who give valuable feedbacks. We wish that
-the toolbox and benchmark could serve the growing research community by
-providing a flexible toolkit to reimplement existing methods and develop their
-own new models. ## Citation If you find this project useful in your research,
-please consider cite: ```bibtex @misc{mmpose2020, title={OpenMMLab Pose
-Estimation Toolbox and Benchmark}, author={MMPose Contributors}, howpublished =
-{\url{https://github.com/open-mmlab/mmpose}}, year={2020} } ``` ## License This
-project is released under the [Apache 2.0 license](LICENSE). ## Projects in
-OpenMMLab - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab
-foundational library for training deep learning models. - [MMCV](https://
-github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer
-vision. - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab
-packages. - [MMClassification](https://github.com/open-mmlab/mmclassification):
-OpenMMLab image classification toolbox and benchmark. - [MMDetection](https://
-github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
-- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's
-next-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
-benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
-OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
-github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
-understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
-OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
-github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
-and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
-OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
-github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
-benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
-fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
-benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
-video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment Framework.
-Keywords: computer vision,pose estimation Platform: UNKNOWN Classifier:
-Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Programming Language :: Python :: 3 Classifier: Programming Language :: Python
-:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
-Programming Language :: Python :: 3.9 Requires-Python: >=3.7 Description-
-Content-Type: text/markdown Provides-Extra: all Provides-Extra: tests Provides-
-Extra: optional Provides-Extra: mim
+[VGG](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#vgg-iclr-2015) (ICLR'2015) - [x] [ResNet](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnet-cvpr-
+2016) (CVPR'2016) - [x] [ResNext](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017) - [x] [SEResNet]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#seresnet-cvpr-2018) (CVPR'2018) - [x] [ShufflenetV1](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv1-
+cvpr-2018) (CVPR'2018) - [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018) -
+[x] [MobilenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018) - [x] [ResNetV1D](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnetv1d-cvpr-
+2019) (CVPR'2019) - [x] [ResNeSt](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020) - [x] [Swin]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#swin-
+cvpr-2021) (CVPR'2021) - [x] [HRFormer](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021) - [x]
+[PVT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#pvt-iccv-2021) (ICCV'2021) - [x] [PVTV2](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvtv2-cvmj-
+2022) (CVMJ'2022)  ### Model Request We will keep up with the latest progress
+of the community, and support more popular algorithms and frameworks. If you
+have any feature requests, please feel free to leave a comment in [MMPose
+Roadmap](https://github.com/open-mmlab/mmpose/issues/2258). ## Contributing We
+appreciate all contributions to improve MMPose. Please refer to
+[CONTRIBUTING.md](https://mmpose.readthedocs.io/en/latest/
+contribution_guide.html) for the contributing guideline. ## Acknowledgement
+MMPose is an open source project that is contributed by researchers and
+engineers from various colleges and companies. We appreciate all the
+contributors who implement their methods or add new features, as well as users
+who give valuable feedbacks. We wish that the toolbox and benchmark could serve
+the growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their own new models. ## Citation If you find this
+project useful in your research, please consider cite: ```bibtex @misc
+{mmpose2020, title={OpenMMLab Pose Estimation Toolbox and Benchmark}, author=
+{MMPose Contributors}, howpublished = {\url{https://github.com/open-mmlab/
+mmpose}}, year={2020} } ``` ## License This project is released under the
+[Apache 2.0 license](LICENSE). ## Projects in OpenMMLab - [MMEngine](https://
+github.com/open-mmlab/mmengine): OpenMMLab foundational library for training
+deep learning models. - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab
+foundational library for computer vision. - [MMPreTrain](https://github.com/
+open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark. -
+[MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced,
+**G**enerative and **I**ntelligent **C**reation toolbox. - [MMDetection](https:
+//github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and
+benchmark. - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d):
+OpenMMLab's next-generation platform for general 3D object detection. -
+[MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object
+detection toolbox and benchmark. - [MMTracking](https://github.com/open-mmlab/
+mmtracking): OpenMMLab video perception toolbox and benchmark. -
+[MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab
+semantic segmentation toolbox and benchmark. - [MMOCR](https://github.com/open-
+mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
+- [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation
+toolbox and benchmark. - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d):
+OpenMMLab 3D human parametric model toolbox and benchmark. - [MMFewShot](https:
+//github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and
+benchmark. - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's
+next-generation action understanding toolbox and benchmark. - [MMFlow](https://
+github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment
+Framework. - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model
+compression toolbox and benchmark. - [MIM](https://github.com/open-mmlab/mim):
+MIM installs OpenMMLab packages. - [Playground](https://github.com/open-mmlab/
+playground): A central hub for gathering and showcasing amazing projects built
+upon OpenMMLab. Keywords: computer vision,pose estimation Platform: UNKNOWN
+Classifier: Development Status :: 4 - Beta Classifier: License :: OSI Approved
+:: Apache Software License Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3 Classifier: Programming
+Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9 Requires-Python: >=3.7
+Description-Content-Type: text/markdown Provides-Extra: all Provides-Extra:
+tests Provides-Extra: optional Provides-Extra: mim
```

### Comparing `mmpose-1.0.0rc1/README.md` & `mmpose-1.1.0/README.md`

 * *Files 19% similar despite different names*

```diff
@@ -14,66 +14,72 @@
       <a href="https://platform.openmmlab.com">
         <i>TRY IT OUT</i>
       </a>
     </sup>
   </div>
   <div>&nbsp;</div>
 
-[![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/1.x/?badge=latest)
+[![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest)
 [![actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)](https://github.com/open-mmlab/mmpose/actions)
-[![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/1.x/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose)
+[![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/latest/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose)
 [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/project/mmpose/)
-[![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE)
+[![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/main/LICENSE)
 [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
 [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
 
-[Documentation](https://mmpose.readthedocs.io/en/1.x/) |
-[Installation](https://mmpose.readthedocs.io/en/1.x/installation.html) |
-[Model Zoo](https://mmpose.readthedocs.io/en/1.x/model_zoo.html) |
-[Papers](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html) |
-[Update News](https://mmpose.readthedocs.io/en/1.x/notes/changelog.html) |
+[Documentation](https://mmpose.readthedocs.io/en/latest/) |
+[Installation](https://mmpose.readthedocs.io/en/latest/installation.html) |
+[Model Zoo](https://mmpose.readthedocs.io/en/latest/model_zoo.html) |
+[Papers](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html) |
+[Update News](https://mmpose.readthedocs.io/en/latest/notes/changelog.html) |
 [Reporting Issues](https://github.com/open-mmlab/mmpose/issues/new/choose) |
 [RTMPose](/projects/rtmpose/)
 
 </div>
 
 <div align="center">
   <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
-    <img src="https://user-images.githubusercontent.com/25839884/218352562-cdded397-b0f3-4ca1-b8dd-a60df8dca75b.png" width="3%" alt="" /></a>
+    <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
   <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
-  <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+  <a href="https://discord.com/channels/1037617289144569886/1072798105428299817" style="text-decoration:none;">
     <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
   <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
   <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
     <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
   <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
   <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
     <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+  <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+  <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+    <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
 </div>
 
 ## Introduction
 
 English | [](README_CN.md)
 
 MMPose is an open-source toolbox for pose estimation based on PyTorch.
 It is a part of the [OpenMMLab project](https://github.com/open-mmlab).
 
-The master branch works with **PyTorch 1.6+**.
+The main branch works with **PyTorch 1.8+**.
 
 https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-ded1-11eb-84f6-24eeddbf4d91.mp4
 
 <br/>
 
 <details close>
 <summary><b>Major Features</b></summary>
 
 - **Support diverse tasks**
 
   We support a wide spectrum of mainstream pose analysis tasks in current research community, including 2d multi-person human pose estimation, 2d hand pose estimation, 2d face landmark detection, 133 keypoint whole-body human pose estimation, 3d human mesh recovery, fashion landmark detection and animal pose estimation.
-  See [Demo](demo/docs/) for more information.
+  See [Demo](demo/docs/en) for more information.
 
 - **Higher efficiency and higher accuracy**
 
   MMPose implements multiple state-of-the-art (SOTA) deep learning models, including both top-down & bottom-up approaches. We achieve faster training speed and higher accuracy than other popular codebases, such as [HRNet](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch).
   See [benchmark.md](docs/en/notes/benchmark.md) for more information.
 
 - **Support for various datasets**
@@ -87,184 +93,238 @@
   pose estimation framework by combining different modules.
   We provide detailed documentation and API reference, as well as unittests.
 
 </details>
 
 ## What's New
 
-- We are excited to release **RTMPose**, a real-time pose estimation framework including:
-
-  - A family of lightweight pose estimation models with state-of-the-art performance
-  - Inference APIs for Python, C++, C#, Java, etc. Easy to integrate into your applications and empower real-time stable pose estimation
-  - Cross-platform deployment with various backends
-  - A step-by-step guide to training and deploying your own models
-
-  Checkout our [project page](/projects/rtmpose/) and [technical report](https://arxiv.org/abs/2303.07399) for more information!
+- We are glad to support 3 new datasets:
+  - (CVPR 2023) [Human-Art](https://github.com/IDEA-Research/HumanArt)
+  - (CVPR 2022) [Animal Kingdom](https://github.com/sutdcv/Animal-Kingdom)
+  - (AAAI 2020) [LaPa](https://github.com/JDAI-CV/lapa-dataset/)
 
-![rtmpose_intro](https://user-images.githubusercontent.com/13503330/219269619-935499e5-bdd9-49ea-8104-3c7796dbd862.png)
+![image](https://github.com/open-mmlab/mmpose/assets/13503330/c9171dbb-7e7a-4c39-98e3-c92932182efb)
 
 - Welcome to [*projects of MMPose*](/projects/README.md), where you can access to the latest features of MMPose, and share your ideas and codes with the community at once. Contribution to MMPose will be simple and smooth:
 
   - Provide an easy and agile way to integrate algorithms, features and applications into MMPose
   - Allow flexible code structure and style; only need a short code review process
   - Build individual projects with full power of MMPose but not bound up with heavy frameworks
   - Checkout new projects:
     - [RTMPose](/projects/rtmpose/)
-    - [YOLOX-Pose (coming soon)](<>)
-    - [MMPose4AIGC (coming soon)](<>)
+    - [YOLOX-Pose](/projects/yolox_pose/)
+    - [MMPose4AIGC](/projects/mmpose4aigc/)
+    - [Simple Keypoints](/projects/skps/)
   - Become a contributors and make MMPose greater. Start your journey from the [example project](/projects/example_project/)
 
 <br/>
 
-- 2022-03-15: MMPose [v1.0.0rc1](https://github.com/open-mmlab/mmpose/releases/tag/v1.0.0rc1) is released. Major updates include:
+- 2023-07-04: MMPose [v1.1.0](https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) is officially released, with the main updates including:
 
-  - Release [RTMPose](/projects/rtmpose/), a high-performance real-time pose estimation framework based on MMPose
-  - Support [ViTPose](/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.md) (NeurIPS'22), [CID](/configs/body_2d_keypoint/cid/coco/hrnet_coco.md) (CVPR'22) and [DEKR](/configs/body_2d_keypoint/dekr/) (CVPR'21)
-  - Add [*Inferencer*](/docs/en/user_guides/inference.md#out-of-the-box-inferencer), a convenient interface for inference and visualization
+  - Support new datasets: Human-Art, Animal Kingdom and LaPa.
+  - Support new config type that is more user-friendly and flexible.
+  - Improve RTMPose with better performance.
+  - Migrate 3D pose estimation models on h36m.
+  - Inference speedup and webcam inference with all demo scripts.
 
-  See the full [release note](https://github.com/open-mmlab/mmpose/releases/tag/v1.0.0rc1) for more exciting updates brought by MMPose v1.0.0rc1!
+  Please refer to the [release notes](https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) for more updates brought by MMPose v1.1.0!
 
-## Installation
+## 0.x / 1.x Migration
 
-Below are quick steps for installation:
+MMPose v1.0.0 is a major update, including many API and config file changes. Currently, a part of the algorithms have been migrated to v1.0.0, and the remaining algorithms will be completed in subsequent versions. We will show the migration progress in the following list.
 
-```shell
-conda create -n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y
-conda activate open-mmlab
-pip install openmim
-git clone -b 1.x https://github.com/open-mmlab/mmpose.git
-cd mmpose
-mim install -e .
-```
+<details close>
+<summary><b>Migration Progress</b></summary>
 
-Please refer to [installation.md](https://mmpose.readthedocs.io/en/1.x/installation.html) for more detailed installation and dataset preparation.
+| Algorithm                         |   Status    |
+| :-------------------------------- | :---------: |
+| MTUT (CVPR 2019)                  |             |
+| MSPN (ArXiv 2019)                 |    done     |
+| InterNet (ECCV 2020)              |             |
+| DEKR (CVPR 2021)                  |    done     |
+| HigherHRNet (CVPR 2020)           |             |
+| DeepPose (CVPR 2014)              |    done     |
+| RLE (ICCV 2021)                   |    done     |
+| SoftWingloss (TIP 2021)           |    done     |
+| VideoPose3D (CVPR 2019)           |    done     |
+| Hourglass (ECCV 2016)             |    done     |
+| LiteHRNet (CVPR 2021)             |    done     |
+| AdaptiveWingloss (ICCV 2019)      |    done     |
+| SimpleBaseline2D (ECCV 2018)      |    done     |
+| PoseWarper (NeurIPS 2019)         |             |
+| SimpleBaseline3D (ICCV 2017)      |    done     |
+| HMR (CVPR 2018)                   |             |
+| UDP (CVPR 2020)                   |    done     |
+| VIPNAS (CVPR 2021)                |    done     |
+| Wingloss (CVPR 2018)              |    done     |
+| DarkPose (CVPR 2020)              |    done     |
+| Associative Embedding (NIPS 2017) | in progress |
+| VoxelPose (ECCV 2020)             |             |
+| RSN (ECCV 2020)                   |    done     |
+| CID (CVPR 2022)                   |    done     |
+| CPM (CVPR 2016)                   |    done     |
+| HRNet (CVPR 2019)                 |    done     |
+| HRNetv2 (TPAMI 2019)              |    done     |
+| SCNet (CVPR 2020)                 |    done     |
+
+</details>
+
+If your algorithm has not been migrated, you can continue to use the [0.x branch](https://github.com/open-mmlab/mmpose/tree/0.x) and [old documentation](https://mmpose.readthedocs.io/en/0.x/).
+
+## Installation
+
+Please refer to [installation.md](https://mmpose.readthedocs.io/en/latest/installation.html) for more detailed installation and dataset preparation.
 
 ## Getting Started
 
 We provided a series of tutorials about the basic usage of MMPose for new users:
 
-- [About Configs](https://mmpose.readthedocs.io/en/1.x/user_guides/configs.html)
-- [Add New Dataset](https://mmpose.readthedocs.io/en/1.x/user_guides/prepare_datasets.html)
-- [Keypoint Encoding & Decoding](https://mmpose.readthedocs.io/en/1.x/user_guides/codecs.html)
-- [Inference with Existing Models](https://mmpose.readthedocs.io/en/1.x/user_guides/inference.html)
-- [Train and Test](https://mmpose.readthedocs.io/en/1.x/user_guides/train_and_test.html)
-- [Visualization Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/visualization.html)
-- [Other Useful Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/useful_tools.html)
+1. For the basic usage of MMPose:
+
+   - [A 20-minute Tour to MMPose](https://mmpose.readthedocs.io/en/latest/guide_to_framework.html)
+   - [Demos](https://mmpose.readthedocs.io/en/latest/demos.html)
+   - [Inference](https://mmpose.readthedocs.io/en/latest/user_guides/inference.html)
+   - [Configs](https://mmpose.readthedocs.io/en/latest/user_guides/configs.html)
+   - [Prepare Datasets](https://mmpose.readthedocs.io/en/latest/user_guides/prepare_datasets.html)
+   - [Train and Test](https://mmpose.readthedocs.io/en/latest/user_guides/train_and_test.html)
+
+2. For developers who wish to develop based on MMPose:
+
+   - [Learn about Codecs](https://mmpose.readthedocs.io/en/latest/advanced_guides/codecs.html)
+   - [Dataflow in MMPose](https://mmpose.readthedocs.io/en/latest/advanced_guides/dataflow.html)
+   - [Implement New Models](https://mmpose.readthedocs.io/en/latest/advanced_guides/implement_new_models.html)
+   - [Customize Datasets](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_datasets.html)
+   - [Customize Data Transforms](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_transforms.html)
+   - [Customize Optimizer](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_optimizer.html)
+   - [Customize Logging](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_logging.html)
+   - [How to Deploy](https://mmpose.readthedocs.io/en/latest/advanced_guides/how_to_deploy.html)
+   - [Model Analysis](https://mmpose.readthedocs.io/en/latest/advanced_guides/model_analysis.html)
+   - [Migration Guide](https://mmpose.readthedocs.io/en/latest/migration.html)
+
+3. For researchers and developers who are willing to contribute to MMPose:
+
+   - [Contribution Guide](https://mmpose.readthedocs.io/en/latest/contribution_guide.html)
+
+4. For some common issues, we provide a FAQ list:
+
+   - [FAQ](https://mmpose.readthedocs.io/en/latest/faq.html)
 
 ## Model Zoo
 
 Results and models are available in the **README.md** of each method's config directory.
-A summary can be found in the [Model Zoo](https://mmpose.readthedocs.io/en/1.x/modelzoo.html) page.
+A summary can be found in the [Model Zoo](https://mmpose.readthedocs.io/en/latest/model_zoo.html) page.
 
-<details open>
+<details close>
 <summary><b>Supported algorithms:</b></summary>
 
-- [x] [DeepPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#deeppose-cvpr-2014) (CVPR'2014)
-- [x] [CPM](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#cpm-cvpr-2016) (CVPR'2016)
-- [x] [Hourglass](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016)
-- [ ] [SimpleBaseline3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017)
-- [ ] [Associative Embedding](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017)
-- [x] [SimpleBaseline2D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018)
-- [x] [DSNT](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#dsnt-2018) (ArXiv'2021)
-- [x] [HRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019)
-- [x] [IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#ipr-eccv-2018) (ECCV'2018)
-- [ ] [VideoPose3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019)
-- [x] [HRNetv2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrnetv2-tpami-2019) (TPAMI'2019)
-- [x] [MSPN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mspn-arxiv-2019) (ArXiv'2019)
-- [x] [SCNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020)
-- [ ] [HigherHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#higherhrnet-cvpr-2020) (CVPR'2020)
-- [x] [RSN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#rsn-eccv-2020) (ECCV'2020)
-- [ ] [InterNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020)
-- [ ] [VoxelPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#voxelpose-eccv-2020) (ECCV'2020)
-- [x] [LiteHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#litehrnet-cvpr-2021) (CVPR'2021)
-- [x] [ViPNAS](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021)
-- [x] [Debias-IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#debias-ipr-iccv-2021) (ICCV'2021)
-- [x] [SimCC](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simcc-eccv-2022) (ECCV'2022)
+- [x] [DeepPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#deeppose-cvpr-2014) (CVPR'2014)
+- [x] [CPM](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#cpm-cvpr-2016) (CVPR'2016)
+- [x] [Hourglass](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016)
+- [x] [SimpleBaseline3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017)
+- [ ] [Associative Embedding](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017)
+- [x] [SimpleBaseline2D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018)
+- [x] [DSNT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#dsnt-2018) (ArXiv'2021)
+- [x] [HRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019)
+- [x] [IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#ipr-eccv-2018) (ECCV'2018)
+- [x] [VideoPose3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019)
+- [x] [HRNetv2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrnetv2-tpami-2019) (TPAMI'2019)
+- [x] [MSPN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mspn-arxiv-2019) (ArXiv'2019)
+- [x] [SCNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020)
+- [ ] [HigherHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#higherhrnet-cvpr-2020) (CVPR'2020)
+- [x] [RSN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#rsn-eccv-2020) (ECCV'2020)
+- [ ] [InterNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020)
+- [ ] [VoxelPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#voxelpose-eccv-2020) (ECCV'2020)
+- [x] [LiteHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#litehrnet-cvpr-2021) (CVPR'2021)
+- [x] [ViPNAS](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021)
+- [x] [Debias-IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#debias-ipr-iccv-2021) (ICCV'2021)
+- [x] [SimCC](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simcc-eccv-2022) (ECCV'2022)
 
 </details>
 
-<details open>
+<details close>
 <summary><b>Supported techniques:</b></summary>
 
-- [ ] [FPN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017)
-- [ ] [FP16](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017)
-- [ ] [Wingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018)
-- [ ] [AdaptiveWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019)
-- [x] [DarkPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#darkpose-cvpr-2020) (CVPR'2020)
-- [x] [UDP](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020)
-- [ ] [Albumentations](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#albumentations-information-2020) (Information'2020)
-- [ ] [SoftWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#softwingloss-tip-2021) (TIP'2021)
-- [x] [RLE](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#rle-iccv-2021) (ICCV'2021)
+- [x] [FPN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017)
+- [x] [FP16](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017)
+- [x] [Wingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018)
+- [x] [AdaptiveWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019)
+- [x] [DarkPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#darkpose-cvpr-2020) (CVPR'2020)
+- [x] [UDP](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020)
+- [x] [Albumentations](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#albumentations-information-2020) (Information'2020)
+- [x] [SoftWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#softwingloss-tip-2021) (TIP'2021)
+- [x] [RLE](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#rle-iccv-2021) (ICCV'2021)
 
 </details>
 
-<details open>
-<summary><b>Supported <a href="https://mmpose.readthedocs.io/en/1.x/dataset_zoo.html">datasets</a>:</b></summary>
+<details close>
+<summary><b>Supported datasets:</b></summary>
 
-- [x] [AFLW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\] (ICCVW'2011)
-- [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013)
-- [x] [COFW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cofw-iccv-2013) \[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013)
-- [x] [MPII](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\] (CVPR'2014)
-- [x] [Human3.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014)
-- [x] [COCO](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-eccv-2014) \[[homepage](http://cocodataset.org/)\] (ECCV'2014)
-- [x] [CMU Panoptic](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015)
-- [x] [DeepFashion](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016)
-- [x] [300W](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#300w-imavis-2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
-- [x] [RHD](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017)
-- [x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017)
-- [x] [AI Challenger](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/AI_Challenger_2017)\] (ArXiv'2017)
-- [x] [MHP](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018)
-- [x] [WFLW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018)
-- [x] [PoseTrack18](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/download.php)\] (CVPR'2018)
-- [x] [OCHuman](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
-- [x] [CrowdPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019)
-- [x] [MPII-TRB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019)
-- [x] [FreiHand](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-freiburg.de/projects/freihand/)\] (ICCV'2019)
-- [x] [Animal-Pose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#animal-pose-iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019)
-- [x] [OneHand10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019)
-- [x] [Vinegar Fly](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Nature Methods'2019)
-- [x] [Desert Locust](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
-- [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
-- [x] [ATRW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020)
-- [x] [Halpe](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/Halpe-FullBody/)\] (CVPR'2020)
-- [x] [COCO-WholeBody](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-wholebody-eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\] (ECCV'2020)
-- [x] [MacaquePose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020)
-- [x] [InterHand2.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/InterHand2.6M/)\] (ECCV'2020)
-- [x] [AP-10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021)
-- [x] [Horse-10](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#horse-10-wacv-2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
+- [x] [AFLW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\] (ICCVW'2011)
+- [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013)
+- [x] [COFW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cofw-iccv-2013) \[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013)
+- [x] [MPII](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\] (CVPR'2014)
+- [x] [Human3.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014)
+- [x] [COCO](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-eccv-2014) \[[homepage](http://cocodataset.org/)\] (ECCV'2014)
+- [x] [CMU Panoptic](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015)
+- [x] [DeepFashion](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016)
+- [x] [300W](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#300w-imavis-2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
+- [x] [RHD](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017)
+- [x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017)
+- [x] [AI Challenger](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/AI_Challenger_2017)\] (ArXiv'2017)
+- [x] [MHP](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018)
+- [x] [WFLW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018)
+- [x] [PoseTrack18](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/download.php)\] (CVPR'2018)
+- [x] [OCHuman](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
+- [x] [CrowdPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019)
+- [x] [MPII-TRB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019)
+- [x] [FreiHand](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-freiburg.de/projects/freihand/)\] (ICCV'2019)
+- [x] [Animal-Pose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#animal-pose-iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019)
+- [x] [OneHand10K](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019)
+- [x] [Vinegar Fly](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Nature Methods'2019)
+- [x] [Desert Locust](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
+- [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
+- [x] [ATRW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020)
+- [x] [Halpe](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/Halpe-FullBody/)\] (CVPR'2020)
+- [x] [COCO-WholeBody](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-wholebody-eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\] (ECCV'2020)
+- [x] [MacaquePose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020)
+- [x] [InterHand2.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/InterHand2.6M/)\] (ECCV'2020)
+- [x] [AP-10K](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021)
+- [x] [Horse-10](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#horse-10-wacv-2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
+- [x] [Human-Art](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#human-art-cvpr-2023) \[[homepage](https://idea-research.github.io/HumanArt/)\] (CVPR'2023)
+- [x] [LaPa](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#lapa-aaai-2020) \[[homepage](https://github.com/JDAI-CV/lapa-dataset)\] (AAAI'2020)
 
 </details>
 
-<details open>
+<details close>
 <summary><b>Supported backbones:</b></summary>
 
-- [x] [AlexNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012)
-- [x] [VGG](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vgg-iclr-2015) (ICLR'2015)
-- [x] [ResNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016)
-- [x] [ResNext](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017)
-- [x] [SEResNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018)
-- [x] [ShufflenetV1](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018)
-- [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018)
-- [x] [MobilenetV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018)
-- [x] [ResNetV1D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnetv1d-cvpr-2019) (CVPR'2019)
-- [x] [ResNeSt](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020)
-- [x] [Swin](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021)
-- [x] [HRFormer](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021)
-- [x] [PVT](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021)
-- [x] [PVTV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvtv2-cvmj-2022) (CVMJ'2022)
+- [x] [AlexNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012)
+- [x] [VGG](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#vgg-iclr-2015) (ICLR'2015)
+- [x] [ResNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016)
+- [x] [ResNext](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017)
+- [x] [SEResNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018)
+- [x] [ShufflenetV1](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018)
+- [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018)
+- [x] [MobilenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018)
+- [x] [ResNetV1D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnetv1d-cvpr-2019) (CVPR'2019)
+- [x] [ResNeSt](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020)
+- [x] [Swin](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021)
+- [x] [HRFormer](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021)
+- [x] [PVT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021)
+- [x] [PVTV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvtv2-cvmj-2022) (CVMJ'2022)
 
 </details>
 
 ### Model Request
 
-We will keep up with the latest progress of the community, and support more popular algorithms and frameworks. If you have any feature requests, please feel free to leave a comment in [MMPose Roadmap](https://github.com/open-mmlab/mmpose/issues/9).
+We will keep up with the latest progress of the community, and support more popular algorithms and frameworks. If you have any feature requests, please feel free to leave a comment in [MMPose Roadmap](https://github.com/open-mmlab/mmpose/issues/2258).
 
 ## Contributing
 
-We appreciate all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://mmpose.readthedocs.io/en/1.x/notes/contribution_guide.html) for the contributing guideline.
+We appreciate all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://mmpose.readthedocs.io/en/latest/contribution_guide.html) for the contributing guideline.
 
 ## Acknowledgement
 
 MMPose is an open source project that is contributed by researchers and engineers from various colleges and companies.
 We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks.
 We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new models.
 
@@ -285,25 +345,24 @@
 
 This project is released under the [Apache 2.0 license](LICENSE).
 
 ## Projects in OpenMMLab
 
 - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
 - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
-- [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-- [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
+- [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
+- [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
 - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
 - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
 - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
+- [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
 - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
 - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
 - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
 - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
-- [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
-- [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
 - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
 - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
-- [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
 - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-- [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
-- [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
 - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment Framework.
+- [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
+- [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
+- [Playground](https://github.com/open-mmlab/playground): A central hub for gathering and showcasing amazing projects built upon OpenMMLab.
```

#### html2text {}

```diff
@@ -1,290 +1,323 @@
                           [resources/mmpose-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
        [![Documentation](https://readthedocs.org/projects/mmpose/badge/
-   ?version=latest)](https://mmpose.readthedocs.io/en/1.x/?badge=latest) [!
+  ?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest) [!
   [actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)]
 (https://github.com/open-mmlab/mmpose/actions) [![codecov](https://codecov.io/
- gh/open-mmlab/mmpose/branch/1.x/graph/badge.svg)](https://codecov.io/gh/open-
-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/
-project/mmpose/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/
-   mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE) [!
-[Average time to resolve an issue](https://isitmaintained.com/badge/resolution/
-    open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues) [!
- [Percentage of issues still open](https://isitmaintained.com/badge/open/open-
+  gh/open-mmlab/mmpose/branch/latest/graph/badge.svg)](https://codecov.io/gh/
+  open-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://
+ pypi.org/project/mmpose/) [![LICENSE](https://img.shields.io/github/license/
+open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/main/LICENSE)
+    [![Average time to resolve an issue](https://isitmaintained.com/badge/
+resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
+[![Percentage of issues still open](https://isitmaintained.com/badge/open/open-
        mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
-         [Documentation](https://mmpose.readthedocs.io/en/1.x/) |
-[Installation](https://mmpose.readthedocs.io/en/1.x/installation.html) |
-    [Model Zoo](https://mmpose.readthedocs.io/en/1.x/model_zoo.html) |
-      [Papers](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-  algorithms.html) | [Update News](https://mmpose.readthedocs.io/en/1.x/
- notes/changelog.html) | [Reporting Issues](https://github.com/open-mmlab/
-         mmpose/issues/new/choose) | [RTMPose](/projects/rtmpose/)
+       [Documentation](https://mmpose.readthedocs.io/en/latest/) |
+        [Installation](https://mmpose.readthedocs.io/en/latest/
+ installation.html) | [Model Zoo](https://mmpose.readthedocs.io/en/latest/
+    model_zoo.html) | [Papers](https://mmpose.readthedocs.io/en/latest/
+        model_zoo_papers/algorithms.html) | [Update News](https://
+mmpose.readthedocs.io/en/latest/notes/changelog.html) | [Reporting Issues]
+  (https://github.com/open-mmlab/mmpose/issues/new/choose) | [RTMPose](/
+                              projects/rtmpose/)
 
 ## Introduction English | [](README_CN.md) MMPose is an open-source
 toolbox for pose estimation based on PyTorch. It is a part of the [OpenMMLab
-project](https://github.com/open-mmlab). The master branch works with **PyTorch
-1.6+**. https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-
+project](https://github.com/open-mmlab). The main branch works with **PyTorch
+1.8+**. https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-
 ded1-11eb-84f6-24eeddbf4d91.mp4
  Major Features - **Support diverse tasks** We support a wide spectrum of
 mainstream pose analysis tasks in current research community, including 2d
 multi-person human pose estimation, 2d hand pose estimation, 2d face landmark
 detection, 133 keypoint whole-body human pose estimation, 3d human mesh
 recovery, fashion landmark detection and animal pose estimation. See [Demo]
-(demo/docs/) for more information. - **Higher efficiency and higher accuracy**
-MMPose implements multiple state-of-the-art (SOTA) deep learning models,
-including both top-down & bottom-up approaches. We achieve faster training
-speed and higher accuracy than other popular codebases, such as [HRNet](https:/
-/github.com/leoxiaobin/deep-high-resolution-net.pytorch). See [benchmark.md]
-(docs/en/notes/benchmark.md) for more information. - **Support for various
-datasets** The toolbox directly supports multiple popular and representative
-datasets, COCO, AIC, MPII, MPII-TRB, OCHuman etc. See [dataset_zoo](docs/en/
-dataset_zoo) for more information. - **Well designed, tested and documented**
-We decompose MMPose into different components and one can easily construct a
-customized pose estimation framework by combining different modules. We provide
-detailed documentation and API reference, as well as unittests.  ## What's New
-- We are excited to release **RTMPose**, a real-time pose estimation framework
-including: - A family of lightweight pose estimation models with state-of-the-
-art performance - Inference APIs for Python, C++, C#, Java, etc. Easy to
-integrate into your applications and empower real-time stable pose estimation -
-Cross-platform deployment with various backends - A step-by-step guide to
-training and deploying your own models Checkout our [project page](/projects/
-rtmpose/) and [technical report](https://arxiv.org/abs/2303.07399) for more
-information! ![rtmpose_intro](https://user-images.githubusercontent.com/
-13503330/219269619-935499e5-bdd9-49ea-8104-3c7796dbd862.png) - Welcome to
+(demo/docs/en) for more information. - **Higher efficiency and higher
+accuracy** MMPose implements multiple state-of-the-art (SOTA) deep learning
+models, including both top-down & bottom-up approaches. We achieve faster
+training speed and higher accuracy than other popular codebases, such as
+[HRNet](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch). See
+[benchmark.md](docs/en/notes/benchmark.md) for more information. - **Support
+for various datasets** The toolbox directly supports multiple popular and
+representative datasets, COCO, AIC, MPII, MPII-TRB, OCHuman etc. See
+[dataset_zoo](docs/en/dataset_zoo) for more information. - **Well designed,
+tested and documented** We decompose MMPose into different components and one
+can easily construct a customized pose estimation framework by combining
+different modules. We provide detailed documentation and API reference, as well
+as unittests.  ## What's New - We are glad to support 3 new datasets: - (CVPR
+2023) [Human-Art](https://github.com/IDEA-Research/HumanArt) - (CVPR 2022)
+[Animal Kingdom](https://github.com/sutdcv/Animal-Kingdom) - (AAAI 2020) [LaPa]
+(https://github.com/JDAI-CV/lapa-dataset/) ![image](https://github.com/open-
+mmlab/mmpose/assets/13503330/c9171dbb-7e7a-4c39-98e3-c92932182efb) - Welcome to
 [*projects of MMPose*](/projects/README.md), where you can access to the latest
 features of MMPose, and share your ideas and codes with the community at once.
 Contribution to MMPose will be simple and smooth: - Provide an easy and agile
 way to integrate algorithms, features and applications into MMPose - Allow
 flexible code structure and style; only need a short code review process -
 Build individual projects with full power of MMPose but not bound up with heavy
 frameworks - Checkout new projects: - [RTMPose](/projects/rtmpose/) - [YOLOX-
-Pose (coming soon)](<>) - [MMPose4AIGC (coming soon)](<>) - Become a
-contributors and make MMPose greater. Start your journey from the [example
-project](/projects/example_project/)
-- 2022-03-15: MMPose [v1.0.0rc1](https://github.com/open-mmlab/mmpose/releases/
-tag/v1.0.0rc1) is released. Major updates include: - Release [RTMPose](/
-projects/rtmpose/), a high-performance real-time pose estimation framework
-based on MMPose - Support [ViTPose](/configs/body_2d_keypoint/topdown_heatmap/
-coco/vitpose_coco.md) (NeurIPS'22), [CID](/configs/body_2d_keypoint/cid/coco/
-hrnet_coco.md) (CVPR'22) and [DEKR](/configs/body_2d_keypoint/dekr/) (CVPR'21)
-- Add [*Inferencer*](/docs/en/user_guides/inference.md#out-of-the-box-
-inferencer), a convenient interface for inference and visualization See the
-full [release note](https://github.com/open-mmlab/mmpose/releases/tag/
-v1.0.0rc1) for more exciting updates brought by MMPose v1.0.0rc1! ##
-Installation Below are quick steps for installation: ```shell conda create -
-n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -
-c pytorch -y conda activate open-mmlab pip install openmim git clone -b 1.x
-https://github.com/open-mmlab/mmpose.git cd mmpose mim install -e . ``` Please
-refer to [installation.md](https://mmpose.readthedocs.io/en/1.x/
-installation.html) for more detailed installation and dataset preparation. ##
-Getting Started We provided a series of tutorials about the basic usage of
-MMPose for new users: - [About Configs](https://mmpose.readthedocs.io/en/1.x/
-user_guides/configs.html) - [Add New Dataset](https://mmpose.readthedocs.io/en/
-1.x/user_guides/prepare_datasets.html) - [Keypoint Encoding & Decoding](https:/
-/mmpose.readthedocs.io/en/1.x/user_guides/codecs.html) - [Inference with
-Existing Models](https://mmpose.readthedocs.io/en/1.x/user_guides/
-inference.html) - [Train and Test](https://mmpose.readthedocs.io/en/1.x/
-user_guides/train_and_test.html) - [Visualization Tools](https://
-mmpose.readthedocs.io/en/1.x/user_guides/visualization.html) - [Other Useful
-Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/useful_tools.html) ##
-Model Zoo Results and models are available in the **README.md** of each
-method's config directory. A summary can be found in the [Model Zoo](https://
-mmpose.readthedocs.io/en/1.x/modelzoo.html) page.  Supported algorithms: - [x]
-[DeepPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+Pose](/projects/yolox_pose/) - [MMPose4AIGC](/projects/mmpose4aigc/) - [Simple
+Keypoints](/projects/skps/) - Become a contributors and make MMPose greater.
+Start your journey from the [example project](/projects/example_project/)
+- 2023-07-04: MMPose [v1.1.0](https://github.com/open-mmlab/mmpose/releases/
+tag/v1.1.0) is officially released, with the main updates including: - Support
+new datasets: Human-Art, Animal Kingdom and LaPa. - Support new config type
+that is more user-friendly and flexible. - Improve RTMPose with better
+performance. - Migrate 3D pose estimation models on h36m. - Inference speedup
+and webcam inference with all demo scripts. Please refer to the [release notes]
+(https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) for more updates
+brought by MMPose v1.1.0! ## 0.x / 1.x Migration MMPose v1.0.0 is a major
+update, including many API and config file changes. Currently, a part of the
+algorithms have been migrated to v1.0.0, and the remaining algorithms will be
+completed in subsequent versions. We will show the migration progress in the
+following list.  Migration Progress | Algorithm | Status | | :-----------------
+--------------- | :---------: | | MTUT (CVPR 2019) | | | MSPN (ArXiv 2019) |
+done | | InterNet (ECCV 2020) | | | DEKR (CVPR 2021) | done | | HigherHRNet
+(CVPR 2020) | | | DeepPose (CVPR 2014) | done | | RLE (ICCV 2021) | done | |
+SoftWingloss (TIP 2021) | done | | VideoPose3D (CVPR 2019) | done | | Hourglass
+(ECCV 2016) | done | | LiteHRNet (CVPR 2021) | done | | AdaptiveWingloss (ICCV
+2019) | done | | SimpleBaseline2D (ECCV 2018) | done | | PoseWarper (NeurIPS
+2019) | | | SimpleBaseline3D (ICCV 2017) | done | | HMR (CVPR 2018) | | | UDP
+(CVPR 2020) | done | | VIPNAS (CVPR 2021) | done | | Wingloss (CVPR 2018) |
+done | | DarkPose (CVPR 2020) | done | | Associative Embedding (NIPS 2017) | in
+progress | | VoxelPose (ECCV 2020) | | | RSN (ECCV 2020) | done | | CID (CVPR
+2022) | done | | CPM (CVPR 2016) | done | | HRNet (CVPR 2019) | done | |
+HRNetv2 (TPAMI 2019) | done | | SCNet (CVPR 2020) | done |  If your algorithm
+has not been migrated, you can continue to use the [0.x branch](https://
+github.com/open-mmlab/mmpose/tree/0.x) and [old documentation](https://
+mmpose.readthedocs.io/en/0.x/). ## Installation Please refer to
+[installation.md](https://mmpose.readthedocs.io/en/latest/installation.html)
+for more detailed installation and dataset preparation. ## Getting Started We
+provided a series of tutorials about the basic usage of MMPose for new users:
+1. For the basic usage of MMPose: - [A 20-minute Tour to MMPose](https://
+mmpose.readthedocs.io/en/latest/guide_to_framework.html) - [Demos](https://
+mmpose.readthedocs.io/en/latest/demos.html) - [Inference](https://
+mmpose.readthedocs.io/en/latest/user_guides/inference.html) - [Configs](https:/
+/mmpose.readthedocs.io/en/latest/user_guides/configs.html) - [Prepare Datasets]
+(https://mmpose.readthedocs.io/en/latest/user_guides/prepare_datasets.html) -
+[Train and Test](https://mmpose.readthedocs.io/en/latest/user_guides/
+train_and_test.html) 2. For developers who wish to develop based on MMPose: -
+[Learn about Codecs](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+codecs.html) - [Dataflow in MMPose](https://mmpose.readthedocs.io/en/latest/
+advanced_guides/dataflow.html) - [Implement New Models](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/implement_new_models.html) -
+[Customize Datasets](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+customize_datasets.html) - [Customize Data Transforms](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/customize_transforms.html) -
+[Customize Optimizer](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+customize_optimizer.html) - [Customize Logging](https://mmpose.readthedocs.io/
+en/latest/advanced_guides/customize_logging.html) - [How to Deploy](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/how_to_deploy.html) - [Model
+Analysis](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+model_analysis.html) - [Migration Guide](https://mmpose.readthedocs.io/en/
+latest/migration.html) 3. For researchers and developers who are willing to
+contribute to MMPose: - [Contribution Guide](https://mmpose.readthedocs.io/en/
+latest/contribution_guide.html) 4. For some common issues, we provide a FAQ
+list: - [FAQ](https://mmpose.readthedocs.io/en/latest/faq.html) ## Model Zoo
+Results and models are available in the **README.md** of each method's config
+directory. A summary can be found in the [Model Zoo](https://
+mmpose.readthedocs.io/en/latest/model_zoo.html) page.  Supported algorithms: -
+[x] [DeepPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#deeppose-cvpr-2014) (CVPR'2014) - [x] [CPM](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#cpm-cvpr-2016)
-(CVPR'2016) - [x] [Hourglass](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016) - [ ]
-[SimpleBaseline3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#cpm-cvpr-2016)
+(CVPR'2016) - [x] [Hourglass](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016) - [x]
+[SimpleBaseline3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017) - [ ] [Associative
-Embedding](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+Embedding](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017) - [x]
-[SimpleBaseline2D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[SimpleBaseline2D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018) - [x] [DSNT](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#dsnt-2018)
-(ArXiv'2021) - [x] [HRNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#dsnt-2018)
+(ArXiv'2021) - [x] [HRNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019) - [x] [IPR](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#ipr-eccv-2018)
-(ECCV'2018) - [ ] [VideoPose3D](https://mmpose.readthedocs.io/en/1.x/
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#ipr-eccv-
+2018) (ECCV'2018) - [x] [VideoPose3D](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019) - [x]
-[HRNetv2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[HRNetv2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 backbones.html#hrnetv2-tpami-2019) (TPAMI'2019) - [x] [MSPN](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mspn-arxiv-2019)
-(ArXiv'2019) - [x] [SCNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mspn-arxiv-
+2019) (ArXiv'2019) - [x] [SCNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020) - [ ]
-[HigherHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[HigherHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 backbones.html#higherhrnet-cvpr-2020) (CVPR'2020) - [x] [RSN](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#rsn-eccv-2020)
-(ECCV'2020) - [ ] [InterNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#rsn-eccv-2020)
+(ECCV'2020) - [ ] [InterNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020) - [ ]
-[VoxelPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[VoxelPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#voxelpose-eccv-2020) (ECCV'2020) - [x] [LiteHRNet](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#litehrnet-cvpr-
-2021) (CVPR'2021) - [x] [ViPNAS](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#litehrnet-cvpr-
+2021) (CVPR'2021) - [x] [ViPNAS](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021) - [x] [Debias-
-IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#debias-ipr-iccv-2021) (ICCV'2021) - [x] [SimCC](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simcc-eccv-2022)
-(ECCV'2022)   Supported techniques: - [ ] [FPN](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017) - [ ] [FP16]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fp16-
-arxiv-2017) (ArXiv'2017) - [ ] [Wingloss](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018) - [ ]
-[AdaptiveWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simcc-eccv-
+2022) (ECCV'2022)   Supported techniques: - [x] [FPN](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fpn-cvpr-2017)
+(CVPR'2017) - [x] [FP16](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017) - [x] [Wingloss]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+techniques.html#wingloss-cvpr-2018) (CVPR'2018) - [x] [AdaptiveWingloss](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/
 techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019) - [x] [DarkPose](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#darkpose-cvpr-
-2020) (CVPR'2020) - [x] [UDP](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020) - [ ]
-[Albumentations](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-techniques.html#albumentations-information-2020) (Information'2020) - [ ]
-[SoftWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#darkpose-
+cvpr-2020) (CVPR'2020) - [x] [UDP](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020) - [x]
+[Albumentations](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+techniques.html#albumentations-information-2020) (Information'2020) - [x]
+[SoftWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 techniques.html#softwingloss-tip-2021) (TIP'2021) - [x] [RLE](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#rle-iccv-2021)
+mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#rle-iccv-2021)
 (ICCV'2021)   Supported datasets: - [x] [AFLW](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://
+en/latest/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://
 www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\]
-(ICCVW'2011) - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/1.x/
+(ICCVW'2011) - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://
 jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013) - [x] [COFW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cofw-iccv-2013) \[
-[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013) - [x]
-[MPII](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cofw-iccv-2013)
+\[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013) -
+[x] [MPII](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\]
-(CVPR'2014) - [x] [Human3.6M](https://mmpose.readthedocs.io/en/1.x/
+(CVPR'2014) - [x] [Human3.6M](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://
 vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014) - [x] [COCO](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-eccv-2014) \[
-[homepage](http://cocodataset.org/)\] (ECCV'2014) - [x] [CMU Panoptic](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-iccv-
-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015) - [x]
-[DeepFashion](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-eccv-2014)
+\[[homepage](http://cocodataset.org/)\] (ECCV'2014) - [x] [CMU Panoptic](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-
+iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015) -
+[x] [DeepFashion](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/
 projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016) - [x] [300W](https:/
-/mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#300w-imavis-2016)
-\[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016) - [x]
-[RHD](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#rhd-
-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/
-datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017) - [x] [CMU Panoptic
-HandDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://
-domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017) - [x] [AI Challenger]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ai-
-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/
-AI_Challenger_2017)\] (ArXiv'2017) - [x] [MHP](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-
-mhp.github.io/dataset)\] (ACM MM'2018) - [x] [WFLW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[
-[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018) - [x]
-[PoseTrack18](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/
-download.php)\] (CVPR'2018) - [x] [OCHuman](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://
-github.com/liruilong940607/OCHumanApi)\] (CVPR'2019) - [x] [CrowdPose](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#crowdpose-cvpr-
-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019) - [x]
-[MPII-TRB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/
-kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019) - [x]
-[FreiHand](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+/mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#300w-imavis-
+2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
+- [x] [RHD](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-
+freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017) -
+[x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage]
+(http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017) - [x] [AI
+Challenger](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/
+AIChallenger/AI_Challenger_2017)\] (ArXiv'2017) - [x] [MHP](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mhp-acm-mm-2018)
+\[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018) - [x] [WFLW]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#wflw-
+cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\]
+(CVPR'2018) - [x] [PoseTrack18](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://
+posetrack.net/users/download.php)\] (CVPR'2018) - [x] [OCHuman](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ochuman-cvpr-
+2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
+- [x] [CrowdPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/
+CrowdPose)\] (CVPR'2019) - [x] [MPII-TRB](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://
+github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019) -
+[x] [FreiHand](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-
 freiburg.de/projects/freihand/)\] (ICCV'2019) - [x] [Animal-Pose](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#animal-pose-iccv-
-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019) -
-[x] [OneHand10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/
-papers/WANG-MCC-2018-10.html)\] (TCSVT'2019) - [x] [Vinegar Fly](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#vinegar-fly-nature-
-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\]
-(Nature Methods'2019) - [x] [Desert Locust](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage]
-(https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [Grvys
-Zebra](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/
-jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [ATRW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \
-[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020) - [x]
-[Halpe](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#animal-pose-
+iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\]
+(ICCV'2019) - [x] [OneHand10K](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://
+www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019) - [x] [Vinegar
+Fly](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/
+jgraving/DeepPoseKit-Data)\] (Nature Methods'2019) - [x] [Desert Locust](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#desert-locust-
+elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\]
+(Elife'2019) - [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://
+github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [ATRW](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#atrw-acm-mm-
+2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020) -
+[x] [Halpe](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/
 Halpe-FullBody/)\] (CVPR'2020) - [x] [COCO-WholeBody](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-wholebody-
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-wholebody-
 eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\]
-(ECCV'2020) - [x] [MacaquePose](https://mmpose.readthedocs.io/en/1.x/
+(ECCV'2020) - [x] [MacaquePose](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://
 www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020) - [x]
-[InterHand2.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[InterHand2.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/
 InterHand2.6M/)\] (ECCV'2020) - [x] [AP-10K](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://
-github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021) - [x] [Horse-10](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#horse-10-wacv-2021)
-\[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
-Supported backbones: - [x] [AlexNet](https://mmpose.readthedocs.io/en/1.x/
+latest/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https:/
+/github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021) - [x] [Horse-10](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#horse-10-wacv-
+2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021) -
+[x] [Human-Art](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#human-art-cvpr-2023) \[[homepage](https://idea-
+research.github.io/HumanArt/)\] (CVPR'2023) - [x] [LaPa](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#lapa-aaai-2020)
+\[[homepage](https://github.com/JDAI-CV/lapa-dataset)\] (AAAI'2020)   Supported
+backbones: - [x] [AlexNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012) - [x]
-[VGG](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vgg-
-iclr-2015) (ICLR'2015) - [x] [ResNet](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016) - [x] [ResNext]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnext-
-cvpr-2017) (CVPR'2017) - [x] [SEResNet](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018) - [x]
-[ShufflenetV1](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018) - [x] [ShufflenetV2](https:/
-/mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv2-
-eccv-2018) (ECCV'2018) - [x] [MobilenetV2](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018) - [x]
-[ResNetV1D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-backbones.html#resnetv1d-cvpr-2019) (CVPR'2019) - [x] [ResNeSt](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnest-arxiv-
-2020) (ArXiv'2020) - [x] [Swin](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021) - [x] [HRFormer]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrformer-
-nips-2021) (NIPS'2021) - [x] [PVT](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021) - [x] [PVTV2](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvtv2-cvmj-2022)
-(CVMJ'2022)  ### Model Request We will keep up with the latest progress of the
-community, and support more popular algorithms and frameworks. If you have any
-feature requests, please feel free to leave a comment in [MMPose Roadmap]
-(https://github.com/open-mmlab/mmpose/issues/9). ## Contributing We appreciate
-all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://
-mmpose.readthedocs.io/en/1.x/notes/contribution_guide.html) for the
-contributing guideline. ## Acknowledgement MMPose is an open source project
-that is contributed by researchers and engineers from various colleges and
-companies. We appreciate all the contributors who implement their methods or
-add new features, as well as users who give valuable feedbacks. We wish that
-the toolbox and benchmark could serve the growing research community by
-providing a flexible toolkit to reimplement existing methods and develop their
-own new models. ## Citation If you find this project useful in your research,
-please consider cite: ```bibtex @misc{mmpose2020, title={OpenMMLab Pose
-Estimation Toolbox and Benchmark}, author={MMPose Contributors}, howpublished =
-{\url{https://github.com/open-mmlab/mmpose}}, year={2020} } ``` ## License This
-project is released under the [Apache 2.0 license](LICENSE). ## Projects in
-OpenMMLab - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab
-foundational library for training deep learning models. - [MMCV](https://
-github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer
-vision. - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab
-packages. - [MMClassification](https://github.com/open-mmlab/mmclassification):
-OpenMMLab image classification toolbox and benchmark. - [MMDetection](https://
-github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
-- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's
-next-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
-benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
-OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
-github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
-understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
-OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
-github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
-and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
-OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
-github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
-benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
-fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
-benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
-video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment Framework.
+[VGG](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#vgg-iclr-2015) (ICLR'2015) - [x] [ResNet](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnet-cvpr-
+2016) (CVPR'2016) - [x] [ResNext](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017) - [x] [SEResNet]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#seresnet-cvpr-2018) (CVPR'2018) - [x] [ShufflenetV1](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv1-
+cvpr-2018) (CVPR'2018) - [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018) -
+[x] [MobilenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018) - [x] [ResNetV1D](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnetv1d-cvpr-
+2019) (CVPR'2019) - [x] [ResNeSt](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020) - [x] [Swin]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#swin-
+cvpr-2021) (CVPR'2021) - [x] [HRFormer](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021) - [x]
+[PVT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#pvt-iccv-2021) (ICCV'2021) - [x] [PVTV2](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvtv2-cvmj-
+2022) (CVMJ'2022)  ### Model Request We will keep up with the latest progress
+of the community, and support more popular algorithms and frameworks. If you
+have any feature requests, please feel free to leave a comment in [MMPose
+Roadmap](https://github.com/open-mmlab/mmpose/issues/2258). ## Contributing We
+appreciate all contributions to improve MMPose. Please refer to
+[CONTRIBUTING.md](https://mmpose.readthedocs.io/en/latest/
+contribution_guide.html) for the contributing guideline. ## Acknowledgement
+MMPose is an open source project that is contributed by researchers and
+engineers from various colleges and companies. We appreciate all the
+contributors who implement their methods or add new features, as well as users
+who give valuable feedbacks. We wish that the toolbox and benchmark could serve
+the growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their own new models. ## Citation If you find this
+project useful in your research, please consider cite: ```bibtex @misc
+{mmpose2020, title={OpenMMLab Pose Estimation Toolbox and Benchmark}, author=
+{MMPose Contributors}, howpublished = {\url{https://github.com/open-mmlab/
+mmpose}}, year={2020} } ``` ## License This project is released under the
+[Apache 2.0 license](LICENSE). ## Projects in OpenMMLab - [MMEngine](https://
+github.com/open-mmlab/mmengine): OpenMMLab foundational library for training
+deep learning models. - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab
+foundational library for computer vision. - [MMPreTrain](https://github.com/
+open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark. -
+[MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced,
+**G**enerative and **I**ntelligent **C**reation toolbox. - [MMDetection](https:
+//github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and
+benchmark. - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d):
+OpenMMLab's next-generation platform for general 3D object detection. -
+[MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object
+detection toolbox and benchmark. - [MMTracking](https://github.com/open-mmlab/
+mmtracking): OpenMMLab video perception toolbox and benchmark. -
+[MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab
+semantic segmentation toolbox and benchmark. - [MMOCR](https://github.com/open-
+mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
+- [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation
+toolbox and benchmark. - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d):
+OpenMMLab 3D human parametric model toolbox and benchmark. - [MMFewShot](https:
+//github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and
+benchmark. - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's
+next-generation action understanding toolbox and benchmark. - [MMFlow](https://
+github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment
+Framework. - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model
+compression toolbox and benchmark. - [MIM](https://github.com/open-mmlab/mim):
+MIM installs OpenMMLab packages. - [Playground](https://github.com/open-mmlab/
+playground): A central hub for gathering and showcasing amazing projects built
+upon OpenMMLab.
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/300w.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/300w.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/aflw.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/aflw.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/aic.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/aic.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/animalpose.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/animalpose.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/ap10k.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/ap10k.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/atrw.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/atrw.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/campus.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/campus.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_aic.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_aic.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_wholebody.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_wholebody.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_wholebody_face.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_wholebody_face.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/coco_wholebody_hand.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/coco_wholebody_hand.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/cofw.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/cofw.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/crowdpose.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/crowdpose.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/deepfashion_full.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/deepfashion_full.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/deepfashion_lower.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/deepfashion_lower.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/deepfashion_upper.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/deepfashion_upper.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/fly.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/fly.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/freihand2d.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/freihand2d.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/h36m.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/h36m.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/halpe.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/halpe.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/horse10.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/horse10.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/interhand2d.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/interhand2d.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/interhand3d.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/interhand3d.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/jhmdb.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/jhmdb.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/locust.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/locust.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/macaque.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/macaque.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mhp.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mhp.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mpi_inf_3dhp.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mpi_inf_3dhp.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mpii.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mpii.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/mpii_trb.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/mpii_trb.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/ochuman.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/ochuman.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/onehand10k.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/onehand10k.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/panoptic_body3d.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/panoptic_body3d.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/panoptic_hand2d.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/panoptic_hand2d.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/posetrack18.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/posetrack18.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/rhd2d.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/rhd2d.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/shelf.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/shelf.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/wflw.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/wflw.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/datasets/zebra.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/datasets/zebra.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/_base_/default_runtime.py` & `mmpose-1.1.0/mmpose/.mim/configs/_base_/default_runtime.py`

 * *Files 13% similar despite different names*

```diff
@@ -21,25 +21,29 @@
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
     dist_cfg=dict(backend='nccl'),
 )
 
 # visualizer
-vis_backends = [dict(type='LocalVisBackend')]
+vis_backends = [
+    dict(type='LocalVisBackend'),
+    # dict(type='TensorboardVisBackend'),
+    # dict(type='WandbVisBackend'),
+]
 visualizer = dict(
     type='PoseLocalVisualizer', vis_backends=vis_backends, name='visualizer')
 
 # logger
 log_processor = dict(
     type='LogProcessor', window_size=50, by_epoch=True, num_digits=6)
 log_level = 'INFO'
 load_from = None
 resume = False
 
 # file I/O backend
-file_client_args = dict(backend='disk')
+backend_args = dict(backend='local')
 
 # training/validation/testing progress
 train_cfg = dict(by_epoch=True)
 val_cfg = dict()
 test_cfg = dict()
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/rtmpose-m_8xb64-210e_ap10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/rtmpose-m_8xb64-210e_ap10k-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -65,22 +64,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=768,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(8, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +96,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'AP10KDataset'
 data_mode = 'topdown'
 data_root = 'data/ap10k/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/pose/ap10k/',
 #         f'{data_root}': 's3://openmmlab/datasets/pose/ap10k/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +133,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w32_8xb64-210e_animalpose-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w48_8xb64-210e_animalpose-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -52,35 +52,35 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256))),
+                num_channels=(48, 96, 192, 384))),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w32-36af842e.pth'),
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=32,
+        in_channels=48,
         out_channels=20,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
@@ -90,25 +90,25 @@
 # base dataset settings
 dataset_type = 'AnimalPoseDataset'
 data_mode = 'topdown'
 data_root = 'data/animalpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w48_8xb64-210e_animalpose-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_fish-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=300, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
-    type='Adam',
+    type='AdamW',
     lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
@@ -23,15 +23,15 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
@@ -52,96 +52,95 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(48, 96)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384))),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+            'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=48,
-        out_channels=20,
+        in_channels=32,
+        out_channels=23,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'AnimalPoseDataset'
+dataset_type = 'AnimalKingdomDataset'
 data_mode = 'topdown'
-data_root = 'data/animalpose/'
+data_root = 'data/ak/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/animalpose_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/ak_P3_fish/train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=24,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/animalpose_val.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/ak_P3_fish/test.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric', ann_file=data_root + 'annotations/animalpose_val.json')
+val_evaluator = [dict(type='PCKAccuracy', thr=0.05), dict(type='AUC')]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res101_8xb64-210e_animalpose-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_res101_8xb64-210e_aic-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -45,41 +45,41 @@
         type='ResNet',
         depth=101,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=20,
+        out_channels=14,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'AnimalPoseDataset'
+dataset_type = 'AicDataset'
 data_mode = 'topdown'
-data_root = 'data/animalpose/'
+data_root = 'data/aic/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,32 +87,36 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/animalpose_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/aic_train.json',
+        data_prefix=dict(img='ai_challenger_keypoint_train_20170902/'
+                         'keypoint_train_images_20170902/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/animalpose_val.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/aic_val.json',
+        data_prefix=dict(img='ai_challenger_keypoint_validation_20170911/'
+                         'keypoint_validation_images_20170911/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
-    type='CocoMetric', ann_file=data_root + 'annotations/animalpose_val.json')
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/aic_val.json',
+    use_area=False)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res152_8xb32-210e_animalpose-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res152_8xb32-210e_animalpose-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -61,25 +61,25 @@
 # base dataset settings
 dataset_type = 'AnimalPoseDataset'
 data_mode = 'topdown'
 data_root = 'data/animalpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res50_8xb64-210e_animalpose-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res152_8xb32-210e_zebra-160x160.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,99 +20,105 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(160, 160), heatmap_size=(40, 40), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=20,
+        out_channels=9,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'AnimalPoseDataset'
+dataset_type = 'ZebraDataset'
 data_mode = 'topdown'
-data_root = 'data/animalpose/'
+data_root = 'data/zebra/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        shift_factor=0.25,
+        rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/animalpose_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/zebra_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/animalpose_val.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/zebra_test.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric', ann_file=data_root + 'annotations/animalpose_val.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/cspnext-m_udp_8xb64-210e_ap10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/cspnext-m_udp_8xb64-210e_ap10k-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -69,35 +69,27 @@
     head=dict(
         type='HeatmapHead',
         in_channels=768,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
-        flip_test=False,
+        flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'AP10KDataset'
 data_mode = 'topdown'
 data_root = 'data/ap10k/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
-#     backend='petrel',
-#     path_mapping=dict({
-#         f'{data_root}': 's3://openmmlab/datasets/pose/ap10k/',
-#         f'{data_root}': 's3://openmmlab/datasets/pose/ap10k/'
-#     }))
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -116,22 +108,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/resnet_ap10k.yml` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/resnet_ap10k.yml`

 * *Files 18% similar despite different names*

```diff
@@ -1,41 +1,35 @@
-Collections:
-- Name: SimpleBaseline2D
-  Paper:
-    Title: Simple baselines for human pose estimation and tracking
-    URL: http://openaccess.thecvf.com/content_ECCV_2018/html/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.html
-  README: https://github.com/open-mmlab/mmpose/blob/master/docs/en/papers/algorithms/simplebaseline2d.md
 Models:
 - Config: configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res50_8xb64-210e_ap10k-256x256.py
   In Collection: SimpleBaseline2D
-  Alias: animal
   Metadata:
     Architecture: &id001
     - SimpleBaseline2D
+    - ResNet
     Training Data: AP-10K
-  Name: topdown_heatmap_res50_ap10k_256x256
+  Name: td-hm_res50_8xb64-210e_ap10k-256x256
   Results:
   - Dataset: AP-10K
     Metrics:
       AP: 0.680
       AP@0.5: 0.926
       AP@0.75: 0.738
-      APL: 0.687
-      APM: 0.552
+      AP (L): 0.687
+      AP (M): 0.552
     Task: Animal 2D Keypoint
   Weights: https://download.openmmlab.com/mmpose/animal/resnet/res50_ap10k_256x256-35760eb8_20211029.pth
 - Config: configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res101_8xb64-210e_ap10k-256x256.py
   In Collection: SimpleBaseline2D
   Metadata:
     Architecture: *id001
     Training Data: AP-10K
-  Name: topdown_heatmap_res101_ap10k_256x256
+  Name: td-hm_res101_8xb64-210e_ap10k-256x256
   Results:
   - Dataset: AP-10K
     Metrics:
       AP: 0.681
       AP@0.5: 0.921
       AP@0.75: 0.751
-      APL: 0.690
-      APM: 0.545
+      AP (L): 0.690
+      AP (M): 0.545
     Task: Animal 2D Keypoint
   Weights: https://download.openmmlab.com/mmpose/animal/resnet/res101_ap10k_256x256-9edfafb9_20211029.pth
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w32_8xb64-210e_ap10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w32_8xb64-210e_ap10k-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,25 +90,25 @@
 # base dataset settings
 dataset_type = 'AP10KDataset'
 data_mode = 'topdown'
 data_root = 'data/ap10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w48_8xb64-210e_ap10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w48_8xb64-210e_ap10k-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,25 +90,25 @@
 # base dataset settings
 dataset_type = 'AP10KDataset'
 data_mode = 'topdown'
 data_root = 'data/ap10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res101_8xb64-210e_ap10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub1-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=40, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,123 +13,110 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=40,
+        milestones=[20, 30],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(32, 32), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(
-        type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
-    ),
+    backbone=dict(type='ResNet', depth=50),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=15,
+        deconv_out_channels=(256, 256),
+        deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
+load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'AP10KDataset'
+dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
-data_root = 'data/ap10k/'
+data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
     batch_size=64,
-    num_workers=4,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/ap10k-train-split1.json',
-        data_prefix=dict(img='data/'),
+        ann_file='annotations/Sub1_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=4,
-    persistent_workers=True,
-    drop_last=False,
-    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/ap10k-val-split1.json',
-        data_prefix=dict(img='data/'),
-        test_mode=True,
-        pipeline=val_pipeline,
-    ))
-test_dataloader = dict(
-    batch_size=32,
-    num_workers=4,
+    num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/ap10k-test-split1.json',
-        data_prefix=dict(img='data/'),
+        ann_file='annotations/Sub1_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
+test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/ap10k-val-split1.json')
-test_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/ap10k-test-split1.json')
+val_evaluator = [
+    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
+]
+test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res50_8xb64-210e_ap10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub3-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=40, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,123 +13,110 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=40,
+        milestones=[20, 30],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(32, 32), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
-    ),
+    backbone=dict(type='ResNet', depth=50),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=15,
+        deconv_out_channels=(256, 256),
+        deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
+load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'AP10KDataset'
+dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
-data_root = 'data/ap10k/'
+data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
     batch_size=64,
-    num_workers=4,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/ap10k-train-split1.json',
-        data_prefix=dict(img='data/'),
+        ann_file='annotations/Sub3_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=4,
-    persistent_workers=True,
-    drop_last=False,
-    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/ap10k-val-split1.json',
-        data_prefix=dict(img='data/'),
-        test_mode=True,
-        pipeline=val_pipeline,
-    ))
-test_dataloader = dict(
-    batch_size=32,
-    num_workers=4,
+    num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/ap10k-test-split1.json',
-        data_prefix=dict(img='data/'),
+        ann_file='annotations/Sub3_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
+test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/ap10k-val-split1.json')
-test_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/ap10k-test-split1.json')
+val_evaluator = [
+    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
+]
+test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res101_8xb64-210e_locust-160x160.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res101_8xb64-210e_locust-160x160.py`

 * *Files 4% similar despite different names*

```diff
@@ -61,28 +61,28 @@
 # base dataset settings
 dataset_type = 'LocustDataset'
 data_mode = 'topdown'
 data_root = 'data/locust/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.25,
         rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res152_8xb32-210e_locust-160x160.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res50_8xb64-210e_locust-160x160.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,15 +20,15 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(160, 160), heatmap_size=(40, 40), sigma=2)
@@ -39,16 +39,16 @@
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=35,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,36 +61,36 @@
 # base dataset settings
 dataset_type = 'LocustDataset'
 data_mode = 'topdown'
 data_root = 'data/locust/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.25,
         rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res50_8xb64-210e_locust-160x160.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res50_8xb64-210e_zebra-160x160.py`

 * *Files 4% similar despite different names*

```diff
@@ -45,44 +45,44 @@
         type='ResNet',
         depth=50,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=35,
+        out_channels=9,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'LocustDataset'
+dataset_type = 'ZebraDataset'
 data_mode = 'topdown'
-data_root = 'data/locust/'
+data_root = 'data/zebra/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.25,
         rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -90,29 +90,29 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/locust_train.json',
+        ann_file='annotations/zebra_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/locust_test.json',
+        ann_file='annotations/zebra_test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res101_8xb64-210e_zebra-160x160.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res101_8xb64-210e_zebra-160x160.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,28 +61,28 @@
 # base dataset settings
 dataset_type = 'ZebraDataset'
 data_mode = 'topdown'
 data_root = 'data/zebra/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.25,
         rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res152_8xb32-210e_zebra-160x160.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res152_8xb32-210e_locust-160x160.py`

 * *Files 4% similar despite different names*

```diff
@@ -45,44 +45,44 @@
         type='ResNet',
         depth=152,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=9,
+        out_channels=35,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'ZebraDataset'
+dataset_type = 'LocustDataset'
 data_mode = 'topdown'
-data_root = 'data/zebra/'
+data_root = 'data/locust/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.25,
         rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -90,29 +90,29 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/zebra_train.json',
+        ann_file='annotations/locust_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/zebra_test.json',
+        ann_file='annotations/locust_test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res50_8xb64-210e_zebra-160x160.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_res50_8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,105 +20,100 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
-
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(160, 160), heatmap_size=(40, 40), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
-    ),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=9,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'ZebraDataset'
+dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
-data_root = 'data/zebra/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale', padding=0.8),
-    dict(type='RandomFlip', direction='horizontal'),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale'),
     dict(
-        type='RandomBBoxTransform',
-        shift_factor=0.25,
-        rotate_factor=180,
+        type='RandomBBoxTransform', rotate_factor=180,
         scale_factor=(0.7, 1.3)),
+    dict(type='RandomFlip', direction='horizontal'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale', padding=0.8),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/zebra_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/zebra_test.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = [
     dict(type='PCKAccuracy', thr=0.2),
     dict(type='AUC'),
-    dict(type='EPE'),
+    dict(type='EPE')
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/coco/ae_hrnet-w32_8xb24-300e_coco-512x512.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/associative_embedding/coco/ae_hrnet-w32_8xb24-300e_coco-512x512.py`

 * *Files 2% similar despite different names*

```diff
@@ -105,15 +105,15 @@
 dataset_type = 'CocoDataset'
 data_mode = 'bottomup'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = []
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(
         type='BottomupResize',
         input_size=codec['input_size'],
         size_factor=32,
         resize_mode='expand'),
     dict(type='PackPoseInputs')
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w32_8xb20-140e_coco-512x512.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w32_8xb20-140e_coco-512x512.py`

 * *Files 5% similar despite different names*

```diff
@@ -68,21 +68,23 @@
                 num_channels=(32, 64, 128, 256),
                 multiscale_output=True)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='CIDHead',
-        in_channels=(32, 64, 128, 256),
+        in_channels=480,
         num_keypoints=17,
         gfd_channels=32,
-        input_transform='resize_concat',
-        input_index=(0, 1, 2, 3),
         coupled_heatmap_loss=dict(type='FocalHeatmapLoss', loss_weight=1.0),
         decoupled_heatmap_loss=dict(type='FocalHeatmapLoss', loss_weight=4.0),
         contrastive_loss=dict(
             type='InfoNCELoss', temperature=0.05, loss_weight=1.0),
         decoder=codec,
     ),
     train_cfg=dict(max_train_instances=200),
@@ -95,23 +97,23 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'bottomup'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='BottomupRandomAffine', input_size=codec['input_size']),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='BottomupGetHeatmapMask'),
     dict(type='PackPoseInputs'),
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(
         type='BottomupResize',
         input_size=codec['input_size'],
         size_factor=64,
         resize_mode='expand'),
     dict(
         type='PackPoseInputs',
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w48_8xb20-140e_coco-512x512.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w48_8xb20-140e_coco-512x512.py`

 * *Files 2% similar despite different names*

```diff
@@ -68,21 +68,23 @@
                 num_channels=(48, 96, 192, 384),
                 multiscale_output=True)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='CIDHead',
-        in_channels=(48, 96, 192, 384),
+        in_channels=720,
         num_keypoints=17,
         gfd_channels=48,
-        input_transform='resize_concat',
-        input_index=(0, 1, 2, 3),
         coupled_heatmap_loss=dict(type='FocalHeatmapLoss', loss_weight=1.0),
         decoupled_heatmap_loss=dict(type='FocalHeatmapLoss', loss_weight=4.0),
         contrastive_loss=dict(
             type='InfoNCELoss', temperature=0.05, loss_weight=1.0),
         decoder=codec,
     ),
     train_cfg=dict(max_train_instances=200),
@@ -95,23 +97,23 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'bottomup'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='BottomupRandomAffine', input_size=codec['input_size']),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='BottomupGetHeatmapMask'),
     dict(type='PackPoseInputs'),
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(
         type='BottomupResize',
         input_size=codec['input_size'],
         size_factor=64,
         resize_mode='expand'),
     dict(
         type='PackPoseInputs',
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w32_8xb10-140e_coco-512x512.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w32_8xb10-140e_coco-512x512.py`

 * *Files 5% similar despite different names*

```diff
@@ -77,20 +77,22 @@
                 num_channels=(32, 64, 128, 256),
                 multiscale_output=True)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='DEKRHead',
-        in_channels=(32, 64, 128, 256),
+        in_channels=480,
         num_keypoints=17,
-        input_transform='resize_concat',
-        input_index=(0, 1, 2, 3),
         heatmap_loss=dict(type='KeypointMSELoss', use_target_weight=True),
         displacement_loss=dict(
             type='SoftWeightSmoothL1Loss',
             use_target_weight=True,
             supervise_empty=False,
             beta=1 / 9,
             loss_weight=0.002,
@@ -117,23 +119,23 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'bottomup'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='BottomupRandomAffine', input_size=codec['input_size']),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='BottomupGetHeatmapMask'),
     dict(type='PackPoseInputs'),
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(
         type='BottomupResize',
         input_size=codec['input_size'],
         size_factor=32,
         resize_mode='expand'),
     dict(
         type='PackPoseInputs',
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w48_8xb10-140e_coco-640x640.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w32_8xb10-300e_crowdpose-512x512.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=140, val_interval=10)
+train_cfg = dict(max_epochs=300, val_interval=20)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=1e-3,
 ))
 
@@ -13,31 +13,31 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=140,
-        milestones=[90, 120],
+        end=300,
+        milestones=[200, 260],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=80)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='SPR',
-    input_size=(640, 640),
-    heatmap_size=(160, 160),
+    input_size=(512, 512),
+    heatmap_size=(128, 128),
     sigma=(4, 2),
     minimal_diagonal_length=32**0.5,
     generate_keypoint_heatmaps=True,
     decode_max_instances=30)
 
 # model settings
 model = dict(
@@ -58,83 +58,83 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(48, 96)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384),
+                num_channels=(32, 64, 128, 256),
                 multiscale_output=True)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+            'pretrain_models/hrnet_w32-36af842e.pth'),
+    ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
     ),
     head=dict(
         type='DEKRHead',
-        in_channels=(48, 96, 192, 384),
-        num_keypoints=17,
-        input_transform='resize_concat',
-        input_index=(0, 1, 2, 3),
-        num_heatmap_filters=48,
+        in_channels=480,
+        num_keypoints=14,
         heatmap_loss=dict(type='KeypointMSELoss', use_target_weight=True),
         displacement_loss=dict(
             type='SoftWeightSmoothL1Loss',
             use_target_weight=True,
             supervise_empty=False,
             beta=1 / 9,
-            loss_weight=0.002,
+            loss_weight=0.004,
         ),
         decoder=codec,
         rescore_cfg=dict(
-            in_channels=74,
-            norm_indexes=(5, 6),
+            in_channels=59,
+            norm_indexes=(0, 1),
             init_cfg=dict(
                 type='Pretrained',
                 checkpoint='https://download.openmmlab.com/mmpose/'
-                'pretrain_models/kpt_rescore_coco-33d58c5c.pth')),
+                'pretrain_models/kpt_rescore_crowdpose-300c7efe.pth')),
     ),
     test_cfg=dict(
         multiscale_test=False,
         flip_test=True,
         nms_dist_thr=0.05,
         shift_heatmap=True,
         align_corners=False))
 
 # enable DDP training when rescore net is used
 find_unused_parameters = True
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CrowdPoseDataset'
 data_mode = 'bottomup'
-data_root = 'data/coco/'
+data_root = 'data/crowdpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='BottomupRandomAffine', input_size=codec['input_size']),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='GenerateTarget', encoder=codec),
-    dict(type='BottomupGetHeatmapMask'),
     dict(type='PackPoseInputs'),
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(
         type='BottomupResize',
         input_size=codec['input_size'],
         size_factor=32,
         resize_mode='expand'),
     dict(
         type='PackPoseInputs',
@@ -150,36 +150,38 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mmpose_crowdpose_trainval.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mmpose_crowdpose_test.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json',
+    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
     nms_mode='none',
     score_mode='keypoint',
-)
+    use_area=False,
+    iou_type='keypoints_crowd',
+    prefix='crowdpose')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w32_8xb10-300e_crowdpose-512x512.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w48_8xb10-140e_coco-640x640.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=300, val_interval=20)
+train_cfg = dict(max_epochs=140, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=1e-3,
 ))
 
@@ -13,31 +13,31 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=300,
-        milestones=[200, 260],
+        end=140,
+        milestones=[90, 120],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=80)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='SPR',
-    input_size=(512, 512),
-    heatmap_size=(128, 128),
+    input_size=(640, 640),
+    heatmap_size=(160, 160),
     sigma=(4, 2),
     minimal_diagonal_length=32**0.5,
     generate_keypoint_heatmaps=True,
     decode_max_instances=30)
 
 # model settings
 model = dict(
@@ -58,81 +58,85 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256),
+                num_channels=(48, 96, 192, 384),
                 multiscale_output=True)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w32-36af842e.pth'),
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+    ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
     ),
     head=dict(
         type='DEKRHead',
-        in_channels=(32, 64, 128, 256),
-        num_keypoints=14,
-        input_transform='resize_concat',
-        input_index=(0, 1, 2, 3),
+        in_channels=720,
+        num_keypoints=17,
+        num_heatmap_filters=48,
         heatmap_loss=dict(type='KeypointMSELoss', use_target_weight=True),
         displacement_loss=dict(
             type='SoftWeightSmoothL1Loss',
             use_target_weight=True,
             supervise_empty=False,
             beta=1 / 9,
-            loss_weight=0.004,
+            loss_weight=0.002,
         ),
         decoder=codec,
         rescore_cfg=dict(
-            in_channels=59,
-            norm_indexes=(0, 1),
+            in_channels=74,
+            norm_indexes=(5, 6),
             init_cfg=dict(
                 type='Pretrained',
                 checkpoint='https://download.openmmlab.com/mmpose/'
-                'pretrain_models/kpt_rescore_crowdpose-300c7efe.pth')),
+                'pretrain_models/kpt_rescore_coco-33d58c5c.pth')),
     ),
     test_cfg=dict(
         multiscale_test=False,
         flip_test=True,
         nms_dist_thr=0.05,
         shift_heatmap=True,
         align_corners=False))
 
 # enable DDP training when rescore net is used
 find_unused_parameters = True
 
 # base dataset settings
-dataset_type = 'CrowdPoseDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'bottomup'
-data_root = 'data/crowdpose/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='BottomupRandomAffine', input_size=codec['input_size']),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='GenerateTarget', encoder=codec),
+    dict(type='BottomupGetHeatmapMask'),
     dict(type='PackPoseInputs'),
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(
         type='BottomupResize',
         input_size=codec['input_size'],
         size_factor=32,
         resize_mode='expand'),
     dict(
         type='PackPoseInputs',
@@ -148,38 +152,36 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_trainval.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=1,
     num_workers=1,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_test.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/person_keypoints_val2017.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json',
     nms_mode='none',
     score_mode='keypoint',
-    use_area=False,
-    iou_type='keypoints_crowd',
-    prefix='crowdpose')
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w48_8xb5-300e_crowdpose-640x640.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w48_8xb5-300e_crowdpose-640x640.py`

 * *Files 9% similar despite different names*

```diff
@@ -77,20 +77,22 @@
                 num_channels=(48, 96, 192, 384),
                 multiscale_output=True)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='DEKRHead',
-        in_channels=(48, 96, 192, 384),
+        in_channels=720,
         num_keypoints=14,
-        input_transform='resize_concat',
-        input_index=(0, 1, 2, 3),
         num_heatmap_filters=48,
         heatmap_loss=dict(type='KeypointMSELoss', use_target_weight=True),
         displacement_loss=dict(
             type='SoftWeightSmoothL1Loss',
             use_target_weight=True,
             supervise_empty=False,
             beta=1 / 9,
@@ -118,22 +120,22 @@
 # base dataset settings
 dataset_type = 'CrowdPoseDataset'
 data_mode = 'bottomup'
 data_root = 'data/crowdpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='BottomupRandomAffine', input_size=codec['input_size']),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs'),
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(
         type='BottomupResize',
         input_size=codec['input_size'],
         size_factor=32,
         resize_mode='expand'),
     dict(
         type='PackPoseInputs',
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_8xb64-210e_coco-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb16-210e_coco-384x288.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,124 +13,109 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=train_cfg['max_epochs'],
+        end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=128)
+
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='IntegralRegressionLabel',
-    input_size=(256, 256),
-    heatmap_size=(64, 64),
-    sigma=2.0,
-    normalize=True)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=50,
+        type='ResNeSt',
+        depth=200,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest200'),
     ),
     head=dict(
-        type='DSNTHead',
+        type='HeatmapHead',
         in_channels=2048,
-        in_featuremap_size=(8, 8),
-        num_joints=17,
-        loss=dict(
-            type='MultipleLossWrapper',
-            losses=[
-                dict(type='SmoothL1Loss', use_target_weight=True),
-                dict(type='KeypointMSELoss', use_target_weight=True)
-            ]),
+        out_channels=17,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        shift_coords=True,
+        flip_mode='heatmap',
         shift_heatmap=True,
-    ),
-    init_cfg=dict(
-        type='Pretrained',
-        checkpoint='https://download.openmmlab.com/mmpose/'
-        'pretrain_models/td-hm_res50_8xb64-210e_coco-256x192.pth'))
+    ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-test_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+val_pipeline = [
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=16,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=16,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file=f'{data_root}person_detection_results/'
+        bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
-        pipeline=test_pipeline,
+        pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=f'{data_root}annotations/person_keypoints_val2017.json')
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_debias-8xb64-210e_coco-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_dsnt-8xb64-210e_coco-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -47,16 +47,14 @@
         depth=50,
     ),
     head=dict(
         type='DSNTHead',
         in_channels=2048,
         in_featuremap_size=(8, 8),
         num_joints=17,
-        debias=True,
-        beta=10.,
         loss=dict(
             type='MultipleLossWrapper',
             losses=[
                 dict(type='SmoothL1Loss', use_target_weight=True),
                 dict(type='JSDiscretLoss', use_target_weight=True)
             ]),
         decoder=codec),
@@ -71,29 +69,27 @@
         'pretrain_models/td-hm_res50_8xb64-210e_coco-256x192.pth'))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 test_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_dsnt-8xb64-210e_coco-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_8xb64-210e_coco-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -51,15 +51,15 @@
         in_channels=2048,
         in_featuremap_size=(8, 8),
         num_joints=17,
         loss=dict(
             type='MultipleLossWrapper',
             losses=[
                 dict(type='SmoothL1Loss', use_target_weight=True),
-                dict(type='JSDiscretLoss', use_target_weight=True)
+                dict(type='KeypointMSELoss', use_target_weight=True)
             ]),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         shift_coords=True,
         shift_heatmap=True,
     ),
@@ -69,29 +69,27 @@
         'pretrain_models/td-hm_res50_8xb64-210e_coco-256x192.pth'))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 test_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -36,16 +36,16 @@
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=1024)
 
 # codec settings
 codec = dict(
     type='SimCCLabel',
-    input_size=(192, 256),
-    sigma=(4.9, 5.66),
+    input_size=(288, 384),
+    sigma=(6., 6.93),
     simcc_split_ratio=2.0,
     normalize=False,
     use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
@@ -65,22 +65,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
+            'rtmposev1/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=1024,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -55,32 +55,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=1.,
-        widen_factor=1.,
+        deepen_factor=0.67,
+        widen_factor=0.75,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=1024,
+        in_channels=768,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(9, 12),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -214,15 +214,15 @@
                 (11, 15),
             ])
     ],
 )
 
 # data loaders
 train_dataloader = dict(
-    batch_size=256,
+    batch_size=128 * 2,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='CombinedDataset',
         metainfo=dict(from_file='configs/_base_/datasets/coco.py'),
         datasets=[dataset_coco, dataset_aic],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -65,22 +65,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
+            'rtmposev1/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=1024,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -55,32 +55,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.67,
-        widen_factor=0.75,
+        deepen_factor=1.,
+        widen_factor=1.,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=768,
+        in_channels=1024,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -214,15 +214,15 @@
                 (11, 15),
             ])
     ],
 )
 
 # data loaders
 train_dataloader = dict(
-    batch_size=128 * 2,
+    batch_size=256,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='CombinedDataset',
         metainfo=dict(from_file='configs/_base_/datasets/coco.py'),
         datasets=[dataset_coco, dataset_aic],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),
+    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.0),
     paramwise_cfg=dict(
         norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))
 
 # learning rate
 param_scheduler = [
     dict(
         type='LinearLR',
@@ -36,16 +36,16 @@
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=1024)
 
 # codec settings
 codec = dict(
     type='SimCCLabel',
-    input_size=(288, 384),
-    sigma=(6., 6.93),
+    input_size=(192, 256),
+    sigma=(4.9, 5.66),
     simcc_split_ratio=2.0,
     normalize=False,
     use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
@@ -55,32 +55,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.67,
-        widen_factor=0.75,
+        deepen_factor=0.33,
+        widen_factor=0.5,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-s_udp-aic-coco_210e-256x192-92f5a029_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=768,
+        in_channels=512,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(9, 12),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -65,22 +65,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=768,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_aic-coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.0),
+    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.),
     paramwise_cfg=dict(
         norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))
 
 # learning rate
 param_scheduler = [
     dict(
         type='LinearLR',
@@ -55,32 +55,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.33,
-        widen_factor=0.5,
+        deepen_factor=0.167,
+        widen_factor=0.375,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-s_udp-aic-coco_210e-256x192-92f5a029_20230130.pth'  # noqa
+            'rtmposev1/cspnext-tiny_udp-aic-coco_210e-256x192-cbed682d_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=512,
+        in_channels=384,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -214,15 +214,15 @@
                 (11, 15),
             ])
     ],
 )
 
 # data loaders
 train_dataloader = dict(
-    batch_size=128 * 2,
+    batch_size=256,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='CombinedDataset',
         metainfo=dict(from_file='configs/_base_/datasets/coco.py'),
         datasets=[dataset_coco, dataset_aic],
@@ -249,20 +249,21 @@
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='coco/AP', rule='greater', max_keep_ckpts=1))
 
 custom_hooks = [
-    dict(
-        type='EMAHook',
-        ema_type='ExpMomentumEMA',
-        momentum=0.0002,
-        update_buffers=True,
-        priority=49),
+    # Turn off EMA while training the tiny model
+    # dict(
+    #     type='EMAHook',
+    #     ema_type='ExpMomentumEMA',
+    #     momentum=0.0002,
+    #     update_buffers=True,
+    #     priority=49),
     dict(
         type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - stage2_num_epochs,
         switch_pipeline=train_pipeline_stage2)
 ]
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -65,22 +65,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-s_udp-aic-coco_210e-256x192-92f5a029_20230130.pth'  # noqa
+            'rtmposev1/cspnext-s_udp-aic-coco_210e-256x192-92f5a029_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=512,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.),
+    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),
     paramwise_cfg=dict(
         norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))
 
 # learning rate
 param_scheduler = [
     dict(
         type='LinearLR',
@@ -55,32 +55,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.167,
-        widen_factor=0.375,
+        deepen_factor=0.67,
+        widen_factor=0.75,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-tiny_udp-aic-coco_210e-256x192-cbed682d_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=384,
+        in_channels=768,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -214,15 +214,15 @@
                 (11, 15),
             ])
     ],
 )
 
 # data loaders
 train_dataloader = dict(
-    batch_size=256,
+    batch_size=128 * 2,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type='CombinedDataset',
         metainfo=dict(from_file='configs/_base_/datasets/coco.py'),
         datasets=[dataset_coco, dataset_aic],
@@ -249,21 +249,20 @@
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='coco/AP', rule='greater', max_keep_ckpts=1))
 
 custom_hooks = [
-    # Turn off EMA while training the tiny model
-    # dict(
-    #     type='EMAHook',
-    #     ema_type='ExpMomentumEMA',
-    #     momentum=0.0002,
-    #     update_buffers=True,
-    #     priority=49),
+    dict(
+        type='EMAHook',
+        ema_type='ExpMomentumEMA',
+        momentum=0.0002,
+        update_buffers=True,
+        priority=49),
     dict(
         type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - stage2_num_epochs,
         switch_pipeline=train_pipeline_stage2)
 ]
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -65,22 +65,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-tiny_udp-aic-coco_210e-256x192-cbed682d_20230130.pth'  # noqa
+            'rtmposev1/cspnext-tiny_udp-aic-coco_210e-256x192-cbed682d_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=384,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +97,25 @@
     test_cfg=dict(flip_test=True))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +134,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/rtmpose-m_8xb64-210e_crowdpose-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/rtmpose-m_8xb32-60e_coco-wholebody-face-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-max_epochs = 210
-stage2_num_epochs = 30
-base_lr = 5e-4
+max_epochs = 60
+stage2_num_epochs = 10
+base_lr = 4e-3
 
-train_cfg = dict(max_epochs=max_epochs, val_interval=10)
+train_cfg = dict(max_epochs=max_epochs, val_interval=1)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
     optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),
     paramwise_cfg=dict(
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -36,16 +35,16 @@
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
 codec = dict(
     type='SimCCLabel',
-    input_size=(192, 256),
-    sigma=(4.9, 5.66),
+    input_size=(256, 256),
+    sigma=(5.66, 5.66),
     simcc_split_ratio=2.0,
     normalize=False,
     use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
@@ -65,22 +64,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=768,
-        out_channels=14,
+        out_channels=68,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -93,32 +92,32 @@
             use_target_weight=True,
             beta=10.,
             label_softmax=True),
         decoder=codec),
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
-dataset_type = 'CrowdPoseDataset'
+dataset_type = 'CocoWholeBodyFaceDataset'
 data_mode = 'topdown'
-data_root = 'data/'
+data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
-#         f'{data_root}': 's3://openmmlab/datasets/',
-#         f'{data_root}': 's3://openmmlab/datasets/'
+#         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
+#         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
+    # dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
     dict(
         type='Albumentation',
         transforms=[
@@ -134,25 +133,25 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
+    # dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
         rotate_factor=60),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -173,48 +172,47 @@
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='crowdpose/annotations/mmpose_crowdpose_trainval.json',
-        data_prefix=dict(img='pose/CrowdPose/images/'),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=10,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='crowdpose/annotations/mmpose_crowdpose_test.json',
-        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
-        data_prefix=dict(img='pose/CrowdPose/images/'),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(
-        save_best='crowdpose/AP', rule='greater', max_keep_ckpts=1))
+        save_best='NME', rule='less', max_keep_ckpts=1, interval=1))
 
 custom_hooks = [
     dict(
         type='EMAHook',
         ema_type='ExpMomentumEMA',
         momentum=0.0002,
         update_buffers=True,
@@ -223,13 +221,11 @@
         type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - stage2_num_epochs,
         switch_pipeline=train_pipeline_stage2)
 ]
 
 # evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'crowdpose/annotations/mmpose_crowdpose_test.json',
-    use_area=False,
-    iou_type='keypoints_crowd',
-    prefix='crowdpose')
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/rtmpose-m_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/rtmpose-m_8xb64-210e_mpii-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -64,22 +64,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=768,
         out_channels=16,
         input_size=codec['input_size'],
-        in_featuremap_size=(8, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -96,25 +96,25 @@
     test_cfg=dict(flip_test=True))
 
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/pose/MPI/',
 #         f'{data_root}': 's3://openmmlab/datasets/pose/MPI/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -133,22 +133,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/mobilenetv2_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/vipnas_coco.yml`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,19 @@
-Collections:
-- Name: SimCC
-  Paper:
-    Title: A Simple Coordinate Classification Perspective for Human Pose Estimation
-    URL: https://arxiv.org/abs/2107.03332
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/algorithms/simcc.md
 Models:
-- Config: configs/body_2d_keypoint/simcc/coco/simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192.py
+- Config: configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py
   In Collection: SimCC
   Metadata:
     Architecture: &id001
     - SimCC
-    - MobilenetV2
+    - ViPNAS
     Training Data: COCO
-  Name: simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192
+  Name: simcc_vipnas-mbv3_8xb64-210e_coco-256x192
   Results:
   - Dataset: COCO
     Metrics:
-      AP: 0.62
-      AP@0.5: 0.855
-      AP@0.75: 0.697
-      AR: 0.678
-      AR@0.5: 0.902
+      AP: 0.695
+      AP@0.5: 0.883
+      AP@0.75: 0.772
+      AR: 0.755
+      AR@0.5: 0.927
     Task: Body 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/simcc/coco/simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192-4b0703bb_20221010.pth
+  Weights: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192-719f3489_20220922.pth
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/resnet_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/resnet_coco.yml`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 Collections:
 - Name: SimCC
   Paper:
     Title: A Simple Coordinate Classification Perspective for Human Pose Estimation
     URL: https://arxiv.org/abs/2107.03332
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/algorithms/simcc.md
+  README: https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/algorithms/simcc.md
 Models:
 - Config: configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb64-210e_coco-256x192.py
   In Collection: SimCC
   Metadata:
     Architecture: &id001
     - SimCC
     - ResNet
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-384x288.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,26 +13,29 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=train_cfg['max_epochs'],
+        end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+
 # codec settings
 codec = dict(
-    type='SimCCLabel', input_size=(192, 256), sigma=6.0, simcc_split_ratio=2.0)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -43,45 +46,43 @@
         widen_factor=1.,
         out_indices=(7, ),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='mmcls://mobilenet_v2',
         )),
     head=dict(
-        type='SimCCHead',
+        type='HeatmapHead',
         in_channels=1280,
         out_channels=17,
-        input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
-        simcc_split_ratio=codec['simcc_split_ratio'],
-        deconv_out_channels=None,
-        loss=dict(type='KLDiscretLoss', use_target_weight=True),
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
-    test_cfg=dict(flip_test=True, ))
+    test_cfg=dict(
+        flip_test=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
+    ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -104,23 +105,20 @@
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file=f'{data_root}person_detection_results/'
+        bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
     ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb32-140e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb32-140e_coco-384x288.py`

 * *Files 4% similar despite different names*

```diff
@@ -44,40 +44,38 @@
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='SimCCHead',
         in_channels=2048,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(9, 12),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         loss=dict(type='KLDiscretLoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(flip_test=True))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 test_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_8xb32-210e_coco-384x288.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,84 +2,92 @@
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=1e-3,
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
-    dict(type='MultiStepLR', milestones=[170, 200], gamma=0.1, by_epoch=True)
+    dict(
+        type='MultiStepLR',
+        begin=0,
+        end=210,
+        milestones=[170, 200],
+        gamma=0.1,
+        by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+
 # codec settings
 codec = dict(
-    type='SimCCLabel', input_size=(192, 256), sigma=6.0, simcc_split_ratio=2.0)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
     head=dict(
-        type='SimCCHead',
+        type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
-        input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
-        simcc_split_ratio=codec['simcc_split_ratio'],
-        loss=dict(type='KLDiscretLoss', use_target_weight=True),
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
-    test_cfg=dict(flip_test=True))
+    test_cfg=dict(
+        flip_test=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
+    ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-test_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+val_pipeline = [
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
@@ -94,23 +102,20 @@
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file=f'{data_root}person_detection_results/'
+        bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
-        pipeline=test_pipeline,
+        pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
     ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,43 +40,41 @@
         bgr_to_rgb=True),
     backbone=dict(type='ViPNAS_MobileNetV3'),
     head=dict(
         type='SimCCHead',
         in_channels=160,
         out_channels=17,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         deconv_type='vipnas',
         deconv_out_channels=(160, 160, 160),
         deconv_num_groups=(160, 160, 160),
         loss=dict(type='KLDiscretLoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/vipnas_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/resnet_dsnt_coco.yml`

 * *Files 25% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 Collections:
-- Name: SimCC
+- Name: DSNT
   Paper:
-    Title: A Simple Coordinate Classification Perspective for Human Pose Estimation
-    URL: https://arxiv.org/abs/2107.03332
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/algorithms/simcc.md
+    Title: Numerical Coordinate Regression with Convolutional Neural Networks
+    URL: https://arxiv.org/abs/1801.07372v2
+  README: https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/algorithms/dsnt.md
 Models:
-- Config: configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py
-  In Collection: SimCC
+- Config: configs/body_2d_keypoint/integral_regression/coco/ipr_res50_dsnt-8xb64-210e_coco-256x256.py
+  In Collection: DSNT
   Metadata:
     Architecture: &id001
-    - SimCC
-    - ViPNAS
+    - DSNT
+    - ResNet
     Training Data: COCO
-  Name: simcc_vipnas-mbv3_8xb64-210e_coco-256x192
+  Name: ipr_res50_dsnt-8xb64-210e_coco-256x256
   Results:
   - Dataset: COCO
     Metrics:
-      AP: 0.695
-      AP@0.5: 0.883
-      AP@0.75: 0.772
-      AR: 0.755
-      AR@0.5: 0.927
+      AP: 0.674
+      AP@0.5: 0.87
+      AP@0.75: 0.744
+      AR: 0.764
+      AR@0.5: 0.928
     Task: Body 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192-719f3489_20220922.pth
+  Weights: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/integral_regression/coco/ipr_res50_dsnt-8xb64-210e_coco-256x256-441eedc0_20220913.pth
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/simcc/mpii/simcc_res50_wo-deconv-8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_alexnet_8xb64-210e_coco-256x192.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,74 +13,69 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=train_cfg['max_epochs'],
+        end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+
 # codec settings
 codec = dict(
-    type='SimCCLabel', input_size=(256, 256), sigma=6.0, simcc_split_ratio=2.0)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(40, 56), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
-    ),
+    backbone=dict(type='AlexNet', num_classes=-1),
     head=dict(
-        type='SimCCHead',
-        in_channels=2048,
-        out_channels=16,
-        input_size=codec['input_size'],
-        in_featuremap_size=(8, 8),
-        simcc_split_ratio=codec['simcc_split_ratio'],
-        deconv_out_channels=None,
-        loss=dict(type='KLDiscretLoss', use_target_weight=True),
+        type='HeatmapHead',
+        in_channels=256,
+        out_channels=17,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        shift_coords=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
-
-file_client_args = dict(backend='disk')
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,35 +83,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file=f'{data_root}/annotations/mpii_gt_val.mat',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/person_keypoints_val2017.json',
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
-
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = dict(
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_hrnet-w32_8xb64-210e_aic-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_hrnet-w32_8xb64-210e_aic-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,25 +90,25 @@
 # base dataset settings
 dataset_type = 'AicDataset'
 data_mode = 'topdown'
 data_root = 'data/aic/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_res101_8xb64-210e_aic-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvtv2-b2_8xb64-210e_coco-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -30,56 +30,63 @@
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
+norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+        type='PyramidVisionTransformerV2',
+        embed_dims=64,
+        num_layers=[3, 4, 6, 3],
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://github.com/whai362/PVT/'
+            'releases/download/v2/pvt_v2_b2.pth'),
     ),
+    neck=dict(type='FeatureMapProcessor', select_index=3),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
-        out_channels=14,
+        in_channels=512,
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'AicDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
-data_root = 'data/aic/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,36 +94,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/aic_train.json',
-        data_prefix=dict(img='ai_challenger_keypoint_train_20170902/'
-                         'keypoint_train_images_20170902/'),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/aic_val.json',
-        data_prefix=dict(img='ai_challenger_keypoint_validation_20170911/'
-                         'keypoint_validation_images_20170911/'),
+        ann_file='annotations/person_keypoints_val2017.json',
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/aic_val.json',
-    use_area=False)
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_aic-coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),
+    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.0),
     paramwise_cfg=dict(
         norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))
 
 # learning rate
 param_scheduler = [
     dict(
         type='LinearLR',
@@ -88,54 +88,54 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=1.,
-        widen_factor=1.,
+        deepen_factor=0.33,
+        widen_factor=0.5,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
             'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-l_8xb256-rsb-a1-600e_in1k-6a760974.pth')),
+            'cspnext-s_imagenet_600e-ea671761.pth')),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=512,
         out_channels=19,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=False,
         output_keypoint_indices=[
             target for _, target in keypoint_mapping_coco
         ]))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -154,22 +154,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -50,54 +50,54 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=1.,
-        widen_factor=1.,
+        deepen_factor=0.67,
+        widen_factor=0.75,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
             'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-l_8xb256-rsb-a1-600e_in1k-6a760974.pth')),
+            'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=768,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -116,22 +116,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_aic-coco-256x192.py`

 * *Files 8% similar despite different names*

```diff
@@ -88,54 +88,54 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.67,
-        widen_factor=0.75,
+        deepen_factor=1.,
+        widen_factor=1.,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
             'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')),
+            'cspnext-l_8xb256-rsb-a1-600e_in1k-6a760974.pth')),
     head=dict(
         type='HeatmapHead',
-        in_channels=768,
+        in_channels=1024,
         out_channels=19,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=False,
         output_keypoint_indices=[
             target for _, target in keypoint_mapping_coco
         ]))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -154,22 +154,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -50,54 +50,54 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.67,
-        widen_factor=0.75,
+        deepen_factor=0.167,
+        widen_factor=0.375,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
             'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')),
+            'cspnext-tiny_imagenet_600e-3a2dd350.pth')),
     head=dict(
         type='HeatmapHead',
-        in_channels=768,
+        in_channels=384,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -116,22 +116,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -191,20 +191,20 @@
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='coco/AP', rule='greater', max_keep_ckpts=1))
 
 custom_hooks = [
-    dict(
-        type='EMAHook',
-        ema_type='ExpMomentumEMA',
-        momentum=0.0002,
-        update_buffers=True,
-        priority=49),
+    # dict(
+    #     type='EMAHook',
+    #     ema_type='ExpMomentumEMA',
+    #     momentum=0.0002,
+    #     update_buffers=True,
+    #     priority=49),
     dict(
         type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - stage2_num_epochs,
         switch_pipeline=train_pipeline_stage2)
 ]
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_aic-coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
-    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.0),
+    optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),
     paramwise_cfg=dict(
         norm_decay_mult=0, bias_decay_mult=0, bypass_duplicate=True))
 
 # learning rate
 param_scheduler = [
     dict(
         type='LinearLR',
@@ -88,54 +88,54 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.33,
-        widen_factor=0.5,
+        deepen_factor=0.67,
+        widen_factor=0.75,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
             'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-s_imagenet_600e-ea671761.pth')),
+            'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')),
     head=dict(
         type='HeatmapHead',
-        in_channels=512,
+        in_channels=768,
         out_channels=19,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=False,
         output_keypoint_indices=[
             target for _, target in keypoint_mapping_coco
         ]))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -154,22 +154,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -79,25 +79,25 @@
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -116,22 +116,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_aic-coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_aic-coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -117,25 +117,25 @@
         ]))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -154,22 +154,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_coco-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -50,54 +50,54 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.167,
-        widen_factor=0.375,
+        deepen_factor=1.,
+        widen_factor=1.,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
             'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-tiny_imagenet_600e-3a2dd350.pth')),
+            'cspnext-l_8xb256-rsb-a1-600e_in1k-6a760974.pth')),
     head=dict(
         type='HeatmapHead',
-        in_channels=384,
+        in_channels=1024,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -116,22 +116,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -191,20 +191,20 @@
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='coco/AP', rule='greater', max_keep_ckpts=1))
 
 custom_hooks = [
-    # dict(
-    #     type='EMAHook',
-    #     ema_type='ExpMomentumEMA',
-    #     momentum=0.0002,
-    #     update_buffers=True,
-    #     priority=49),
+    dict(
+        type='EMAHook',
+        ema_type='ExpMomentumEMA',
+        momentum=0.0002,
+        update_buffers=True,
+        priority=49),
     dict(
         type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - stage2_num_epochs,
         switch_pipeline=train_pipeline_stage2)
 ]
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hourglass_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hourglass_coco.yml`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 Collections:
 - Name: Hourglass
   Paper:
     Title: Stacked hourglass networks for human pose estimation
     URL: https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29
-  README:  https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/backbones/hourglass.md
+  README:  https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/backbones/hourglass.md
 Models:
 - Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-256x256.py
   In Collection: Hourglass
   Metadata:
     Architecture: &id001
     - Hourglass
     Training Data: COCO
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_augmentation_coco.yml`

 * *Files 14% similar despite different names*

```diff
@@ -1,73 +1,56 @@
 Collections:
-- Name: HRNet
+- Name: Albumentations
   Paper:
-    Title: Deep high-resolution representation learning for human pose estimation
-    URL: http://openaccess.thecvf.com/content_CVPR_2019/html/Sun_Deep_High-Resolution_Representation_Learning_for_Human_Pose_Estimation_CVPR_2019_paper.html
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/backbones/hrnet.md
+    Title: 'Albumentations: fast and flexible image augmentations'
+    URL: https://www.mdpi.com/649002
+  README: https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/techniques/albumentations.md
 Models:
-- Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py
-  In Collection: HRNet
-  Alias: human
+- Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_coarsedropout-8xb64-210e_coco-256x192.py
+  In Collection: Albumentations
   Metadata:
     Architecture: &id001
     - HRNet
     Training Data: COCO
-  Name: td-hm_hrnet-w32_8xb64-210e_coco-256x192
+  Name: td-hm_hrnet-w32_coarsedropout-8xb64-210e_coco-256x192
   Results:
   - Dataset: COCO
     Metrics:
-      AP: 0.746
-      AP@0.5: 0.904
-      AP@0.75: 0.819
-      AR: 0.799
-      AR@0.5: 0.942
+      AP: 0.753
+      AP@0.5: 0.908
+      AP@0.75: 0.822
+      AR: 0.805
+      AR@0.5: 0.944
     Task: Body 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth
-- Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-384x288.py
-  In Collection: HRNet
+  Weights: https://download.openmmlab.com/mmpose/top_down/augmentation/hrnet_w32_coco_256x192_coarsedropout-0f16a0ce_20210320.pth
+- Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_gridmask-8xb64-210e_coco-256x192.py
+  In Collection: Albumentations
   Metadata:
     Architecture: *id001
     Training Data: COCO
-  Name: td-hm_hrnet-w32_8xb64-210e_coco-384x288
+  Name: td-hm_hrnet-w32_gridmask-8xb64-210e_coco-256x192
   Results:
   - Dataset: COCO
     Metrics:
-      AP: 0.76
+      AP: 0.752
       AP@0.5: 0.906
-      AP@0.75: 0.83
-      AR: 0.81
+      AP@0.75: 0.825
+      AR: 0.804
       AR@0.5: 0.943
     Task: Body 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_384x288-d9f0d786_20200708.pth
-- Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192.py
-  In Collection: HRNet
+  Weights: https://download.openmmlab.com/mmpose/top_down/augmentation/hrnet_w32_coco_256x192_gridmask-868180df_20210320.pth
+- Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_photometric-8xb64-210e_coco-256x192.py
+  In Collection: Albumentations
   Metadata:
     Architecture: *id001
     Training Data: COCO
-  Name: td-hm_hrnet-w48_8xb32-210e_coco-256x192
+  Name: td-hm_hrnet-w32_photometric-8xb64-210e_coco-256x192
   Results:
   - Dataset: COCO
     Metrics:
-      AP: 0.756
-      AP@0.5: 0.907
+      AP: 0.754
+      AP@0.5: 0.908
       AP@0.75: 0.825
-      AR: 0.806
-      AR@0.5: 0.942
-    Task: Body 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth
-- Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-384x288.py
-  In Collection: HRNet
-  Metadata:
-    Architecture: *id001
-    Training Data: COCO
-  Name: td-hm_hrnet-w48_8xb32-210e_coco-384x288
-  Results:
-  - Dataset: COCO
-    Metrics:
-      AP: 0.767
-      AP@0.5: 0.91
-      AP@0.75: 0.831
-      AR: 0.816
-      AR@0.5: 0.946
+      AR: 0.805
+      AR@0.5: 0.943
     Task: Body 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_384x288-314c8528_20200708.pth
+  Weights: https://download.openmmlab.com/mmpose/top_down/augmentation/hrnet_w32_coco_256x192_photometric-308cf591_20210320.pth
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/litehrnet_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/litehrnet_coco.yml`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 Collections:
 - Name: LiteHRNet
   Paper:
     Title: 'Lite-HRNet: A Lightweight High-Resolution Network'
     URL: https://arxiv.org/abs/2104.06403
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/backbones/litehrnet.md
+  README: https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/backbones/litehrnet.md
 Models:
 - Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb64-210e_coco-256x192.py
   In Collection: LiteHRNet
   Metadata:
     Architecture: &id001
     - LiteHRNet
     Training Data: COCO
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/mspn_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/mspn_coco.yml`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 Collections:
 - Name: MSPN
   Paper:
     Title: Rethinking on Multi-Stage Networks for Human Pose Estimation
     URL: https://arxiv.org/abs/1901.00148
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/backbones/mspn.md
+  README: https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/backbones/mspn.md
 Models:
 - Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mspn50_8xb32-210e_coco-256x192.py
   In Collection: MSPN
   Metadata:
     Architecture: &id001
     - MSPN
     Training Data: COCO
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xmspn50_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_4xmspn50_8xb32-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -47,69 +47,69 @@
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='MSPN',
         unit_channels=256,
-        num_stages=2,
+        num_stages=4,
         num_units=4,
         num_blocks=[3, 4, 6, 3],
         norm_cfg=dict(type='BN'),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='torchvision://resnet50',
         )),
     head=dict(
         type='MSPNHead',
         out_shape=(64, 48),
         unit_channels=256,
         out_channels=17,
-        num_stages=2,
+        num_stages=4,
         num_units=4,
         norm_cfg=dict(type='BN'),
         # each sub list is for a stage
         # and each element in each list is for a unit
-        level_indices=[0, 1, 2, 3] + [1, 2, 3, 4],
+        level_indices=[0, 1, 2, 3] * 3 + [1, 2, 3, 4],
         loss=([
             dict(
                 type='KeypointMSELoss',
                 use_target_weight=True,
                 loss_weight=0.25)
         ] * 3 + [
             dict(
                 type='KeypointOHKMMSELoss',
                 use_target_weight=True,
                 loss_weight=1.)
-        ]) * 2,
+        ]) * 4,
         decoder=codec[-1]),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xrsn50_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn50_8xb32-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 # multiple kernel_sizes of heatmap gaussian for 'Megvii' approach.
-kernel_sizes = [15, 11, 9, 7, 5]
+kernel_sizes = [11, 9, 7, 5]
 codec = [
     dict(
         type='MegviiHeatmap',
         input_size=(192, 256),
         heatmap_size=(48, 64),
         kernel_size=kernel_size) for kernel_size in kernel_sizes
 ]
@@ -47,68 +47,68 @@
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='RSN',
         unit_channels=256,
-        num_stages=2,
+        num_stages=1,
         num_units=4,
         num_blocks=[3, 4, 6, 3],
         num_steps=4,
         norm_cfg=dict(type='BN'),
     ),
     head=dict(
         type='MSPNHead',
         out_shape=(64, 48),
         unit_channels=256,
         out_channels=17,
-        num_stages=2,
+        num_stages=1,
         num_units=4,
         norm_cfg=dict(type='BN'),
         # each sub list is for a stage
         # and each element in each list is for a unit
-        level_indices=[0, 1, 2, 3] + [1, 2, 3, 4],
-        loss=([
+        level_indices=[0, 1, 2, 3],
+        loss=[
             dict(
                 type='KeypointMSELoss',
                 use_target_weight=True,
                 loss_weight=0.25)
         ] * 3 + [
             dict(
                 type='KeypointOHKMMSELoss',
                 use_target_weight=True,
                 loss_weight=1.)
-        ]) * 2,
+        ],
         decoder=codec[-1]),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xmspn50_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xmspn50_8xb32-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -91,25 +91,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xrsn50_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn18_8xb32-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,40 +2,40 @@
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=5e-3,
+    lr=2e-2,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[170, 200],
+        milestones=[170, 190, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 # multiple kernel_sizes of heatmap gaussian for 'Megvii' approach.
-kernel_sizes = [15, 11, 9, 7, 5]
+kernel_sizes = [11, 9, 7, 5]
 codec = [
     dict(
         type='MegviiHeatmap',
         input_size=(192, 256),
         heatmap_size=(48, 64),
         kernel_size=kernel_size) for kernel_size in kernel_sizes
 ]
@@ -47,68 +47,68 @@
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='RSN',
         unit_channels=256,
-        num_stages=3,
+        num_stages=1,
         num_units=4,
-        num_blocks=[3, 4, 6, 3],
+        num_blocks=[2, 2, 2, 2],
         num_steps=4,
         norm_cfg=dict(type='BN'),
     ),
     head=dict(
         type='MSPNHead',
         out_shape=(64, 48),
         unit_channels=256,
         out_channels=17,
-        num_stages=3,
+        num_stages=1,
         num_units=4,
         norm_cfg=dict(type='BN'),
         # each sub list is for a stage
         # and each element in each list is for a unit
-        level_indices=[0, 1, 2, 3] * 2 + [1, 2, 3, 4],
-        loss=([
+        level_indices=[0, 1, 2, 3],
+        loss=[
             dict(
                 type='KeypointMSELoss',
                 use_target_weight=True,
                 loss_weight=0.25)
         ] * 3 + [
             dict(
                 type='KeypointOHKMMSELoss',
                 use_target_weight=True,
                 loss_weight=1.)
-        ]) * 3,
+        ],
         decoder=codec[-1]),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_4xmspn50_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mspn50_8xb32-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 # multiple kernel_sizes of heatmap gaussian for 'Megvii' approach.
-kernel_sizes = [15, 11, 9, 7, 5]
+kernel_sizes = [11, 9, 7, 5]
 codec = [
     dict(
         type='MegviiHeatmap',
         input_size=(192, 256),
         heatmap_size=(48, 64),
         kernel_size=kernel_size) for kernel_size in kernel_sizes
 ]
@@ -47,69 +47,69 @@
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='MSPN',
         unit_channels=256,
-        num_stages=4,
+        num_stages=1,
         num_units=4,
         num_blocks=[3, 4, 6, 3],
         norm_cfg=dict(type='BN'),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='torchvision://resnet50',
         )),
     head=dict(
         type='MSPNHead',
         out_shape=(64, 48),
         unit_channels=256,
         out_channels=17,
-        num_stages=4,
+        num_stages=1,
         num_units=4,
         norm_cfg=dict(type='BN'),
         # each sub list is for a stage
         # and each element in each list is for a unit
-        level_indices=[0, 1, 2, 3] * 3 + [1, 2, 3, 4],
-        loss=([
+        level_indices=[0, 1, 2, 3],
+        loss=[
             dict(
                 type='KeypointMSELoss',
                 use_target_weight=True,
                 loss_weight=0.25)
         ] * 3 + [
             dict(
                 type='KeypointOHKMMSELoss',
                 use_target_weight=True,
                 loss_weight=1.)
-        ]) * 4,
+        ],
         decoder=codec[-1]),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge-simple_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,16 +8,16 @@
     imports=['mmpose.engine.optim_wrappers.layer_decay_optim_wrapper'],
     allow_failed_imports=False)
 
 optim_wrapper = dict(
     optimizer=dict(
         type='AdamW', lr=5e-4, betas=(0.9, 0.999), weight_decay=0.1),
     paramwise_cfg=dict(
-        num_layers=12,
-        layer_decay_rate=0.75,
+        num_layers=32,
+        layer_decay_rate=0.85,
         custom_keys={
             'bias': dict(decay_multi=0.0),
             'pos_embed': dict(decay_mult=0.0),
             'relative_position_bias_table': dict(decay_mult=0.0),
             'norm': dict(decay_mult=0.0),
         },
     ),
@@ -55,61 +55,63 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
-        arch='base',
+        type='mmpretrain.VisionTransformer',
+        arch='huge',
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
-        drop_path_rate=0.3,
+        drop_path_rate=0.55,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_base.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_huge.pth'),
     ),
+    neck=dict(type='FeatureMapProcessor', scale_factor=4.0, apply_relu=True),
     head=dict(
         type='HeatmapHead',
-        in_channels=768,
+        in_channels=1280,
         out_channels=17,
         deconv_out_channels=[],
         deconv_kernel_sizes=[],
+        final_layer=dict(kernel_size=3, padding=1),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec,
-        extra=dict(upsample=4, final_conv_kernel=3),
     ),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge_8xb64-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,16 +8,16 @@
     imports=['mmpose.engine.optim_wrappers.layer_decay_optim_wrapper'],
     allow_failed_imports=False)
 
 optim_wrapper = dict(
     optimizer=dict(
         type='AdamW', lr=5e-4, betas=(0.9, 0.999), weight_decay=0.1),
     paramwise_cfg=dict(
-        num_layers=12,
-        layer_decay_rate=0.75,
+        num_layers=32,
+        layer_decay_rate=0.85,
         custom_keys={
             'bias': dict(decay_multi=0.0),
             'pos_embed': dict(decay_mult=0.0),
             'relative_position_bias_table': dict(decay_mult=0.0),
             'norm': dict(decay_mult=0.0),
         },
     ),
@@ -55,30 +55,31 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
-        arch='base',
+        type='mmpretrain.VisionTransformer',
+        arch='huge',
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
-        drop_path_rate=0.3,
+        drop_path_rate=0.55,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_base.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_huge.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=768,
+        in_channels=1280,
         out_channels=17,
         deconv_out_channels=(256, 256),
         deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
@@ -89,25 +90,25 @@
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge-simple_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,16 +8,16 @@
     imports=['mmpose.engine.optim_wrappers.layer_decay_optim_wrapper'],
     allow_failed_imports=False)
 
 optim_wrapper = dict(
     optimizer=dict(
         type='AdamW', lr=5e-4, betas=(0.9, 0.999), weight_decay=0.1),
     paramwise_cfg=dict(
-        num_layers=32,
-        layer_decay_rate=0.85,
+        num_layers=24,
+        layer_decay_rate=0.8,
         custom_keys={
             'bias': dict(decay_multi=0.0),
             'pos_embed': dict(decay_mult=0.0),
             'relative_position_bias_table': dict(decay_mult=0.0),
             'norm': dict(decay_mult=0.0),
         },
     ),
@@ -55,61 +55,63 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
-        arch='huge',
+        type='mmpretrain.VisionTransformer',
+        arch='large',
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
-        drop_path_rate=0.55,
+        drop_path_rate=0.5,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_huge.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_large.pth'),
     ),
+    neck=dict(type='FeatureMapProcessor', scale_factor=4.0, apply_relu=True),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
+        in_channels=1024,
         out_channels=17,
         deconv_out_channels=[],
         deconv_kernel_sizes=[],
+        final_layer=dict(kernel_size=3, padding=1),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec,
-        extra=dict(upsample=4, final_conv_kernel=3),
     ),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,16 +8,16 @@
     imports=['mmpose.engine.optim_wrappers.layer_decay_optim_wrapper'],
     allow_failed_imports=False)
 
 optim_wrapper = dict(
     optimizer=dict(
         type='AdamW', lr=5e-4, betas=(0.9, 0.999), weight_decay=0.1),
     paramwise_cfg=dict(
-        num_layers=32,
-        layer_decay_rate=0.85,
+        num_layers=24,
+        layer_decay_rate=0.8,
         custom_keys={
             'bias': dict(decay_multi=0.0),
             'pos_embed': dict(decay_mult=0.0),
             'relative_position_bias_table': dict(decay_mult=0.0),
             'norm': dict(decay_mult=0.0),
         },
     ),
@@ -55,30 +55,31 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
-        arch='huge',
+        type='mmpretrain.VisionTransformer',
+        arch='large',
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
-        drop_path_rate=0.55,
+        drop_path_rate=0.5,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_huge.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_large.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
+        in_channels=1024,
         out_channels=17,
         deconv_out_channels=(256, 256),
         deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
@@ -89,25 +90,25 @@
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base_8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -8,16 +8,16 @@
     imports=['mmpose.engine.optim_wrappers.layer_decay_optim_wrapper'],
     allow_failed_imports=False)
 
 optim_wrapper = dict(
     optimizer=dict(
         type='AdamW', lr=5e-4, betas=(0.9, 0.999), weight_decay=0.1),
     paramwise_cfg=dict(
-        num_layers=24,
-        layer_decay_rate=0.8,
+        num_layers=12,
+        layer_decay_rate=0.75,
         custom_keys={
             'bias': dict(decay_multi=0.0),
             'pos_embed': dict(decay_mult=0.0),
             'relative_position_bias_table': dict(decay_mult=0.0),
             'norm': dict(decay_mult=0.0),
         },
     ),
@@ -55,61 +55,60 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
-        arch='large',
+        type='mmpretrain.VisionTransformer',
+        arch='base',
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
-        drop_path_rate=0.5,
+        drop_path_rate=0.3,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_large.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_base.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=768,
         out_channels=17,
-        deconv_out_channels=[],
-        deconv_kernel_sizes=[],
+        deconv_out_channels=(256, 256),
+        deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec,
-        extra=dict(upsample=4, final_conv_kernel=3),
-    ),
+        decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,16 +8,16 @@
     imports=['mmpose.engine.optim_wrappers.layer_decay_optim_wrapper'],
     allow_failed_imports=False)
 
 optim_wrapper = dict(
     optimizer=dict(
         type='AdamW', lr=5e-4, betas=(0.9, 0.999), weight_decay=0.1),
     paramwise_cfg=dict(
-        num_layers=24,
-        layer_decay_rate=0.8,
+        num_layers=12,
+        layer_decay_rate=0.75,
         custom_keys={
             'bias': dict(decay_multi=0.0),
             'pos_embed': dict(decay_mult=0.0),
             'relative_position_bias_table': dict(decay_mult=0.0),
             'norm': dict(decay_mult=0.0),
         },
     ),
@@ -55,59 +55,63 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
-        arch='large',
+        type='mmpretrain.VisionTransformer',
+        arch='base',
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
-        drop_path_rate=0.5,
+        drop_path_rate=0.3,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_large.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_base.pth'),
     ),
+    neck=dict(type='FeatureMapProcessor', scale_factor=4.0, apply_relu=True),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=768,
         out_channels=17,
-        deconv_out_channels=(256, 256),
-        deconv_kernel_sizes=(4, 4),
+        deconv_out_channels=[],
+        deconv_kernel_sizes=[],
+        final_layer=dict(kernel_size=3, padding=1),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
+        decoder=codec,
+    ),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small-simple_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small_8xb64-210e_coco-256x192.py`

 * *Files 7% similar despite different names*

```diff
@@ -55,66 +55,65 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
+        type='mmpretrain.VisionTransformer',
         arch={
             'embed_dims': 384,
             'num_layers': 12,
             'num_heads': 12,
             'feedforward_channels': 384 * 4
         },
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
         drop_path_rate=0.1,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_small.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_small.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=384,
         out_channels=17,
-        deconv_out_channels=[],
-        deconv_kernel_sizes=[],
+        deconv_out_channels=(256, 256),
+        deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec,
-        extra=dict(upsample=4, final_conv_kernel=3),
-    ),
+        decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small-simple_8xb64-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,64 +55,68 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='mmcls.VisionTransformer',
+        type='mmpretrain.VisionTransformer',
         arch={
             'embed_dims': 384,
             'num_layers': 12,
             'num_heads': 12,
             'feedforward_channels': 384 * 4
         },
         img_size=(256, 192),
         patch_size=16,
         qkv_bias=True,
         drop_path_rate=0.1,
         with_cls_token=False,
-        output_cls_token=False,
+        out_type='featmap',
         patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='pretrained/mae_pretrain_vit_small.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'v1/pretrained_models/mae_pretrain_vit_small.pth'),
     ),
+    neck=dict(type='FeatureMapProcessor', scale_factor=4.0, apply_relu=True),
     head=dict(
         type='HeatmapHead',
         in_channels=384,
         out_channels=17,
-        deconv_out_channels=(256, 256),
-        deconv_kernel_sizes=(4, 4),
+        deconv_out_channels=[],
+        deconv_kernel_sizes=[],
+        final_layer=dict(kernel_size=3, padding=1),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
+        decoder=codec,
+    ),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_alexnet_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_mobilenetv2_8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,98 +20,101 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(40, 56), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='AlexNet', num_classes=-1),
+    backbone=dict(
+        type='MobileNetV2',
+        widen_factor=1.,
+        out_indices=(7, ),
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://mobilenet_v2')),
     head=dict(
         type='HeatmapHead',
-        in_channels=256,
-        out_channels=17,
+        in_channels=1280,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_cpm_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_cpm_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(36, 48), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(24, 32), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -49,15 +49,15 @@
         num_stages=6),
     head=dict(
         type='CPMHead',
         in_channels=17,
         out_channels=17,
         num_stages=6,
         deconv_out_channels=None,
-        has_final_layer=False,
+        final_layer=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -65,33 +65,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_cpm_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest101_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,37 +27,33 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(24, 32), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='CPM',
-        in_channels=3,
-        out_channels=17,
-        feat_channels=128,
-        num_stages=6),
+        type='ResNeSt',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest101'),
+    ),
     head=dict(
-        type='CPMHead',
-        in_channels=17,
+        type='HeatmapHead',
+        in_channels=2048,
         out_channels=17,
-        num_stages=6,
-        deconv_out_channels=None,
-        has_final_layer=False,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -65,25 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -62,25 +62,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-384x384.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hourglass52_8xb32-210e_coco-384x384.py`

 * *Files 3% similar despite different names*

```diff
@@ -62,25 +62,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-base_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-base_8xb32-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -110,26 +110,26 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-base_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-384x288.py`

 * *Files 5% similar despite different names*

```diff
@@ -48,59 +48,59 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='HRFormer',
         in_channels=3,
         norm_cfg=norm_cfg,
         extra=dict(
-            drop_path_rate=0.2,
+            drop_path_rate=0.1,
             with_rpe=True,
             stage1=dict(
                 num_modules=1,
                 num_branches=1,
                 block='BOTTLENECK',
                 num_blocks=(2, ),
                 num_channels=(64, ),
                 num_heads=[2],
-                mlp_ratios=[4]),
+                num_mlp_ratios=[4]),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='HRFORMERBLOCK',
                 num_blocks=(2, 2),
-                num_channels=(78, 156),
-                num_heads=[2, 4],
+                num_channels=(32, 64),
+                num_heads=[1, 2],
                 mlp_ratios=[4, 4],
                 window_sizes=[7, 7]),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='HRFORMERBLOCK',
                 num_blocks=(2, 2, 2),
-                num_channels=(78, 156, 312),
-                num_heads=[2, 4, 8],
+                num_channels=(32, 64, 128),
+                num_heads=[1, 2, 4],
                 mlp_ratios=[4, 4, 4],
                 window_sizes=[7, 7, 7]),
             stage4=dict(
                 num_modules=2,
                 num_branches=4,
                 block='HRFORMERBLOCK',
                 num_blocks=(2, 2, 2, 2),
-                num_channels=(78, 156, 312, 624),
-                num_heads=[2, 4, 8, 16],
+                num_channels=(32, 64, 128, 256),
+                num_heads=[1, 2, 4, 8],
                 mlp_ratios=[4, 4, 4, 4],
                 window_sizes=[7, 7, 7, 7])),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrformer_base-32815020_20220226.pth'),
+            'pretrain_models/hrformer_small-09516375_20220226.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=78,
+        in_channels=32,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
@@ -110,26 +110,26 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -110,26 +110,26 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrformer-small_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-merge.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,17 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(
-    optimizer=dict(
-        type='AdamW',
-        lr=5e-4,
-        betas=(0.9, 0.999),
-        weight_decay=0.01,
-    ),
-    paramwise_cfg=dict(
-        custom_keys={'relative_position_bias_table': dict(decay_mult=0.)}))
+optim_wrapper = dict(optimizer=dict(
+    type='Adam',
+    lr=5e-4,
+))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
@@ -25,78 +20,63 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='HRFormer',
+        type='HRNet',
         in_channels=3,
-        norm_cfg=norm_cfg,
         extra=dict(
-            drop_path_rate=0.1,
-            with_rpe=True,
             stage1=dict(
                 num_modules=1,
                 num_branches=1,
                 block='BOTTLENECK',
-                num_blocks=(2, ),
-                num_channels=(64, ),
-                num_heads=[2],
-                num_mlp_ratios=[4]),
+                num_blocks=(4, ),
+                num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
-                block='HRFORMERBLOCK',
-                num_blocks=(2, 2),
-                num_channels=(32, 64),
-                num_heads=[1, 2],
-                mlp_ratios=[4, 4],
-                window_sizes=[7, 7]),
+                block='BASIC',
+                num_blocks=(4, 4),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
-                block='HRFORMERBLOCK',
-                num_blocks=(2, 2, 2),
-                num_channels=(32, 64, 128),
-                num_heads=[1, 2, 4],
-                mlp_ratios=[4, 4, 4],
-                window_sizes=[7, 7, 7]),
+                block='BASIC',
+                num_blocks=(4, 4, 4),
+                num_channels=(32, 64, 128)),
             stage4=dict(
-                num_modules=2,
+                num_modules=3,
                 num_branches=4,
-                block='HRFORMERBLOCK',
-                num_blocks=(2, 2, 2, 2),
-                num_channels=(32, 64, 128, 256),
-                num_heads=[1, 2, 4, 8],
-                mlp_ratios=[4, 4, 4, 4],
-                window_sizes=[7, 7, 7, 7])),
+                block='BASIC',
+                num_blocks=(4, 4, 4, 4),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrformer_small-09516375_20220226.pth'),
+            'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=32,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
@@ -110,44 +90,80 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
+# train datasets
+dataset_coco = dict(
+    type=dataset_type,
+    data_root=data_root,
+    data_mode=data_mode,
+    ann_file='annotations/person_keypoints_train2017.json',
+    data_prefix=dict(img='train2017/'),
+    pipeline=[],
+)
+
+dataset_aic = dict(
+    type='AicDataset',
+    data_root='data/aic/',
+    data_mode=data_mode,
+    ann_file='annotations/aic_train.json',
+    data_prefix=dict(img='ai_challenger_keypoint_train_20170902/'
+                     'keypoint_train_images_20170902/'),
+    pipeline=[
+        dict(
+            type='KeypointConverter',
+            num_keypoints=17,
+            mapping=[
+                (0, 6),
+                (1, 8),
+                (2, 10),
+                (3, 5),
+                (4, 7),
+                (5, 9),
+                (6, 12),
+                (7, 14),
+                (8, 16),
+                (9, 11),
+                (10, 13),
+                (11, 15),
+            ])
+    ],
+)
+
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        type='CombinedDataset',
+        metainfo=dict(from_file='configs/_base_/datasets/coco.py'),
+        datasets=[dataset_coco, dataset_aic],
         pipeline=train_pipeline,
+        test_mode=False,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
@@ -165,10 +181,7 @@
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
     ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
-
-# fp16 settings
-fp16 = dict(loss_scale='dynamic')
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,19 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(192, 256),
+    heatmap_size=(48, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -90,25 +94,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-384x288.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,25 +90,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-combine.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-combine.py`

 * *Files 2% similar despite different names*

```diff
@@ -132,25 +132,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # train datasets
 dataset_coco = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-aic-256x192-merge.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_coarsedropout-8xb64-210e_coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -67,16 +67,18 @@
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w32-36af842e.pth'),
+            prefix='backbone.',
+            checkpoint='https://download.openmmlab.com/mmpose/v1/'
+            'body_2d_keypoint/topdown_heatmap/coco/'
+            'td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=32,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
@@ -90,80 +92,56 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(
+        type='Albumentation',
+        transforms=[
+            dict(
+                type='CoarseDropout',
+                max_holes=8,
+                max_height=40,
+                max_width=40,
+                min_holes=1,
+                min_height=10,
+                min_width=10,
+                p=0.5),
+        ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
-# train datasets
-dataset_coco = dict(
-    type=dataset_type,
-    data_root=data_root,
-    data_mode=data_mode,
-    ann_file='annotations/person_keypoints_train2017.json',
-    data_prefix=dict(img='train2017/'),
-    pipeline=[],
-)
-
-dataset_aic = dict(
-    type='AicDataset',
-    data_root='data/aic/',
-    data_mode=data_mode,
-    ann_file='annotations/aic_train.json',
-    data_prefix=dict(img='ai_challenger_keypoint_train_20170902/'
-                     'keypoint_train_images_20170902/'),
-    pipeline=[
-        dict(
-            type='KeypointConverter',
-            num_keypoints=17,
-            mapping=[
-                (0, 6),
-                (1, 8),
-                (2, 10),
-                (3, 5),
-                (4, 7),
-                (5, 9),
-                (6, 12),
-                (7, 14),
-                (8, 16),
-                (9, 11),
-                (10, 13),
-                (11, 15),
-            ])
-    ],
-)
-
 # data loaders
 train_dataloader = dict(
     batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
-        type='CombinedDataset',
-        metainfo=dict(from_file='configs/_base_/datasets/coco.py'),
-        datasets=[dataset_coco, dataset_aic],
+        type=dataset_type,
+        data_root=data_root,
+        data_mode=data_mode,
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
-        test_mode=False,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_coarsedropout-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='UDPHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -52,86 +52,71 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256))),
+                num_channels=(48, 96, 192, 384))),
         init_cfg=dict(
             type='Pretrained',
-            prefix='backbone.',
-            checkpoint='https://download.openmmlab.com/mmpose/v1/'
-            'body_2d_keypoint/topdown_heatmap/coco/'
-            'td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=32,
+        in_channels=48,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(
-        type='Albumentation',
-        transforms=[
-            dict(
-                type='CoarseDropout',
-                max_holes=8,
-                max_height=40,
-                max_width=40,
-                min_holes=1,
-                min_height=10,
-                min_width=10,
-                p=0.5),
-        ]),
+    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -28,17 +28,17 @@
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
+    input_size=(288, 384),
+    heatmap_size=(72, 96),
+    sigma=3,
     unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
@@ -94,25 +94,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_dark-8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,19 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(288, 384),
-    heatmap_size=(72, 96),
-    sigma=3,
-    unbiased=True)
+    type='UDPHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -56,71 +52,71 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256))),
+                num_channels=(48, 96, 192, 384))),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w32-36af842e.pth'),
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=32,
+        in_channels=48,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_gridmask-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_photometric-8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -92,35 +92,26 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(
-        type='Albumentation',
-        transforms=[
-            dict(
-                type='GridDropout',
-                unit_size_min=10,
-                unit_size_max=40,
-                random_offset=True,
-                p=0.5),
-        ]),
+    dict(type='PhotometricDistortion'),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_photometric-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-384x288.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,19 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(288, 384),
+    heatmap_size=(72, 96),
+    sigma=3,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -52,37 +56,35 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256))),
+                num_channels=(48, 96, 192, 384))),
         init_cfg=dict(
             type='Pretrained',
-            prefix='backbone.',
-            checkpoint='https://download.openmmlab.com/mmpose/v1/'
-            'body_2d_keypoint/topdown_heatmap/coco/'
-            'td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=32,
+        in_channels=48,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
@@ -92,34 +94,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='PhotometricDistortion'),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,25 +90,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-8xb64-210e_coco-384x288.py`

 * *Files 1% similar despite different names*

```diff
@@ -90,25 +90,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-regress-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_udp-regress-8xb64-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -95,25 +95,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-384x288.py`

 * *Files 5% similar despite different names*

```diff
@@ -23,19 +23,20 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -73,42 +74,42 @@
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=48,
-        out_channels=17,
+        out_channels=133,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -116,35 +117,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,19 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap',
+    input_size=(192, 256),
+    heatmap_size=(48, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -90,25 +94,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -23,23 +23,20 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -77,42 +74,42 @@
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=48,
-        out_channels=17,
+        out_channels=133,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -120,35 +117,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_dark-8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,19 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(288, 384),
-    heatmap_size=(72, 96),
-    sigma=3,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -94,25 +90,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-384x288.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='UDPHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -80,37 +80,37 @@
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
-        shift_heatmap=False,
+        shift_heatmap=True,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
+    dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
+    dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
     batch_size=32,
     num_workers=2,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_udp-8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_gridmask-8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='UDPHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -52,71 +52,83 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(48, 96)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384))),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+            prefix='backbone.',
+            checkpoint='https://download.openmmlab.com/mmpose/v1/'
+            'body_2d_keypoint/topdown_heatmap/coco/'
+            'td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=48,
+        in_channels=32,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
-        shift_heatmap=False,
+        shift_heatmap=True,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
+    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(
+        type='Albumentation',
+        transforms=[
+            dict(
+                type='GridDropout',
+                unit_size_min=10,
+                unit_size_max=40,
+                random_offset=True,
+                p=0.5),
+        ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
+    dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb32-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -44,15 +44,15 @@
     backbone=dict(
         type='LiteHRNet',
         in_channels=3,
         extra=dict(
             stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
             num_stages=3,
             stages_spec=dict(
-                num_modules=(2, 4, 2),
+                num_modules=(3, 8, 3),
                 num_branches=(2, 3, 4),
                 num_blocks=(2, 2, 2),
                 module_type=('LITE', 'LITE', 'LITE'),
                 with_fuse=(True, True, True),
                 reduce_ratios=(8, 8, 8),
                 num_channels=(
                     (40, 80),
@@ -77,28 +77,28 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb32-210e_coco-384x288.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -77,36 +77,36 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -77,36 +77,36 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-30_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_litehrnet-18_8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -44,15 +44,15 @@
     backbone=dict(
         type='LiteHRNet',
         in_channels=3,
         extra=dict(
             stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
             num_stages=3,
             stages_spec=dict(
-                num_modules=(3, 8, 3),
+                num_modules=(2, 4, 2),
                 num_branches=(2, 3, 4),
                 num_blocks=(2, 2, 2),
                 module_type=('LITE', 'LITE', 'LITE'),
                 with_fuse=(True, True, True),
                 reduce_ratios=(8, 8, 8),
                 num_channels=(
                     (40, 80),
@@ -77,28 +77,28 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,24 +38,21 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='MobileNetV2',
-        widen_factor=1.,
-        out_indices=(7, ),
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='mmcls://mobilenet_v2',
-        )),
+        type='ResNeXt',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnext50_32x4d'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
+        in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -64,25 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,24 +38,21 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='MobileNetV2',
-        widen_factor=1.,
-        out_indices=(7, ),
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='mmcls://mobilenet_v2',
-        )),
+        type='ResNeXt',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnext50_32x4d'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
+        in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -64,25 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mspn50_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xmspn50_8xb32-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 # multiple kernel_sizes of heatmap gaussian for 'Megvii' approach.
-kernel_sizes = [11, 9, 7, 5]
+kernel_sizes = [15, 11, 9, 7, 5]
 codec = [
     dict(
         type='MegviiHeatmap',
         input_size=(192, 256),
         heatmap_size=(48, 64),
         kernel_size=kernel_size) for kernel_size in kernel_sizes
 ]
@@ -47,69 +47,69 @@
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='MSPN',
         unit_channels=256,
-        num_stages=1,
+        num_stages=2,
         num_units=4,
         num_blocks=[3, 4, 6, 3],
         norm_cfg=dict(type='BN'),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='torchvision://resnet50',
         )),
     head=dict(
         type='MSPNHead',
         out_shape=(64, 48),
         unit_channels=256,
         out_channels=17,
-        num_stages=1,
+        num_stages=2,
         num_units=4,
         norm_cfg=dict(type='BN'),
         # each sub list is for a stage
         # and each element in each list is for a unit
-        level_indices=[0, 1, 2, 3],
-        loss=[
+        level_indices=[0, 1, 2, 3] + [1, 2, 3, 4],
+        loss=([
             dict(
                 type='KeypointMSELoss',
                 use_target_weight=True,
                 loss_weight=0.25)
         ] * 3 + [
             dict(
                 type='KeypointOHKMMSELoss',
                 use_target_weight=True,
                 loss_weight=1.)
-        ],
+        ]) * 2,
         decoder=codec[-1]),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvt-s_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvt-s_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -46,19 +46,19 @@
         type='PyramidVisionTransformer',
         num_layers=[3, 4, 6, 3],
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://github.com/whai362/PVT/'
             'releases/download/v2/pvt_small.pth'),
     ),
+    neck=dict(type='FeatureMapProcessor', select_index=3),
     head=dict(
         type='HeatmapHead',
-        in_channels=(64, 128, 320, 512),
+        in_channels=512,
         out_channels=17,
-        input_index=3,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -66,26 +66,26 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_pvtv2-b2_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,39 +27,33 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='PyramidVisionTransformerV2',
-        embed_dims=64,
-        num_layers=[3, 4, 6, 3],
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://github.com/whai362/PVT/'
-            'releases/download/v2/pvt_v2_b2.pth'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(64, 128, 320, 512),
+        in_channels=2048,
         out_channels=17,
-        input_index=3,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -67,26 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-384x288.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,15 +23,16 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
@@ -45,41 +46,41 @@
         type='ResNet',
         depth=101,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=133,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +88,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_mobilenetv2_8xb32-60e_coco-wholebody-face-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,121 +1,122 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=60, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=5e-4,
+    lr=2e-3,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[170, 200],
+        milestones=[40, 55],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
-    ),
+        type='MobileNetV2',
+        widen_factor=1.,
+        out_indices=(7, ),
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://mobilenet_v2')),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
-        out_channels=17,
+        in_channels=1280,
+        out_channels=68,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyFaceDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_dark-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/td-reg_res50_8xb64-210e_onehand10k-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,67 +23,64 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
-codec = dict(
-    type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
-    unbiased=True)
+codec = dict(type='RegressionLabel', input_size=(256, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
+    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='HeatmapHead',
+        type='RegressionHead',
         in_channels=2048,
-        out_channels=17,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        num_joints=21,
+        loss=dict(type='SmoothL1Loss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'OneHand10KDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/onehand10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -91,35 +88,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/onehand10k_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/onehand10k_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_dark-8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-320x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,23 +23,19 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(288, 384),
-    heatmap_size=(72, 96),
-    sigma=3,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 320), heatmap_size=(64, 80), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -49,41 +45,41 @@
         type='ResNet',
         depth=101,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=14,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CrowdPoseDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/crowdpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -91,35 +87,37 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mmpose_crowdpose_trainval.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mmpose_crowdpose_test.json',
+        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
+    use_area=False,
+    iou_type='keypoints_crowd',
+    prefix='crowdpose')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_res50_8xb32-60e_coco-wholebody-face-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,85 +1,86 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=60, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=5e-4,
+    lr=2e-3,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[170, 200],
+        milestones=[40, 55],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
-    ),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=68,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyFaceDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +88,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,66 +20,67 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=133,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +88,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_dark-8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hourglass52_8xb32-60e_coco-wholebody-face-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,89 +1,88 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=60, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=5e-4,
+    lr=2e-3,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[170, 200],
+        milestones=[40, 55],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        type='HourglassNet',
+        num_stacks=1,
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=17,
+        type='CPMHead',
+        in_channels=256,
+        out_channels=68,
+        num_stages=1,
+        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyFaceDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -91,35 +90,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_dark-8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb16-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,40 +20,35 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=128)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(288, 384),
-    heatmap_size=(72, 96),
-    sigma=3,
-    unbiased=True,
-    blur_kernel_size=17)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        type='ResNeSt',
+        depth=269,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest269'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -66,46 +61,46 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=16,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=16,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vgg16-bn_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,21 +38,22 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        type='VGG',
+        depth=16,
+        norm_cfg=dict(type='BN'),
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://vgg16_bn'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
+        in_channels=512,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -61,25 +62,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb48-210e_coco-384x288.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,15 +20,15 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=384)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
@@ -38,17 +38,16 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        type='SEResNet',
+        depth=152,
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,33 +60,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=48,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb32-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,32 +27,27 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        type='SEResNet',
+        depth=152,
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -65,33 +60,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_rle-8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,77 +13,69 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
+        end=train_cfg['max_epochs'],
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
 # codec settings
-codec = dict(
-    type='MSRAHeatmap',
-    input_size=(288, 384),
-    heatmap_size=(72, 96),
-    sigma=3,
-    unbiased=True)
+codec = dict(type='RegressionLabel', input_size=(192, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
+    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='HeatmapHead',
+        type='RLEHead',
         in_channels=2048,
-        out_channels=17,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        num_joints=17,
+        loss=dict(type='RLELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_coords=True,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -106,20 +98,24 @@
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
+        bbox_file=f'{data_root}person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    ann_file=f'{data_root}annotations/person_keypoints_val2017.json',
+    score_mode='bbox_rle')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest101_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res152_8xb64-210e_crowdpose-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,102 +20,104 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeSt',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest101'),
+        type='ResNet',
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=14,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CrowdPoseDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/crowdpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mmpose_crowdpose_trainval.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mmpose_crowdpose_test.json',
+        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
+    use_area=False,
+    iou_type='keypoints_crowd',
+    prefix='crowdpose')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest101_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,63 +23,63 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeSt',
+        type='ResNet',
         depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest101'),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=14,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CrowdPoseDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/crowdpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +87,37 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mmpose_crowdpose_trainval.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mmpose_crowdpose_test.json',
+        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
+    use_area=False,
+    iou_type='keypoints_crowd',
+    prefix='crowdpose')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb16-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res50_8xb64-210e_crowdpose-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,102 +20,104 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=128)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeSt',
-        depth=200,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest200'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=14,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CrowdPoseDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/crowdpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=16,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mmpose_crowdpose_trainval.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mmpose_crowdpose_test.json',
+        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
+    use_area=False,
+    iou_type='keypoints_crowd',
+    prefix='crowdpose')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_mobilenetv2_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,21 +38,24 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeSt',
-        depth=200,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest200'),
-    ),
+        type='MobileNetV2',
+        widen_factor=1.,
+        out_indices=(7, ),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='mmcls://mobilenet_v2',
+        )),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
+        in_channels=1280,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -61,25 +64,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb16-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_res50_8xb32-210e_onehand10k-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,102 +20,105 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=128)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeSt',
-        depth=269,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest269'),
-    ),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='torchvision://resnet50',
+        )),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'OneHand10KDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/onehand10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/onehand10k_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/onehand10k_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb64-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,17 +38,18 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeSt',
-        depth=269,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest269'),
+        type='ResNeXt',
+        depth=101,
+        init_cfg=dict(
+            type='Pretrained', checkpoint='mmcls://resnext101_32x4d'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,33 +62,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest50_8xb64-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -61,25 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest50_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest200_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,28 +27,28 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNeSt',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest50'),
+        depth=200,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest200'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,25 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb32-210e_coco-384x288.py`

 * *Files 6% similar despite different names*

```diff
@@ -38,17 +38,18 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
+        type='ResNeXt',
         depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet101_v1d'),
+        init_cfg=dict(
+            type='Pretrained', checkpoint='mmcls://resnext101_32x4d'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,25 +62,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_mobilenetv2_8xb64-210e_rhd2d-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,63 +23,67 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet101_v1d'),
-    ),
+        type='MobileNetV2',
+        widen_factor=1.,
+        out_indices=(7, ),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='mmcls://mobilenet_v2',
+        )),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
-        out_channels=17,
+        in_channels=1280,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'Rhd2DDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/rhd/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +91,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/rhd_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/rhd_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d152_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-384x288.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,99 +23,99 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet152_v1d'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=133,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d152_8xb48-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res101_8xb64-210e_ap10k-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -20,102 +20,116 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=384)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet152_v1d'),
+        type='ResNet',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'AP10KDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/ap10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=48,
-    num_workers=2,
+    batch_size=64,
+    num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/ap10k-train-split1.json',
+        data_prefix=dict(img='data/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=2,
+    num_workers=4,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/ap10k-val-split1.json',
+        data_prefix=dict(img='data/'),
+        test_mode=True,
+        pipeline=val_pipeline,
+    ))
+test_dataloader = dict(
+    batch_size=32,
+    num_workers=4,
+    persistent_workers=True,
+    drop_last=False,
+    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
+    dataset=dict(
+        type=dataset_type,
+        data_root=data_root,
+        data_mode=data_mode,
+        ann_file='annotations/ap10k-test-split1.json',
+        data_prefix=dict(img='data/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
-test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
-test_evaluator = val_evaluator
+    ann_file=data_root + 'annotations/ap10k-val-split1.json')
+test_evaluator = dict(
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/ap10k-test-split1.json')
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet50_8xb64-210e_coco-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -38,17 +38,20 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
+        type='SCNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet50_v1d'),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/scnet50-7ef0a199.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,25 +64,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d50_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,28 +27,28 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet50_v1d'),
+        type='ResNet',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,25 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_scnet50_8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,64 +23,65 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeXt',
-        depth=101,
+        type='SCNet',
+        depth=50,
         init_cfg=dict(
-            type='Pretrained', checkpoint='mmcls://resnext101_32x4d'),
-    ),
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/scnet50-7ef0a199.pth')),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,35 +89,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext101_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-skirt-256x192.py`

 * *Files 7% similar despite different names*

```diff
@@ -20,67 +20,67 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=64)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeXt',
-        depth=101,
-        init_cfg=dict(
-            type='Pretrained', checkpoint='mmcls://resnext101_32x4d'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,35 +88,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='train/deepfashion2_skirt_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='validation/deepfashion2_skirt_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res152_8xb64-210e_mpii-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,101 +22,97 @@
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
 # codec settings
-codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+codec = dict(type='RegressionLabel', input_size=(256, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeXt',
+        type='ResNet',
         depth=152,
-        init_cfg=dict(
-            type='Pretrained', checkpoint='mmcls://resnext152_32x4d'),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
+    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='HeatmapHead',
+        type='RegressionHead',
         in_channels=2048,
-        out_channels=17,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        num_joints=16,
+        loss=dict(type='SmoothL1Loss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_coords=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
+
+file_client_args = dict(backend='disk')
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage', file_client_args=file_client_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage', file_client_args=file_client_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file=f'{data_root}/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb48-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_mobilenetv2_8xb64-210e_onehand10k-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,103 +20,106 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=384)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeXt',
-        depth=152,
+        type='MobileNetV2',
+        widen_factor=1.,
+        out_indices=(7, ),
         init_cfg=dict(
-            type='Pretrained', checkpoint='mmcls://resnext152_32x4d'),
-    ),
+            type='Pretrained',
+            checkpoint='mmcls://mobilenet_v2',
+        )),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
-        out_channels=17,
+        in_channels=1280,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'OneHand10KDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/onehand10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=48,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/onehand10k_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/onehand10k_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_res50_8xb64-210e_rhd2d-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,63 +23,66 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeXt',
+        type='ResNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnext50_32x4d'),
-    ),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='torchvision://resnet50',
+        )),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'Rhd2DDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/rhd/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +90,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/rhd_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/rhd_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext50_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_8xb64-210e_coco-wholebody-256x192.py`

 * *Files 7% similar despite different names*

```diff
@@ -23,63 +23,66 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeXt',
+        type='ViPNAS_ResNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnext50_32x4d'),
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=17,
+        type='ViPNASHead',
+        in_channels=608,
+        out_channels=133,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +90,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn18_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xrsn50_8xb32-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,40 +2,40 @@
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=2e-2,
+    lr=5e-3,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[170, 190, 200],
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 # multiple kernel_sizes of heatmap gaussian for 'Megvii' approach.
-kernel_sizes = [11, 9, 7, 5]
+kernel_sizes = [15, 11, 9, 7, 5]
 codec = [
     dict(
         type='MegviiHeatmap',
         input_size=(192, 256),
         heatmap_size=(48, 64),
         kernel_size=kernel_size) for kernel_size in kernel_sizes
 ]
@@ -47,68 +47,68 @@
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='RSN',
         unit_channels=256,
-        num_stages=1,
+        num_stages=2,
         num_units=4,
-        num_blocks=[2, 2, 2, 2],
+        num_blocks=[3, 4, 6, 3],
         num_steps=4,
         norm_cfg=dict(type='BN'),
     ),
     head=dict(
         type='MSPNHead',
         out_shape=(64, 48),
         unit_channels=256,
         out_channels=17,
-        num_stages=1,
+        num_stages=2,
         num_units=4,
         norm_cfg=dict(type='BN'),
         # each sub list is for a stage
         # and each element in each list is for a unit
-        level_indices=[0, 1, 2, 3],
-        loss=[
+        level_indices=[0, 1, 2, 3] + [1, 2, 3, 4],
+        loss=([
             dict(
                 type='KeypointMSELoss',
                 use_target_weight=True,
                 loss_weight=0.25)
         ] * 3 + [
             dict(
                 type='KeypointOHKMMSELoss',
                 use_target_weight=True,
                 loss_weight=1.)
-        ],
+        ]) * 2,
         decoder=codec[-1]),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_rsn50_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xrsn50_8xb32-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 # multiple kernel_sizes of heatmap gaussian for 'Megvii' approach.
-kernel_sizes = [11, 9, 7, 5]
+kernel_sizes = [15, 11, 9, 7, 5]
 codec = [
     dict(
         type='MegviiHeatmap',
         input_size=(192, 256),
         heatmap_size=(48, 64),
         kernel_size=kernel_size) for kernel_size in kernel_sizes
 ]
@@ -47,68 +47,68 @@
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='RSN',
         unit_channels=256,
-        num_stages=1,
+        num_stages=3,
         num_units=4,
         num_blocks=[3, 4, 6, 3],
         num_steps=4,
         norm_cfg=dict(type='BN'),
     ),
     head=dict(
         type='MSPNHead',
         out_shape=(64, 48),
         unit_channels=256,
         out_channels=17,
-        num_stages=1,
+        num_stages=3,
         num_units=4,
         norm_cfg=dict(type='BN'),
         # each sub list is for a stage
         # and each element in each list is for a unit
-        level_indices=[0, 1, 2, 3],
-        loss=[
+        level_indices=[0, 1, 2, 3] * 2 + [1, 2, 3, 4],
+        loss=([
             dict(
                 type='KeypointMSELoss',
                 use_target_weight=True,
                 loss_weight=0.25)
         ] * 3 + [
             dict(
                 type='KeypointOHKMMSELoss',
                 use_target_weight=True,
                 loss_weight=1.)
-        ],
+        ]) * 3,
         decoder=codec[-1]),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=False,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='GenerateTarget', multilevel=True, encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec[0]['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet101_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,20 +38,17 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SCNet',
+        type='ResNetV1d',
         depth=101,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet101-94250a77.pth'),
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet101_v1d'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -64,47 +61,47 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
-    num_workers=1,
+    batch_size=64,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=1,
+    num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet101_8xb48-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_8xb32-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,15 +20,15 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
@@ -38,20 +38,17 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SCNet',
+        type='ResNet',
         depth=101,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet101-94250a77.pth'),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -64,46 +61,46 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=48,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet50_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,38 +20,35 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SCNet',
+        type='ResNet',
         depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet50-7ef0a199.pth'),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -64,47 +61,47 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
-    num_workers=1,
+    batch_size=64,
+    num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=1,
+    num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_scnet50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,31 +27,32 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(192, 256),
+    heatmap_size=(48, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SCNet',
+        type='ResNet',
         depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet50-7ef0a199.pth'),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -64,25 +65,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res50_8xb64-210e_ap10k-256x256.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,102 +20,116 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet101'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'AP10KDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/ap10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
-    num_workers=2,
+    batch_size=64,
+    num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/ap10k-train-split1.json',
+        data_prefix=dict(img='data/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=2,
+    num_workers=4,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/ap10k-val-split1.json',
+        data_prefix=dict(img='data/'),
+        test_mode=True,
+        pipeline=val_pipeline,
+    ))
+test_dataloader = dict(
+    batch_size=32,
+    num_workers=4,
+    persistent_workers=True,
+    drop_last=False,
+    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
+    dataset=dict(
+        type=dataset_type,
+        data_root=data_root,
+        data_mode=data_mode,
+        ann_file='annotations/ap10k-test-split1.json',
+        data_prefix=dict(img='data/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
-test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
-test_evaluator = val_evaluator
+    ann_file=data_root + 'annotations/ap10k-val-split1.json')
+test_evaluator = dict(
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/ap10k-test-split1.json')
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res152_8xb32-210e_mpii-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -20,102 +20,98 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet101'),
+        type='ResNet',
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=100, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,108 +13,126 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=100,
+        milestones=[50, 70],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='AUC', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(224, 224), heatmap_size=(56, 56), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=152,
-    ),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=21,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'FreiHandDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/freihand/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        shift_factor=0.25,
+        rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale', padding=0.8),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/freihand_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/freihand_val.json',
+        data_prefix=dict(img=''),
+        test_mode=True,
+        pipeline=val_pipeline,
+    ))
+test_dataloader = dict(
+    batch_size=32,
+    num_workers=2,
+    persistent_workers=True,
+    drop_last=False,
+    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
+    dataset=dict(
+        type=dataset_type,
+        data_root=data_root,
+        data_mode=data_mode,
+        ann_file='annotations/freihand_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
-test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet152_8xb48-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-short-sleeved-dress-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,101 +20,103 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=384)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=152,
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=17,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=48,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='train/deepfashion2_short_sleeved_dress_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='validation/deepfashion2_short_sleeved_dress_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_cpm_8xb64-210e_mpii-368x368.py`

 * *Files 8% similar despite different names*

```diff
@@ -23,63 +23,70 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(368, 368), heatmap_size=(46, 46), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet50'),
-    ),
+        type='CPM',
+        in_channels=3,
+        out_channels=16,
+        feat_channels=128,
+        num_stages=6),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=17,
+        type='CPMHead',
+        in_channels=16,
+        out_channels=16,
+        num_stages=6,
+        deconv_out_channels=None,
+        final_layer=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        shift_prob=0,
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +94,32 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-t-p4-w7_8xb32-210e_coco-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,39 +20,56 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
+norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet50'),
+        type='SwinTransformer',
+        embed_dims=96,
+        depths=[2, 2, 6, 2],
+        num_heads=[3, 6, 12, 24],
+        window_size=7,
+        mlp_ratio=4,
+        qkv_bias=True,
+        qk_scale=None,
+        drop_rate=0.,
+        attn_drop_rate=0.,
+        drop_path_rate=0.2,
+        patch_norm=True,
+        out_indices=(3, ),
+        with_cp=False,
+        convert_weights=True,
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://github.com/SwinTransformer/storage/releases/'
+            'download/v1.0.0/swin_tiny_patch4_window7_224.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
+        in_channels=768,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -61,33 +78,34 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv1_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet101_8xb64-210e_mpii-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -23,63 +23,65 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ShuffleNetV1',
-        groups=3,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v1'),
+        type='SCNet',
+        depth=101,
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/scnet101-94250a77.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=960,
-        out_channels=17,
+        in_channels=2048,
+        out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +89,32 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv1_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv2_8xb64-210e_mpii-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,63 +23,62 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ShuffleNetV1',
-        groups=3,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v1'),
+        type='ShuffleNetV2',
+        widen_factor=1.0,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v2'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=960,
-        out_channels=17,
+        in_channels=1024,
+        out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,35 +86,32 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv2_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb48-210e_coco-384x288.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,39 +20,40 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=384)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ShuffleNetV2',
-        widen_factor=1.0,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v2'),
+        type='ResNeXt',
+        depth=152,
+        init_cfg=dict(
+            type='Pretrained', checkpoint='mmcls://resnext152_32x4d'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -61,33 +62,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=48,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_shufflenetv2_8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnext152_8xb32-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,32 +27,33 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ShuffleNetV2',
-        widen_factor=1.0,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v2'),
+        type='ResNeXt',
+        depth=152,
+        init_cfg=dict(
+            type='Pretrained', checkpoint='mmcls://resnext152_32x4d'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -61,33 +62,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-b-p4-w7_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_litehrnet-w18_8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,81 +23,79 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SwinTransformer',
-        embed_dims=128,
-        depths=[2, 2, 18, 2],
-        num_heads=[4, 8, 16, 32],
-        window_size=7,
-        mlp_ratio=4,
-        qkv_bias=True,
-        qk_scale=None,
-        drop_rate=0.,
-        attn_drop_rate=0.,
-        drop_path_rate=0.3,
-        patch_norm=True,
-        out_indices=(3, ),
-        with_cp=False,
-        convert_weights=True,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://github.com/SwinTransformer/storage/releases/'
-            'download/v1.0.0/swin_base_patch4_window7_224_22k.pth'),
-    ),
+        type='LiteHRNet',
+        in_channels=3,
+        extra=dict(
+            stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
+            num_stages=3,
+            stages_spec=dict(
+                num_modules=(2, 4, 2),
+                num_branches=(2, 3, 4),
+                num_blocks=(2, 2, 2),
+                module_type=('LITE', 'LITE', 'LITE'),
+                with_fuse=(True, True, True),
+                reduce_ratios=(8, 8, 8),
+                num_channels=(
+                    (40, 80),
+                    (40, 80, 160),
+                    (40, 80, 160, 320),
+                )),
+            with_head=True,
+        )),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
-        out_channels=17,
+        in_channels=40,
+        out_channels=21,
+        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -105,35 +103,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-b-p4-w7_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-384x288.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,26 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
+optim_wrapper = dict(
+    optimizer=dict(
+        type='AdamW',
+        lr=5e-4,
+        betas=(0.9, 0.999),
+        weight_decay=0.01,
+    ),
+    paramwise_cfg=dict(
+        custom_keys={
+            'absolute_pos_embed': dict(decay_mult=0.),
+            'relative_position_bias_table': dict(decay_mult=0.),
+            'norm': dict(decay_mult=0.)
+        }))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
@@ -40,36 +49,36 @@
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='SwinTransformer',
-        embed_dims=128,
+        embed_dims=192,
         depths=[2, 2, 18, 2],
-        num_heads=[4, 8, 16, 32],
-        window_size=12,
+        num_heads=[6, 12, 24, 48],
+        window_size=7,
         mlp_ratio=4,
         qkv_bias=True,
         qk_scale=None,
         drop_rate=0.,
         attn_drop_rate=0.,
-        drop_path_rate=0.3,
+        drop_path_rate=0.5,
         patch_norm=True,
         out_indices=(3, ),
         with_cp=False,
         convert_weights=True,
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://github.com/SwinTransformer/storage/releases/'
             'download/v1.0.0/swin_base_patch4_window12_384_22k.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=1536,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -78,26 +87,26 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -87,26 +87,26 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,17 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(
-    optimizer=dict(
-        type='AdamW',
-        lr=5e-4,
-        betas=(0.9, 0.999),
-        weight_decay=0.01,
-    ),
-    paramwise_cfg=dict(
-        custom_keys={
-            'absolute_pos_embed': dict(decay_mult=0.),
-            'relative_position_bias_table': dict(decay_mult=0.),
-            'norm': dict(decay_mult=0.)
-        }))
+optim_wrapper = dict(optimizer=dict(
+    type='Adam',
+    lr=5e-4,
+))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
@@ -29,120 +20,131 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(256, 256),
+    heatmap_size=(64, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SwinTransformer',
-        embed_dims=192,
-        depths=[2, 2, 18, 2],
-        num_heads=[6, 12, 24, 48],
-        window_size=7,
-        mlp_ratio=4,
-        qkv_bias=True,
-        qk_scale=None,
-        drop_rate=0.,
-        attn_drop_rate=0.,
-        drop_path_rate=0.5,
-        patch_norm=True,
-        out_indices=(3, ),
-        with_cp=False,
-        convert_weights=True,
+        type='HRNet',
+        in_channels=3,
+        extra=dict(
+            stage1=dict(
+                num_modules=1,
+                num_branches=1,
+                block='BOTTLENECK',
+                num_blocks=(4, ),
+                num_channels=(64, )),
+            stage2=dict(
+                num_modules=1,
+                num_branches=2,
+                block='BASIC',
+                num_blocks=(4, 4),
+                num_channels=(48, 96)),
+            stage3=dict(
+                num_modules=4,
+                num_branches=3,
+                block='BASIC',
+                num_blocks=(4, 4, 4),
+                num_channels=(48, 96, 192)),
+            stage4=dict(
+                num_modules=3,
+                num_branches=4,
+                block='BASIC',
+                num_blocks=(4, 4, 4, 4),
+                num_channels=(48, 96, 192, 384))),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='https://github.com/SwinTransformer/storage/releases/'
-            'download/v1.0.0/swin_base_patch4_window12_384_22k.pth'),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1536,
-        out_channels=17,
+        in_channels=48,
+        out_channels=16,
+        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-t-p4-w7_8xb32-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-30_8xb64-210e_mpii-256x256.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,120 +20,118 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SwinTransformer',
-        embed_dims=96,
-        depths=[2, 2, 6, 2],
-        num_heads=[3, 6, 12, 24],
-        window_size=7,
-        mlp_ratio=4,
-        qkv_bias=True,
-        qk_scale=None,
-        drop_rate=0.,
-        attn_drop_rate=0.,
-        drop_path_rate=0.2,
-        patch_norm=True,
-        out_indices=(3, ),
-        with_cp=False,
-        convert_weights=True,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://github.com/SwinTransformer/storage/releases/'
-            'download/v1.0.0/swin_tiny_patch4_window7_224.pth'),
-    ),
+        type='LiteHRNet',
+        in_channels=3,
+        extra=dict(
+            stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
+            num_stages=3,
+            stages_spec=dict(
+                num_modules=(3, 8, 3),
+                num_branches=(2, 3, 4),
+                num_blocks=(2, 2, 2),
+                module_type=('LITE', 'LITE', 'LITE'),
+                with_fuse=(True, True, True),
+                reduce_ratios=(8, 8, 8),
+                num_channels=(
+                    (40, 80),
+                    (40, 80, 160),
+                    (40, 80, 160, 320),
+                )),
+            with_head=True,
+        )),
     head=dict(
         type='HeatmapHead',
-        in_channels=768,
-        out_channels=17,
+        in_channels=40,
+        out_channels=16,
+        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        shift_prob=0,
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vgg16-bn_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet50_8xb64-210e_mpii-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -23,64 +23,65 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='VGG',
-        depth=16,
-        norm_cfg=dict(type='BN'),
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://vgg16_bn'),
+        type='SCNet',
+        depth=50,
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/scnet50-7ef0a199.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=512,
-        out_channels=17,
+        in_channels=2048,
+        out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,35 +89,32 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-mbv3_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnest269_8xb32-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -37,21 +37,23 @@
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ViPNAS_MobileNetV3'),
+    backbone=dict(
+        type='ResNeSt',
+        depth=269,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnest269'),
+    ),
     head=dict(
-        type='ViPNASHead',
-        in_channels=160,
+        type='HeatmapHead',
+        in_channels=2048,
         out_channels=17,
-        deconv_out_channels=(160, 160, 160),
-        deconv_num_groups=(160, 160, 160),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -59,36 +61,33 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-res50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -37,18 +37,22 @@
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ViPNAS_ResNet', depth=50),
+    backbone=dict(
+        type='SEResNet',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet101'),
+    ),
     head=dict(
-        type='ViPNASHead',
-        in_channels=608,
+        type='HeatmapHead',
+        in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -57,28 +61,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.yml` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.yml`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 Collections:
 - Name: ViTPose
   Paper:
     Title: 'ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation'
     URL: https://arxiv.org/abs/2204.12484
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/algorithms/vitpose.md
+  README: https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/algorithms/vitpose.md
   Metadata:
     Training Resources: 8x A100 GPUs
 Models:
 - Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-small_8xb64-210e_coco-256x192.py
   In Collection: ViTPose
   Metadata:
     Architecture: &id001
@@ -112,29 +112,29 @@
     Metrics:
       AP: 0.756
       AP@0.5: 0.906
       AP@0.75: 0.826
       AR: 0.809
       AR@0.5: 0.946
     Task: Body 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192-fd73707d_20230314.pth
+  Weights: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192-0b8234ea_20230407.pth
 - Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192.py
   In Collection: ViTPose
   Alias: vitpose-l
   Metadata:
     Architecture: *id002
     Model Size: Large
     Training Data: COCO
   Name: td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192
   Results:
   - Dataset: COCO
     Metrics:
-      AP: 0.781
+      AP: 0.780
       AP@0.5: 0.914
-      AP@0.75: 0.853
+      AP@0.75: 0.851
       AR: 0.833
       AR@0.5: 0.952
     Task: Body 2D Keypoint
   Weights: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192-3a7ee9e1_20230314.pth
 - Config: configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-huge-simple_8xb64-210e_coco-256x192.py
   In Collection: ViTPose
   Alias: vitpose-h
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/cspnext-m_udp_8xb64-210e_crowpose-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/rtmpose-m_8xb64-210e_crowdpose-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
 max_epochs = 210
 stage2_num_epochs = 30
-base_lr = 4e-3
+base_lr = 5e-4
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
@@ -20,30 +20,34 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
 codec = dict(
-    type='UDPHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='SimCCLabel',
+    input_size=(192, 256),
+    sigma=(4.9, 5.66),
+    simcc_split_ratio=2.0,
+    normalize=False,
+    use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -59,45 +63,58 @@
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
-            checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
-            'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')),
+            checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+        )),
     head=dict(
-        type='HeatmapHead',
+        type='RTMCCHead',
         in_channels=768,
         out_channels=14,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        input_size=codec['input_size'],
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
+        simcc_split_ratio=codec['simcc_split_ratio'],
+        final_layer_kernel_size=7,
+        gau_cfg=dict(
+            hidden_dims=256,
+            s=128,
+            expansion_factor=2,
+            dropout_rate=0.,
+            drop_path=0.,
+            act_fn='SiLU',
+            use_rel_bias=False,
+            pos_enc=False),
+        loss=dict(
+            type='KLDiscretLoss',
+            use_target_weight=True,
+            beta=10.,
+            label_softmax=True),
         decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=False,
-    ))
+    test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CrowdPoseDataset'
 data_mode = 'topdown'
 data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/',
 #         f'{data_root}': 's3://openmmlab/datasets/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -116,22 +133,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -169,15 +186,15 @@
         data_mode=data_mode,
         ann_file='crowdpose/annotations/mmpose_crowdpose_trainval.json',
         data_prefix=dict(img='pose/CrowdPose/images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=2,
+    num_workers=10,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_hrnet-w32_8xb64-210e_crowdpose-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w32_8xb64-210e_animalpose-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -23,19 +23,19 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -73,42 +73,42 @@
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=32,
-        out_channels=14,
+        out_channels=20,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CrowdPoseDataset'
+dataset_type = 'AnimalPoseDataset'
 data_mode = 'topdown'
-data_root = 'data/crowdpose/'
+data_root = 'data/animalpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -116,37 +116,32 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_trainval.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/animalpose_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_test.json',
-        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/animalpose_val.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
-    use_area=False,
-    iou_type='keypoints_crowd',
-    prefix='crowdpose')
+    type='CocoMetric', ann_file=data_root + 'annotations/animalpose_val.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub3-368x368.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=40, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,111 +13,115 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=40,
+        milestones=[20, 30],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(368, 368), heatmap_size=(46, 46), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
-    ),
+        type='CPM',
+        in_channels=3,
+        out_channels=15,
+        feat_channels=128,
+        num_stages=6),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=14,
+        type='CPMHead',
+        in_channels=15,
+        out_channels=15,
+        num_stages=6,
+        deconv_out_channels=None,
+        final_layer=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CrowdPoseDataset'
+dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
-data_root = 'data/crowdpose/'
+data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_trainval.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/Sub3_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_test.json',
-        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/Sub3_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
-    use_area=False,
-    iou_type='keypoints_crowd',
-    prefix='crowdpose')
+val_evaluator = [
+    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-320x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub2-256x256.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=20, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,73 +13,74 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=20,
+        milestones=[8, 15],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 320), heatmap_size=(64, 80), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(
-        type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
-    ),
+    backbone=dict(type='ResNet', depth=50),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=14,
+        out_channels=15,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
+load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'CrowdPoseDataset'
+dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
-data_root = 'data/crowdpose/'
+data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,37 +88,33 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_trainval.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/Sub2_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_test.json',
-        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/Sub2_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
-    use_area=False,
-    iou_type='keypoints_crowd',
-    prefix='crowdpose')
+val_evaluator = [
+    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res152_8xb64-210e_crowdpose-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub3-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=20, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,73 +13,74 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=20,
+        milestones=[8, 15],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(
-        type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
-    ),
+    backbone=dict(type='ResNet', depth=50),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=14,
+        out_channels=15,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
+load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'CrowdPoseDataset'
+dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
-data_root = 'data/crowdpose/'
+data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,37 +88,33 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_trainval.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/Sub3_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_test.json',
-        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/Sub3_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
-    use_area=False,
-    iou_type='keypoints_crowd',
-    prefix='crowdpose')
+val_evaluator = [
+    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res50_8xb64-210e_crowdpose-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_8xb64-210e_coco-wholebody-256x192.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,63 +23,65 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='crowdpose/AP', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
-    ),
+    backbone=dict(type='ViPNAS_MobileNetV3'),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=14,
+        type='ViPNASHead',
+        in_channels=160,
+        out_channels=133,
+        deconv_out_channels=(160, 160, 160),
+        deconv_num_groups=(160, 160, 160),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CrowdPoseDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
-data_root = 'data/crowdpose/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -87,37 +89,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_trainval.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mmpose_crowdpose_test.json',
-        bbox_file='data/crowdpose/annotations/det_for_crowd_test_0.1_0.5.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=data_root + 'annotations/mmpose_crowdpose_test.json',
-    use_area=False,
-    iou_type='keypoints_crowd',
-    prefix='crowdpose')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub1-368x368.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub1-368x368.py`

 * *Files 4% similar despite different names*

```diff
@@ -50,15 +50,15 @@
         num_stages=6),
     head=dict(
         type='CPMHead',
         in_channels=15,
         out_channels=15,
         num_stages=6,
         deconv_out_channels=None,
-        has_final_layer=False,
+        final_layer=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -66,28 +66,28 @@
 # base dataset settings
 dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
 data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub2-368x368.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub2-368x368.py`

 * *Files 6% similar despite different names*

```diff
@@ -50,15 +50,15 @@
         num_stages=6),
     head=dict(
         type='CPMHead',
         in_channels=15,
         out_channels=15,
         num_stages=6,
         deconv_out_channels=None,
-        has_final_layer=False,
+        final_layer=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -66,28 +66,28 @@
 # base dataset settings
 dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
 data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub3-368x368.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub2-256x256.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,104 +20,99 @@
         end=40,
         milestones=[20, 30],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='PCK', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(368, 368), heatmap_size=(46, 46), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(32, 32), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(
-        type='CPM',
-        in_channels=3,
-        out_channels=15,
-        feat_channels=128,
-        num_stages=6),
+    backbone=dict(type='ResNet', depth=50),
     head=dict(
-        type='CPMHead',
-        in_channels=15,
+        type='HeatmapHead',
+        in_channels=2048,
         out_channels=15,
-        num_stages=6,
-        deconv_out_channels=None,
-        has_final_layer=False,
+        deconv_out_channels=(256, 256),
+        deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
+load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
 dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
 data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub3_train.json',
+        ann_file='annotations/Sub2_train.json',
         data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub3_test.json',
+        ann_file='annotations/Sub2_test.json',
         data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub1-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub1-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=40, val_interval=1)
+train_cfg = dict(max_epochs=20, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,46 +13,44 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=40,
-        milestones=[20, 30],
+        end=20,
+        milestones=[8, 15],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='PCK', rule='greater', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(32, 32), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(type='ResNet', depth=50),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=15,
-        deconv_out_channels=(256, 256),
-        deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -61,28 +59,28 @@
 # base dataset settings
 dataset_type = 'JhmdbDataset'
 data_mode = 'topdown'
 data_root = 'data/jhmdb/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub2-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-384x288.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=40, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,76 +13,73 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=40,
-        milestones=[20, 30],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(32, 32), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ResNet', depth=50),
+    backbone=dict(
+        type='SEResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=15,
-        deconv_out_channels=(256, 256),
-        deconv_kernel_sizes=(4, 4),
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
-load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'JhmdbDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
-data_root = 'data/jhmdb/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -90,33 +87,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub2_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub2_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_val2017.json',
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = [
-    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
-]
+val_evaluator = dict(
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub3-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res101_dark-8xb64-210e_coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=40, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,76 +13,77 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=40,
-        milestones=[20, 30],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(32, 32), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(192, 256),
+    heatmap_size=(48, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ResNet', depth=50),
+    backbone=dict(
+        type='ResNet',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=15,
-        deconv_out_channels=(256, 256),
-        deconv_kernel_sizes=(4, 4),
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
-load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'JhmdbDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
-data_root = 'data/jhmdb/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -90,33 +91,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub3_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub3_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_val2017.json',
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = [
-    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
-]
+val_evaluator = dict(
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub1-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-18_8xb64-210e_mpii-256x256.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=20, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,74 +13,92 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=20,
-        milestones=[8, 15],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ResNet', depth=50),
+    backbone=dict(
+        type='LiteHRNet',
+        in_channels=3,
+        extra=dict(
+            stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
+            num_stages=3,
+            stages_spec=dict(
+                num_modules=(2, 4, 2),
+                num_branches=(2, 3, 4),
+                num_blocks=(2, 2, 2),
+                module_type=('LITE', 'LITE', 'LITE'),
+                with_fuse=(True, True, True),
+                reduce_ratios=(8, 8, 8),
+                num_channels=(
+                    (40, 80),
+                    (40, 80, 160),
+                    (40, 80, 160, 320),
+                )),
+            with_head=True,
+        )),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
-        out_channels=15,
+        in_channels=40,
+        out_channels=16,
+        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
-load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'JhmdbDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/jhmdb/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
+        shift_prob=0,
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,33 +106,32 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub1_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub1_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = [
-    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
-]
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub2-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet50_8xb64-210e_coco-256x192.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=20, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,74 +13,73 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=20,
-        milestones=[8, 15],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ResNet', depth=50),
+    backbone=dict(
+        type='SEResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=15,
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
-load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'JhmdbDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
-data_root = 'data/jhmdb/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,33 +87,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub2_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub2_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_val2017.json',
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = [
-    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
-]
+val_evaluator = dict(
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub3-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-res50_8xb64-210e_coco-256x192.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=20, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,74 +13,72 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=20,
-        milestones=[8, 15],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='PCK', rule='greater', interval=1))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ResNet', depth=50),
+    backbone=dict(type='ViPNAS_ResNet', depth=50),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=15,
+        type='ViPNASHead',
+        in_channels=608,
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
-load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_mpii_256x256-418ffc88_20200812.pth'  # noqa: E501
 
 # base dataset settings
-dataset_type = 'JhmdbDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
-data_root = 'data/jhmdb/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
+    dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,33 +86,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub3_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/Sub3_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_val2017.json',
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = [
-    dict(type='JhmdbPCKAccuracy', thr=0.2, norm_item=['bbox', 'torso']),
-]
+val_evaluator = dict(
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cspnext-m_udp_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cspnext-m_udp_8xb64-210e_mpii-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -78,25 +78,25 @@
     ))
 
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/pose/MPI/',
 #         f'{data_root}': 's3://openmmlab/datasets/pose/MPI/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -115,22 +115,22 @@
                 min_width=0.2,
                 p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_cpm_8xb64-210e_mpii-368x368.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb32-210e_mpii-384x384.py`

 * *Files 10% similar despite different names*

```diff
@@ -27,37 +27,34 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(368, 368), heatmap_size=(46, 46), sigma=2)
+    type='MSRAHeatmap', input_size=(384, 384), heatmap_size=(96, 96), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='CPM',
-        in_channels=3,
-        out_channels=16,
-        feat_channels=128,
-        num_stages=6),
+        type='HourglassNet',
+        num_stacks=1,
+    ),
     head=dict(
         type='CPMHead',
-        in_channels=16,
+        in_channels=256,
         out_channels=16,
-        num_stages=6,
+        num_stages=1,
         deconv_out_channels=None,
-        has_final_layer=False,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -65,36 +62,32 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        shift_prob=0,
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb32-210e_mpii-384x384.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnext152_8xb32-210e_mpii-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,41 +20,41 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(384, 384), heatmap_size=(96, 96), sigma=2)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='HourglassNet',
-        num_stacks=1,
+        type='ResNeXt',
+        depth=152,
+        init_cfg=dict(
+            type='Pretrained', checkpoint='mmcls://resnext152_32x4d'),
     ),
     head=dict(
-        type='CPMHead',
-        in_channels=256,
+        type='HeatmapHead',
+        in_channels=2048,
         out_channels=16,
-        num_stages=1,
-        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -62,24 +62,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet50_8xb64-210e_mpii-256x256.py`

 * *Files 10% similar despite different names*

```diff
@@ -38,23 +38,22 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='HourglassNet',
-        num_stacks=1,
+        type='SEResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet50'),
     ),
     head=dict(
-        type='CPMHead',
-        in_channels=256,
+        type='HeatmapHead',
+        in_channels=2048,
         out_channels=16,
-        num_stages=1,
-        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -62,24 +61,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_dark-8xb64-210e_mpii-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,19 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(256, 256),
+    heatmap_size=(64, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -90,45 +94,45 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=16,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/mpii_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=16,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_dark-8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_8xb64-210e_mpii-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,19 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(256, 256),
-    heatmap_size=(64, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -94,45 +90,45 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=16,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/mpii_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=16,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_8xb64-210e_mpii-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,24 +90,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_mammal-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=300, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
-    type='Adam',
+    type='AdamW',
     lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
@@ -27,19 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(256, 256),
-    heatmap_size=(64, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -56,95 +52,95 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(48, 96)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384))),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+            'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=48,
-        out_channels=16,
+        in_channels=32,
+        out_channels=23,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'AnimalKingdomDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/ak/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
+        ann_file='annotations/ak_P3_mammal/train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=24,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        ann_file='annotations/ak_P3_mammal/test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [dict(type='PCKAccuracy', thr=0.05), dict(type='AUC')]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-18_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_reptile-256x256.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=300, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
-    type='Adam',
+    type='AdamW',
     lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
@@ -38,100 +38,109 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='LiteHRNet',
+        type='HRNet',
         in_channels=3,
         extra=dict(
-            stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
-            num_stages=3,
-            stages_spec=dict(
-                num_modules=(2, 4, 2),
-                num_branches=(2, 3, 4),
-                num_blocks=(2, 2, 2),
-                module_type=('LITE', 'LITE', 'LITE'),
-                with_fuse=(True, True, True),
-                reduce_ratios=(8, 8, 8),
-                num_channels=(
-                    (40, 80),
-                    (40, 80, 160),
-                    (40, 80, 160, 320),
-                )),
-            with_head=True,
-        )),
+            stage1=dict(
+                num_modules=1,
+                num_branches=1,
+                block='BOTTLENECK',
+                num_blocks=(4, ),
+                num_channels=(64, )),
+            stage2=dict(
+                num_modules=1,
+                num_branches=2,
+                block='BASIC',
+                num_blocks=(4, 4),
+                num_channels=(32, 64)),
+            stage3=dict(
+                num_modules=4,
+                num_branches=3,
+                block='BASIC',
+                num_blocks=(4, 4, 4),
+                num_channels=(32, 64, 128)),
+            stage4=dict(
+                num_modules=3,
+                num_branches=4,
+                block='BASIC',
+                num_blocks=(4, 4, 4, 4),
+                num_channels=(32, 64, 128, 256))),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w32-36af842e.pth'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=40,
-        out_channels=16,
+        in_channels=32,
+        out_channels=23,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'AnimalKingdomDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/ak/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        shift_prob=0,
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
+        ann_file='annotations/ak_P3_reptile/train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=24,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        ann_file='annotations/ak_P3_reptile/test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [dict(type='PCKAccuracy', thr=0.05), dict(type='AUC')]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_litehrnet-30_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_amphibian-256x256.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=300, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
-    type='Adam',
+    type='AdamW',
     lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
@@ -38,100 +38,109 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='LiteHRNet',
+        type='HRNet',
         in_channels=3,
         extra=dict(
-            stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
-            num_stages=3,
-            stages_spec=dict(
-                num_modules=(3, 8, 3),
-                num_branches=(2, 3, 4),
-                num_blocks=(2, 2, 2),
-                module_type=('LITE', 'LITE', 'LITE'),
-                with_fuse=(True, True, True),
-                reduce_ratios=(8, 8, 8),
-                num_channels=(
-                    (40, 80),
-                    (40, 80, 160),
-                    (40, 80, 160, 320),
-                )),
-            with_head=True,
-        )),
+            stage1=dict(
+                num_modules=1,
+                num_branches=1,
+                block='BOTTLENECK',
+                num_blocks=(4, ),
+                num_channels=(64, )),
+            stage2=dict(
+                num_modules=1,
+                num_branches=2,
+                block='BASIC',
+                num_blocks=(4, 4),
+                num_channels=(32, 64)),
+            stage3=dict(
+                num_modules=4,
+                num_branches=3,
+                block='BASIC',
+                num_blocks=(4, 4, 4),
+                num_channels=(32, 64, 128)),
+            stage4=dict(
+                num_modules=3,
+                num_branches=4,
+                block='BASIC',
+                num_blocks=(4, 4, 4, 4),
+                num_channels=(32, 64, 128, 256))),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w32-36af842e.pth'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=40,
-        out_channels=16,
+        in_channels=32,
+        out_channels=23,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'AnimalKingdomDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/ak/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        shift_prob=0,
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
+        ann_file='annotations/ak_P3_amphibian/train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=24,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        ann_file='annotations/ak_P3_amphibian/test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [dict(type='PCKAccuracy', thr=0.05), dict(type='AUC')]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_mobilenetv2_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv1_8xb64-210e_mpii-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -38,22 +38,21 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='MobileNetV2',
-        widen_factor=1.,
-        out_indices=(7, ),
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://mobilenet_v2'),
+        type='ShuffleNetV1',
+        groups=3,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v1'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
+        in_channels=960,
         out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -62,24 +61,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res101_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet101_8xb64-210e_mpii-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -38,17 +38,17 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
+        type='SEResNet',
         depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,24 +61,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res152_8xb32-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hourglass52_8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,62 +23,66 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        type='HourglassNet',
+        num_stacks=1,
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=16,
+        type='CPMHead',
+        in_channels=256,
+        out_channels=21,
+        num_stages=1,
+        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=180.0,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -86,32 +90,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res50_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_8xb64-210e_mpii-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -22,63 +22,59 @@
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
-
 # codec settings
-codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+codec = dict(type='RegressionLabel', input_size=(256, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
+    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='HeatmapHead',
+        type='RegressionHead',
         in_channels=2048,
-        out_channels=16,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        num_joints=16,
+        loss=dict(type='SmoothL1Loss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_coords=True,
     ))
 
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -101,17 +97,20 @@
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        headbox_file=f'{data_root}/annotations/mpii_gt_val.mat',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+
 # evaluators
 val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d101_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-sling-dress-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,65 +20,67 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet101_v1d'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=16,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -86,32 +88,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='train/deepfashion2_sling_dress_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
-        data_prefix=dict(img='images/'),
+        ann_file='validation/deepfashion2_sling_dress_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d152_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res50_8xb64-210e_mpii-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -38,17 +38,17 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet152_v1d'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,24 +61,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d50_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_res101_8xb64-210e_mpii-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -38,17 +38,17 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNetV1d',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet50_v1d'),
+        type='ResNet',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,24 +61,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnext152_8xb32-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet152_8xb32-210e_mpii-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -38,18 +38,16 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNeXt',
+        type='SEResNet',
         depth=152,
-        init_cfg=dict(
-            type='Pretrained', checkpoint='mmcls://resnext152_32x4d'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -62,24 +60,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet101_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-vest-dress-256x192.py`

 * *Files 10% similar despite different names*

```diff
@@ -20,68 +20,67 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=64)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SCNet',
-        depth=101,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet101-94250a77.pth'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=16,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -89,32 +88,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='train/deepfashion2_vest_dress_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
-        data_prefix=dict(img='images/'),
+        ann_file='validation/deepfashion2_vest_dress_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet50_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-243frm-supv_8xb128-80e_h36m.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,120 +1,128 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
+vis_backends = [
+    dict(type='LocalVisBackend'),
+]
+visualizer = dict(
+    type='Pose3dLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=80, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
+optim_wrapper = dict(optimizer=dict(type='Adam', lr=1e-3))
 
 # learning policy
 param_scheduler = [
-    dict(
-        type='LinearLR', begin=0, end=500, start_factor=0.001,
-        by_epoch=False),  # warm-up
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=210,
-        milestones=[170, 200],
-        gamma=0.1,
-        by_epoch=True)
+    dict(type='ExponentialLR', gamma=0.975, end=80, by_epoch=True)
 ]
 
-# automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=1024)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(
+        type='CheckpointHook',
+        save_best='MPJPE',
+        rule='less',
+        max_keep_ckpts=1),
+    logger=dict(type='LoggerHook', interval=20),
+)
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='VideoPoseLifting',
+    num_keypoints=17,
+    zero_center=True,
+    root_index=0,
+    remove_root=False)
 
 # model settings
 model = dict(
-    type='TopdownPoseEstimator',
-    data_preprocessor=dict(
-        type='PoseDataPreprocessor',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        bgr_to_rgb=True),
+    type='PoseLifter',
     backbone=dict(
-        type='SCNet',
-        depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet50-7ef0a199.pth'),
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=4,
+        kernel_sizes=(3, 3, 3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=16,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        type='TemporalRegressionHead',
+        in_channels=1024,
+        num_joints=17,
+        loss=dict(type='MPJPELoss'),
+        decoder=codec,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
-data_mode = 'topdown'
-data_root = 'data/mpii/'
+dataset_type = 'Human36mDataset'
+data_root = 'data/h36m/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(
+        type='RandomFlipAroundRoot',
+        keypoints_flip_cfg=dict(),
+        target_flip_cfg=dict(),
+    ),
     dict(type='GenerateTarget', encoder=codec),
-    dict(type='PackPoseInputs')
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='PackPoseInputs')
+    dict(type='GenerateTarget', encoder=codec),
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_train.npz',
+        seq_len=243,
+        causal=False,
+        pad_video_seq=True,
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
-    ))
+    ),
+)
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_test.npz',
+        seq_len=243,
+        causal=False,
+        pad_video_seq=True,
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
         data_prefix=dict(img='images/'),
-        test_mode=True,
         pipeline=val_pipeline,
+        test_mode=True,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [
+    dict(type='MPJPE', mode='mpjpe'),
+    dict(type='MPJPE', mode='p-mpjpe')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet101_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d152_8xb64-210e_mpii-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -38,17 +38,17 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet101'),
+        type='ResNetV1d',
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet152_v1d'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
@@ -61,24 +61,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet152_8xb32-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res50_8xb64-210e_animalpose-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,97 +20,99 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=152,
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=16,
+        out_channels=20,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'AnimalPoseDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/animalpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/animalpose_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
-        data_prefix=dict(img='images/'),
+        ann_file='annotations/animalpose_val.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = dict(
+    type='CocoMetric', ann_file=data_root + 'annotations/animalpose_val.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet50_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb64-210e_mpii-256x256.py`

 * *Files 9% similar despite different names*

```diff
@@ -38,22 +38,23 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SEResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet50'),
+        type='HourglassNet',
+        num_stacks=1,
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
+        type='CPMHead',
+        in_channels=256,
         out_channels=16,
+        num_stages=1,
+        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -61,24 +62,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv1_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_wingloss_8xb64-210e_wflw-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -22,96 +22,101 @@
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
-
 # codec settings
-codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+codec = dict(type='RegressionLabel', input_size=(256, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ShuffleNetV1',
-        groups=3,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v1'),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
+    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='HeatmapHead',
-        in_channels=960,
-        out_channels=16,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        type='RegressionHead',
+        in_channels=2048,
+        num_joints=98,
+        loss=dict(type='WingLoss', use_target_weight=True),
         decoder=codec),
+    train_cfg=dict(),
     test_cfg=dict(
         flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_coords=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'WFLWDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
+data_root = 'data/wflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(
+        type='RandomBBoxTransform',
+        scale_factor=[0.75, 1.25],
+        rotate_factor=60),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
-# data loaders
+# dataloaders
 train_dataloader = dict(
     batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
+        ann_file='annotations/face_landmarks_wflw_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        ann_file='annotations/face_landmarks_wflw_test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='NME', rule='less'))
+
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = dict(
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv2_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d50_8xb64-210e_mpii-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -38,21 +38,21 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ShuffleNetV2',
-        widen_factor=1.0,
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://shufflenet_v2'),
+        type='ResNetV1d',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet50_v1d'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1024,
+        in_channels=2048,
         out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -61,24 +61,24 @@
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(
         save_best='posetrack18/Total AP', rule='greater', interval=1))
 
 # load from the pretrained model
-load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth'  # noqa: E501
+load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth'  # noqa: E501
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 norm_cfg = dict(type='SyncBN', requires_grad=True)
@@ -58,31 +58,31 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256))),
+                num_channels=(48, 96, 192, 384))),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=32,
+        in_channels=48,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
@@ -92,26 +92,26 @@
 # base dataset settings
 dataset_type = 'PoseTrack18Dataset'
 data_mode = 'topdown'
 data_root = 'data/posetrack18/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-384x288.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(
         save_best='posetrack18/Total AP', rule='greater', interval=1))
 
 # load from the pretrained model
-load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-384x288-ca5956af_20220909.pth'  # noqa: E501
+load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-384x288-c161b7de_20220915.pth'  # noqa: E501
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 norm_cfg = dict(type='SyncBN', requires_grad=True)
@@ -58,31 +58,31 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256))),
+                num_channels=(48, 96, 192, 384))),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=32,
+        in_channels=48,
         out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
@@ -92,26 +92,26 @@
 # base dataset settings
 dataset_type = 'PoseTrack18Dataset'
 data_mode = 'topdown'
 data_root = 'data/posetrack18/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-256x192.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=20, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,37 +13,32 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=20,
-        milestones=[10, 15],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(
-        save_best='posetrack18/Total AP', rule='greater', interval=1))
-
-# load from the pretrained model
-load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth'  # noqa: E501
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
@@ -58,60 +53,63 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(48, 96)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384))),
+                num_channels=(32, 64, 128, 256))),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=48,
-        out_channels=17,
+        in_channels=32,
+        out_channels=133,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'PoseTrack18Dataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
-data_root = 'data/posetrack18/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -119,37 +117,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/posetrack18_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/posetrack18_val.json',
-        # comment `bbox_file` and '`filter_cfg` if use gt bbox for evaluation
-        bbox_file='data/posetrack18/annotations/'
-        'posetrack18_val_human_detections.json',
-        filter_cfg=dict(bbox_score_thr=0.4),
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 val_evaluator = dict(
-    type='PoseTrack18Metric',
-    ann_file=data_root + 'annotations/posetrack18_val.json',
-)
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=20, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,37 +13,36 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=20,
-        milestones=[10, 15],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(
-        save_best='posetrack18/Total AP', rule='greater', interval=1))
-
-# load from the pretrained model
-load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-384x288-c161b7de_20220915.pth'  # noqa: E501
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap',
+    input_size=(288, 384),
+    heatmap_size=(72, 96),
+    sigma=3,
+    unbiased=True)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
@@ -71,85 +70,85 @@
                 num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(48, 96, 192, 384))),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=48,
-        out_channels=17,
+        out_channels=133,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'PoseTrack18Dataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
-data_root = 'data/posetrack18/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/posetrack18_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/posetrack18_val.json',
-        # comment `bbox_file` and '`filter_cfg` if use gt bbox for evaluation
-        bbox_file='data/posetrack18/annotations/'
-        'posetrack18_val_human_detections.json',
-        filter_cfg=dict(bbox_score_thr=0.4),
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 val_evaluator = dict(
-    type='PoseTrack18Metric',
-    ann_file=data_root + 'annotations/posetrack18_val.json',
-)
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_res50_8xb64-20e_posetrack18-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d152_8xb32-210e_coco-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=20, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,114 +13,109 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=20,
-        milestones=[10, 15],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(
-        save_best='posetrack18/Total AP', rule='greater', interval=1))
-
-# load from the pretrained model
-load_from = 'https://download.openmmlab.com/mmpose/top_down/resnet/res50_coco_256x192-ec54d7f3_20200709.pth'  # noqa: E501
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
-norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        type='ResNetV1d',
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet152_v1d'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
         out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'PoseTrack18Dataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
-data_root = 'data/posetrack18/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/posetrack18_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_train2017.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/posetrack18_val.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/person_keypoints_val2017.json',
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='PoseTrack18Metric',
-    ann_file=data_root + 'annotations/posetrack18_val.json',
-)
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_mobilenetv2_rle-pretrained-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_mobilenetv2_rle-pretrained-8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -60,29 +60,27 @@
 )
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_8xb64-210e_coco-256x192.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,16 +35,16 @@
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     neck=dict(type='GlobalAveragePooling'),
     head=dict(
         type='RegressionHead',
         in_channels=2048,
         num_joints=17,
         loss=dict(type='SmoothL1Loss', use_target_weight=True),
@@ -57,25 +57,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_rle-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-8xb64-210e_coco-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=5e-4,
+    lr=1e-3,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
@@ -35,16 +35,16 @@
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     neck=dict(type='GlobalAveragePooling'),
     head=dict(
         type='RLEHead',
         in_channels=2048,
         num_joints=17,
         loss=dict(type='RLELoss', use_target_weight=True),
@@ -57,25 +57,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -35,16 +35,16 @@
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
     ),
     neck=dict(type='GlobalAveragePooling'),
     head=dict(
         type='RegressionHead',
         in_channels=2048,
         num_joints=17,
         loss=dict(type='SmoothL1Loss', use_target_weight=True),
@@ -57,25 +57,25 @@
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb64-210e_coco-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,80 +2,74 @@
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=5e-4,
+    lr=1e-3,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=train_cfg['max_epochs'],
-        milestones=[170, 200],
-        gamma=0.1,
-        by_epoch=True)
+    dict(type='MultiStepLR', milestones=[170, 200], gamma=0.1, by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(192, 256))
+codec = dict(
+    type='SimCCLabel', input_size=(192, 256), sigma=6.0, simcc_split_ratio=2.0)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RLEHead',
+        type='SimCCHead',
         in_channels=2048,
-        num_joints=17,
-        loss=dict(type='RLELoss', use_target_weight=True),
+        out_channels=17,
+        input_size=codec['input_size'],
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
+        simcc_split_ratio=codec['simcc_split_ratio'],
+        loss=dict(type='KLDiscretLoss', use_target_weight=True),
         decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        shift_coords=True,
-    ))
+    test_cfg=dict(flip_test=True))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
-val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+test_pipeline = [
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -102,20 +96,19 @@
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_val2017.json',
         bbox_file=f'{data_root}person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
-        pipeline=val_pipeline,
+        pipeline=test_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=f'{data_root}annotations/person_keypoints_val2017.json',
-    score_mode='bbox_rle')
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res152_dark-8xb32-210e_coco-384x288.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,77 +13,86 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=train_cfg['max_epochs'],
+        end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
+
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(288, 384))
+codec = dict(
+    type='MSRAHeatmap',
+    input_size=(288, 384),
+    heatmap_size=(72, 96),
+    sigma=3,
+    unbiased=True,
+    blur_kernel_size=17)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=152,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RLEHead',
+        type='HeatmapHead',
         in_channels=2048,
-        num_joints=17,
-        loss=dict(type='RLELoss', use_target_weight=True),
+        out_channels=17,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        shift_coords=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
     ))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
@@ -98,24 +107,20 @@
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file=f'{data_root}person_detection_results/'
+        bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=f'{data_root}annotations/person_keypoints_val2017.json',
-    score_mode='bbox_rle')
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,69 +13,74 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=train_cfg['max_epochs'],
+        end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
+# hooks
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(192, 256))
+codec = dict(
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RegressionHead',
+        type='HeatmapHead',
         in_channels=2048,
-        num_joints=17,
-        loss=dict(type='SmoothL1Loss', use_target_weight=True),
+        out_channels=133,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        shift_coords=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -83,38 +88,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file=f'{data_root}person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=f'{data_root}annotations/person_keypoints_val2017.json')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,120 +2,120 @@
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=1e-3,
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=train_cfg['max_epochs'],
+        end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
+# hooks
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(192, 256))
+codec = dict(
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RLEHead',
+        type='HeatmapHead',
         in_channels=2048,
-        num_joints=17,
-        loss=dict(type='RLELoss', use_target_weight=True),
+        out_channels=133,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        shift_coords=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_train2017.json',
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/person_keypoints_val2017.json',
-        bbox_file=f'{data_root}person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
-
-# evaluators
 val_evaluator = dict(
-    type='CocoMetric',
-    ann_file=f'{data_root}annotations/person_keypoints_val2017.json',
-    score_mode='bbox_rle')
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-pretrained-8xb64-210e_coco-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_debias-8xb64-210e_coco-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=1e-3,
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
@@ -23,65 +23,75 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(192, 256))
+codec = dict(
+    type='IntegralRegressionLabel',
+    input_size=(256, 256),
+    heatmap_size=(64, 64),
+    sigma=2.0,
+    normalize=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            prefix='backbone.',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/td-hm_res50_8xb64-210e_coco-256x192.pth'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RLEHead',
+        type='DSNTHead',
         in_channels=2048,
+        in_featuremap_size=(8, 8),
         num_joints=17,
-        loss=dict(type='RLELoss', use_target_weight=True),
+        debias=True,
+        beta=10.,
+        loss=dict(
+            type='MultipleLossWrapper',
+            losses=[
+                dict(type='SmoothL1Loss', use_target_weight=True),
+                dict(type='JSDiscretLoss', use_target_weight=True)
+            ]),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         shift_coords=True,
-    ))
+        shift_heatmap=True,
+    ),
+    init_cfg=dict(
+        type='Pretrained',
+        checkpoint='https://download.openmmlab.com/mmpose/'
+        'pretrain_models/td-hm_res50_8xb64-210e_coco-256x192.pth'))
 
 # base dataset settings
 dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 test_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -118,10 +128,9 @@
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # evaluators
 val_evaluator = dict(
     type='CocoMetric',
-    ann_file=f'{data_root}annotations/person_keypoints_val2017.json',
-    score_mode='bbox_rle')
+    ann_file=f'{data_root}annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res101_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res101_8xb64-210e_mpii-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -55,28 +55,26 @@
     ))
 
 # base dataset settings
 dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
 data_root = 'data/mpii/'
 
-file_client_args = dict(backend='disk')
-
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res152_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-long-sleeved-dress-256x192.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,63 +20,67 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=64)
+
+# hooks
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(256, 256))
+codec = dict(
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RegressionHead',
+        type='HeatmapHead',
         in_channels=2048,
-        num_joints=16,
-        loss=dict(type='SmoothL1Loss', use_target_weight=True),
+        out_channels=294,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        shift_coords=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
-
-file_client_args = dict(backend='disk')
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -84,35 +88,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='train/deepfashion2_long_sleeved_dress_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file=f'{data_root}/annotations/mpii_gt_val.mat',
-        data_prefix=dict(img='images/'),
+        ann_file='validation/deepfashion2_long_sleeved_dress_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
-
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_3xb64-210e_deepfasion2-shorts-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,63 +20,67 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=192)
+
+# hooks
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(256, 256))
+codec = dict(
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RegressionHead',
+        type='HeatmapHead',
         in_channels=2048,
-        num_joints=16,
-        loss=dict(type='SmoothL1Loss', use_target_weight=True),
+        out_channels=294,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        shift_coords=True,
+        flip_mode='heatmap',
+        shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
-
-file_client_args = dict(backend='disk')
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -84,35 +88,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
-        data_prefix=dict(img='images/'),
+        ann_file='train/deepfashion2_shorts_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file=f'{data_root}/annotations/mpii_gt_val.mat',
-        data_prefix=dict(img='images/'),
+        ann_file='validation/deepfashion2_shorts_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
-
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_rle-8xb64-210e_mpii-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_8xb64-210e_wflw-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -40,79 +40,83 @@
     backbone=dict(
         type='ResNet',
         depth=50,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RLEHead',
+        type='RegressionHead',
         in_channels=2048,
-        num_joints=16,
-        loss=dict(type='RLELoss', use_target_weight=True),
+        num_joints=98,
+        loss=dict(type='SmoothL1Loss', use_target_weight=True),
         decoder=codec),
+    train_cfg=dict(),
     test_cfg=dict(
         flip_test=True,
         shift_coords=True,
     ))
 
 # base dataset settings
-dataset_type = 'MpiiDataset'
+dataset_type = 'WFLWDataset'
 data_mode = 'topdown'
-data_root = 'data/mpii/'
-
-file_client_args = dict(backend='disk')
+data_root = 'data/wflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomBBoxTransform', shift_prob=0),
+    dict(
+        type='RandomBBoxTransform',
+        scale_factor=[0.75, 1.25],
+        rotate_factor=60),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
-# data loaders
+# dataloaders
 train_dataloader = dict(
     batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_train.json',
+        ann_file='annotations/face_landmarks_wflw_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/mpii_val.json',
-        headbox_file=f'{data_root}/annotations/mpii_gt_val.mat',
+        ann_file='annotations/face_landmarks_wflw_test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='NME', rule='less'))
 
 # evaluators
-val_evaluator = dict(type='MpiiPCKAccuracy')
+val_evaluator = dict(
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/rtmpose-m_8xb32-60e_coco-wholebody-face-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-max_epochs = 60
-stage2_num_epochs = 10
+max_epochs = 270
+stage2_num_epochs = 30
 base_lr = 4e-3
 
-train_cfg = dict(max_epochs=max_epochs, val_interval=1)
+train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
 optim_wrapper = dict(
     type='OptimWrapper',
     optimizer=dict(type='AdamW', lr=base_lr, weight_decay=0.05),
     paramwise_cfg=dict(
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -36,16 +35,16 @@
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
 codec = dict(
     type='SimCCLabel',
-    input_size=(256, 256),
-    sigma=(5.66, 5.66),
+    input_size=(192, 256),
+    sigma=(4.9, 5.66),
     simcc_split_ratio=2.0,
     normalize=False,
     use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
@@ -55,32 +54,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.67,
-        widen_factor=0.75,
+        deepen_factor=1.,
+        widen_factor=1.,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=768,
-        out_channels=68,
+        in_channels=1024,
+        out_channels=133,
         input_size=codec['input_size'],
-        in_featuremap_size=(8, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -93,32 +92,32 @@
             use_target_weight=True,
             beta=10.,
             label_softmax=True),
         decoder=codec),
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyFaceDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    # dict(type='RandomHalfBody'),
+    dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
     dict(
         type='Albumentation',
         transforms=[
@@ -134,25 +133,25 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    # dict(type='RandomHalfBody'),
+    dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
         rotate_factor=60),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -173,15 +172,15 @@
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
@@ -198,22 +197,24 @@
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(
-        save_best='NME', rule='less', max_keep_ckpts=1, interval=1))
+        save_best='coco-wholebody/AP', rule='greater', max_keep_ckpts=1))
 
 custom_hooks = [
     dict(
         type='EMAHook',
         ema_type='ExpMomentumEMA',
         momentum=0.0002,
         update_buffers=True,
@@ -222,11 +223,10 @@
         type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - stage2_num_epochs,
         switch_pipeline=train_pipeline_stage2)
 ]
 
 # evaluators
 val_evaluator = dict(
-    type='NME',
-    norm_mode='keypoint_distance',
-)
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/rtmpose-m_8xb64-60e_wflw-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/rtmpose-m_8xb64-60e_wflw-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -65,22 +64,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=768,
         out_channels=98,
         input_size=codec['input_size'],
-        in_featuremap_size=(8, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +96,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'WFLWDataset'
 data_mode = 'topdown'
 data_root = 'data/wflw/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/pose/WFLW/',
 #         f'{data_root}': 's3://openmmlab/datasets/pose/WFLW/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     # dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +133,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     # dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/td-hm_hrnetv2-w18_8xb64-60e_300w-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_dark-8xb64-60e_wflw-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -30,15 +30,16 @@
 default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap',
     input_size=(256, 256),
     heatmap_size=(64, 64),
-    sigma=1.5)
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -73,52 +74,54 @@
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(18, 36, 72, 144),
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
-        out_channels=68,
+        in_channels=270,
+        out_channels=98,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'Face300WDataset'
+dataset_type = 'WFLWDataset'
 data_mode = 'topdown'
-data_root = 'data/300w/'
+data_root = 'data/wflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_prob=0,
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -126,29 +129,29 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/face_landmarks_300w_train.json',
+        ann_file='annotations/face_landmarks_wflw_train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/face_landmarks_300w_valid.json',
+        ann_file='annotations/face_landmarks_wflw_test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_8xb64-60e_aflw-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_dark-8xb64-60e_aflw-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,15 +27,19 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(256, 256),
+    heatmap_size=(64, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -70,19 +74,21 @@
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(18, 36, 72, 144),
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=19,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
@@ -94,28 +100,28 @@
 # base dataset settings
 dataset_type = 'AFLWDataset'
 data_mode = 'topdown'
 data_root = 'data/aflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_prob=0,
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_dark-8xb64-60e_aflw-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_8xb64-60e_aflw-256x256.py`

 * *Files 2% similar despite different names*

```diff
@@ -27,19 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(256, 256),
-    heatmap_size=(64, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -74,19 +70,21 @@
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(18, 36, 72, 144),
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=19,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
@@ -98,28 +96,28 @@
 # base dataset settings
 dataset_type = 'AFLWDataset'
 data_mode = 'topdown'
 data_root = 'data/aflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_prob=0,
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hourglass52_8xb32-60e_coco-wholebody-face-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_scnet50_8xb32-60e_coco-wholebody-face-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -38,23 +38,24 @@
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='HourglassNet',
-        num_stacks=1,
-    ),
+        type='SCNet',
+        depth=50,
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/scnet50-7ef0a199.pth')),
     head=dict(
-        type='CPMHead',
-        in_channels=256,
+        type='HeatmapHead',
+        in_channels=2048,
         out_channels=68,
-        num_stages=1,
-        deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
@@ -62,27 +63,27 @@
 # base dataset settings
 dataset_type = 'CocoWholeBodyFaceDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_8xb32-60e_coco-wholebody-face-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_dark-8xb32-60e_coco-wholebody-face-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,15 +27,19 @@
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(256, 256),
+    heatmap_size=(64, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -69,19 +73,21 @@
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18')),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=68,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         decoder=codec),
     test_cfg=dict(
@@ -93,27 +99,27 @@
 # base dataset settings
 dataset_type = 'CocoWholeBodyFaceDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_dark-8xb32-60e_coco-wholebody-face-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_8xb32-60e_coco-wholebody-face-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,19 +27,15 @@
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(256, 256),
-    heatmap_size=(64, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -73,19 +69,21 @@
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18')),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=68,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         decoder=codec),
     test_cfg=dict(
@@ -97,27 +95,27 @@
 # base dataset settings
 dataset_type = 'CocoWholeBodyFaceDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_mobilenetv2_8xb32-60e_coco-wholebody-face-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_2xb64-210e_deepfasion2-trousers-256x192.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,122 +1,122 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=60, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=2e-3,
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[40, 55],
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=128)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='MobileNetV2',
-        widen_factor=1.,
-        out_indices=(7, ),
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://mobilenet_v2')),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
-        out_channels=68,
+        in_channels=2048,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyFaceDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='train/deepfashion2_trousers_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='validation/deepfashion2_trousers_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='NME',
-    norm_mode='keypoint_distance',
-)
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_res50_8xb32-60e_coco-wholebody-face-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-vest-256x192.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,121 +1,122 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=60, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=2e-3,
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[40, 55],
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=68,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyFaceDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='train/deepfashion2_vest_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='validation/deepfashion2_vest_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='NME',
-    norm_mode='keypoint_distance',
-)
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_scnet50_8xb32-60e_coco-wholebody-face-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-384x288.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,89 +1,86 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=60, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=2e-3,
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
         end=210,
-        milestones=[40, 55],
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SCNet',
-        depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet50-7ef0a199.pth')),
+        type='ResNet',
+        depth=152,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=68,
+        out_channels=133,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyFaceDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -108,17 +105,17 @@
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = dict(
-    type='NME',
-    norm_mode='keypoint_distance',
-)
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/td-hm_hrnetv2-w18_8xb64-60e_cofw-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/td-hm_hrnetv2-w18_8xb64-60e_cofw-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -73,19 +73,21 @@
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(18, 36, 72, 144),
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=29,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
@@ -97,28 +99,28 @@
 # base dataset settings
 dataset_type = 'COFWDataset'
 data_mode = 'topdown'
 data_root = 'data/cofw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_prob=0,
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_wflw.yml` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/hrnetv2_300w.yml`

 * *Files 27% similar despite different names*

```diff
@@ -1,27 +1,23 @@
 Collections:
 - Name: HRNetv2
   Paper:
     Title: Deep High-Resolution Representation Learning for Visual Recognition
-    URL: https://arxiv.org/abs/1908.07919
-  README: https://github.com/open-mmlab/mmpose/blob/1.x/docs/src/papers/backbones/hrnetv2.md
+    URL: https://ieeexplore.ieee.org/abstract/document/9052469/
+  README: https://github.com/open-mmlab/mmpose/blob/main/docs/src/papers/backbones/hrnetv2.md
 Models:
-- Config: configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_8xb64-60e_wflw-256x256.py
+- Config: configs/face_2d_keypoint/topdown_heatmap/300w/td-hm_hrnetv2-w18_8xb64-60e_300w-256x256.py
   In Collection: HRNetv2
-  Alias: face
   Metadata:
     Architecture:
     - HRNetv2
-    Training Data: WFLW
-  Name: topdown_heatmap_hrnetv2_w18_wflw_256x256
+    Training Data: 300W
+  Name: td-hm_hrnetv2-w18_8xb64-60e_300w-256x256
   Results:
-  - Dataset: WFLW
+  - Dataset: 300W
     Metrics:
-      NME blur: 4.58
-      NME expression: 4.33
-      NME illumination: 3.99
-      NME makeup: 3.94
-      NME occlusion: 4.83
-      NME pose: 6.97
-      NME test: 4.06
+      NME challenge: 5.64
+      NME common: 2.92
+      NME full: 3.45
+      NME test: 4.1
     Task: Face 2D Keypoint
-  Weights: https://download.openmmlab.com/mmpose/face/hrnetv2/hrnetv2_w18_wflw_256x256-2bf032a6_20210125.pth
+  Weights: https://download.openmmlab.com/mmpose/face/hrnetv2/hrnetv2_w18_300w_256x256-eea53406_20211019.pth
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_8xb64-60e_wflw-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P1-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,37 +1,37 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=60, val_interval=1)
+train_cfg = dict(max_epochs=300, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=2e-3,
+    type='AdamW',
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=60,
-        milestones=[40, 55],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
@@ -52,105 +52,95 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(18, 36)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(18, 36, 72)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(18, 36, 72, 144),
-                multiscale_output=True),
-            upsample=dict(mode='bilinear', align_corners=False)),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
-            type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(18, 36, 72, 144),
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
-        out_channels=98,
+        in_channels=32,
+        out_channels=23,
         deconv_out_channels=None,
-        conv_out_channels=(270, ),
-        conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'WFLWDataset'
+dataset_type = 'AnimalKingdomDataset'
 data_mode = 'topdown'
-data_root = 'data/wflw/'
+data_root = 'data/ak/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        shift_prob=0,
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/face_landmarks_wflw_train.json',
+        ann_file='annotations/ak_P1/train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=24,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/face_landmarks_wflw_test.json',
+        ann_file='annotations/ak_P1/test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='NME',
-    norm_mode='keypoint_distance',
-)
+val_evaluator = [dict(type='PCKAccuracy', thr=0.05), dict(type='AUC')]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_awing-8xb64-60e_wflw-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P2-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,37 +1,37 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=60, val_interval=1)
+train_cfg = dict(max_epochs=300, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=2e-3,
+    type='AdamW',
+    lr=5e-4,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=60,
-        milestones=[40, 55],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
@@ -52,105 +52,95 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(18, 36)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(18, 36, 72)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(18, 36, 72, 144),
-                multiscale_output=True),
-            upsample=dict(mode='bilinear', align_corners=False)),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
-            type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
+            type='Pretrained',
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(18, 36, 72, 144),
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
-        out_channels=98,
+        in_channels=32,
+        out_channels=23,
         deconv_out_channels=None,
-        conv_out_channels=(270, ),
-        conv_kernel_sizes=(1, ),
-        loss=dict(type='AdaptiveWingLoss', use_target_weight=True),
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'WFLWDataset'
+dataset_type = 'AnimalKingdomDataset'
 data_mode = 'topdown'
-data_root = 'data/wflw/'
+data_root = 'data/ak/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        shift_prob=0,
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/face_landmarks_wflw_train.json',
+        ann_file='annotations/ak_P2/train.json',
         data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=24,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/face_landmarks_wflw_test.json',
+        ann_file='annotations/ak_P2/test.json',
         data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = dict(
-    type='NME',
-    norm_mode='keypoint_distance',
-)
+val_evaluator = [dict(type='PCKAccuracy', thr=0.05), dict(type='AUC')]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_dark-8xb64-60e_wflw-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_8xb64-60e_wflw-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -27,19 +27,15 @@
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(256, 256),
-    heatmap_size=(64, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -74,19 +70,21 @@
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
     ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=(18, 36, 72, 144),
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=98,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
@@ -98,28 +96,28 @@
 # base dataset settings
 dataset_type = 'WFLWDataset'
 data_mode = 'topdown'
 data_root = 'data/wflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
         shift_prob=0,
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose-m_8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose-m_8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -65,22 +64,22 @@
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
         in_channels=768,
         out_channels=21,
         input_size=codec['input_size'],
-        in_featuremap_size=(8, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +96,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     # dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.5, 1.5],
         rotate_factor=180),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
@@ -135,22 +134,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     # dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
         rotate_factor=180),
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hourglass52_8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_softwingloss_8xb64-210e_wflw-256x256.py`

 * *Files 10% similar despite different names*

```diff
@@ -20,104 +20,103 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
-
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
-codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+codec = dict(type='RegressionLabel', input_size=(256, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='HourglassNet',
-        num_stacks=1,
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
+    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='CPMHead',
-        in_channels=256,
-        out_channels=21,
-        num_stages=1,
-        deconv_out_channels=None,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        type='RegressionHead',
+        in_channels=2048,
+        num_joints=98,
+        loss=dict(type='SoftWingLoss', use_target_weight=True),
         decoder=codec),
+    train_cfg=dict(),
     test_cfg=dict(
         flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_coords=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyHandDataset'
+dataset_type = 'WFLWDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/wflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform',
-        rotate_factor=180.0,
-        scale_factor=(0.7, 1.3)),
+        scale_factor=[0.75, 1.25],
+        rotate_factor=60),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
-# data loaders
+# dataloaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/face_landmarks_wflw_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/face_landmarks_wflw_test.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE')
-]
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='NME', rule='less'))
+
+# evaluators
+val_evaluator = dict(
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_8xb64-210e_onehand10k-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,18 +20,19 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
@@ -67,86 +68,91 @@
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
-            type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18')),
+            type='Pretrained',
+            checkpoint='open-mmlab://msra/hrnetv2_w18',
+        )),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=21,
         deconv_out_channels=None,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyHandDataset'
+dataset_type = 'OneHand10KDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/onehand10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform', rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/onehand10k_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/onehand10k_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = [
     dict(type='PCKAccuracy', thr=0.2),
     dict(type='AUC'),
-    dict(type='EPE')
+    dict(type='EPE'),
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_dark-8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 3% similar despite different names*

```diff
@@ -26,19 +26,15 @@
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(256, 256),
-    heatmap_size=(64, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -72,19 +68,21 @@
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18')),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=21,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         decoder=codec),
     test_cfg=dict(
@@ -96,26 +94,26 @@
 # base dataset settings
 dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
+    dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform', rotate_factor=180,
         scale_factor=(0.7, 1.3)),
-    dict(type='RandomFlip', direction='horizontal'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_litehrnet-w18_8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_udp-8xb64-210e_rhd2d-256x256.py`

 * *Files 7% similar despite different names*

```diff
@@ -20,117 +20,139 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='UDPHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='LiteHRNet',
+        type='HRNet',
         in_channels=3,
         extra=dict(
-            stem=dict(stem_channels=32, out_channels=32, expand_ratio=1),
-            num_stages=3,
-            stages_spec=dict(
-                num_modules=(2, 4, 2),
-                num_branches=(2, 3, 4),
-                num_blocks=(2, 2, 2),
-                module_type=('LITE', 'LITE', 'LITE'),
-                with_fuse=(True, True, True),
-                reduce_ratios=(8, 8, 8),
-                num_channels=(
-                    (40, 80),
-                    (40, 80, 160),
-                    (40, 80, 160, 320),
-                )),
-            with_head=True,
+            stage1=dict(
+                num_modules=1,
+                num_branches=1,
+                block='BOTTLENECK',
+                num_blocks=(4, ),
+                num_channels=(64, )),
+            stage2=dict(
+                num_modules=1,
+                num_branches=2,
+                block='BASIC',
+                num_blocks=(4, 4),
+                num_channels=(18, 36)),
+            stage3=dict(
+                num_modules=4,
+                num_branches=3,
+                block='BASIC',
+                num_blocks=(4, 4, 4),
+                num_channels=(18, 36, 72)),
+            stage4=dict(
+                num_modules=3,
+                num_branches=4,
+                block='BASIC',
+                num_blocks=(4, 4, 4, 4),
+                num_channels=(18, 36, 72, 144),
+                multiscale_output=True),
+            upsample=dict(mode='bilinear', align_corners=False)),
+        init_cfg=dict(
+            type='Pretrained',
+            checkpoint='open-mmlab://msra/hrnetv2_w18',
         )),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=40,
+        in_channels=270,
         out_channels=21,
         deconv_out_channels=None,
+        conv_out_channels=(270, ),
+        conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_heatmap=False,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyHandDataset'
+dataset_type = 'Rhd2DDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/rhd/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
+    dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform', rotate_factor=180,
         scale_factor=(0.7, 1.3)),
-    dict(type='RandomFlip', direction='horizontal'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/rhd_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/rhd_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = [
     dict(type='PCKAccuracy', thr=0.2),
     dict(type='AUC'),
-    dict(type='EPE')
+    dict(type='EPE'),
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_mobilenetv2_8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-short-sleeved-outwear-256x192.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,101 +20,104 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
+
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='MobileNetV2',
-        widen_factor=1.,
-        out_indices=(7, ),
-        init_cfg=dict(type='Pretrained', checkpoint='mmcls://mobilenet_v2')),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
-        out_channels=21,
+        in_channels=2048,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyHandDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
     dict(type='RandomFlip', direction='horizontal'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='train/deepfashion2_short_sleeved_outwear_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='validation/'
+        'deepfashion2_short_sleeved_outwear_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = [
     dict(type='PCKAccuracy', thr=0.2),
     dict(type='AUC'),
-    dict(type='EPE')
+    dict(type='EPE'),
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_res50_8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-long-sleeved-outwear-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,100 +20,104 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
+
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=21,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyHandDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
     dict(type='RandomFlip', direction='horizontal'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='train/deepfashion2_long_sleeved_outwear_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='validation/'
+        'deepfashion2_long_sleeved_outwear_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = [
     dict(type='PCKAccuracy', thr=0.2),
     dict(type='AUC'),
-    dict(type='EPE')
+    dict(type='EPE'),
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_scnet50_8xb32-210e_coco-wholebody-hand-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res101_8xb64-210e_animalpose-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,103 +20,99 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
+
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='SCNet',
-        depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/scnet50-7ef0a199.pth')),
+        type='ResNet',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=21,
+        out_channels=20,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyHandDataset'
+dataset_type = 'AnimalPoseDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/animalpose/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
     dict(type='RandomFlip', direction='horizontal'),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/animalpose_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/animalpose_val.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE')
-]
+# evaluators
+val_evaluator = dict(
+    type='CocoMetric', ann_file=data_root + 'annotations/animalpose_val.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-sling-256x192.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=100, val_interval=1)
+train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,125 +13,109 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=100,
-        milestones=[50, 70],
+        end=210,
+        milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(save_best='AUC', rule='greater', interval=1))
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(224, 224), heatmap_size=(56, 56), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=21,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'FreiHandDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/freihand/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale', padding=0.8),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform',
-        shift_factor=0.25,
-        rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale', padding=0.8),
+    dict(type='LoadImage'),
+    dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
     batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/freihand_train.json',
-        data_prefix=dict(img=''),
+        ann_file='train/deepfashion2_sling_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/freihand_val.json',
-        data_prefix=dict(img=''),
-        test_mode=True,
-        pipeline=val_pipeline,
-    ))
-test_dataloader = dict(
-    batch_size=32,
-    num_workers=2,
-    persistent_workers=True,
-    drop_last=False,
-    sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/freihand_test.json',
-        data_prefix=dict(img=''),
+        ann_file='validation/deepfashion2_sling_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
+test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = [
     dict(type='PCKAccuracy', thr=0.2),
     dict(type='AUC'),
     dict(type='EPE'),
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_8xb64-210e_onehand10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_awing-8xb64-60e_wflw-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,37 +1,37 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=60, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
-    lr=5e-4,
+    lr=2e-3,
 ))
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=60,
+        milestones=[40, 55],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='NME', rule='less', interval=1))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
@@ -68,53 +68,56 @@
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
-            type='Pretrained',
-            checkpoint='open-mmlab://msra/hrnetv2_w18',
-        )),
+            type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18'),
+    ),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
-        out_channels=21,
+        in_channels=270,
+        out_channels=98,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        loss=dict(type='AdaptiveWingLoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'OneHand10KDataset'
+dataset_type = 'WFLWDataset'
 data_mode = 'topdown'
-data_root = 'data/onehand10k/'
+data_root = 'data/wflw/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+        type='RandomBBoxTransform',
+        shift_prob=0,
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -122,35 +125,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/onehand10k_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/face_landmarks_wflw_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/onehand10k_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/face_landmarks_wflw_test.json',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE'),
-]
+val_evaluator = dict(
+    type='NME',
+    norm_mode='keypoint_distance',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_dark-8xb64-210e_onehand10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_dark-8xb32-210e_coco-wholebody-hand-256x256.py`

 * *Files 4% similar despite different names*

```diff
@@ -20,19 +20,18 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
 default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
-
 # codec settings
 codec = dict(
     type='MSRAHeatmap',
     input_size=(256, 256),
     heatmap_size=(64, 64),
     sigma=2,
     unbiased=True)
@@ -72,89 +71,88 @@
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
-            type='Pretrained',
-            checkpoint='open-mmlab://msra/hrnetv2_w18',
-        )),
+            type='Pretrained', checkpoint='open-mmlab://msra/hrnetv2_w18')),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=21,
         deconv_out_channels=None,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'OneHand10KDataset'
+dataset_type = 'CocoWholeBodyHandDataset'
 data_mode = 'topdown'
-data_root = 'data/onehand10k/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform', rotate_factor=180,
         scale_factor=(0.7, 1.3)),
+    dict(type='RandomFlip', direction='horizontal'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/onehand10k_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/onehand10k_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
 val_evaluator = [
     dict(type='PCKAccuracy', thr=0.2),
     dict(type='AUC'),
-    dict(type='EPE'),
+    dict(type='EPE')
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_udp-8xb64-210e_onehand10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_udp-8xb64-210e_onehand10k-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -71,19 +71,21 @@
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='open-mmlab://msra/hrnetv2_w18',
         )),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=21,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
@@ -95,26 +97,26 @@
 # base dataset settings
 dataset_type = 'OneHand10KDataset'
 data_mode = 'topdown'
 data_root = 'data/onehand10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform', rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_mobilenetv2_8xb64-210e_onehand10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-81frm-supv_8xb128-80e_h36m.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,125 +1,128 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
+vis_backends = [
+    dict(type='LocalVisBackend'),
+]
+visualizer = dict(
+    type='Pose3dLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=80, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
+optim_wrapper = dict(optimizer=dict(type='Adam', lr=1e-3))
 
 # learning policy
 param_scheduler = [
-    dict(
-        type='LinearLR', begin=0, end=500, start_factor=0.001,
-        by_epoch=False),  # warm-up
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=210,
-        milestones=[170, 200],
-        gamma=0.1,
-        by_epoch=True)
+    dict(type='ExponentialLR', gamma=0.975, end=80, by_epoch=True)
 ]
 
-# automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=1024)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(
+        type='CheckpointHook',
+        save_best='MPJPE',
+        rule='less',
+        max_keep_ckpts=1),
+    logger=dict(type='LoggerHook', interval=20),
+)
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='VideoPoseLifting',
+    num_keypoints=17,
+    zero_center=True,
+    root_index=0,
+    remove_root=False)
 
 # model settings
 model = dict(
-    type='TopdownPoseEstimator',
-    data_preprocessor=dict(
-        type='PoseDataPreprocessor',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        bgr_to_rgb=True),
+    type='PoseLifter',
     backbone=dict(
-        type='MobileNetV2',
-        widen_factor=1.,
-        out_indices=(7, ),
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='mmcls://mobilenet_v2',
-        )),
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=3,
+        kernel_sizes=(3, 3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
+    ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=1280,
-        out_channels=21,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        type='TemporalRegressionHead',
+        in_channels=1024,
+        num_joints=17,
+        loss=dict(type='MPJPELoss'),
+        decoder=codec,
     ))
 
 # base dataset settings
-dataset_type = 'OneHand10KDataset'
-data_mode = 'topdown'
-data_root = 'data/onehand10k/'
+dataset_type = 'Human36mDataset'
+data_root = 'data/h36m/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='RandomFlip', direction='horizontal'),
     dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+        type='RandomFlipAroundRoot',
+        keypoints_flip_cfg=dict(),
+        target_flip_cfg=dict(),
+    ),
     dict(type='GenerateTarget', encoder=codec),
-    dict(type='PackPoseInputs')
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='PackPoseInputs')
+    dict(type='GenerateTarget', encoder=codec),
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_train.npz',
+        seq_len=81,
+        causal=False,
+        pad_video_seq=True,
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/onehand10k_train.json',
-        data_prefix=dict(img=''),
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
-    ))
+    ),
+)
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_test.npz',
+        seq_len=81,
+        causal=False,
+        pad_video_seq=True,
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/onehand10k_test.json',
-        data_prefix=dict(img=''),
-        test_mode=True,
+        data_prefix=dict(img='images/'),
         pipeline=val_pipeline,
+        test_mode=True,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE'),
+    dict(type='MPJPE', mode='mpjpe'),
+    dict(type='MPJPE', mode='p-mpjpe')
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_res50_8xb32-210e_onehand10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-supv_8xb128-80e_h36m.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,124 +1,128 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
+vis_backends = [
+    dict(type='LocalVisBackend'),
+]
+visualizer = dict(
+    type='Pose3dLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=80, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
+optim_wrapper = dict(optimizer=dict(type='Adam', lr=1e-3))
 
 # learning policy
 param_scheduler = [
-    dict(
-        type='LinearLR', begin=0, end=500, start_factor=0.001,
-        by_epoch=False),  # warm-up
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=210,
-        milestones=[170, 200],
-        gamma=0.1,
-        by_epoch=True)
+    dict(type='ExponentialLR', gamma=0.975, end=80, by_epoch=True)
 ]
 
-# automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=1024)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(
+        type='CheckpointHook',
+        save_best='MPJPE',
+        rule='less',
+        max_keep_ckpts=1),
+    logger=dict(type='LoggerHook', interval=20),
+)
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='VideoPoseLifting',
+    num_keypoints=17,
+    zero_center=True,
+    root_index=0,
+    remove_root=False)
 
 # model settings
 model = dict(
-    type='TopdownPoseEstimator',
-    data_preprocessor=dict(
-        type='PoseDataPreprocessor',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        bgr_to_rgb=True),
+    type='PoseLifter',
     backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='torchvision://resnet50',
-        )),
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=2,
+        kernel_sizes=(3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
+    ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=21,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        type='TemporalRegressionHead',
+        in_channels=1024,
+        num_joints=17,
+        loss=dict(type='MPJPELoss'),
+        decoder=codec,
     ))
 
 # base dataset settings
-dataset_type = 'OneHand10KDataset'
-data_mode = 'topdown'
-data_root = 'data/onehand10k/'
+dataset_type = 'Human36mDataset'
+data_root = 'data/h36m/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='RandomFlip', direction='horizontal'),
     dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+        type='RandomFlipAroundRoot',
+        keypoints_flip_cfg=dict(),
+        target_flip_cfg=dict(),
+    ),
     dict(type='GenerateTarget', encoder=codec),
-    dict(type='PackPoseInputs')
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='PackPoseInputs')
+    dict(type='GenerateTarget', encoder=codec),
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_train.npz',
+        seq_len=27,
+        causal=False,
+        pad_video_seq=True,
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/onehand10k_train.json',
-        data_prefix=dict(img=''),
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
-    ))
+    ),
+)
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_test.npz',
+        seq_len=27,
+        causal=False,
+        pad_video_seq=True,
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/onehand10k_test.json',
-        data_prefix=dict(img=''),
-        test_mode=True,
+        data_prefix=dict(img='images/'),
         pipeline=val_pipeline,
+        test_mode=True,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE'),
+    dict(type='MPJPE', mode='mpjpe'),
+    dict(type='MPJPE', mode='p-mpjpe')
 ]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_8xb64-210e_rhd2d-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-384x288.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,19 +23,20 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -52,69 +53,63 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(18, 36)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(18, 36, 72)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(18, 36, 72, 144),
-                multiscale_output=True),
-            upsample=dict(mode='bilinear', align_corners=False)),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='open-mmlab://msra/hrnetv2_w18',
-        )),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w32-36af842e.pth'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
-        out_channels=21,
+        in_channels=32,
+        out_channels=133,
         deconv_out_channels=None,
-        conv_out_channels=(270, ),
-        conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'Rhd2DDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
-data_root = 'data/rhd/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -122,35 +117,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
-val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE'),
-]
+val_evaluator = dict(
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_dark-8xb64-210e_rhd2d-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_dark-8xb64-210e_onehand10k-256x256.py`

 * *Files 6% similar despite different names*

```diff
@@ -75,50 +75,52 @@
                 num_channels=(18, 36, 72, 144),
                 multiscale_output=True),
             upsample=dict(mode='bilinear', align_corners=False)),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='open-mmlab://msra/hrnetv2_w18',
         )),
+    neck=dict(
+        type='FeatureMapProcessor',
+        concat=True,
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
+        in_channels=270,
         out_channels=21,
         deconv_out_channels=None,
         conv_out_channels=(270, ),
         conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'Rhd2DDataset'
+dataset_type = 'OneHand10KDataset'
 data_mode = 'topdown'
-data_root = 'data/rhd/'
+data_root = 'data/onehand10k/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(
         type='RandomBBoxTransform', rotate_factor=180,
         scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -126,29 +128,29 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_train.json',
+        ann_file='annotations/onehand10k_train.json',
         data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_test.json',
+        ann_file='annotations/onehand10k_test.json',
         data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_udp-8xb64-210e_rhd2d-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_dark-8xb64-210e_coco-wholebody-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,19 +23,24 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='UDPHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(192, 256),
+    heatmap_size=(48, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -52,69 +57,63 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(18, 36)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(18, 36, 72)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(18, 36, 72, 144),
-                multiscale_output=True),
-            upsample=dict(mode='bilinear', align_corners=False)),
+                num_channels=(32, 64, 128, 256))),
         init_cfg=dict(
             type='Pretrained',
-            checkpoint='open-mmlab://msra/hrnetv2_w18',
-        )),
+            checkpoint='https://download.openmmlab.com/mmpose/'
+            'pretrain_models/hrnet_w32-36af842e.pth'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=[18, 36, 72, 144],
-        input_index=(0, 1, 2, 3),
-        input_transform='resize_concat',
-        out_channels=21,
+        in_channels=32,
+        out_channels=133,
         deconv_out_channels=None,
-        conv_out_channels=(270, ),
-        conv_kernel_sizes=(1, ),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
-        shift_heatmap=False,
+        shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'Rhd2DDataset'
+dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
-data_root = 'data/rhd/'
+data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+    dict(type='RandomHalfBody'),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -122,35 +121,34 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/coco_wholebody_val_v1.0.json',
+        data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-# evaluators
-val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE'),
-]
+val_evaluator = dict(
+    type='CocoWholeBodyMetric',
+    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_mobilenetv2_8xb64-210e_rhd2d-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-long-sleeved-shirt-256x192.py`

 * *Files 9% similar despite different names*

```diff
@@ -23,67 +23,64 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='MobileNetV2',
-        widen_factor=1.,
-        out_indices=(7, ),
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='mmcls://mobilenet_v2',
-        )),
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=1280,
-        out_channels=21,
+        in_channels=2048,
+        out_channels=294,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'Rhd2DDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/rhd/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -91,30 +88,30 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_train.json',
-        data_prefix=dict(img=''),
+        ann_file='train/deepfashion2_long_sleeved_shirt_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_test.json',
-        data_prefix=dict(img=''),
+        ann_file='validation/deepfashion2_long_sleeved_shirt_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = [
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_res50_8xb64-210e_rhd2d-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_mobilenetv2_8xb64-210e_mpii-256x256.py`

 * *Files 9% similar despite different names*

```diff
@@ -23,66 +23,63 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='torchvision://resnet50',
-        )),
+        type='MobileNetV2',
+        widen_factor=1.,
+        out_indices=(7, ),
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://mobilenet_v2'),
+    ),
     head=dict(
         type='HeatmapHead',
-        in_channels=2048,
-        out_channels=21,
+        in_channels=1280,
+        out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'Rhd2DDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/rhd/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -90,35 +87,32 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
-val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE'),
-]
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/td-reg_res50_8xb64-210e_onehand10k-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_rle-8xb64-210e_mpii-256x256.py`

 * *Files 5% similar despite different names*

```diff
@@ -22,17 +22,14 @@
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
-# hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
-
 # codec settings
 codec = dict(type='RegressionLabel', input_size=(256, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
@@ -43,44 +40,41 @@
     backbone=dict(
         type='ResNet',
         depth=50,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RegressionHead',
+        type='RLEHead',
         in_channels=2048,
-        num_joints=21,
-        loss=dict(type='SmoothL1Loss', use_target_weight=True),
+        num_joints=16,
+        loss=dict(type='RLELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_coords=True,
     ))
 
 # base dataset settings
-dataset_type = 'OneHand10KDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/onehand10k/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,35 +82,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/onehand10k_train.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/onehand10k_test.json',
-        data_prefix=dict(img=''),
+        ann_file='annotations/mpii_val.json',
+        headbox_file=f'{data_root}/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# hooks
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
+
 # evaluators
-val_evaluator = [
-    dict(type='PCKAccuracy', thr=0.2),
-    dict(type='AUC'),
-    dict(type='EPE'),
-]
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/td-reg_res50_8xb64-210e_rhd2d-256x256.py` & `mmpose-1.1.0/mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_6xb64-210e_deepfasion2-short-sleeved-shirt-256x192.py`

 * *Files 10% similar despite different names*

```diff
@@ -20,67 +20,67 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=384)
 
 # hooks
-default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
+default_hooks = dict(
+    logger=dict(type='LoggerHook', interval=10),
+    checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
-codec = dict(type='RegressionLabel', input_size=(256, 256))
+codec = dict(
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
         depth=50,
         init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
-    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='RegressionHead',
+        type='HeatmapHead',
         in_channels=2048,
-        num_joints=21,
-        loss=dict(type='SmoothL1Loss', use_target_weight=True),
+        out_channels=294,
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'Rhd2DDataset'
+dataset_type = 'DeepFashion2Dataset'
 data_mode = 'topdown'
-data_root = 'data/rhd/'
+data_root = 'data/deepfasion2/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(
-        type='RandomBBoxTransform', rotate_factor=180,
-        scale_factor=(0.7, 1.3)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -88,30 +88,30 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_train.json',
-        data_prefix=dict(img=''),
+        ann_file='train/deepfashion2_short_sleeved_shirt_train.json',
+        data_prefix=dict(img='train/image/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/rhd_test.json',
-        data_prefix=dict(img=''),
+        ann_file='validation/deepfashion2_short_sleeved_shirt_validation.json',
+        data_prefix=dict(img='validation/image/'),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # evaluators
 val_evaluator = [
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-m_8xb64-270e_coco-wholebody-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -36,16 +35,16 @@
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
 codec = dict(
     type='SimCCLabel',
-    input_size=(288, 384),
-    sigma=(6., 6.93),
+    input_size=(192, 256),
+    sigma=(4.9, 5.66),
     simcc_split_ratio=2.0,
     normalize=False,
     use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
@@ -55,32 +54,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=1.,
-        widen_factor=1.,
+        deepen_factor=0.67,
+        widen_factor=0.75,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
+            'rtmposev1/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=1024,
+        in_channels=768,
         out_channels=133,
         input_size=codec['input_size'],
-        in_featuremap_size=(9, 12),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +96,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +133,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -173,15 +172,15 @@
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
@@ -198,14 +197,16 @@
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-m_udp_8xb64-210e_coco-wholebody-256x192.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-max_epochs = 270
+max_epochs = 210
 stage2_num_epochs = 30
 base_lr = 4e-3
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
@@ -20,102 +20,83 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
 codec = dict(
-    type='SimCCLabel',
-    input_size=(192, 256),
-    sigma=(4.9, 5.66),
-    simcc_split_ratio=2.0,
-    normalize=False,
-    use_dark=False)
+    type='UDPHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=1.,
-        widen_factor=1.,
+        deepen_factor=0.67,
+        widen_factor=0.75,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
-            checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
-        )),
+            checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
+            'rtmdet/cspnext_rsb_pretrain/'
+            'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')),
     head=dict(
-        type='RTMCCHead',
-        in_channels=1024,
+        type='HeatmapHead',
+        in_channels=768,
         out_channels=133,
-        input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
-        simcc_split_ratio=codec['simcc_split_ratio'],
-        final_layer_kernel_size=7,
-        gau_cfg=dict(
-            hidden_dims=256,
-            s=128,
-            expansion_factor=2,
-            dropout_rate=0.,
-            drop_path=0.,
-            act_fn='SiLU',
-            use_rel_bias=False,
-            pos_enc=False),
-        loss=dict(
-            type='KLDiscretLoss',
-            use_target_weight=True,
-            beta=10.,
-            label_softmax=True),
+        loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
-    test_cfg=dict(flip_test=True, ))
+    test_cfg=dict(
+        flip_test=False,
+        flip_mode='heatmap',
+        shift_heatmap=False,
+    ))
 
 # base dataset settings
 dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +115,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-m_8xb64-270e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -36,16 +35,16 @@
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # codec settings
 codec = dict(
     type='SimCCLabel',
-    input_size=(192, 256),
-    sigma=(4.9, 5.66),
+    input_size=(288, 384),
+    sigma=(6., 6.93),
     simcc_split_ratio=2.0,
     normalize=False,
     use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
@@ -55,32 +54,32 @@
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.67,
-        widen_factor=0.75,
+        deepen_factor=1.,
+        widen_factor=1.,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
             checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
-            'rtmpose/cspnext-m_udp-aic-coco_210e-256x192-f2f7d6f6_20230130.pth'  # noqa
+            'rtmposev1/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
         )),
     head=dict(
         type='RTMCCHead',
-        in_channels=768,
+        in_channels=1024,
         out_channels=133,
         input_size=codec['input_size'],
-        in_featuremap_size=(6, 8),
+        in_featuremap_size=tuple([s // 32 for s in codec['input_size']]),
         simcc_split_ratio=codec['simcc_split_ratio'],
         final_layer_kernel_size=7,
         gau_cfg=dict(
             hidden_dims=256,
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
@@ -97,25 +96,25 @@
     test_cfg=dict(flip_test=True, ))
 
 # base dataset settings
 dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -134,22 +133,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -173,15 +172,15 @@
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
@@ -198,14 +197,16 @@
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
         ann_file='annotations/coco_wholebody_val_v1.0.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
+        bbox_file='data/coco/person_detection_results/'
+        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-l_udp_8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-l_udp_8xb64-210e_coco-wholebody-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,15 +20,14 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
@@ -79,25 +78,25 @@
     ))
 
 # base dataset settings
 dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -116,22 +115,22 @@
                 min_width=0.2,
                 p=1.0),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-m_udp_8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-l_8xb256-420e_humanart-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-max_epochs = 210
+max_epochs = 420
 stage2_num_epochs = 30
 base_lr = 4e-3
 
 train_cfg = dict(max_epochs=max_epochs, val_interval=10)
 randomness = dict(seed=21)
 
 # optimizer
@@ -20,84 +20,102 @@
     dict(
         type='LinearLR',
         start_factor=1.0e-5,
         by_epoch=False,
         begin=0,
         end=1000),
     dict(
-        # use cosine lr from 150 to 300 epoch
+        # use cosine lr from 210 to 420 epoch
         type='CosineAnnealingLR',
         eta_min=base_lr * 0.05,
         begin=max_epochs // 2,
         end=max_epochs,
         T_max=max_epochs // 2,
         by_epoch=True,
         convert_to_iter_based=True),
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=1024)
 
 # codec settings
 codec = dict(
-    type='UDPHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='SimCCLabel',
+    input_size=(192, 256),
+    sigma=(4.9, 5.66),
+    simcc_split_ratio=2.0,
+    normalize=False,
+    use_dark=False)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         _scope_='mmdet',
         type='CSPNeXt',
         arch='P5',
         expand_ratio=0.5,
-        deepen_factor=0.67,
-        widen_factor=0.75,
+        deepen_factor=1.,
+        widen_factor=1.,
         out_indices=(4, ),
         channel_attention=True,
         norm_cfg=dict(type='SyncBN'),
         act_cfg=dict(type='SiLU'),
         init_cfg=dict(
             type='Pretrained',
             prefix='backbone.',
-            checkpoint='https://download.openmmlab.com/mmdetection/v3.0/'
-            'rtmdet/cspnext_rsb_pretrain/'
-            'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')),
+            checkpoint='https://download.openmmlab.com/mmpose/v1/projects/'
+            'rtmpose/cspnext-l_udp-aic-coco_210e-256x192-273b7631_20230130.pth'  # noqa
+        )),
     head=dict(
-        type='HeatmapHead',
-        in_channels=768,
-        out_channels=133,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        type='RTMCCHead',
+        in_channels=1024,
+        out_channels=17,
+        input_size=codec['input_size'],
+        in_featuremap_size=(6, 8),
+        simcc_split_ratio=codec['simcc_split_ratio'],
+        final_layer_kernel_size=7,
+        gau_cfg=dict(
+            hidden_dims=256,
+            s=128,
+            expansion_factor=2,
+            dropout_rate=0.,
+            drop_path=0.,
+            act_fn='SiLU',
+            use_rel_bias=False,
+            pos_enc=False),
+        loss=dict(
+            type='KLDiscretLoss',
+            use_target_weight=True,
+            beta=10.,
+            label_softmax=True),
         decoder=codec),
-    test_cfg=dict(
-        flip_test=False,
-        flip_mode='heatmap',
-        shift_heatmap=False,
-    ))
+    test_cfg=dict(flip_test=True))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'HumanArtDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/'
 
-file_client_args = dict(backend='disk')
-# file_client_args = dict(
+backend_args = dict(backend='local')
+# backend_args = dict(
 #     backend='petrel',
 #     path_mapping=dict({
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/',
 #         f'{data_root}': 's3://openmmlab/datasets/detection/coco/'
 #     }))
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform', scale_factor=[0.6, 1.4], rotate_factor=80),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='mmdet.YOLOXHSVRandomAug'),
@@ -110,28 +128,28 @@
                 type='CoarseDropout',
                 max_holes=1,
                 max_height=0.4,
                 max_width=0.4,
                 min_holes=1,
                 min_height=0.2,
                 min_width=0.2,
-                p=1.0),
+                p=1.),
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 train_pipeline_stage2 = [
-    dict(type='LoadImage', file_client_args=file_client_args),
+    dict(type='LoadImage', backend_args=backend_args),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         shift_factor=0.,
         scale_factor=[0.75, 1.25],
@@ -155,47 +173,48 @@
         ]),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=256,
     num_workers=10,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='HumanArt/annotations/training_humanart_coco.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=10,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='HumanArt/annotations/validation_humanart.json',
+        # bbox_file=f'{data_root}HumanArt/person_detection_results/'
+        # 'HumanArt_validation_detections_AP_H_56_person.json',
+        data_prefix=dict(img=''),
         test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(
-        save_best='coco-wholebody/AP', rule='greater', max_keep_ckpts=1))
+    checkpoint=dict(save_best='coco/AP', rule='greater', max_keep_ckpts=1))
 
 custom_hooks = [
     dict(
         type='EMAHook',
         ema_type='ExpMomentumEMA',
         momentum=0.0002,
         update_buffers=True,
@@ -204,10 +223,10 @@
         type='mmdet.PipelineSwitchHook',
         switch_epoch=max_epochs - stage2_num_epochs,
         switch_pipeline=train_pipeline_stage2)
 ]
 
 # evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'HumanArt/annotations/validation_humanart.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_hrnet-w32_8xb64-210e_humanart-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,16 +23,15 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
@@ -74,42 +73,42 @@
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=32,
-        out_channels=133,
+        out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'HumanArtDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -117,34 +116,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='HumanArt/annotations/training_humanart_coco.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='HumanArt/annotations/validation_humanart.json',
+        bbox_file=f'{data_root}HumanArt/person_detection_results/'
+        'HumanArt_validation_detections_AP_H_56_person.json',
+        data_prefix=dict(img=''),
         test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'HumanArt/annotations/validation_humanart.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_hrnet-w48_8xb32-210e_humanart-256x192.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,20 +23,19 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -53,98 +52,99 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(32, 64)),
+                num_channels=(48, 96)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(32, 64, 128)),
+                num_channels=(48, 96, 192)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(32, 64, 128, 256))),
+                num_channels=(48, 96, 192, 384))),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w32-36af842e.pth'),
+            'pretrain_models/hrnet_w48-8ef0771d.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=32,
-        out_channels=133,
+        in_channels=48,
+        out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'HumanArtDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='HumanArt/annotations/training_humanart_coco.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='HumanArt/annotations/validation_humanart.json',
+        bbox_file=f'{data_root}HumanArt/person_detection_results/'
+        'HumanArt_validation_detections_AP_H_56_person.json',
+        data_prefix=dict(img=''),
         test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'HumanArt/annotations/validation_humanart.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_dark-8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py`

 * *Files 8% similar despite different names*

```diff
@@ -23,24 +23,19 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -78,42 +73,42 @@
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
             'pretrain_models/hrnet_w32-36af842e.pth'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=32,
-        out_channels=133,
+        out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -121,34 +116,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
-        test_mode=True,
+        ann_file='annotations/person_keypoints_val2017.json',
         bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
+        test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-256x192.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=20, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,32 +13,37 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=20,
+        milestones=[10, 15],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+    checkpoint=dict(
+        save_best='posetrack18/Total AP', rule='greater', interval=1))
+
+# load from the pretrained model
+load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth'  # noqa: E501
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
+norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
@@ -53,98 +58,98 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(48, 96)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384))),
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+                num_channels=(32, 64, 128, 256))),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=48,
-        out_channels=133,
+        in_channels=32,
+        out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'PoseTrack18Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/posetrack18/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/posetrack18_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/posetrack18_val.json',
+        # comment `bbox_file` and '`filter_cfg` if use gt bbox for evaluation
+        bbox_file='data/posetrack18/annotations/'
+        'posetrack18_val_human_detections.json',
+        filter_cfg=dict(bbox_score_thr=0.4),
+        data_prefix=dict(img=''),
         test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='PoseTrack18Metric',
+    ann_file=data_root + 'annotations/posetrack18_val.json',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-small_8xb64-210e_humanart-256x192.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,17 +1,33 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
 train_cfg = dict(max_epochs=210, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
+custom_imports = dict(
+    imports=['mmpose.engine.optim_wrappers.layer_decay_optim_wrapper'],
+    allow_failed_imports=False)
+
+optim_wrapper = dict(
+    optimizer=dict(
+        type='AdamW', lr=5e-4, betas=(0.9, 0.999), weight_decay=0.1),
+    paramwise_cfg=dict(
+        num_layers=12,
+        layer_decay_rate=0.8,
+        custom_keys={
+            'bias': dict(decay_multi=0.0),
+            'pos_embed': dict(decay_mult=0.0),
+            'relative_position_bias_table': dict(decay_mult=0.0),
+            'norm': dict(decay_mult=0.0),
+        },
+    ),
+    constructor='LayerDecayOptimWrapperConstructor',
+    clip_grad=dict(max_norm=1., norm_type=2),
+)
 
 # learning policy
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
@@ -24,127 +40,116 @@
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+    checkpoint=dict(save_best='coco/AP', rule='greater', max_keep_ckpts=1))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='UDPHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='HRNet',
-        in_channels=3,
-        extra=dict(
-            stage1=dict(
-                num_modules=1,
-                num_branches=1,
-                block='BOTTLENECK',
-                num_blocks=(4, ),
-                num_channels=(64, )),
-            stage2=dict(
-                num_modules=1,
-                num_branches=2,
-                block='BASIC',
-                num_blocks=(4, 4),
-                num_channels=(48, 96)),
-            stage3=dict(
-                num_modules=4,
-                num_branches=3,
-                block='BASIC',
-                num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
-            stage4=dict(
-                num_modules=3,
-                num_branches=4,
-                block='BASIC',
-                num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384))),
+        type='mmpretrain.VisionTransformer',
+        arch={
+            'embed_dims': 384,
+            'num_layers': 12,
+            'num_heads': 12,
+            'feedforward_channels': 384 * 4
+        },
+        img_size=(256, 192),
+        patch_size=16,
+        qkv_bias=True,
+        drop_path_rate=0.1,
+        with_cls_token=False,
+        out_type='featmap',
+        patch_cfg=dict(padding=2),
         init_cfg=dict(
             type='Pretrained',
             checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+            'v1/pretrained_models/mae_pretrain_vit_small.pth'),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=48,
-        out_channels=133,
-        deconv_out_channels=None,
+        in_channels=384,
+        out_channels=17,
+        deconv_out_channels=(256, 256),
+        deconv_kernel_sizes=(4, 4),
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
-        shift_heatmap=True,
+        shift_heatmap=False,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+data_root = 'data/'
+dataset_type = 'HumanArtDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(type='TopdownAffine', input_size=codec['input_size'], use_udp=True),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
-    num_workers=2,
+    batch_size=64,
+    num_workers=4,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='HumanArt/annotations/training_humanart_coco.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
-    num_workers=2,
+    num_workers=4,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='HumanArt/annotations/validation_humanart.json',
+        bbox_file=f'{data_root}HumanArt/person_detection_results/'
+        'HumanArt_validation_detections_AP_H_56_person.json',
+        data_prefix=dict(img=''),
         test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'HumanArt/annotations/validation_humanart.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-384x288.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=20, val_interval=1)
 
 # optimizer
 optim_wrapper = dict(optimizer=dict(
     type='Adam',
     lr=5e-4,
 ))
 
@@ -13,36 +13,37 @@
 param_scheduler = [
     dict(
         type='LinearLR', begin=0, end=500, start_factor=0.001,
         by_epoch=False),  # warm-up
     dict(
         type='MultiStepLR',
         begin=0,
-        end=210,
-        milestones=[170, 200],
+        end=20,
+        milestones=[10, 15],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+    checkpoint=dict(
+        save_best='posetrack18/Total AP', rule='greater', interval=1))
+
+# load from the pretrained model
+load_from = 'https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-384x288-ca5956af_20220909.pth'  # noqa: E501
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(288, 384),
-    heatmap_size=(72, 96),
-    sigma=3,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
+norm_cfg = dict(type='SyncBN', requires_grad=True)
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
@@ -57,98 +58,98 @@
                 num_blocks=(4, ),
                 num_channels=(64, )),
             stage2=dict(
                 num_modules=1,
                 num_branches=2,
                 block='BASIC',
                 num_blocks=(4, 4),
-                num_channels=(48, 96)),
+                num_channels=(32, 64)),
             stage3=dict(
                 num_modules=4,
                 num_branches=3,
                 block='BASIC',
                 num_blocks=(4, 4, 4),
-                num_channels=(48, 96, 192)),
+                num_channels=(32, 64, 128)),
             stage4=dict(
                 num_modules=3,
                 num_branches=4,
                 block='BASIC',
                 num_blocks=(4, 4, 4, 4),
-                num_channels=(48, 96, 192, 384))),
-        init_cfg=dict(
-            type='Pretrained',
-            checkpoint='https://download.openmmlab.com/mmpose/'
-            'pretrain_models/hrnet_w48-8ef0771d.pth'),
+                num_channels=(32, 64, 128, 256))),
     ),
     head=dict(
         type='HeatmapHead',
-        in_channels=48,
-        out_channels=133,
+        in_channels=32,
+        out_channels=17,
         deconv_out_channels=None,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'PoseTrack18Dataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/posetrack18/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
+
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/posetrack18_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/posetrack18_val.json',
+        # comment `bbox_file` and '`filter_cfg` if use gt bbox for evaluation
+        bbox_file='data/posetrack18/annotations/'
+        'posetrack18_val_human_detections.json',
+        filter_cfg=dict(bbox_score_thr=0.4),
+        data_prefix=dict(img=''),
         test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='PoseTrack18Metric',
+    ann_file=data_root + 'annotations/posetrack18_val.json',
+)
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/td-reg_res50_8xb64-210e_rhd2d-256x256.py`

 * *Files 10% similar despite different names*

```diff
@@ -20,102 +20,103 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='AUC', rule='greater'))
 
 # codec settings
-codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+codec = dict(type='RegressionLabel', input_size=(256, 256))
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
         type='ResNet',
-        depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
+    neck=dict(type='GlobalAveragePooling'),
     head=dict(
-        type='HeatmapHead',
+        type='RegressionHead',
         in_channels=2048,
-        out_channels=133,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
+        num_joints=21,
+        loss=dict(type='SmoothL1Loss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'Rhd2DDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/rhd/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform', rotate_factor=180,
+        scale_factor=(0.7, 1.3)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/rhd_train.json',
+        data_prefix=dict(img=''),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/rhd_test.json',
+        data_prefix=dict(img=''),
         test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+# evaluators
+val_evaluator = [
+    dict(type='PCKAccuracy', thr=0.2),
+    dict(type='AUC'),
+    dict(type='EPE'),
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_resnetv1d101_8xb64-210e_mpii-256x256.py`

 * *Files 11% similar despite different names*

```diff
@@ -20,102 +20,98 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=256)
+auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='PCK', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap', input_size=(256, 256), heatmap_size=(64, 64), sigma=2)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
+        type='ResNetV1d',
         depth=101,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet101'),
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet101_v1d'),
     ),
     head=dict(
         type='HeatmapHead',
         in_channels=2048,
-        out_channels=133,
+        out_channels=16,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'MpiiDataset'
 data_mode = 'topdown'
-data_root = 'data/coco/'
+data_root = 'data/mpii/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(type='RandomBBoxTransform', shift_prob=0),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        ann_file='annotations/mpii_train.json',
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
+        ann_file='annotations/mpii_val.json',
+        headbox_file='data/mpii/annotations/mpii_gt_val.mat',
+        data_prefix=dict(img='images/'),
         test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+# evaluators
+val_evaluator = dict(type='MpiiPCKAccuracy')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-semi-supv_8xb64-200e_h36m.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,121 +1,117 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
+vis_backends = [
+    dict(type='LocalVisBackend'),
+]
+visualizer = dict(
+    type='Pose3dLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = None
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
 
 # learning policy
-param_scheduler = [
-    dict(
-        type='LinearLR', begin=0, end=500, start_factor=0.001,
-        by_epoch=False),  # warm-up
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=210,
-        milestones=[170, 200],
-        gamma=0.1,
-        by_epoch=True)
-]
 
-# automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=1024)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+    checkpoint=dict(
+        type='CheckpointHook',
+        save_best='MPJPE',
+        rule='less',
+        max_keep_ckpts=1),
+    logger=dict(type='LoggerHook', interval=20),
+)
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='VideoPoseLifting',
+    num_keypoints=17,
+    zero_center=True,
+    root_index=0,
+    remove_root=False)
 
 # model settings
 model = dict(
-    type='TopdownPoseEstimator',
-    data_preprocessor=dict(
-        type='PoseDataPreprocessor',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        bgr_to_rgb=True),
+    type='PoseLifter',
     backbone=dict(
-        type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=2,
+        kernel_sizes=(3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=133,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
-    ))
+        type='TemporalRegressionHead',
+        in_channels=1024,
+        num_joints=17,
+        loss=dict(type='MPJPELoss'),
+        decoder=codec,
+    ),
+    traj_backbone=dict(
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=2,
+        kernel_sizes=(3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
+    ),
+    traj_head=dict(
+        type='TrajectoryRegressionHead',
+        in_channels=1024,
+        num_joints=1,
+        loss=dict(type='MPJPELoss', use_target_weight=True),
+        decoder=codec,
+    ),
+    semi_loss=dict(
+        type='SemiSupervisionLoss',
+        joint_parents=[0, 0, 1, 2, 0, 4, 5, 0, 7, 8, 9, 8, 11, 12, 8, 14, 15],
+        warmup_iterations=1311376 // 64 // 8 * 5),
+)
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
-data_mode = 'topdown'
-data_root = 'data/coco/'
+dataset_type = 'Human36mDataset'
+data_root = 'data/h36m/'
 
 # pipelines
-train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='GenerateTarget', encoder=codec),
-    dict(type='PackPoseInputs')
-]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='PackPoseInputs')
+    dict(type='GenerateTarget', encoder=codec),
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 
 # data loaders
-train_dataloader = dict(
-    batch_size=32,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
-        pipeline=train_pipeline,
-    ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_test.npz',
+        seq_len=27,
+        causal=False,
+        pad_video_seq=True,
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
-        test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='images/'),
         pipeline=val_pipeline,
+        test_mode=True,
     ))
 test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+# evaluators
+val_evaluator = [
+    dict(type='MPJPE', mode='mpjpe'),
+    dict(type='MPJPE', mode='p-mpjpe'),
+    dict(type='MPJPE', mode='n-mpjpe')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-semi-supv-cpn-ft_8xb64-200e_h36m.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,121 +1,119 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
+vis_backends = [
+    dict(type='LocalVisBackend'),
+]
+visualizer = dict(
+    type='Pose3dLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = None
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
 
 # learning policy
-param_scheduler = [
-    dict(
-        type='LinearLR', begin=0, end=500, start_factor=0.001,
-        by_epoch=False),  # warm-up
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=210,
-        milestones=[170, 200],
-        gamma=0.1,
-        by_epoch=True)
-]
 
-# automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=1024)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+    checkpoint=dict(
+        type='CheckpointHook',
+        save_best='MPJPE',
+        rule='less',
+        max_keep_ckpts=1),
+    logger=dict(type='LoggerHook', interval=20),
+)
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='VideoPoseLifting',
+    num_keypoints=17,
+    zero_center=True,
+    root_index=0,
+    remove_root=False)
 
 # model settings
 model = dict(
-    type='TopdownPoseEstimator',
-    data_preprocessor=dict(
-        type='PoseDataPreprocessor',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        bgr_to_rgb=True),
+    type='PoseLifter',
     backbone=dict(
-        type='ResNet',
-        depth=152,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet152'),
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=2,
+        kernel_sizes=(3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=133,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
-    ))
+        type='TemporalRegressionHead',
+        in_channels=1024,
+        num_joints=17,
+        loss=dict(type='MPJPELoss'),
+        decoder=codec,
+    ),
+    traj_backbone=dict(
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=2,
+        kernel_sizes=(3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
+    ),
+    traj_head=dict(
+        type='TrajectoryRegressionHead',
+        in_channels=1024,
+        num_joints=1,
+        loss=dict(type='MPJPELoss', use_target_weight=True),
+        decoder=codec,
+    ),
+    semi_loss=dict(
+        type='SemiSupervisionLoss',
+        joint_parents=[0, 0, 1, 2, 0, 4, 5, 0, 7, 8, 9, 8, 11, 12, 8, 14, 15],
+        warmup_iterations=1311376 // 64 // 8 * 5),
+)
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
-data_mode = 'topdown'
-data_root = 'data/coco/'
+dataset_type = 'Human36mDataset'
+data_root = 'data/h36m/'
 
 # pipelines
-train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='GenerateTarget', encoder=codec),
-    dict(type='PackPoseInputs')
-]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='PackPoseInputs')
+    dict(type='GenerateTarget', encoder=codec),
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 
 # data loaders
-train_dataloader = dict(
-    batch_size=32,
-    num_workers=2,
-    persistent_workers=True,
-    sampler=dict(type='DefaultSampler', shuffle=True),
-    dataset=dict(
-        type=dataset_type,
-        data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
-        pipeline=train_pipeline,
-    ))
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=64,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_test.npz',
+        seq_len=27,
+        causal=False,
+        pad_video_seq=True,
+        keypoint_2d_src='detection',
+        keypoint_2d_det_file='joint_2d_det_files/cpn_ft_h36m_dbb_test.npy',
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
-        test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='images/'),
         pipeline=val_pipeline,
+        test_mode=True,
     ))
 test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+# evaluators
+val_evaluator = [
+    dict(type='MPJPE', mode='mpjpe'),
+    dict(type='MPJPE', mode='p-mpjpe'),
+    dict(type='MPJPE', mode='n-mpjpe')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-243frm-supv-cpn-ft_8xb128-200e_h36m.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,121 +1,132 @@
 _base_ = ['../../../_base_/default_runtime.py']
 
+vis_backends = [
+    dict(type='LocalVisBackend'),
+]
+visualizer = dict(
+    type='Pose3dLocalVisualizer', vis_backends=vis_backends, name='visualizer')
+
 # runtime
-train_cfg = dict(max_epochs=210, val_interval=10)
+train_cfg = dict(max_epochs=200, val_interval=10)
 
 # optimizer
-optim_wrapper = dict(optimizer=dict(
-    type='Adam',
-    lr=5e-4,
-))
+optim_wrapper = dict(optimizer=dict(type='Adam', lr=1e-4))
 
 # learning policy
 param_scheduler = [
-    dict(
-        type='LinearLR', begin=0, end=500, start_factor=0.001,
-        by_epoch=False),  # warm-up
-    dict(
-        type='MultiStepLR',
-        begin=0,
-        end=210,
-        milestones=[170, 200],
-        gamma=0.1,
-        by_epoch=True)
+    dict(type='ExponentialLR', gamma=0.98, end=200, by_epoch=True)
 ]
 
-# automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=1024)
 
 # hooks
 default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+    checkpoint=dict(
+        type='CheckpointHook',
+        save_best='MPJPE',
+        rule='less',
+        max_keep_ckpts=1),
+    logger=dict(type='LoggerHook', interval=20),
+)
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='VideoPoseLifting',
+    num_keypoints=17,
+    zero_center=True,
+    root_index=0,
+    remove_root=False)
 
 # model settings
 model = dict(
-    type='TopdownPoseEstimator',
-    data_preprocessor=dict(
-        type='PoseDataPreprocessor',
-        mean=[123.675, 116.28, 103.53],
-        std=[58.395, 57.12, 57.375],
-        bgr_to_rgb=True),
+    type='PoseLifter',
     backbone=dict(
-        type='ResNet',
-        depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+        type='TCN',
+        in_channels=2 * 17,
+        stem_channels=1024,
+        num_blocks=4,
+        kernel_sizes=(3, 3, 3, 3, 3),
+        dropout=0.25,
+        use_stride_conv=True,
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
-        out_channels=133,
-        loss=dict(type='KeypointMSELoss', use_target_weight=True),
-        decoder=codec),
-    test_cfg=dict(
-        flip_test=True,
-        flip_mode='heatmap',
-        shift_heatmap=True,
+        type='TemporalRegressionHead',
+        in_channels=1024,
+        num_joints=17,
+        loss=dict(type='MPJPELoss'),
+        decoder=codec,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
-data_mode = 'topdown'
-data_root = 'data/coco/'
+dataset_type = 'Human36mDataset'
+data_root = 'data/h36m/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='RandomFlip', direction='horizontal'),
-    dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
+    dict(
+        type='RandomFlipAroundRoot',
+        keypoints_flip_cfg=dict(),
+        target_flip_cfg=dict(),
+    ),
     dict(type='GenerateTarget', encoder=codec),
-    dict(type='PackPoseInputs')
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
-    dict(type='GetBBoxCenterScale'),
-    dict(type='TopdownAffine', input_size=codec['input_size']),
-    dict(type='PackPoseInputs')
+    dict(type='GenerateTarget', encoder=codec),
+    dict(
+        type='PackPoseInputs',
+        meta_keys=('id', 'category_id', 'target_img_path', 'flip_indices',
+                   'target_root'))
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_train.npz',
+        seq_len=243,
+        causal=False,
+        pad_video_seq=True,
+        keypoint_2d_src='detection',
+        keypoint_2d_det_file='joint_2d_det_files/cpn_ft_h36m_dbb_train.npy',
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
-        data_prefix=dict(img='train2017/'),
+        data_prefix=dict(img='images/'),
         pipeline=train_pipeline,
-    ))
+    ),
+)
 val_dataloader = dict(
-    batch_size=32,
+    batch_size=128,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
+        ann_file='annotation_body3d/fps50/h36m_test.npz',
+        seq_len=243,
+        causal=False,
+        pad_video_seq=True,
+        keypoint_2d_src='detection',
+        keypoint_2d_det_file='joint_2d_det_files/cpn_ft_h36m_dbb_test.npy',
+        camera_param_file='annotation_body3d/cameras.pkl',
         data_root=data_root,
-        data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
-        test_mode=True,
-        bbox_file='data/coco/person_detection_results/'
-        'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='images/'),
         pipeline=val_pipeline,
+        test_mode=True,
     ))
 test_dataloader = val_dataloader
 
-val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+# evaluators
+val_evaluator = [
+    dict(type='MPJPE', mode='mpjpe'),
+    dict(type='MPJPE', mode='p-mpjpe')
+]
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-384x288.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_dark-8xb64-210e_coco-wholebody-256x192.py`

 * *Files 7% similar despite different names*

```diff
@@ -28,32 +28,35 @@
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
+    type='MSRAHeatmap',
+    input_size=(192, 256),
+    heatmap_size=(48, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ResNet',
+        type='ViPNAS_ResNet',
         depth=50,
-        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
     ),
     head=dict(
-        type='HeatmapHead',
-        in_channels=2048,
+        type='ViPNASHead',
+        in_channels=608,
         out_channels=133,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
@@ -62,25 +65,28 @@
 # base dataset settings
 dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(type='RandomBBoxTransform'),
+    dict(
+        type='RandomBBoxTransform',
+        rotate_factor=60,
+        scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_dark-8xb64-210e_coco-wholebody-256x192.py`

 * *Files 3% similar despite different names*

```diff
@@ -28,15 +28,19 @@
 
 # hooks
 default_hooks = dict(
     checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap',
+    input_size=(192, 256),
+    heatmap_size=(48, 64),
+    sigma=2,
+    unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
@@ -60,28 +64,28 @@
 # base dataset settings
 dataset_type = 'CocoWholeBodyDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
     dict(
         type='RandomBBoxTransform',
         rotate_factor=60,
         scale_factor=(0.75, 1.25)),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_dark-8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_res50_dark-8xb64-210e_coco-384x288.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,69 +23,67 @@
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
 auto_scale_lr = dict(base_batch_size=512)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
     type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
+    input_size=(288, 384),
+    heatmap_size=(72, 96),
+    sigma=3,
     unbiased=True)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
-    backbone=dict(type='ViPNAS_MobileNetV3'),
+    backbone=dict(
+        type='ResNet',
+        depth=50,
+        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50'),
+    ),
     head=dict(
-        type='ViPNASHead',
-        in_channels=160,
-        out_channels=133,
-        deconv_out_channels=(160, 160, 160),
-        deconv_num_groups=(160, 160, 160),
+        type='HeatmapHead',
+        in_channels=2048,
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
@@ -93,34 +91,35 @@
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
-        test_mode=True,
+        ann_file='annotations/person_keypoints_val2017.json',
         bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
+        test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_seresnet101_8xb32-210e_coco-384x288.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,104 +20,102 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap', input_size=(192, 256), heatmap_size=(48, 64), sigma=2)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ViPNAS_ResNet',
-        depth=50,
+        type='SEResNet',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://se-resnet101'),
     ),
     head=dict(
-        type='ViPNASHead',
-        in_channels=608,
-        out_channels=133,
+        type='HeatmapHead',
+        in_channels=2048,
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
-        test_mode=True,
+        ann_file='annotations/person_keypoints_val2017.json',
         bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
+        test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_dark-8xb64-210e_coco-wholebody-256x192.py` & `mmpose-1.1.0/mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_resnetv1d101_8xb32-210e_coco-384x288.py`

 * *Files 12% similar despite different names*

```diff
@@ -20,108 +20,102 @@
         end=210,
         milestones=[170, 200],
         gamma=0.1,
         by_epoch=True)
 ]
 
 # automatically scaling LR based on the actual training batch size
-auto_scale_lr = dict(base_batch_size=512)
+auto_scale_lr = dict(base_batch_size=256)
 
 # hooks
-default_hooks = dict(
-    checkpoint=dict(save_best='coco-wholebody/AP', rule='greater'))
+default_hooks = dict(checkpoint=dict(save_best='coco/AP', rule='greater'))
 
 # codec settings
 codec = dict(
-    type='MSRAHeatmap',
-    input_size=(192, 256),
-    heatmap_size=(48, 64),
-    sigma=2,
-    unbiased=True)
+    type='MSRAHeatmap', input_size=(288, 384), heatmap_size=(72, 96), sigma=3)
 
 # model settings
 model = dict(
     type='TopdownPoseEstimator',
     data_preprocessor=dict(
         type='PoseDataPreprocessor',
         mean=[123.675, 116.28, 103.53],
         std=[58.395, 57.12, 57.375],
         bgr_to_rgb=True),
     backbone=dict(
-        type='ViPNAS_ResNet',
-        depth=50,
+        type='ResNetV1d',
+        depth=101,
+        init_cfg=dict(type='Pretrained', checkpoint='mmcls://resnet101_v1d'),
     ),
     head=dict(
-        type='ViPNASHead',
-        in_channels=608,
-        out_channels=133,
+        type='HeatmapHead',
+        in_channels=2048,
+        out_channels=17,
         loss=dict(type='KeypointMSELoss', use_target_weight=True),
         decoder=codec),
     test_cfg=dict(
         flip_test=True,
         flip_mode='heatmap',
         shift_heatmap=True,
     ))
 
 # base dataset settings
-dataset_type = 'CocoWholeBodyDataset'
+dataset_type = 'CocoDataset'
 data_mode = 'topdown'
 data_root = 'data/coco/'
 
 # pipelines
 train_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='RandomFlip', direction='horizontal'),
     dict(type='RandomHalfBody'),
-    dict(
-        type='RandomBBoxTransform',
-        rotate_factor=60,
-        scale_factor=(0.75, 1.25)),
+    dict(type='RandomBBoxTransform'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='GenerateTarget', encoder=codec),
     dict(type='PackPoseInputs')
 ]
 val_pipeline = [
-    dict(type='LoadImage', file_client_args={{_base_.file_client_args}}),
+    dict(type='LoadImage'),
     dict(type='GetBBoxCenterScale'),
     dict(type='TopdownAffine', input_size=codec['input_size']),
     dict(type='PackPoseInputs')
 ]
 
 # data loaders
 train_dataloader = dict(
-    batch_size=64,
+    batch_size=32,
     num_workers=2,
     persistent_workers=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_train_v1.0.json',
+        ann_file='annotations/person_keypoints_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=train_pipeline,
     ))
 val_dataloader = dict(
     batch_size=32,
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
     sampler=dict(type='DefaultSampler', shuffle=False, round_up=False),
     dataset=dict(
         type=dataset_type,
         data_root=data_root,
         data_mode=data_mode,
-        ann_file='annotations/coco_wholebody_val_v1.0.json',
-        data_prefix=dict(img='val2017/'),
-        test_mode=True,
+        ann_file='annotations/person_keypoints_val2017.json',
         bbox_file='data/coco/person_detection_results/'
         'COCO_val2017_detections_AP_H_56_person.json',
+        data_prefix=dict(img='val2017/'),
+        test_mode=True,
         pipeline=val_pipeline,
     ))
 test_dataloader = val_dataloader
 
+# evaluators
 val_evaluator = dict(
-    type='CocoWholeBodyMetric',
-    ann_file=data_root + 'annotations/coco_wholebody_val_v1.0.json')
+    type='CocoMetric',
+    ann_file=data_root + 'annotations/person_keypoints_val2017.json')
 test_evaluator = val_evaluator
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/bottomup_demo.py` & `mmpose-1.1.0/mmpose/.mim/demo/bottomup_demo.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,49 +1,53 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import mimetypes
 import os
-import tempfile
+import time
 from argparse import ArgumentParser
 
+import cv2
 import json_tricks as json
 import mmcv
 import mmengine
+import numpy as np
 
 from mmpose.apis import inference_bottomup, init_model
 from mmpose.registry import VISUALIZERS
 from mmpose.structures import split_instances
 
 
-def process_one_image(args, img_path, pose_estimator, visualizer,
-                      show_interval):
+def process_one_image(args,
+                      img,
+                      pose_estimator,
+                      visualizer=None,
+                      show_interval=0):
     """Visualize predicted keypoints (and heatmaps) of one image."""
 
     # inference a single image
-    batch_results = inference_bottomup(pose_estimator, img_path)
+    batch_results = inference_bottomup(pose_estimator, img)
     results = batch_results[0]
 
     # show the results
-    img = mmcv.imread(img_path, channel_order='rgb')
-
-    out_file = None
-    if args.output_root:
-        out_file = f'{args.output_root}/{os.path.basename(img_path)}'
-
-    visualizer.add_datasample(
-        'result',
-        img,
-        data_sample=results,
-        draw_gt=False,
-        draw_bbox=False,
-        draw_heatmap=args.draw_heatmap,
-        show_kpt_idx=args.show_kpt_idx,
-        show=args.show,
-        wait_time=show_interval,
-        out_file=out_file,
-        kpt_score_thr=args.kpt_thr)
+    if isinstance(img, str):
+        img = mmcv.imread(img, channel_order='rgb')
+    elif isinstance(img, np.ndarray):
+        img = mmcv.bgr2rgb(img)
+
+    if visualizer is not None:
+        visualizer.add_datasample(
+            'result',
+            img,
+            data_sample=results,
+            draw_gt=False,
+            draw_bbox=False,
+            draw_heatmap=args.draw_heatmap,
+            show_kpt_idx=args.show_kpt_idx,
+            show=args.show,
+            wait_time=show_interval,
+            kpt_thr=args.kpt_thr)
 
     return results.pred_instances
 
 
 def parse_args():
     parser = ArgumentParser()
     parser.add_argument('config', help='Config file')
@@ -85,24 +89,33 @@
         default=3,
         help='Keypoint radius for visualization')
     parser.add_argument(
         '--thickness',
         type=int,
         default=1,
         help='Link thickness for visualization')
+    parser.add_argument(
+        '--show-interval', type=int, default=0, help='Sleep seconds per frame')
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
     assert args.show or (args.output_root != '')
     assert args.input != ''
+
+    output_file = None
     if args.output_root:
         mmengine.mkdir_or_exist(args.output_root)
+        output_file = os.path.join(args.output_root,
+                                   os.path.basename(args.input))
+        if args.input == 'webcam':
+            output_file += '.mp4'
+
     if args.save_predictions:
         assert args.output_root != ''
         args.pred_save_path = f'{args.output_root}/results_' \
             f'{os.path.splitext(os.path.basename(args.input))[0]}.json'
 
     # build the model from a config file and a checkpoint file
     if args.draw_heatmap:
@@ -112,56 +125,91 @@
 
     model = init_model(
         args.config,
         args.checkpoint,
         device=args.device,
         cfg_options=cfg_options)
 
-    # init visualizer
+    # build visualizer
     model.cfg.visualizer.radius = args.radius
     model.cfg.visualizer.line_width = args.thickness
     visualizer = VISUALIZERS.build(model.cfg.visualizer)
     visualizer.set_dataset_meta(model.dataset_meta)
 
-    input_type = mimetypes.guess_type(args.input)[0].split('/')[0]
+    if args.input == 'webcam':
+        input_type = 'webcam'
+    else:
+        input_type = mimetypes.guess_type(args.input)[0].split('/')[0]
+
     if input_type == 'image':
+        # inference
         pred_instances = process_one_image(
             args, args.input, model, visualizer, show_interval=0)
-        pred_instances_list = split_instances(pred_instances)
 
-    elif input_type == 'video':
-        tmp_folder = tempfile.TemporaryDirectory()
-        video = mmcv.VideoReader(args.input)
-        progressbar = mmengine.ProgressBar(len(video))
-        video.cvt2frames(tmp_folder.name, show_progress=False)
-        output_root = args.output_root
-        args.output_root = tmp_folder.name
+        if args.save_predictions:
+            pred_instances_list = split_instances(pred_instances)
+
+        if output_file:
+            img_vis = visualizer.get_image()
+            mmcv.imwrite(mmcv.rgb2bgr(img_vis), output_file)
+
+    elif input_type in ['webcam', 'video']:
+
+        if args.input == 'webcam':
+            cap = cv2.VideoCapture(0)
+        else:
+            cap = cv2.VideoCapture(args.input)
+
+        video_writer = None
         pred_instances_list = []
+        frame_idx = 0
+
+        while cap.isOpened():
+            success, frame = cap.read()
+            frame_idx += 1
+
+            if not success:
+                break
+
+            pred_instances = process_one_image(args, frame, model, visualizer,
+                                               0.001)
+
+            if args.save_predictions:
+                # save prediction results
+                pred_instances_list.append(
+                    dict(
+                        frame_id=frame_idx,
+                        instances=split_instances(pred_instances)))
+
+            # output videos
+            if output_file:
+                frame_vis = visualizer.get_image()
+
+                if video_writer is None:
+                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
+                    # the size of the image with visualization may vary
+                    # depending on the presence of heatmaps
+                    video_writer = cv2.VideoWriter(
+                        output_file,
+                        fourcc,
+                        25,  # saved fps
+                        (frame_vis.shape[1], frame_vis.shape[0]))
+
+                video_writer.write(mmcv.rgb2bgr(frame_vis))
+
+            # press ESC to exit
+            if cv2.waitKey(5) & 0xFF == 27:
+                break
+
+            time.sleep(args.show_interval)
+
+        if video_writer:
+            video_writer.release()
 
-        for frame_id, img_fname in enumerate(os.listdir(tmp_folder.name)):
-            pred_instances = process_one_image(
-                args,
-                f'{tmp_folder.name}/{img_fname}',
-                model,
-                visualizer,
-                show_interval=1)
-            progressbar.update()
-            pred_instances_list.append(
-                dict(
-                    frame_id=frame_id,
-                    instances=split_instances(pred_instances)))
-
-        if output_root:
-            mmcv.frames2video(
-                tmp_folder.name,
-                f'{output_root}/{os.path.basename(args.input)}',
-                fps=video.fps,
-                fourcc='mp4v',
-                show_progress=False)
-        tmp_folder.cleanup()
+        cap.release()
 
     else:
         args.save_predictions = False
         raise ValueError(
             f'file {os.path.basename(args.input)} has invalid format.')
 
     if args.save_predictions:
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/image_demo.py` & `mmpose-1.1.0/mmpose/.mim/demo/image_demo.py`

 * *Files 18% similar despite different names*

```diff
@@ -22,14 +22,37 @@
         help='Visualize the predicted heatmap')
     parser.add_argument(
         '--show-kpt-idx',
         action='store_true',
         default=False,
         help='Whether to show the index of keypoints')
     parser.add_argument(
+        '--skeleton-style',
+        default='mmpose',
+        type=str,
+        choices=['mmpose', 'openpose'],
+        help='Skeleton style selection')
+    parser.add_argument(
+        '--kpt-thr',
+        type=float,
+        default=0.3,
+        help='Visualizing keypoint thresholds')
+    parser.add_argument(
+        '--radius',
+        type=int,
+        default=3,
+        help='Keypoint radius for visualization')
+    parser.add_argument(
+        '--thickness',
+        type=int,
+        default=1,
+        help='Link thickness for visualization')
+    parser.add_argument(
+        '--alpha', type=float, default=0.8, help='The transparency of bboxes')
+    parser.add_argument(
         '--show',
         action='store_true',
         default=False,
         help='whether to show img')
     args = parser.parse_args()
     return args
 
@@ -46,30 +69,37 @@
     model = init_model(
         args.config,
         args.checkpoint,
         device=args.device,
         cfg_options=cfg_options)
 
     # init visualizer
+    model.cfg.visualizer.radius = args.radius
+    model.cfg.visualizer.alpha = args.alpha
+    model.cfg.visualizer.line_width = args.thickness
+
     visualizer = VISUALIZERS.build(model.cfg.visualizer)
-    visualizer.set_dataset_meta(model.dataset_meta)
+    visualizer.set_dataset_meta(
+        model.dataset_meta, skeleton_style=args.skeleton_style)
 
     # inference a single image
     batch_results = inference_topdown(model, args.img)
     results = merge_data_samples(batch_results)
 
     # show the results
     img = imread(args.img, channel_order='rgb')
     visualizer.add_datasample(
         'result',
         img,
         data_sample=results,
         draw_gt=False,
         draw_bbox=True,
+        kpt_thr=args.kpt_thr,
         draw_heatmap=args.draw_heatmap,
         show_kpt_idx=args.show_kpt_idx,
+        skeleton_style=args.skeleton_style,
         show=args.show,
         out_file=args.out_file)
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_1class.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_1class.py`

 * *Files 3% similar despite different names*

```diff
@@ -213,25 +213,23 @@
             nms=dict(type='nms', iou_threshold=0.5),
             max_per_img=100)))
 
 # dataset settings
 dataset_type = 'CocoDataset'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 train_pipeline = [
-    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(type='LoadImageFromFile'),
     dict(type='LoadAnnotations', with_bbox=True),
     dict(type='Resize', scale=(1333, 800), keep_ratio=True),
     dict(type='RandomFlip', prob=0.5),
     dict(type='PackDetInputs')
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(type='LoadImageFromFile'),
     dict(type='Resize', scale=(1333, 800), keep_ratio=True),
     # If you don't have a gt annotation, delete the pipeline
     dict(type='LoadAnnotations', with_bbox=True),
     dict(
         type='PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                    'scale_factor'))
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_coco.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_coco.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_1class.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_1class.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py`

 * *Files 2% similar despite different names*

```diff
@@ -139,25 +139,23 @@
         # e.g., nms=dict(type='soft_nms', iou_threshold=0.5, min_score=0.05)
     ))
 
 # dataset settings
 dataset_type = 'CocoDataset'
 data_root = 'data/coco/'
 
-file_client_args = dict(backend='disk')
-
 train_pipeline = [
-    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(type='LoadImageFromFile'),
     dict(type='LoadAnnotations', with_bbox=True),
     dict(type='Resize', scale=(1333, 800), keep_ratio=True),
     dict(type='RandomFlip', prob=0.5),
     dict(type='PackDetInputs')
 ]
 test_pipeline = [
-    dict(type='LoadImageFromFile', file_client_args=file_client_args),
+    dict(type='LoadImageFromFile'),
     dict(type='Resize', scale=(1333, 800), keep_ratio=True),
     # If you don't have a gt annotation, delete the pipeline
     dict(type='LoadAnnotations', with_bbox=True),
     dict(
         type='PackDetInputs',
         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',
                    'scale_factor'))
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/mask_rcnn_r50_fpn_2x_coco.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/mask_rcnn_r50_fpn_2x_coco.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/ssdlite_mobilenetv2-scratch_8xb24-600e_coco.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/ssdlite_mobilenetv2-scratch_8xb24-600e_coco.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/yolov3_d53_320_273e_coco.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/yolov3_d53_320_273e_coco.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmdetection_cfg/yolox-s_8xb8-300e_coco-face.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmdetection_cfg/yolox-s_8xb8-300e_coco-face.py`

 * *Files 9% similar despite different names*

```diff
@@ -105,15 +105,15 @@
             reduction='sum',
             loss_weight=1.0),
         loss_l1=dict(type='L1Loss', reduction='sum', loss_weight=1.0)),
     train_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),
     test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.65)))
 data_root = 'data/coco/'
 dataset_type = 'CocoDataset'
-file_client_args = dict(backend='disk')
+backend_args = dict(backend='local')
 train_pipeline = [
     dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
     dict(
         type='RandomAffine', scaling_ratio_range=(0.1, 2),
         border=(-320, -320)),
     dict(
         type='MixUp',
@@ -134,17 +134,15 @@
     type='MultiImageMixDataset',
     dataset=dict(
         type='CocoDataset',
         data_root='data/coco/',
         ann_file='annotations/instances_train2017.json',
         data_prefix=dict(img='train2017/'),
         pipeline=[
-            dict(
-                type='LoadImageFromFile',
-                file_client_args=dict(backend='disk')),
+            dict(type='LoadImageFromFile', backend_args=dict(backend='local')),
             dict(type='LoadAnnotations', with_bbox=True)
         ],
         filter_cfg=dict(filter_empty_gt=False, min_size=32)),
     pipeline=[
         dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
         dict(
             type='RandomAffine',
@@ -163,15 +161,15 @@
             pad_to_square=True,
             pad_val=dict(img=(114.0, 114.0, 114.0))),
         dict(
             type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),
         dict(type='PackDetInputs')
     ])
 test_pipeline = [
-    dict(type='LoadImageFromFile', file_client_args=dict(backend='disk')),
+    dict(type='LoadImageFromFile', backend_args=dict(backend='local')),
     dict(type='Resize', scale=(640, 640), keep_ratio=True),
     dict(
         type='Pad',
         pad_to_square=True,
         pad_val=dict(img=(114.0, 114.0, 114.0))),
     dict(type='LoadAnnotations', with_bbox=True),
     dict(
@@ -190,15 +188,15 @@
             type='CocoDataset',
             data_root='data/coco/',
             ann_file='annotations/coco_face_train.json',
             data_prefix=dict(img='train2017/'),
             pipeline=[
                 dict(
                     type='LoadImageFromFile',
-                    file_client_args=dict(backend='disk')),
+                    backend_args=dict(backend='local')),
                 dict(type='LoadAnnotations', with_bbox=True)
             ],
             filter_cfg=dict(filter_empty_gt=False, min_size=32),
             metainfo=dict(CLASSES=('person', ), PALETTE=(220, 20, 60))),
         pipeline=[
             dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
             dict(
@@ -232,17 +230,15 @@
     dataset=dict(
         type='CocoDataset',
         data_root='data/coco/',
         ann_file='annotations/coco_face_val.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=[
-            dict(
-                type='LoadImageFromFile',
-                file_client_args=dict(backend='disk')),
+            dict(type='LoadImageFromFile', backend_args=dict(backend='local')),
             dict(type='Resize', scale=(640, 640), keep_ratio=True),
             dict(
                 type='Pad',
                 pad_to_square=True,
                 pad_val=dict(img=(114.0, 114.0, 114.0))),
             dict(type='LoadAnnotations', with_bbox=True),
             dict(
@@ -260,17 +256,15 @@
     dataset=dict(
         type='CocoDataset',
         data_root='data/coco/',
         ann_file='annotations/coco_face_val.json',
         data_prefix=dict(img='val2017/'),
         test_mode=True,
         pipeline=[
-            dict(
-                type='LoadImageFromFile',
-                file_client_args=dict(backend='disk')),
+            dict(type='LoadImageFromFile', backend_args=dict(backend='local')),
             dict(type='Resize', scale=(640, 640), keep_ratio=True),
             dict(
                 type='Pad',
                 pad_to_square=True,
                 pad_val=dict(img=(114.0, 114.0, 114.0))),
             dict(type='LoadAnnotations', with_bbox=True),
             dict(
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmtracking_cfg/deepsort_faster-rcnn_fpn_4e_mot17-private-half.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmtracking_cfg/deepsort_faster-rcnn_fpn_4e_mot17-private-half.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/mmtracking_cfg/tracktor_faster-rcnn_r50_fpn_4e_mot17-private.py` & `mmpose-1.1.0/mmpose/.mim/demo/mmtracking_cfg/tracktor_faster-rcnn_r50_fpn_4e_mot17-private.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/demo/topdown_demo_with_mmdet.py` & `mmpose-1.1.0/mmpose/.mim/demo/topdown_demo_with_mmdet.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import mimetypes
 import os
-import tempfile
+import time
 from argparse import ArgumentParser
 
+import cv2
 import json_tricks as json
 import mmcv
 import mmengine
 import numpy as np
 
 from mmpose.apis import inference_topdown
 from mmpose.apis import init_model as init_pose_estimator
@@ -19,50 +20,54 @@
 try:
     from mmdet.apis import inference_detector, init_detector
     has_mmdet = True
 except (ImportError, ModuleNotFoundError):
     has_mmdet = False
 
 
-def process_one_image(args, img_path, detector, pose_estimator, visualizer,
-                      show_interval):
+def process_one_image(args,
+                      img,
+                      detector,
+                      pose_estimator,
+                      visualizer=None,
+                      show_interval=0):
     """Visualize predicted keypoints (and heatmaps) of one image."""
 
     # predict bbox
-    det_result = inference_detector(detector, img_path)
+    det_result = inference_detector(detector, img)
     pred_instance = det_result.pred_instances.cpu().numpy()
     bboxes = np.concatenate(
         (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)
     bboxes = bboxes[np.logical_and(pred_instance.labels == args.det_cat_id,
                                    pred_instance.scores > args.bbox_thr)]
     bboxes = bboxes[nms(bboxes, args.nms_thr), :4]
 
     # predict keypoints
-    pose_results = inference_topdown(pose_estimator, img_path, bboxes)
+    pose_results = inference_topdown(pose_estimator, img, bboxes)
     data_samples = merge_data_samples(pose_results)
 
     # show the results
-    img = mmcv.imread(img_path, channel_order='rgb')
-
-    out_file = None
-    if args.output_root:
-        out_file = f'{args.output_root}/{os.path.basename(img_path)}'
-
-    visualizer.add_datasample(
-        'result',
-        img,
-        data_sample=data_samples,
-        draw_gt=False,
-        draw_heatmap=args.draw_heatmap,
-        draw_bbox=args.draw_bbox,
-        show_kpt_idx=args.show_kpt_idx,
-        show=args.show,
-        wait_time=show_interval,
-        out_file=out_file,
-        kpt_score_thr=args.kpt_thr)
+    if isinstance(img, str):
+        img = mmcv.imread(img, channel_order='rgb')
+    elif isinstance(img, np.ndarray):
+        img = mmcv.bgr2rgb(img)
+
+    if visualizer is not None:
+        visualizer.add_datasample(
+            'result',
+            img,
+            data_sample=data_samples,
+            draw_gt=False,
+            draw_heatmap=args.draw_heatmap,
+            draw_bbox=args.draw_bbox,
+            show_kpt_idx=args.show_kpt_idx,
+            skeleton_style=args.skeleton_style,
+            show=args.show,
+            wait_time=show_interval,
+            kpt_thr=args.kpt_thr)
 
     # if there is no instance detected, return None
     return data_samples.get('pred_instances', None)
 
 
 def main():
     """Visualize the demo images.
@@ -106,48 +111,68 @@
         help='Bounding box score threshold')
     parser.add_argument(
         '--nms-thr',
         type=float,
         default=0.3,
         help='IoU threshold for bounding box NMS')
     parser.add_argument(
-        '--kpt-thr', type=float, default=0.3, help='Keypoint score threshold')
+        '--kpt-thr',
+        type=float,
+        default=0.3,
+        help='Visualizing keypoint thresholds')
     parser.add_argument(
         '--draw-heatmap',
         action='store_true',
         default=False,
         help='Draw heatmap predicted by the model')
     parser.add_argument(
         '--show-kpt-idx',
         action='store_true',
         default=False,
         help='Whether to show the index of keypoints')
     parser.add_argument(
+        '--skeleton-style',
+        default='mmpose',
+        type=str,
+        choices=['mmpose', 'openpose'],
+        help='Skeleton style selection')
+    parser.add_argument(
         '--radius',
         type=int,
         default=3,
         help='Keypoint radius for visualization')
     parser.add_argument(
         '--thickness',
         type=int,
         default=1,
         help='Link thickness for visualization')
     parser.add_argument(
+        '--show-interval', type=int, default=0, help='Sleep seconds per frame')
+    parser.add_argument(
+        '--alpha', type=float, default=0.8, help='The transparency of bboxes')
+    parser.add_argument(
         '--draw-bbox', action='store_true', help='Draw bboxes of instances')
 
     assert has_mmdet, 'Please install mmdet to run the demo.'
 
     args = parser.parse_args()
 
     assert args.show or (args.output_root != '')
     assert args.input != ''
     assert args.det_config is not None
     assert args.det_checkpoint is not None
+
+    output_file = None
     if args.output_root:
         mmengine.mkdir_or_exist(args.output_root)
+        output_file = os.path.join(args.output_root,
+                                   os.path.basename(args.input))
+        if args.input == 'webcam':
+            output_file += '.mp4'
+
     if args.save_predictions:
         assert args.output_root != ''
         args.pred_save_path = f'{args.output_root}/results_' \
             f'{os.path.splitext(os.path.basename(args.input))[0]}.json'
 
     # build detector
     detector = init_detector(
@@ -158,65 +183,98 @@
     pose_estimator = init_pose_estimator(
         args.pose_config,
         args.pose_checkpoint,
         device=args.device,
         cfg_options=dict(
             model=dict(test_cfg=dict(output_heatmaps=args.draw_heatmap))))
 
-    # init visualizer
+    # build visualizer
     pose_estimator.cfg.visualizer.radius = args.radius
+    pose_estimator.cfg.visualizer.alpha = args.alpha
     pose_estimator.cfg.visualizer.line_width = args.thickness
     visualizer = VISUALIZERS.build(pose_estimator.cfg.visualizer)
     # the dataset_meta is loaded from the checkpoint and
     # then pass to the model in init_pose_estimator
-    visualizer.set_dataset_meta(pose_estimator.dataset_meta)
+    visualizer.set_dataset_meta(
+        pose_estimator.dataset_meta, skeleton_style=args.skeleton_style)
+
+    if args.input == 'webcam':
+        input_type = 'webcam'
+    else:
+        input_type = mimetypes.guess_type(args.input)[0].split('/')[0]
 
-    input_type = mimetypes.guess_type(args.input)[0].split('/')[0]
     if input_type == 'image':
-        pred_instances = process_one_image(
-            args,
-            args.input,
-            detector,
-            pose_estimator,
-            visualizer,
-            show_interval=0)
-        pred_instances_list = split_instances(pred_instances)
-
-    elif input_type == 'video':
-        tmp_folder = tempfile.TemporaryDirectory()
-        video = mmcv.VideoReader(args.input)
-        progressbar = mmengine.ProgressBar(len(video))
-        video.cvt2frames(tmp_folder.name, show_progress=False)
-        output_root = args.output_root
-        args.output_root = tmp_folder.name
+
+        # inference
+        pred_instances = process_one_image(args, args.input, detector,
+                                           pose_estimator, visualizer)
+
+        if args.save_predictions:
+            pred_instances_list = split_instances(pred_instances)
+
+        if output_file:
+            img_vis = visualizer.get_image()
+            mmcv.imwrite(mmcv.rgb2bgr(img_vis), output_file)
+
+    elif input_type in ['webcam', 'video']:
+
+        if args.input == 'webcam':
+            cap = cv2.VideoCapture(0)
+        else:
+            cap = cv2.VideoCapture(args.input)
+
+        video_writer = None
         pred_instances_list = []
+        frame_idx = 0
+
+        while cap.isOpened():
+            success, frame = cap.read()
+            frame_idx += 1
+
+            if not success:
+                break
+
+            # topdown pose estimation
+            pred_instances = process_one_image(args, frame, detector,
+                                               pose_estimator, visualizer,
+                                               0.001)
+
+            if args.save_predictions:
+                # save prediction results
+                pred_instances_list.append(
+                    dict(
+                        frame_id=frame_idx,
+                        instances=split_instances(pred_instances)))
+
+            # output videos
+            if output_file:
+                frame_vis = visualizer.get_image()
+
+                if video_writer is None:
+                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
+                    # the size of the image with visualization may vary
+                    # depending on the presence of heatmaps
+                    video_writer = cv2.VideoWriter(
+                        output_file,
+                        fourcc,
+                        25,  # saved fps
+                        (frame_vis.shape[1], frame_vis.shape[0]))
+
+                video_writer.write(mmcv.rgb2bgr(frame_vis))
+
+            # press ESC to exit
+            if cv2.waitKey(5) & 0xFF == 27:
+                break
+
+            time.sleep(args.show_interval)
+
+        if video_writer:
+            video_writer.release()
 
-        for frame_id, img_fname in enumerate(os.listdir(tmp_folder.name)):
-            pred_instances = process_one_image(
-                args,
-                f'{tmp_folder.name}/{img_fname}',
-                detector,
-                pose_estimator,
-                visualizer,
-                show_interval=1)
-
-            progressbar.update()
-            pred_instances_list.append(
-                dict(
-                    frame_id=frame_id,
-                    instances=split_instances(pred_instances)))
-
-        if output_root:
-            mmcv.frames2video(
-                tmp_folder.name,
-                f'{output_root}/{os.path.basename(args.input)}',
-                fps=video.fps,
-                fourcc='mp4v',
-                show_progress=False)
-        tmp_folder.cleanup()
+        cap.release()
 
     else:
         args.save_predictions = False
         raise ValueError(
             f'file {os.path.basename(args.input)} has invalid format.')
 
     if args.save_predictions:
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/analysis_tools/analyze_logs.py` & `mmpose-1.1.0/mmpose/.mim/tools/analysis_tools/analyze_logs.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/analysis_tools/print_config.py` & `mmpose-1.1.0/mmpose/.mim/tools/analysis_tools/print_config.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/h36m_to_coco.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/h36m_to_coco.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/mat2json.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/mat2json.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_animalpose_dataset.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_animalpose_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_cofw_dataset.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_cofw_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_deepposekit_dataset.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_deepposekit_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/parse_macaquepose_dataset.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/parse_macaquepose_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/preprocess_h36m.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/preprocess_h36m.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dataset_converters/preprocess_mpi_inf_3dhp.py` & `mmpose-1.1.0/mmpose/.mim/tools/dataset_converters/preprocess_mpi_inf_3dhp.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/dist_test.sh` & `mmpose-1.1.0/mmpose/.mim/tools/dist_test.sh`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/misc/browse_dataset.py` & `mmpose-1.1.0/mmpose/.mim/tools/misc/browse_dataset.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import argparse
 import os
 import os.path as osp
 
 import mmcv
 import mmengine
+import mmengine.fileio as fileio
 import numpy as np
 from mmengine import Config, DictAction
 from mmengine.registry import build_from_cfg, init_default_scope
 from mmengine.structures import InstanceData
 
 from mmpose.registry import DATASETS, VISUALIZERS
 from mmpose.structures import PoseDataSample
@@ -75,19 +76,20 @@
 
 
 def main():
     args = parse_args()
     cfg = Config.fromfile(args.config)
     if args.cfg_options is not None:
         cfg.merge_from_dict(args.cfg_options)
-    file_client_args = cfg.get('file_client_args', dict(backend='disk'))
-    file_client = mmengine.FileClient(**file_client_args)
+    backend_args = cfg.get('backend_args', dict(backend='local'))
 
     # register all modules in mmpose into the registries
-    init_default_scope(cfg.get('default_scope', 'mmpose'))
+    scope = cfg.get('default_scope', 'mmpose')
+    if scope is not None:
+        init_default_scope(scope)
 
     if args.mode == 'original':
         cfg[f'{args.phase}_dataloader'].dataset.pipeline = []
     else:
         # pack transformed keypoints for visualization
         cfg[f'{args.phase}_dataloader'].dataset.pipeline[
             -1].pack_transformed = True
@@ -117,15 +119,15 @@
                      next_item['keypoints_visible']))
                 item['bbox'] = np.concatenate(
                     (item['bbox'], next_item['bbox']))
                 progress_bar.update()
                 continue
             else:
                 img_path = item['img_path']
-                img_bytes = file_client.get(img_path)
+                img_bytes = fileio.get(img_path, backend_args=backend_args)
                 img = mmcv.imfrombytes(img_bytes, channel_order='bgr')
 
                 # forge pseudo data_sample
                 gt_instances = InstanceData()
                 gt_instances.keypoints = item['keypoints']
                 gt_instances.keypoints_visible = item['keypoints_visible']
                 gt_instances.bboxes = item['bbox']
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/misc/keypoints2coco_without_mmdet.py` & `mmpose-1.1.0/mmpose/.mim/tools/misc/keypoints2coco_without_mmdet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/misc/publish_model.py` & `mmpose-1.1.0/mmpose/.mim/tools/misc/publish_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -37,15 +37,15 @@
                 f'please set --save-keys.',
                 logger='current')
             checkpoint.pop(k, None)
 
     # if it is necessary to remove some sensitive data in checkpoint['meta'],
     # add the code here.
 
-    if digit_version(TORCH_VERSION) >= digit_version('1.6.0'):
+    if digit_version(TORCH_VERSION) >= digit_version('1.8.0'):
         torch.save(checkpoint, out_file, _use_new_zipfile_serialization=False)
     else:
         torch.save(checkpoint, out_file)
     sha = subprocess.check_output(['sha256sum', out_file]).decode()
     if out_file.endswith('.pth'):
         out_file_name = out_file[:-4]
     else:
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/slurm_test.sh` & `mmpose-1.1.0/mmpose/.mim/tools/slurm_test.sh`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/slurm_train.sh` & `mmpose-1.1.0/mmpose/.mim/tools/slurm_train.sh`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/test.py` & `mmpose-1.1.0/mmpose/.mim/tools/test.py`

 * *Files 6% similar despite different names*

```diff
@@ -56,14 +56,28 @@
     if 'LOCAL_RANK' not in os.environ:
         os.environ['LOCAL_RANK'] = str(args.local_rank)
     return args
 
 
 def merge_args(cfg, args):
     """Merge CLI arguments to config."""
+
+    cfg.launcher = args.launcher
+    cfg.load_from = args.checkpoint
+
+    # -------------------- work directory --------------------
+    # work_dir is determined in this priority: CLI > segment in file > filename
+    if args.work_dir is not None:
+        # update configs according to CLI args if args.work_dir is not None
+        cfg.work_dir = args.work_dir
+    elif cfg.get('work_dir', None) is None:
+        # use config filename as default work_dir if cfg.work_dir is None
+        cfg.work_dir = osp.join('./work_dirs',
+                                osp.splitext(osp.basename(args.config))[0])
+
     # -------------------- visualization --------------------
     if args.show or (args.show_dir is not None):
         assert 'visualization' in cfg.default_hooks, \
             'PoseVisualizationHook is not set in the ' \
             '`default_hooks` field of config. Please set ' \
             '`visualization=dict(type="PoseVisualizationHook")`'
 
@@ -76,41 +90,31 @@
 
     # -------------------- Dump predictions --------------------
     if args.dump is not None:
         assert args.dump.endswith(('.pkl', '.pickle')), \
             'The dump file must be a pkl file.'
         dump_metric = dict(type='DumpResults', out_file_path=args.dump)
         if isinstance(cfg.test_evaluator, (list, tuple)):
-            cfg.test_evaluator = list(cfg.test_evaluator).append(dump_metric)
+            cfg.test_evaluator = [*cfg.test_evaluator, dump_metric]
         else:
             cfg.test_evaluator = [cfg.test_evaluator, dump_metric]
 
+    # -------------------- Other arguments --------------------
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+
     return cfg
 
 
 def main():
     args = parse_args()
 
     # load config
     cfg = Config.fromfile(args.config)
     cfg = merge_args(cfg, args)
-    cfg.launcher = args.launcher
-    if args.cfg_options is not None:
-        cfg.merge_from_dict(args.cfg_options)
-
-    # work_dir is determined in this priority: CLI > segment in file > filename
-    if args.work_dir is not None:
-        # update configs according to CLI args if args.work_dir is not None
-        cfg.work_dir = args.work_dir
-    elif cfg.get('work_dir', None) is None:
-        # use config filename as default work_dir if cfg.work_dir is None
-        cfg.work_dir = osp.join('./work_dirs',
-                                osp.splitext(osp.basename(args.config))[0])
-
-    cfg.load_from = args.checkpoint
 
     # build the runner from config
     runner = Runner.from_cfg(cfg)
 
     if args.out:
 
         class SaveMetricHook(Hook):
```

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/torchserve/mmpose2torchserve.py` & `mmpose-1.1.0/mmpose/.mim/tools/torchserve/mmpose2torchserve.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/torchserve/mmpose_handler.py` & `mmpose-1.1.0/mmpose/.mim/tools/torchserve/mmpose_handler.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/torchserve/test_torchserver.py` & `mmpose-1.1.0/mmpose/.mim/tools/torchserve/test_torchserver.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/.mim/tools/train.py` & `mmpose-1.1.0/mmpose/.mim/tools/train.py`

 * *Files 8% similar despite different names*

```diff
@@ -61,15 +61,18 @@
         'Note that the quotation marks are necessary and that no white space '
         'is allowed.')
     parser.add_argument(
         '--launcher',
         choices=['none', 'pytorch', 'slurm', 'mpi'],
         default='none',
         help='job launcher')
-    parser.add_argument('--local_rank', type=int, default=0)
+    # When using PyTorch version >= 2.0.0, the `torch.distributed.launch`
+    # will pass the `--local-rank` parameter to `tools/train.py` instead
+    # of `--local_rank`.
+    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)
     args = parser.parse_args()
     if 'LOCAL_RANK' not in os.environ:
         os.environ['LOCAL_RANK'] = str(args.local_rank)
 
     return args
 
 
@@ -89,16 +92,17 @@
     elif cfg.get('work_dir', None) is None:
         # use config filename as default work_dir if cfg.work_dir is None
         cfg.work_dir = osp.join('./work_dirs',
                                 osp.splitext(osp.basename(args.config))[0])
 
     # enable automatic-mixed-precision training
     if args.amp is True:
-        optim_wrapper = cfg.optim_wrapper.get('type', 'OptimWrapper')
-        assert optim_wrapper in ['OptimWrapper', 'AmpOptimWrapper'], \
+        from mmengine.optim import AmpOptimWrapper, OptimWrapper
+        optim_wrapper = cfg.optim_wrapper.get('type', OptimWrapper)
+        assert optim_wrapper in (OptimWrapper, AmpOptimWrapper), \
             '`--amp` is not supported custom optimizer wrapper type ' \
             f'`{optim_wrapper}.'
         cfg.optim_wrapper.type = 'AmpOptimWrapper'
         cfg.optim_wrapper.setdefault('loss_scale', 'dynamic')
 
     # resume training
     if args.resume == 'auto':
@@ -108,15 +112,15 @@
         cfg.resume = True
         cfg.load_from = args.resume
 
     # enable auto scale learning rate
     if args.auto_scale_lr:
         cfg.auto_scale_lr.enable = True
 
-    # visualization-
+    # visualization
     if args.show or (args.show_dir is not None):
         assert 'visualization' in cfg.default_hooks, \
             'PoseVisualizationHook is not set in the ' \
             '`default_hooks` field of config. Please set ' \
             '`visualization=dict(type="PoseVisualizationHook")`'
 
         cfg.default_hooks.visualization.enable = True
```

### Comparing `mmpose-1.0.0rc1/mmpose/__init__.py` & `mmpose-1.1.0/mmpose/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/apis/inference.py` & `mmpose-1.1.0/mmpose/apis/inference.py`

 * *Files 16% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 from typing import List, Optional, Union
 
 import numpy as np
 import torch
 import torch.nn as nn
 from mmengine.config import Config
 from mmengine.dataset import Compose, pseudo_collate
+from mmengine.model.utils import revert_sync_batchnorm
 from mmengine.registry import init_default_scope
 from mmengine.runner import load_checkpoint
 from PIL import Image
 
 from mmpose.datasets.datasets.utils import parse_pose_metainfo
 from mmpose.models.builder import build_pose_estimator
 from mmpose.structures import PoseDataSample
@@ -91,17 +92,20 @@
     if cfg_options is not None:
         config.merge_from_dict(cfg_options)
     elif 'init_cfg' in config.model.backbone:
         config.model.backbone.init_cfg = None
     config.model.train_cfg = None
 
     # register all modules in mmpose into the registries
-    init_default_scope(config.get('default_scope', 'mmpose'))
+    scope = config.get('default_scope', 'mmpose')
+    if scope is not None:
+        init_default_scope(scope)
 
     model = build_pose_estimator(config.model)
+    model = revert_sync_batchnorm(model)
     # get dataset_meta in this priority: checkpoint > config > default (COCO)
     dataset_meta = None
 
     if checkpoint is not None:
         ckpt = load_checkpoint(model, checkpoint, map_location='cpu')
 
         if 'dataset_meta' in ckpt.get('meta', {}):
@@ -143,18 +147,20 @@
 
     Returns:
         List[:obj:`PoseDataSample`]: The inference results. Specifically, the
         predicted keypoints and scores are saved at
         ``data_sample.pred_instances.keypoints`` and
         ``data_sample.pred_instances.keypoint_scores``.
     """
-    init_default_scope(model.cfg.get('default_scope', 'mmpose'))
+    scope = model.cfg.get('default_scope', 'mmpose')
+    if scope is not None:
+        init_default_scope(scope)
     pipeline = Compose(model.cfg.test_dataloader.dataset.pipeline)
 
-    if bboxes is None:
+    if bboxes is None or len(bboxes) == 0:
         # get bbox from the image size
         if isinstance(img, str):
             w, h = Image.open(img).size
         else:
             h, w = img.shape[:2]
 
         bboxes = np.array([[0, 0, w, h]], dtype=np.float32)
@@ -217,7 +223,40 @@
     data = pipeline(data_info)
     batch = pseudo_collate([data])
 
     with torch.no_grad():
         results = model.test_step(batch)
 
     return results
+
+
+def collect_multi_frames(video, frame_id, indices, online=False):
+    """Collect multi frames from the video.
+
+    Args:
+        video (mmcv.VideoReader): A VideoReader of the input video file.
+        frame_id (int): index of the current frame
+        indices (list(int)): index offsets of the frames to collect
+        online (bool): inference mode, if set to True, can not use future
+            frame information.
+
+    Returns:
+        list(ndarray): multi frames collected from the input video file.
+    """
+    num_frames = len(video)
+    frames = []
+    # put the current frame at first
+    frames.append(video[frame_id])
+    # use multi frames for inference
+    for idx in indices:
+        # skip current frame
+        if idx == 0:
+            continue
+        support_idx = frame_id + idx
+        # online mode, can not use future frame information
+        if online:
+            support_idx = np.clip(support_idx, 0, frame_id)
+        else:
+            support_idx = np.clip(support_idx, 0, num_frames - 1)
+        frames.append(video[support_idx])
+
+    return frames
```

### Comparing `mmpose-1.0.0rc1/mmpose/apis/inferencers/base_mmpose_inferencer.py` & `mmpose-1.1.0/mmpose/apis/inferencers/base_mmpose_inferencer.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import mimetypes
 import os
-import shutil
-import tempfile
 import warnings
 from collections import defaultdict
-from typing import (Any, Callable, Dict, Generator, List, Optional, Sequence,
-                    Union)
+from typing import (Callable, Dict, Generator, Iterable, List, Optional,
+                    Sequence, Union)
 
 import cv2
 import mmcv
 import mmengine
 import numpy as np
 import torch.nn as nn
 from mmengine.config import Config, ConfigDict
 from mmengine.dataset import Compose
 from mmengine.fileio import (get_file_backend, isdir, join_path,
                              list_dir_or_file)
 from mmengine.infer.infer import BaseInferencer
+from mmengine.registry import init_default_scope
 from mmengine.runner.checkpoint import _load_checkpoint_to_model
 from mmengine.structures import InstanceData
+from mmengine.utils import mkdir_or_exist
 
 from mmpose.apis.inference import dataset_meta_from_config
 from mmpose.structures import PoseDataSample, split_instances
 
 InstanceList = List[InstanceData]
 InputType = Union[str, np.ndarray]
 InputsType = Union[InputType, Sequence[InputType]]
@@ -32,24 +32,19 @@
 ConfigType = Union[Config, ConfigDict]
 ResType = Union[Dict, List[Dict], InstanceData, List[InstanceData]]
 
 
 class BaseMMPoseInferencer(BaseInferencer):
     """The base class for MMPose inferencers."""
 
-    preprocess_kwargs: set = {'bbox_thr', 'nms_thr'}
+    preprocess_kwargs: set = {'bbox_thr', 'nms_thr', 'bboxes'}
     forward_kwargs: set = set()
     visualize_kwargs: set = {
-        'return_vis',
-        'show',
-        'wait_time',
-        'radius',
-        'thickness',
-        'kpt_thr',
-        'vis_out_dir',
+        'return_vis', 'show', 'wait_time', 'draw_bbox', 'radius', 'thickness',
+        'kpt_thr', 'vis_out_dir', 'black_background'
     }
     postprocess_kwargs: set = {'pred_out_dir'}
 
     def _load_weights_to_model(self, model: nn.Module,
                                checkpoint: Optional[dict],
                                cfg: Optional[ConfigType]) -> None:
         """Loading model weights and meta information from cfg and checkpoint.
@@ -78,15 +73,15 @@
         else:
             warnings.warn('Checkpoint is not loaded, and the inference '
                           'result is calculated by the randomly initialized '
                           'model!')
             model.dataset_meta = dataset_meta_from_config(
                 cfg, dataset_mode='train')
 
-    def _inputs_to_list(self, inputs: InputsType) -> list:
+    def _inputs_to_list(self, inputs: InputsType) -> Iterable:
         """Preprocess the inputs to a list.
 
         Preprocess inputs to a list according to its type:
 
         - list or tuple: return inputs
         - str:
             - Directory path: return all files in the directory
@@ -121,29 +116,34 @@
                 inputs.sort()
             else:
                 # if inputs is a path to a video file, it will be converted
                 # to a list containing separated frame filenames
                 input_type = mimetypes.guess_type(inputs)[0].split('/')[0]
                 if input_type == 'video':
                     self._video_input = True
-                    # split video frames into a temporary folder
-                    frame_folder = tempfile.TemporaryDirectory()
                     video = mmcv.VideoReader(inputs)
                     self.video_info = dict(
                         fps=video.fps,
                         name=os.path.basename(inputs),
-                        frame_folder=frame_folder)
-                    video.cvt2frames(frame_folder.name, show_progress=False)
-                    frames = sorted(list_dir_or_file(frame_folder.name))
-                    inputs = [join_path(frame_folder.name, f) for f in frames]
+                        writer=None,
+                        width=video.width,
+                        height=video.height,
+                        predictions=[])
+                    inputs = video
+                elif input_type == 'image':
+                    inputs = [inputs]
+                else:
+                    raise ValueError(f'Expected input to be an image, video, '
+                                     f'or folder, but received {inputs} of '
+                                     f'type {input_type}.')
 
-        if not isinstance(inputs, (list, tuple)):
+        elif isinstance(inputs, np.ndarray):
             inputs = [inputs]
 
-        return list(inputs)
+        return inputs
 
     def _get_webcam_inputs(self, inputs: str) -> Generator:
         """Sets up and returns a generator function that reads frames from a
         webcam input. The generator function returns a new frame each time it
         is iterated over.
 
         Args:
@@ -177,97 +177,125 @@
         vcap = cv2.VideoCapture(camera_id)
         if not vcap.isOpened():
             warnings.warn(f'Cannot open camera (ID={camera_id})')
             return []
 
         # Set video input flag and metadata.
         self._video_input = True
-        self.video_info = dict(fps=10, name='webcam.mp4', frame_folder=None)
-
-        # Set up webcam reader generator function.
-        self._window_closing = False
+        (major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')
+        if int(major_ver) < 3:
+            fps = vcap.get(cv2.cv.CV_CAP_PROP_FPS)
+            width = vcap.get(cv2.cv.CV_CAP_PROP_FRAME_WIDTH)
+            height = vcap.get(cv2.cv.CV_CAP_PROP_FRAME_HEIGHT)
+        else:
+            fps = vcap.get(cv2.CAP_PROP_FPS)
+            width = vcap.get(cv2.CAP_PROP_FRAME_WIDTH)
+            height = vcap.get(cv2.CAP_PROP_FRAME_HEIGHT)
+        self.video_info = dict(
+            fps=fps,
+            name='webcam.mp4',
+            writer=None,
+            width=width,
+            height=height,
+            predictions=[])
 
         def _webcam_reader() -> Generator:
             while True:
-                if self._window_closing:
+                if cv2.waitKey(5) & 0xFF == 27:
                     vcap.release()
                     break
 
                 ret_val, frame = vcap.read()
                 if not ret_val:
                     break
 
                 yield frame
 
         return _webcam_reader()
 
-    def _visualization_window_on_close(self, event):
-        self._window_closing = True
-
     def _init_pipeline(self, cfg: ConfigType) -> Callable:
         """Initialize the test pipeline.
 
         Args:
             cfg (ConfigType): model config path or dict
 
         Returns:
             A pipeline to handle various input data, such as ``str``,
             ``np.ndarray``. The returned pipeline will be used to process
             a single data.
         """
+        scope = cfg.get('default_scope', 'mmpose')
+        if scope is not None:
+            init_default_scope(scope)
         return Compose(cfg.test_dataloader.dataset.pipeline)
 
-    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):
+    def update_model_visualizer_settings(self, **kwargs):
+        """Update the settings of models and visualizer according to inference
+        arguments."""
+
+        pass
+
+    def preprocess(self,
+                   inputs: InputsType,
+                   batch_size: int = 1,
+                   bboxes: Optional[List] = None,
+                   **kwargs):
         """Process the inputs into a model-feedable format.
 
         Args:
             inputs (InputsType): Inputs given by user.
             batch_size (int): batch size. Defaults to 1.
 
         Yields:
             Any: Data processed by the ``pipeline`` and ``collate_fn``.
             List[str or np.ndarray]: List of original inputs in the batch
         """
 
         for i, input in enumerate(inputs):
-            data_infos = self.preprocess_single(input, index=i, **kwargs)
+            bbox = bboxes[i] if bboxes else []
+            data_infos = self.preprocess_single(
+                input, index=i, bboxes=bbox, **kwargs)
             # only supports inference with batch size 1
             yield self.collate_fn(data_infos), [input]
 
     def visualize(self,
                   inputs: list,
                   preds: List[PoseDataSample],
                   return_vis: bool = False,
                   show: bool = False,
+                  draw_bbox: bool = False,
                   wait_time: float = 0,
                   radius: int = 3,
                   thickness: int = 1,
                   kpt_thr: float = 0.3,
                   vis_out_dir: str = '',
                   window_name: str = '',
-                  window_close_event_handler: Optional[Callable] = None
-                  ) -> List[np.ndarray]:
+                  black_background: bool = False,
+                  **kwargs) -> List[np.ndarray]:
         """Visualize predictions.
 
         Args:
             inputs (list): Inputs preprocessed by :meth:`_inputs_to_list`.
             preds (Any): Predictions of the model.
             return_vis (bool): Whether to return images with predicted results.
             show (bool): Whether to display the image in a popup window.
                 Defaults to False.
             wait_time (float): The interval of show (ms). Defaults to 0
+            draw_bbox (bool): Whether to draw the bounding boxes.
+                Defaults to False
             radius (int): Keypoint radius for visualization. Defaults to 3
             thickness (int): Link thickness for visualization. Defaults to 1
             kpt_thr (float): The threshold to visualize the keypoints.
                 Defaults to 0.3
             vis_out_dir (str, optional): Directory to save visualization
                 results w/o predictions. If left as empty, no file will
                 be saved. Defaults to ''.
             window_name (str, optional): Title of display window.
-            window_close_event_handler (callable, optional):
+            black_background (bool, optional): Whether to plot keypoints on a
+                black image instead of the input image. Defaults to False.
 
         Returns:
             List[np.ndarray]: Visualization results.
         """
         if (not return_vis) and (not show) and (not vis_out_dir):
             return
 
@@ -280,56 +308,69 @@
 
         results = []
 
         for single_input, pred in zip(inputs, preds):
             if isinstance(single_input, str):
                 img = mmcv.imread(single_input, channel_order='rgb')
             elif isinstance(single_input, np.ndarray):
-                img = mmcv.bgr2rgb(single_input.copy())
+                img = mmcv.bgr2rgb(single_input)
             else:
                 raise ValueError('Unsupported input type: '
                                  f'{type(single_input)}')
+            if black_background:
+                img = img * 0
 
             img_name = os.path.basename(pred.metainfo['img_path'])
-
-            if vis_out_dir:
-                if self._video_input:
-                    out_file = join_path(vis_out_dir, 'vis_frames', img_name)
-                else:
-                    out_file = join_path(vis_out_dir, img_name)
-            else:
-                out_file = None
+            window_name = window_name if window_name else img_name
 
             # since visualization and inference utilize the same process,
             # the wait time is reduced when a video input is utilized,
             # thereby eliminating the issue of inference getting stuck.
             wait_time = 1e-5 if self._video_input else wait_time
 
-            window_name = window_name if window_name else img_name
-
             visualization = self.visualizer.add_datasample(
                 window_name,
                 img,
                 pred,
                 draw_gt=False,
+                draw_bbox=draw_bbox,
                 show=show,
                 wait_time=wait_time,
-                out_file=out_file,
-                kpt_score_thr=kpt_thr)
+                kpt_thr=kpt_thr,
+                **kwargs)
             results.append(visualization)
 
-            if show and not hasattr(self, '_window_close_cid'):
-                if window_close_event_handler is None:
-                    window_close_event_handler = \
-                        self._visualization_window_on_close
-                self._window_close_cid = \
-                    self.visualizer.manager.canvas.mpl_connect(
-                        'close_event',
-                        window_close_event_handler
-                    )
+            if vis_out_dir:
+                out_img = mmcv.rgb2bgr(visualization)
+                _, file_extension = os.path.splitext(vis_out_dir)
+                if file_extension:
+                    dir_name = os.path.dirname(vis_out_dir)
+                    file_name = os.path.basename(vis_out_dir)
+                else:
+                    dir_name = vis_out_dir
+                    file_name = None
+                mkdir_or_exist(dir_name)
+
+                if self._video_input:
+
+                    if self.video_info['writer'] is None:
+                        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
+                        if file_name is None:
+                            file_name = os.path.basename(
+                                self.video_info['name'])
+                        out_file = join_path(dir_name, file_name)
+                        self.video_info['writer'] = cv2.VideoWriter(
+                            out_file, fourcc, self.video_info['fps'],
+                            (visualization.shape[1], visualization.shape[0]))
+                    self.video_info['writer'].write(out_img)
+
+                else:
+                    file_name = file_name if file_name else img_name
+                    out_file = join_path(dir_name, file_name)
+                    mmcv.imwrite(out_img, out_file)
 
         if return_vis:
             return results
         else:
             return []
 
     def postprocess(
@@ -375,70 +416,54 @@
         for pred in preds:
             if not return_datasample:
                 # convert datasamples to list of instance predictions
                 pred = split_instances(pred.pred_instances)
             result_dict['predictions'].append(pred)
 
         if pred_out_dir != '':
-            if self._video_input:
-                pred_out_dir = join_path(pred_out_dir, 'pred_frames')
-
             for pred, data_sample in zip(result_dict['predictions'], preds):
-                fname = os.path.splitext(
-                    os.path.basename(
-                        data_sample.metainfo['img_path']))[0] + '.json'
-                mmengine.dump(
-                    pred, join_path(pred_out_dir, fname), indent='  ')
+                if self._video_input:
+                    # For video or webcam input, predictions for each frame
+                    # are gathered in the 'predictions' key of 'video_info'
+                    # dictionary. All frame predictions are then stored into
+                    # a single file after processing all frames.
+                    self.video_info['predictions'].append(pred)
+                else:
+                    # For non-video inputs, predictions are stored in separate
+                    # JSON files. The filename is determined by the basename
+                    # of the input image path with a '.json' extension. The
+                    # predictions are then dumped into this file.
+                    fname = os.path.splitext(
+                        os.path.basename(
+                            data_sample.metainfo['img_path']))[0] + '.json'
+                    mmengine.dump(
+                        pred, join_path(pred_out_dir, fname), indent='  ')
 
         return result_dict
 
-    def _merge_outputs(self, vis_out_dir: str, pred_out_dir: str,
-                       **kwargs: Dict[str, Any]) -> None:
-        """Merge the visualized frames and predicted instance outputs and save
-        them.
-
-        Args:
-            vis_out_dir (str): Path to the directory where the visualized
-                frames are saved.
-            pred_out_dir (str): Path to the directory where the predicted
-                instance outputs are saved.
-            **kwargs: Other arguments that are not used in this method.
+    def _finalize_video_processing(
+        self,
+        pred_out_dir: str = '',
+    ):
+        """Finalize video processing by releasing the video writer and saving
+        predictions to a file.
+
+        This method should be called after completing the video processing. It
+        releases the video writer, if it exists, and saves the predictions to a
+        JSON file if a prediction output directory is provided.
         """
-        assert self._video_input
 
-        if vis_out_dir != '':
-            vis_frame_out_dir = join_path(vis_out_dir, 'vis_frames')
-            if not isdir(vis_frame_out_dir) or len(
-                    os.listdir(vis_frame_out_dir)) == 0:
-                warnings.warn(
-                    f'{vis_frame_out_dir} does not exist or is empty.')
-            else:
-                mmcv.frames2video(
-                    vis_frame_out_dir,
-                    join_path(vis_out_dir, self.video_info['name']),
-                    fps=self.video_info['fps'],
-                    fourcc='mp4v',
-                    show_progress=False)
-                shutil.rmtree(vis_frame_out_dir)
+        # Release the video writer if it exists
+        if self.video_info['writer'] is not None:
+            self.video_info['writer'].release()
+
+        # Save predictions
+        if pred_out_dir:
+            fname = os.path.splitext(
+                os.path.basename(self.video_info['name']))[0] + '.json'
+            predictions = [
+                dict(frame_id=i, instances=pred)
+                for i, pred in enumerate(self.video_info['predictions'])
+            ]
 
-        if pred_out_dir != '':
-            pred_frame_out_dir = join_path(pred_out_dir, 'pred_frames')
-            if not isdir(pred_frame_out_dir) or len(
-                    os.listdir(pred_frame_out_dir)) == 0:
-                warnings.warn(
-                    f'{pred_frame_out_dir} does not exist or is empty.')
-            else:
-                predictions = []
-                pred_files = list_dir_or_file(pred_frame_out_dir)
-                for frame_id, pred_file in enumerate(sorted(pred_files)):
-                    predictions.append({
-                        'frame_id':
-                        frame_id,
-                        'instances':
-                        mmengine.load(
-                            join_path(pred_frame_out_dir, pred_file))
-                    })
-                fname = os.path.splitext(
-                    os.path.basename(self.video_info['name']))[0] + '.json'
-                mmengine.dump(
-                    predictions, join_path(pred_out_dir, fname), indent='  ')
-                shutil.rmtree(pred_frame_out_dir)
+            mmengine.dump(
+                predictions, join_path(pred_out_dir, fname), indent='  ')
```

### Comparing `mmpose-1.0.0rc1/mmpose/apis/inferencers/pose2d_inferencer.py` & `mmpose-1.1.0/mmpose/apis/inferencers/mmpose_inferencer.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,193 +1,131 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import warnings
-from typing import Dict, List, Optional, Sequence, Tuple, Union
+from typing import Dict, List, Optional, Sequence, Union
 
-import mmcv
 import numpy as np
+import torch
 from mmengine.config import Config, ConfigDict
 from mmengine.infer.infer import ModelType
-from mmengine.registry import init_default_scope
 from mmengine.structures import InstanceData
-from rich.progress import track
 
-from mmpose.evaluation.functional import nms
-from mmpose.registry import DATASETS, INFERENCERS
-from mmpose.structures import merge_data_samples
 from .base_mmpose_inferencer import BaseMMPoseInferencer
-from .utils import default_det_models
-
-try:
-    from mmdet.apis.det_inferencer import DetInferencer
-    mmdet_available = True
-except (ImportError, ModuleNotFoundError):
-    mmdet_available = False
+from .pose2d_inferencer import Pose2DInferencer
+from .pose3d_inferencer import Pose3DInferencer
 
 InstanceList = List[InstanceData]
 InputType = Union[str, np.ndarray]
 InputsType = Union[InputType, Sequence[InputType]]
 PredType = Union[InstanceData, InstanceList]
 ImgType = Union[np.ndarray, Sequence[np.ndarray]]
 ConfigType = Union[Config, ConfigDict]
 ResType = Union[Dict, List[Dict], InstanceData, List[InstanceData]]
 
 
-@INFERENCERS.register_module(name='pose-estimation')
-@INFERENCERS.register_module()
-class Pose2DInferencer(BaseMMPoseInferencer):
-    """The inferencer for 2D pose estimation.
+class MMPoseInferencer(BaseMMPoseInferencer):
+    """MMPose Inferencer. It's a unified inferencer interface for pose
+    estimation task, currently including: Pose2D. and it can be used to perform
+    2D keypoint detection.
 
     Args:
-        model (str, optional): Pretrained 2D pose estimation algorithm.
+        pose2d (str, optional): Pretrained 2D pose estimation algorithm.
             It's the path to the config file or the model name defined in
             metafile. For example, it could be:
 
             - model alias, e.g. ``'body'``,
             - config name, e.g. ``'simcc_res50_8xb64-210e_coco-256x192'``,
             - config path
 
             Defaults to ``None``.
-        weights (str, optional): Path to the checkpoint. If it is not
-            specified and "model" is a model name of metafile, the weights
-            will be loaded from metafile. Defaults to None.
+        pose2d_weights (str, optional): Path to the custom checkpoint file of
+            the selected pose2d model. If it is not specified and "pose2d" is
+            a model name of metafile, the weights will be loaded from
+            metafile. Defaults to None.
         device (str, optional): Device to run inference. If None, the
             available device will be automatically used. Defaults to None.
         scope (str, optional): The scope of the model. Defaults to "mmpose".
         det_model(str, optional): Config path or alias of detection model.
             Defaults to None.
         det_weights(str, optional): Path to the checkpoints of detection
             model. Defaults to None.
         det_cat_ids(int or list[int], optional): Category id for
             detection model. Defaults to None.
+        output_heatmaps (bool, optional): Flag to visualize predicted
+            heatmaps. If set to None, the default setting from the model
+            config will be used. Default is None.
     """
 
-    preprocess_kwargs: set = {'bbox_thr', 'nms_thr'}
-    forward_kwargs: set = set()
+    preprocess_kwargs: set = {
+        'bbox_thr', 'nms_thr', 'bboxes', 'use_oks_tracking', 'tracking_thr',
+        'norm_pose_2d'
+    }
+    forward_kwargs: set = {'rebase_keypoint_height'}
     visualize_kwargs: set = {
-        'return_vis',
-        'show',
-        'wait_time',
-        'radius',
-        'thickness',
-        'kpt_thr',
-        'vis_out_dir',
+        'return_vis', 'show', 'wait_time', 'draw_bbox', 'radius', 'thickness',
+        'kpt_thr', 'vis_out_dir', 'skeleton_style', 'draw_heatmap',
+        'black_background'
     }
     postprocess_kwargs: set = {'pred_out_dir'}
 
     def __init__(self,
-                 model: Union[ModelType, str],
-                 weights: Optional[str] = None,
+                 pose2d: Optional[str] = None,
+                 pose2d_weights: Optional[str] = None,
+                 pose3d: Optional[str] = None,
+                 pose3d_weights: Optional[str] = None,
                  device: Optional[str] = None,
-                 scope: Optional[str] = 'mmpose',
+                 scope: str = 'mmpose',
                  det_model: Optional[Union[ModelType, str]] = None,
                  det_weights: Optional[str] = None,
-                 det_cat_ids: Optional[Union[int, Tuple]] = None) -> None:
+                 det_cat_ids: Optional[Union[int, List]] = None) -> None:
 
-        init_default_scope(scope)
-        super().__init__(
-            model=model, weights=weights, device=device, scope=scope)
-
-        # assign dataset metainfo to self.visualizer
-        self.visualizer.set_dataset_meta(self.model.dataset_meta)
-
-        # initialize detector for top-down models
-        if self.cfg.data_mode == 'topdown':
-            if det_model is None:
-                det_model = DATASETS.get(
-                    self.cfg.dataset_type).__module__.split(
-                        'datasets.')[-1].split('.')[0].lower()
-                det_info = default_det_models[det_model]
-                det_model, det_weights, det_cat_ids = det_info[
-                    'model'], det_info['weights'], det_info['cat_ids']
-
-            if mmdet_available:
-                self.detector = DetInferencer(
-                    det_model, det_weights, device=device)
-            else:
-                raise RuntimeError(
-                    'MMDetection (v3.0.0rc6 or above) is required to '
-                    'build inferencers for top-down pose estimation models.')
-
-            if isinstance(det_cat_ids, (tuple, list)):
-                self.det_cat_ids = det_cat_ids
-            else:
-                self.det_cat_ids = (det_cat_ids, )
-
-        self._video_input = False
-
-    def preprocess_single(self,
-                          input: InputType,
-                          index: int,
-                          bbox_thr: float = 0.3,
-                          nms_thr: float = 0.3):
-        """Process a single input into a model-feedable format.
+        self.visualizer = None
+        if pose3d is not None:
+            self.inferencer = Pose3DInferencer(pose3d, pose3d_weights, pose2d,
+                                               pose2d_weights, device, scope,
+                                               det_model, det_weights,
+                                               det_cat_ids)
+        elif pose2d is not None:
+            self.inferencer = Pose2DInferencer(pose2d, pose2d_weights, device,
+                                               scope, det_model, det_weights,
+                                               det_cat_ids)
+        else:
+            raise ValueError('Either 2d or 3d pose estimation algorithm '
+                             'should be provided.')
+
+    def preprocess(self, inputs: InputsType, batch_size: int = 1, **kwargs):
+        """Process the inputs into a model-feedable format.
 
         Args:
-            input (InputType): Input given by user.
-            index (int): index of the input
-            bbox_thr (float): threshold for bounding box detection.
-                Defaults to 0.3.
-            nms_thr (float): IoU threshold for bounding box NMS.
-                Defaults to 0.3.
+            inputs (InputsType): Inputs given by user.
+            batch_size (int): batch size. Defaults to 1.
 
         Yields:
             Any: Data processed by the ``pipeline`` and ``collate_fn``.
+            List[str or np.ndarray]: List of original inputs in the batch
         """
 
-        if isinstance(input, str):
-            data_info = dict(img_path=input)
-        else:
-            data_info = dict(img=input, img_path=f'{index}.jpg'.rjust(10, '0'))
-        data_info.update(self.model.dataset_meta)
+        for i, input in enumerate(inputs):
+            data_batch = {}
+            data_infos = self.inferencer.preprocess_single(
+                input, index=i, **kwargs)
+            data_batch = self.inferencer.collate_fn(data_infos)
+            # only supports inference with batch size 1
+            yield data_batch, [input]
+
+    @torch.no_grad()
+    def forward(self, inputs: InputType, **forward_kwargs) -> PredType:
+        """Forward the inputs to the model.
 
-        if self.cfg.data_mode == 'topdown':
-            det_results = self.detector(
-                input, return_datasample=True)['predictions']
-            pred_instance = det_results[0].pred_instances.cpu().numpy()
-            bboxes = np.concatenate(
-                (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)
-
-            label_mask = np.zeros(len(bboxes), dtype=np.uint8)
-            for cat_id in self.det_cat_ids:
-                label_mask = np.logical_or(label_mask,
-                                           pred_instance.labels == cat_id)
-
-            bboxes = bboxes[np.logical_and(label_mask,
-                                           pred_instance.scores > bbox_thr)]
-            bboxes = bboxes[nms(bboxes, nms_thr)]
-
-            data_infos = []
-            if len(bboxes) > 0:
-                for bbox in bboxes:
-                    inst = data_info.copy()
-                    inst['bbox'] = bbox[None, :4]
-                    inst['bbox_score'] = bbox[4:5]
-                    data_infos.append(self.pipeline(inst))
-            else:
-                inst = data_info.copy()
-
-                # get bbox from the image size
-                if isinstance(input, str):
-                    input = mmcv.imread(input)
-                h, w = input.shape[:2]
-
-                inst['bbox'] = np.array([[0, 0, w, h]], dtype=np.float32)
-                inst['bbox_score'] = np.ones(1, dtype=np.float32)
-                data_infos.append(self.pipeline(inst))
-
-        else:  # bottom-up
-            data_infos = [self.pipeline(data_info)]
-
-        return data_infos
-
-    def forward(self, inputs: Union[dict, tuple]):
-        data_samples = super().forward(inputs)
-        if self.cfg.data_mode == 'topdown':
-            data_samples = [merge_data_samples(data_samples)]
-        return data_samples
+        Args:
+            inputs (InputsType): The inputs to be forwarded.
+
+        Returns:
+            Dict: The prediction results. Possibly with keys "pose2d".
+        """
+        return self.inferencer.forward(inputs, **forward_kwargs)
 
     def __call__(
         self,
         inputs: InputsType,
         return_datasample: bool = False,
         batch_size: int = 1,
         out_dir: Optional[str] = None,
@@ -214,44 +152,88 @@
         """
         if out_dir is not None:
             if 'vis_out_dir' not in kwargs:
                 kwargs['vis_out_dir'] = f'{out_dir}/visualizations'
             if 'pred_out_dir' not in kwargs:
                 kwargs['pred_out_dir'] = f'{out_dir}/predictions'
 
+        kwargs = {
+            key: value
+            for key, value in kwargs.items()
+            if key in set.union(self.inferencer.preprocess_kwargs,
+                                self.inferencer.forward_kwargs,
+                                self.inferencer.visualize_kwargs,
+                                self.inferencer.postprocess_kwargs)
+        }
         (
             preprocess_kwargs,
             forward_kwargs,
             visualize_kwargs,
             postprocess_kwargs,
         ) = self._dispatch_kwargs(**kwargs)
 
+        self.inferencer.update_model_visualizer_settings(**kwargs)
+
         # preprocessing
         if isinstance(inputs, str) and inputs.startswith('webcam'):
-            inputs = self._get_webcam_inputs(inputs)
+            inputs = self.inferencer._get_webcam_inputs(inputs)
             batch_size = 1
             if not visualize_kwargs.get('show', False):
                 warnings.warn('The display mode is closed when using webcam '
                               'input. It will be turned on automatically.')
             visualize_kwargs['show'] = True
         else:
-            inputs = self._inputs_to_list(inputs)
+            inputs = self.inferencer._inputs_to_list(inputs)
+        self._video_input = self.inferencer._video_input
+        if self._video_input:
+            self.video_info = self.inferencer.video_info
 
         inputs = self.preprocess(
             inputs, batch_size=batch_size, **preprocess_kwargs)
 
+        # forward
+        if 'bbox_thr' in self.inferencer.forward_kwargs:
+            forward_kwargs['bbox_thr'] = preprocess_kwargs.get('bbox_thr', -1)
+
         preds = []
-        if not hasattr(self, 'detector'):
-            inputs = track(inputs, description='Inference')
 
         for proc_inputs, ori_inputs in inputs:
             preds = self.forward(proc_inputs, **forward_kwargs)
 
             visualization = self.visualize(ori_inputs, preds,
                                            **visualize_kwargs)
             results = self.postprocess(preds, visualization, return_datasample,
                                        **postprocess_kwargs)
             yield results
 
-        # merge visualization and prediction results
         if self._video_input:
-            self._merge_outputs(**visualize_kwargs, **postprocess_kwargs)
+            self._finalize_video_processing(
+                postprocess_kwargs.get('pred_out_dir', ''))
+
+    def visualize(self, inputs: InputsType, preds: PredType,
+                  **kwargs) -> List[np.ndarray]:
+        """Visualize predictions.
+
+        Args:
+            inputs (list): Inputs preprocessed by :meth:`_inputs_to_list`.
+            preds (Any): Predictions of the model.
+            return_vis (bool): Whether to return images with predicted results.
+            show (bool): Whether to display the image in a popup window.
+                Defaults to False.
+            show_interval (int): The interval of show (s). Defaults to 0
+            radius (int): Keypoint radius for visualization. Defaults to 3
+            thickness (int): Link thickness for visualization. Defaults to 1
+            kpt_thr (float): The threshold to visualize the keypoints.
+                Defaults to 0.3
+            vis_out_dir (str, optional): directory to save visualization
+                results w/o predictions. If left as empty, no file will
+                be saved. Defaults to ''.
+
+        Returns:
+            List[np.ndarray]: Visualization results.
+        """
+        window_name = ''
+        if self.inferencer._video_input:
+            window_name = self.inferencer.video_info['name']
+
+        return self.inferencer.visualize(
+            inputs, preds, window_name=window_name, **kwargs)
```

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/__init__.py` & `mmpose-1.1.0/mmpose/codecs/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .associative_embedding import AssociativeEmbedding
 from .decoupled_heatmap import DecoupledHeatmap
+from .image_pose_lifting import ImagePoseLifting
 from .integral_regression_label import IntegralRegressionLabel
 from .megvii_heatmap import MegviiHeatmap
 from .msra_heatmap import MSRAHeatmap
 from .regression_label import RegressionLabel
 from .simcc_label import SimCCLabel
 from .spr import SPR
 from .udp_heatmap import UDPHeatmap
+from .video_pose_lifting import VideoPoseLifting
 
 __all__ = [
     'MSRAHeatmap', 'MegviiHeatmap', 'UDPHeatmap', 'RegressionLabel',
     'SimCCLabel', 'IntegralRegressionLabel', 'AssociativeEmbedding', 'SPR',
-    'DecoupledHeatmap'
+    'DecoupledHeatmap', 'VideoPoseLifting', 'ImagePoseLifting'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/associative_embedding.py` & `mmpose-1.1.0/mmpose/codecs/associative_embedding.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/base.py` & `mmpose-1.1.0/mmpose/codecs/base.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/decoupled_heatmap.py` & `mmpose-1.1.0/mmpose/codecs/decoupled_heatmap.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/integral_regression_label.py` & `mmpose-1.1.0/mmpose/codecs/integral_regression_label.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/megvii_heatmap.py` & `mmpose-1.1.0/mmpose/codecs/megvii_heatmap.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/msra_heatmap.py` & `mmpose-1.1.0/mmpose/codecs/msra_heatmap.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/regression_label.py` & `mmpose-1.1.0/mmpose/codecs/regression_label.py`

 * *Files 0% similar despite different names*

```diff
@@ -74,15 +74,15 @@
 
         Args:
             encoded (np.ndarray): Coordinates in shape (N, K, D)
 
         Returns:
             tuple:
             - keypoints (np.ndarray): Decoded coordinates in shape (N, K, D)
-            - socres (np.ndarray): The keypoint scores in shape (N, K).
+            - scores (np.ndarray): The keypoint scores in shape (N, K).
                 It usually represents the confidence of the keypoint prediction
         """
 
         if encoded.shape[-1] == 2:
             N, K, _ = encoded.shape
             normalized_coords = encoded.copy()
             scores = np.ones((N, K), dtype=np.float32)
```

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/simcc_label.py` & `mmpose-1.1.0/mmpose/codecs/simcc_label.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/spr.py` & `mmpose-1.1.0/mmpose/codecs/spr.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/udp_heatmap.py` & `mmpose-1.1.0/mmpose/codecs/udp_heatmap.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/utils/__init__.py` & `mmpose-1.1.0/mmpose/codecs/utils/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,20 +4,20 @@
                                generate_unbiased_gaussian_heatmaps)
 from .instance_property import (get_diagonal_lengths, get_instance_bbox,
                                 get_instance_root)
 from .offset_heatmap import (generate_displacement_heatmap,
                              generate_offset_heatmap)
 from .post_processing import (batch_heatmap_nms, gaussian_blur,
                               gaussian_blur1d, get_heatmap_maximum,
-                              get_simcc_maximum)
+                              get_simcc_maximum, get_simcc_normalized)
 from .refinement import (refine_keypoints, refine_keypoints_dark,
                          refine_keypoints_dark_udp, refine_simcc_dark)
 
 __all__ = [
     'generate_gaussian_heatmaps', 'generate_udp_gaussian_heatmaps',
     'generate_unbiased_gaussian_heatmaps', 'gaussian_blur',
     'get_heatmap_maximum', 'get_simcc_maximum', 'generate_offset_heatmap',
     'batch_heatmap_nms', 'refine_keypoints', 'refine_keypoints_dark',
     'refine_keypoints_dark_udp', 'generate_displacement_heatmap',
     'refine_simcc_dark', 'gaussian_blur1d', 'get_diagonal_lengths',
-    'get_instance_root', 'get_instance_bbox'
+    'get_instance_root', 'get_instance_bbox', 'get_simcc_normalized'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/utils/gaussian_heatmap.py` & `mmpose-1.1.0/mmpose/codecs/utils/gaussian_heatmap.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/utils/instance_property.py` & `mmpose-1.1.0/mmpose/codecs/utils/instance_property.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/utils/offset_heatmap.py` & `mmpose-1.1.0/mmpose/codecs/utils/offset_heatmap.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/utils/post_processing.py` & `mmpose-1.1.0/mmpose/codecs/utils/post_processing.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,14 +5,43 @@
 import cv2
 import numpy as np
 import torch
 import torch.nn.functional as F
 from torch import Tensor
 
 
+def get_simcc_normalized(batch_pred_simcc, sigma=None):
+    """Normalize the predicted SimCC.
+
+    Args:
+        batch_pred_simcc (torch.Tensor): The predicted SimCC.
+        sigma (float): The sigma of the Gaussian distribution.
+
+    Returns:
+        torch.Tensor: The normalized SimCC.
+    """
+    B, K, _ = batch_pred_simcc.shape
+
+    # Scale and clamp the tensor
+    if sigma is not None:
+        batch_pred_simcc = batch_pred_simcc / (sigma * np.sqrt(np.pi * 2))
+    batch_pred_simcc = batch_pred_simcc.clamp(min=0)
+
+    # Compute the binary mask
+    mask = (batch_pred_simcc.amax(dim=-1) > 1).reshape(B, K, 1)
+
+    # Normalize the tensor using the maximum value
+    norm = (batch_pred_simcc / batch_pred_simcc.amax(dim=-1).reshape(B, K, 1))
+
+    # Apply normalization
+    batch_pred_simcc = torch.where(mask, norm, batch_pred_simcc)
+
+    return batch_pred_simcc
+
+
 def get_simcc_maximum(simcc_x: np.ndarray,
                       simcc_y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
     """Get maximum response location and value from simcc representations.
 
     Note:
         instance number: N
         num_keypoints: K
```

### Comparing `mmpose-1.0.0rc1/mmpose/codecs/utils/refinement.py` & `mmpose-1.1.0/mmpose/codecs/utils/refinement.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/builder.py` & `mmpose-1.1.0/mmpose/datasets/builder.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/dataset_wrappers.py` & `mmpose-1.1.0/mmpose/datasets/dataset_wrappers.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/__init__.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+from .animalkingdom_dataset import AnimalKingdomDataset
 from .animalpose_dataset import AnimalPoseDataset
 from .ap10k_dataset import AP10KDataset
 from .atrw_dataset import ATRWDataset
 from .fly_dataset import FlyDataset
 from .horse10_dataset import Horse10Dataset
 from .locust_dataset import LocustDataset
 from .macaque_dataset import MacaqueDataset
 from .zebra_dataset import ZebraDataset
 
 __all__ = [
     'AnimalPoseDataset', 'AP10KDataset', 'Horse10Dataset', 'MacaqueDataset',
-    'FlyDataset', 'LocustDataset', 'ZebraDataset', 'ATRWDataset'
+    'FlyDataset', 'LocustDataset', 'ZebraDataset', 'ATRWDataset',
+    'AnimalKingdomDataset'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/animalpose_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/animalpose_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/ap10k_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/ap10k_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/atrw_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/atrw_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/fly_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/fly_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/horse10_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/horse10_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/locust_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/locust_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/macaque_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/macaque_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/animal/zebra_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/animal/zebra_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/base/base_coco_style_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/base/base_coco_style_dataset.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,16 +3,16 @@
 import os.path as osp
 from copy import deepcopy
 from itertools import filterfalse, groupby
 from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
 from mmengine.dataset import BaseDataset, force_full_init
-from mmengine.fileio import load
-from mmengine.utils import check_file_exist, is_list_of
+from mmengine.fileio import exists, get_local_path, load
+from mmengine.utils import is_list_of
 from xtcocotools.coco import COCO
 
 from mmpose.registry import DATASETS
 from mmpose.structures.bbox import bbox_xywh2xyxy
 from ..utils import parse_pose_metainfo
 
 
@@ -191,36 +191,37 @@
                     instance_list, image_list)
 
         return data_list
 
     def _load_annotations(self) -> Tuple[List[dict], List[dict]]:
         """Load data from annotations in COCO format."""
 
-        check_file_exist(self.ann_file)
+        assert exists(self.ann_file), 'Annotation file does not exist'
 
-        coco = COCO(self.ann_file)
+        with get_local_path(self.ann_file) as local_path:
+            self.coco = COCO(local_path)
         # set the metainfo about categories, which is a list of dict
         # and each dict contains the 'id', 'name', etc. about this category
-        self._metainfo['CLASSES'] = coco.loadCats(coco.getCatIds())
+        self._metainfo['CLASSES'] = self.coco.loadCats(self.coco.getCatIds())
 
         instance_list = []
         image_list = []
 
-        for img_id in coco.getImgIds():
-            img = coco.loadImgs(img_id)[0]
+        for img_id in self.coco.getImgIds():
+            img = self.coco.loadImgs(img_id)[0]
             img.update({
                 'img_id':
                 img_id,
                 'img_path':
                 osp.join(self.data_prefix['img'], img['file_name']),
             })
             image_list.append(img)
 
-            ann_ids = coco.getAnnIds(imgIds=img_id)
-            for ann in coco.loadAnns(ann_ids):
+            ann_ids = self.coco.getAnnIds(imgIds=img_id)
+            for ann in self.coco.loadAnns(ann_ids):
 
                 instance_info = self.parse_data_info(
                     dict(raw_ann_info=ann, raw_img_info=img))
 
                 # skip invalid instance annotation.
                 if not instance_info:
                     continue
@@ -376,36 +377,36 @@
                     data_list_bu.append(data_info_bu)
 
         return data_list_bu
 
     def _load_detection_results(self) -> List[dict]:
         """Load data from detection results with dummy keypoint annotations."""
 
-        check_file_exist(self.ann_file)
-        check_file_exist(self.bbox_file)
-
+        assert exists(self.ann_file), 'Annotation file does not exist'
+        assert exists(self.bbox_file), 'Bbox file does not exist'
         # load detection results
         det_results = load(self.bbox_file)
         assert is_list_of(det_results, dict)
 
         # load coco annotations to build image id-to-name index
-        coco = COCO(self.ann_file)
+        with get_local_path(self.ann_file) as local_path:
+            self.coco = COCO(local_path)
         # set the metainfo about categories, which is a list of dict
         # and each dict contains the 'id', 'name', etc. about this category
-        self._metainfo['CLASSES'] = coco.loadCats(coco.getCatIds())
+        self._metainfo['CLASSES'] = self.coco.loadCats(self.coco.getCatIds())
 
         num_keypoints = self.metainfo['num_keypoints']
         data_list = []
         id_ = 0
         for det in det_results:
             # remove non-human instances
             if det['category_id'] != 1:
                 continue
 
-            img = coco.loadImgs(det['image_id'])[0]
+            img = self.coco.loadImgs(det['image_id'])[0]
 
             img_path = osp.join(self.data_prefix['img'], img['file_name'])
             bbox_xywh = np.array(
                 det['bbox'][:4], dtype=np.float32).reshape(1, 4)
             bbox = bbox_xywh2xyxy(bbox_xywh)
             bbox_score = np.array(det['score'], dtype=np.float32).reshape(1)
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/__init__.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,18 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .aic_dataset import AicDataset
 from .coco_dataset import CocoDataset
 from .crowdpose_dataset import CrowdPoseDataset
+from .humanart_dataset import HumanArtDataset
 from .jhmdb_dataset import JhmdbDataset
 from .mhp_dataset import MhpDataset
 from .mpii_dataset import MpiiDataset
 from .mpii_trb_dataset import MpiiTrbDataset
 from .ochuman_dataset import OCHumanDataset
 from .posetrack18_dataset import PoseTrack18Dataset
 from .posetrack18_video_dataset import PoseTrack18VideoDataset
 
 __all__ = [
     'CocoDataset', 'MpiiDataset', 'MpiiTrbDataset', 'AicDataset',
     'CrowdPoseDataset', 'OCHumanDataset', 'MhpDataset', 'PoseTrack18Dataset',
-    'JhmdbDataset', 'PoseTrack18VideoDataset'
+    'JhmdbDataset', 'PoseTrack18VideoDataset', 'HumanArtDataset'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/aic_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/aic_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/coco_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/coco_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/crowdpose_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/crowdpose_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/jhmdb_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/jhmdb_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/mhp_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/mhp_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/mpii_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/mpii_dataset.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import json
 import os.path as osp
 from typing import Callable, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
-from mmengine.utils import check_file_exist
+from mmengine.fileio import exists, get_local_path
 from scipy.io import loadmat
 
 from mmpose.registry import DATASETS
 from mmpose.structures.bbox import bbox_cs2xyxy
 from ..base import BaseCocoStyleDataset
 
 
@@ -133,34 +133,36 @@
             test_mode=test_mode,
             lazy_init=lazy_init,
             max_refetch=max_refetch)
 
     def _load_annotations(self) -> Tuple[List[dict], List[dict]]:
         """Load data from annotations in MPII format."""
 
-        check_file_exist(self.ann_file)
-        with open(self.ann_file) as anno_file:
-            anns = json.load(anno_file)
+        assert exists(self.ann_file), 'Annotation file does not exist'
+        with get_local_path(self.ann_file) as local_path:
+            with open(local_path) as anno_file:
+                self.anns = json.load(anno_file)
 
         if self.headbox_file:
-            check_file_exist(self.headbox_file)
-            headbox_dict = loadmat(self.headbox_file)
-            headboxes_src = np.transpose(headbox_dict['headboxes_src'],
+            assert exists(self.headbox_file), 'Headbox file does not exist'
+            with get_local_path(self.headbox_file) as local_path:
+                self.headbox_dict = loadmat(local_path)
+            headboxes_src = np.transpose(self.headbox_dict['headboxes_src'],
                                          [2, 0, 1])
             SC_BIAS = 0.6
 
         instance_list = []
         image_list = []
         used_img_ids = set()
         ann_id = 0
 
         # mpii bbox scales are normalized with factor 200.
         pixel_std = 200.
 
-        for idx, ann in enumerate(anns):
+        for idx, ann in enumerate(self.anns):
             center = np.array(ann['center'], dtype=np.float32)
             scale = np.array([ann['scale'], ann['scale']],
                              dtype=np.float32) * pixel_std
 
             # Adjust center/scale slightly to avoid cropping limbs
             if center[0] != -1:
                 center[1] = center[1] + 15. / pixel_std * scale[1]
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/mpii_trb_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/mpii_trb_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import json
 import os.path as osp
 from typing import List, Tuple
 
 import numpy as np
-from mmengine.utils import check_file_exist
+from mmengine.fileio import exists, get_local_path
 
 from mmpose.registry import DATASETS
 from mmpose.structures.bbox import bbox_cs2xyxy
 from ..base import BaseCocoStyleDataset
 
 
 @DATASETS.register_module()
@@ -102,28 +102,29 @@
     """
 
     METAINFO: dict = dict(from_file='configs/_base_/datasets/mpii_trb.py')
 
     def _load_annotations(self) -> Tuple[List[dict], List[dict]]:
         """Load data from annotations in MPII-TRB format."""
 
-        check_file_exist(self.ann_file)
-        with open(self.ann_file) as anno_file:
-            data = json.load(anno_file)
+        assert exists(self.ann_file), 'Annotation file does not exist'
+        with get_local_path(self.ann_file) as local_path:
+            with open(local_path) as anno_file:
+                self.data = json.load(anno_file)
 
-        imgid2info = {img['id']: img for img in data['images']}
+        imgid2info = {img['id']: img for img in self.data['images']}
 
         instance_list = []
         image_list = []
         used_img_ids = set()
 
         # mpii-trb bbox scales are normalized with factor 200.
         pixel_std = 200.
 
-        for ann in data['annotations']:
+        for ann in self.data['annotations']:
             img_id = ann['image_id']
 
             # center, scale in shape [1, 2] and bbox in [1, 4]
             center = np.array([ann['center']], dtype=np.float32)
             scale = np.array([[ann['scale'], ann['scale']]],
                              dtype=np.float32) * pixel_std
             bbox = bbox_cs2xyxy(center, scale)
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/ochuman_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/ochuman_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/posetrack18_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/posetrack18_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/body/posetrack18_video_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/body/posetrack18_video_dataset.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 from typing import Callable, List, Optional, Sequence, Union
 
 import numpy as np
-from mmengine.fileio import load
-from mmengine.utils import check_file_exist, is_list_of
+from mmengine.fileio import exists, get_local_path, load
+from mmengine.utils import is_list_of
 from xtcocotools.coco import COCO
 
 from mmpose.registry import DATASETS
 from mmpose.structures.bbox import bbox_xywh2xyxy
 from ..base import BaseCocoStyleDataset
 
 
@@ -283,30 +283,30 @@
             'id': ann['id'],
         }
 
         return data_info
 
     def _load_detection_results(self) -> List[dict]:
         """Load data from detection results with dummy keypoint annotations."""
-
-        check_file_exist(self.ann_file)
-        check_file_exist(self.bbox_file)
+        assert exists(self.ann_file), 'Annotation file does not exist'
+        assert exists(self.bbox_file), 'Bbox file does not exist'
 
         # load detection results
         det_results = load(self.bbox_file)
         assert is_list_of(det_results, dict)
 
         # load coco annotations to build image id-to-name index
-        coco = COCO(self.ann_file)
+        with get_local_path(self.ann_file) as local_path:
+            self.coco = COCO(local_path)
 
         # mapping image name to id
         name2id = {}
         # mapping image id to name
         id2name = {}
-        for img_id, image in coco.imgs.items():
+        for img_id, image in self.coco.imgs.items():
             file_name = image['file_name']
             id2name[img_id] = file_name
             name2id[file_name] = img_id
 
         num_keypoints = self.metainfo['num_keypoints']
         data_list = []
         id_ = 0
@@ -329,15 +329,15 @@
             if 'nframes' in det:
                 nframes = int(det['nframes'])
             else:
                 if 'image_name' in det:
                     img_id = name2id[det['image_name']]
                 else:
                     img_id = det['image_id']
-                img_ann = coco.loadImgs(img_id)[0]
+                img_ann = self.coco.loadImgs(img_id)[0]
                 nframes = int(img_ann['nframes'])
 
             # deal with multiple image paths
             img_paths: list = []
             if 'image_name' in det:
                 image_name = det['image_name']
             else:
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/face/aflw_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/face/aflw_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/face/coco_wholebody_face_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/face/coco_wholebody_face_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/face/cofw_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/face/cofw_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/face/face_300w_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/face/face_300w_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/face/wflw_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/face/wflw_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/fashion/deepfashion_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/fashion/deepfashion_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/coco_wholebody_hand_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/hand/coco_wholebody_hand_dataset.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
 from typing import List, Tuple
 
 import numpy as np
-from mmengine.utils import check_file_exist
+from mmengine.fileio import exists, get_local_path
 from xtcocotools.coco import COCO
 
 from mmpose.registry import DATASETS
 from mmpose.structures.bbox import bbox_xywh2xyxy
 from ..base import BaseCocoStyleDataset
 
 
@@ -83,34 +83,35 @@
 
     METAINFO: dict = dict(
         from_file='configs/_base_/datasets/coco_wholebody_hand.py')
 
     def _load_annotations(self) -> Tuple[List[dict], List[dict]]:
         """Load data from annotations in COCO format."""
 
-        check_file_exist(self.ann_file)
+        assert exists(self.ann_file), 'Annotation file does not exist'
 
-        coco = COCO(self.ann_file)
+        with get_local_path(self.ann_file) as local_path:
+            self.coco = COCO(local_path)
         instance_list = []
         image_list = []
         id = 0
 
-        for img_id in coco.getImgIds():
-            img = coco.loadImgs(img_id)[0]
+        for img_id in self.coco.getImgIds():
+            img = self.coco.loadImgs(img_id)[0]
 
             img.update({
                 'img_id':
                 img_id,
                 'img_path':
                 osp.join(self.data_prefix['img'], img['file_name']),
             })
             image_list.append(img)
 
-            ann_ids = coco.getAnnIds(imgIds=img_id, iscrowd=False)
-            anns = coco.loadAnns(ann_ids)
+            ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)
+            anns = self.coco.loadAnns(ann_ids)
             for ann in anns:
                 for type in ['left', 'right']:
                     # filter invalid hand annotations, there might be two
                     # valid instances (left and right hand) in one image
                     if ann[f'{type}hand_valid'] and max(
                             ann[f'{type}hand_kpts']) > 0:
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/freihand_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/hand/freihand_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/onehand10k_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/hand/onehand10k_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/panoptic_hand2d_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/hand/panoptic_hand2d_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/hand/rhd2d_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/hand/rhd2d_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/utils.py` & `mmpose-1.1.0/mmpose/datasets/datasets/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -170,14 +170,19 @@
         parsed['skeleton_link_colors'].append(sk.get('color', [96, 96, 255]))
 
     # parse extra information
     parsed['dataset_keypoint_weights'] = np.array(
         metainfo['joint_weights'], dtype=np.float32)
     parsed['sigmas'] = np.array(metainfo['sigmas'], dtype=np.float32)
 
+    if 'stats_info' in metainfo:
+        parsed['stats_info'] = {}
+        for name, val in metainfo['stats_info'].items():
+            parsed['stats_info'][name] = np.array(val, dtype=np.float32)
+
     # formatting
     def _map(src, mapping: dict):
         if isinstance(src, (list, tuple)):
             cls = type(src)
             return cls(_map(s, mapping) for s in src)
         else:
             return mapping[src]
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/wholebody/coco_wholebody_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/wholebody/coco_wholebody_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/datasets/wholebody/halpe_dataset.py` & `mmpose-1.1.0/mmpose/datasets/datasets/wholebody/halpe_dataset.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/samplers.py` & `mmpose-1.1.0/mmpose/datasets/samplers.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/transforms/__init__.py` & `mmpose-1.1.0/mmpose/datasets/transforms/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,16 +4,17 @@
 from .common_transforms import (Albumentation, GenerateTarget,
                                 GetBBoxCenterScale, PhotometricDistortion,
                                 RandomBBoxTransform, RandomFlip,
                                 RandomHalfBody)
 from .converting import KeypointConverter
 from .formatting import PackPoseInputs
 from .loading import LoadImage
+from .pose3d_transforms import RandomFlipAroundRoot
 from .topdown_transforms import TopdownAffine
 
 __all__ = [
     'GetBBoxCenterScale', 'RandomBBoxTransform', 'RandomFlip',
     'RandomHalfBody', 'TopdownAffine', 'Albumentation',
     'PhotometricDistortion', 'PackPoseInputs', 'LoadImage',
     'BottomupGetHeatmapMask', 'BottomupRandomAffine', 'BottomupResize',
-    'GenerateTarget', 'KeypointConverter'
+    'GenerateTarget', 'KeypointConverter', 'RandomFlipAroundRoot'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/transforms/bottomup_transforms.py` & `mmpose-1.1.0/mmpose/datasets/transforms/bottomup_transforms.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/transforms/common_transforms.py` & `mmpose-1.1.0/mmpose/datasets/transforms/common_transforms.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,16 +61,18 @@
         Args:
             results (dict): The result dict
 
         Returns:
             dict: The result dict.
         """
         if 'bbox_center' in results and 'bbox_scale' in results:
-            warnings.warn('Use the existing "bbox_center" and "bbox_scale". '
-                          'The padding will still be applied.')
+            rank, _ = get_dist_info()
+            if rank == 0:
+                warnings.warn('Use the existing "bbox_center" and "bbox_scale"'
+                              '. The padding will still be applied.')
             results['bbox_scale'] *= self.padding
 
         else:
             bbox = results['bbox']
             center, scale = bbox_xyxy2cs(bbox, padding=self.padding)
 
             results['bbox_center'] = center
@@ -643,14 +645,16 @@
             rank, _ = get_dist_info()
             if rank == 0 and not hasattr(
                     albumentations.augmentations.transforms, obj_type):
                 warnings.warn(
                     f'{obj_type} is not pixel-level transformations. '
                     'Please use with caution.')
             obj_cls = getattr(albumentations, obj_type)
+        elif isinstance(obj_type, type):
+            obj_cls = obj_type
         else:
             raise TypeError(f'type must be a str, but got {type(obj_type)}')
 
         if 'transforms' in args:
             args['transforms'] = [
                 self.albu_builder(transform)
                 for transform in args['transforms']
@@ -905,18 +909,21 @@
                  encoder: MultiConfig,
                  target_type: Optional[str] = None,
                  multilevel: bool = False,
                  use_dataset_keypoint_weights: bool = False) -> None:
         super().__init__()
 
         if target_type is not None:
-            warnings.warn(
-                'The argument `target_type` is deprecated in GenerateTarget. '
-                'The target type and encoded keys will be determined by '
-                'encoder(s).', DeprecationWarning)
+            rank, _ = get_dist_info()
+            if rank == 0:
+                warnings.warn(
+                    'The argument `target_type` is deprecated in'
+                    ' GenerateTarget. The target type and encoded '
+                    'keys will be determined by encoder(s).',
+                    DeprecationWarning)
 
         self.encoder_cfg = deepcopy(encoder)
         self.multilevel = multilevel
         self.use_dataset_keypoint_weights = use_dataset_keypoint_weights
 
         if isinstance(self.encoder_cfg, list):
             self.encoder = [
@@ -1020,14 +1027,24 @@
                     w *= results['dataset_keypoint_weights']
             else:
                 encoded['keypoint_weights'] *= results[
                     'dataset_keypoint_weights']
 
         results.update(encoded)
 
+        if results.get('keypoint_weights', None) is not None:
+            results['transformed_keypoints_visible'] = results[
+                'keypoint_weights']
+        elif results.get('keypoints', None) is not None:
+            results['transformed_keypoints_visible'] = results[
+                'keypoints_visible']
+        else:
+            raise ValueError('GenerateTarget requires \'keypoint_weights\' or'
+                             ' \'keypoints_visible\' in the results.')
+
         return results
 
     def __repr__(self) -> str:
         """print the basic information of the transform.
 
         Returns:
             str: Formatted string.
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/transforms/formatting.py` & `mmpose-1.1.0/mmpose/datasets/transforms/formatting.py`

 * *Files 23% similar despite different names*

```diff
@@ -24,22 +24,48 @@
         torch.Tensor: The output tensor.
     """
 
     if isinstance(img, np.ndarray):
         if len(img.shape) < 3:
             img = np.expand_dims(img, -1)
 
+        img = np.ascontiguousarray(img)
         tensor = torch.from_numpy(img).permute(2, 0, 1).contiguous()
     else:
         assert is_seq_of(img, np.ndarray)
         tensor = torch.stack([image_to_tensor(_img) for _img in img])
 
     return tensor
 
 
+def keypoints_to_tensor(keypoints: Union[np.ndarray, Sequence[np.ndarray]]
+                        ) -> torch.torch.Tensor:
+    """Translate keypoints or sequence of keypoints to tensor. Multiple
+    keypoints tensors will be stacked.
+
+    Args:
+        keypoints (np.ndarray | Sequence[np.ndarray]): The keypoints or
+            keypoints sequence.
+
+    Returns:
+        torch.Tensor: The output tensor.
+    """
+    if isinstance(keypoints, np.ndarray):
+        keypoints = np.ascontiguousarray(keypoints)
+        N = keypoints.shape[0]
+        keypoints = keypoints.transpose(1, 2, 0).reshape(-1, N)
+        tensor = torch.from_numpy(keypoints).contiguous()
+    else:
+        assert is_seq_of(keypoints, np.ndarray)
+        tensor = torch.stack(
+            [keypoints_to_tensor(_keypoints) for _keypoints in keypoints])
+
+    return tensor
+
+
 @TRANSFORMS.register_module()
 class PackPoseInputs(BaseTransform):
     """Pack the inputs data for pose estimation.
 
     The ``img_meta`` item is always populated. The contents of the
     ``img_meta`` dictionary depends on ``meta_keys``. By default it includes:
 
@@ -84,25 +110,31 @@
         'bbox': 'bboxes',
         'head_size': 'head_size',
         'bbox_center': 'bbox_centers',
         'bbox_scale': 'bbox_scales',
         'bbox_score': 'bbox_scores',
         'keypoints': 'keypoints',
         'keypoints_visible': 'keypoints_visible',
+        'lifting_target': 'lifting_target',
+        'lifting_target_visible': 'lifting_target_visible',
     }
 
     # items in `label_mapping_table` will be packed into
     # PoseDataSample.gt_instance_labels and converted to Tensor. These items
     # will be used for computing losses
     label_mapping_table = {
         'keypoint_labels': 'keypoint_labels',
+        'lifting_target_label': 'lifting_target_label',
+        'lifting_target_weights': 'lifting_target_weights',
+        'trajectory_weights': 'trajectory_weights',
         'keypoint_x_labels': 'keypoint_x_labels',
         'keypoint_y_labels': 'keypoint_y_labels',
         'keypoint_weights': 'keypoint_weights',
-        'instance_coords': 'instance_coords'
+        'instance_coords': 'instance_coords',
+        'transformed_keypoints_visible': 'keypoints_visible',
     }
 
     # items in `field_mapping_table` will be packed into
     # PoseDataSample.gt_fields and converted to Tensor. These items will be
     # used for computing losses
     field_mapping_table = {
         'heatmaps': 'heatmaps',
@@ -132,39 +164,60 @@
         Returns:
             dict:
 
             - 'inputs' (obj:`torch.Tensor`): The forward data of models.
             - 'data_samples' (obj:`PoseDataSample`): The annotation info of the
                 sample.
         """
-        # Pack image(s)
+        # Pack image(s) for 2d pose estimation
         if 'img' in results:
             img = results['img']
-            img_tensor = image_to_tensor(img)
+            inputs_tensor = image_to_tensor(img)
+        # Pack keypoints for 3d pose-lifting
+        elif 'lifting_target' in results and 'keypoints' in results:
+            if 'keypoint_labels' in results:
+                keypoints = results['keypoint_labels']
+            else:
+                keypoints = results['keypoints']
+            inputs_tensor = keypoints_to_tensor(keypoints)
 
         data_sample = PoseDataSample()
 
         # pack instance data
         gt_instances = InstanceData()
         for key, packed_key in self.instance_mapping_table.items():
             if key in results:
+                if 'lifting_target' in results and key in {
+                        'keypoints', 'keypoints_visible'
+                }:
+                    continue
                 gt_instances.set_field(results[key], packed_key)
 
         # pack `transformed_keypoints` for visualizing data transform
         # and augmentation results
         if self.pack_transformed and 'transformed_keypoints' in results:
             gt_instances.set_field(results['transformed_keypoints'],
                                    'transformed_keypoints')
+        if self.pack_transformed and \
+                'transformed_keypoints_visible' in results:
+            gt_instances.set_field(results['transformed_keypoints_visible'],
+                                   'transformed_keypoints_visible')
 
         data_sample.gt_instances = gt_instances
 
         # pack instance labels
         gt_instance_labels = InstanceData()
         for key, packed_key in self.label_mapping_table.items():
             if key in results:
+                # For pose-lifting, store only target-related fields
+                if 'lifting_target_label' in results and key in {
+                        'keypoint_labels', 'keypoint_weights',
+                        'transformed_keypoints_visible'
+                }:
+                    continue
                 if isinstance(results[key], list):
                     # A list of labels is usually generated by combined
                     # multiple encoders (See ``GenerateTarget`` in
                     # mmpose/datasets/transforms/common_transforms.py)
                     # In this case, labels in list should have the same
                     # shape and will be stacked.
                     _labels = np.stack(results[key])
@@ -197,15 +250,15 @@
         if gt_fields:
             data_sample.gt_fields = gt_fields.to_tensor()
 
         img_meta = {k: results[k] for k in self.meta_keys if k in results}
         data_sample.set_metainfo(img_meta)
 
         packed_results = dict()
-        packed_results['inputs'] = img_tensor
+        packed_results['inputs'] = inputs_tensor
         packed_results['data_samples'] = data_sample
 
         return packed_results
 
     def __repr__(self) -> str:
         """print the basic information of the transform.
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/transforms/loading.py` & `mmpose-1.1.0/mmpose/datasets/transforms/loading.py`

 * *Files 14% similar despite different names*

```diff
@@ -29,17 +29,16 @@
             Defaults to False.
         color_type (str): The flag argument for :func:``mmcv.imfrombytes``.
             Defaults to 'color'.
         imdecode_backend (str): The image decoding backend type. The backend
             argument for :func:``mmcv.imfrombytes``.
             See :func:``mmcv.imfrombytes`` for details.
             Defaults to 'cv2'.
-        file_client_args (dict): Arguments to instantiate a FileClient.
-            See :class:`mmengine.fileio.FileClient` for details.
-            Defaults to ``dict(backend='disk')``.
+        backend_args (dict, optional): Arguments to instantiate the preifx of
+            uri corresponding backend. Defaults to None.
         ignore_empty (bool): Whether to allow loading empty image or file path
             not existent. Defaults to False.
     """
 
     def transform(self, results: dict) -> Optional[dict]:
         """The transform function of :class:`LoadImage`.
```

### Comparing `mmpose-1.0.0rc1/mmpose/datasets/transforms/topdown_transforms.py` & `mmpose-1.1.0/mmpose/datasets/transforms/topdown_transforms.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/engine/hooks/ema_hook.py` & `mmpose-1.1.0/mmpose/engine/hooks/ema_hook.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/engine/hooks/visualization_hook.py` & `mmpose-1.1.0/mmpose/engine/hooks/visualization_hook.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os
 import warnings
 from typing import Optional, Sequence
 
 import mmcv
 import mmengine
+import mmengine.fileio as fileio
 from mmengine.hooks import Hook
 from mmengine.runner import Runner
 from mmengine.visualization import Visualizer
 
 from mmpose.registry import HOOKS
 from mmpose.structures import PoseDataSample, merge_data_samples
 
@@ -37,71 +38,68 @@
         interval (int): The interval of visualization. Defaults to 50.
         score_thr (float): The threshold to visualize the bboxes
             and masks. Defaults to 0.3.
         show (bool): Whether to display the drawn image. Default to False.
         wait_time (float): The interval of show (s). Defaults to 0.
         out_dir (str, optional): directory where painted images
             will be saved in testing process.
-        file_client_args (dict): Arguments to instantiate a FileClient.
-            See :class:`mmengine.fileio.FileClient` for details.
-            Defaults to ``dict(backend='disk')``.
+        backend_args (dict, optional): Arguments to instantiate the preifx of
+            uri corresponding backend. Defaults to None.
     """
 
-    def __init__(self,
-                 enable: bool = False,
-                 interval: int = 50,
-                 score_thr: float = 0.3,
-                 show: bool = False,
-                 wait_time: float = 0.,
-                 out_dir: Optional[str] = None,
-                 file_client_args: dict = dict(backend='disk')):
+    def __init__(
+        self,
+        enable: bool = False,
+        interval: int = 50,
+        kpt_thr: float = 0.3,
+        show: bool = False,
+        wait_time: float = 0.,
+        out_dir: Optional[str] = None,
+        backend_args: Optional[dict] = None,
+    ):
         self._visualizer: Visualizer = Visualizer.get_current_instance()
         self.interval = interval
-        self.score_thr = score_thr
+        self.kpt_thr = kpt_thr
         self.show = show
         if self.show:
             # No need to think about vis backends.
             self._visualizer._vis_backends = {}
             warnings.warn('The show is True, it means that only '
                           'the prediction results are visualized '
                           'without storing data, so vis_backends '
                           'needs to be excluded.')
 
         self.wait_time = wait_time
-        self.file_client_args = file_client_args.copy()
-        self.file_client = None
         self.enable = enable
         self.out_dir = out_dir
         self._test_index = 0
+        self.backend_args = backend_args
 
     def after_val_iter(self, runner: Runner, batch_idx: int, data_batch: dict,
                        outputs: Sequence[PoseDataSample]) -> None:
         """Run after every ``self.interval`` validation iterations.
 
         Args:
             runner (:obj:`Runner`): The runner of the validation process.
             batch_idx (int): The index of the current batch in the val loop.
             data_batch (dict): Data from dataloader.
             outputs (Sequence[:obj:`PoseDataSample`]): Outputs from model.
         """
         if self.enable is False:
             return
 
-        if self.file_client is None:
-            self.file_client = mmengine.FileClient(**self.file_client_args)
-
         self._visualizer.set_dataset_meta(runner.val_evaluator.dataset_meta)
 
         # There is no guarantee that the same batch of images
         # is visualized for each evaluation.
         total_curr_iter = runner.iter + batch_idx
 
         # Visualize only the first data
         img_path = data_batch['data_samples'][0].get('img_path')
-        img_bytes = self.file_client.get(img_path)
+        img_bytes = fileio.get(img_path, backend_args=self.backend_args)
         img = mmcv.imfrombytes(img_bytes, channel_order='rgb')
         data_sample = outputs[0]
 
         # revert the heatmap on the original image
         data_sample = merge_data_samples([data_sample])
 
         if total_curr_iter % self.interval == 0:
@@ -110,15 +108,15 @@
                 img,
                 data_sample=data_sample,
                 draw_gt=False,
                 draw_bbox=True,
                 draw_heatmap=True,
                 show=self.show,
                 wait_time=self.wait_time,
-                kpt_score_thr=self.score_thr,
+                kpt_thr=self.kpt_thr,
                 step=total_curr_iter)
 
     def after_test_iter(self, runner: Runner, batch_idx: int, data_batch: dict,
                         outputs: Sequence[PoseDataSample]) -> None:
         """Run after every testing iterations.
 
         Args:
@@ -131,24 +129,21 @@
             return
 
         if self.out_dir is not None:
             self.out_dir = os.path.join(runner.work_dir, runner.timestamp,
                                         self.out_dir)
             mmengine.mkdir_or_exist(self.out_dir)
 
-        if self.file_client is None:
-            self.file_client = mmengine.FileClient(**self.file_client_args)
-
         self._visualizer.set_dataset_meta(runner.test_evaluator.dataset_meta)
 
         for data_sample in outputs:
             self._test_index += 1
 
             img_path = data_sample.get('img_path')
-            img_bytes = self.file_client.get(img_path)
+            img_bytes = fileio.get(img_path, backend_args=self.backend_args)
             img = mmcv.imfrombytes(img_bytes, channel_order='rgb')
             data_sample = merge_data_samples([data_sample])
 
             out_file = None
             if self.out_dir is not None:
                 out_file_name, postfix = os.path.basename(img_path).rsplit(
                     '.', 1)
@@ -164,10 +159,10 @@
                 img,
                 data_sample=data_sample,
                 show=self.show,
                 draw_gt=False,
                 draw_bbox=True,
                 draw_heatmap=True,
                 wait_time=self.wait_time,
-                kpt_score_thr=self.score_thr,
+                kpt_thr=self.kpt_thr,
                 out_file=out_file,
                 step=self._test_index)
```

### Comparing `mmpose-1.0.0rc1/mmpose/engine/optim_wrappers/layer_decay_optim_wrapper.py` & `mmpose-1.1.0/mmpose/engine/optim_wrappers/layer_decay_optim_wrapper.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/functional/__init__.py` & `mmpose-1.1.0/mmpose/evaluation/functional/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from .keypoint_eval import (keypoint_auc, keypoint_epe, keypoint_nme,
-                            keypoint_pck_accuracy,
+from .keypoint_eval import (keypoint_auc, keypoint_epe, keypoint_mpjpe,
+                            keypoint_nme, keypoint_pck_accuracy,
                             multilabel_classification_accuracy,
                             pose_pck_accuracy, simcc_pck_accuracy)
 from .nms import nms, oks_nms, soft_oks_nms
 
 __all__ = [
     'keypoint_pck_accuracy', 'keypoint_auc', 'keypoint_nme', 'keypoint_epe',
     'pose_pck_accuracy', 'multilabel_classification_accuracy',
-    'simcc_pck_accuracy', 'nms', 'oks_nms', 'soft_oks_nms'
+    'simcc_pck_accuracy', 'nms', 'oks_nms', 'soft_oks_nms', 'keypoint_mpjpe'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/functional/keypoint_eval.py` & `mmpose-1.1.0/mmpose/evaluation/functional/keypoint_eval.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from typing import Optional, Tuple
 
 import numpy as np
 
 from mmpose.codecs.utils import get_heatmap_maximum, get_simcc_maximum
+from .mesh_eval import compute_similarity_transform
 
 
 def _calc_distances(preds: np.ndarray, gts: np.ndarray, mask: np.ndarray,
                     norm_factor: np.ndarray) -> np.ndarray:
     """Calculate the normalized distances between preds and target.
 
     Note:
@@ -94,15 +95,15 @@
         - avg_acc (float): Averaged accuracy across all keypoints.
         - cnt (int): Number of valid keypoints.
     """
     distances = _calc_distances(pred, gt, mask, norm_factor)
     acc = np.array([_distance_acc(d, thr) for d in distances])
     valid_acc = acc[acc >= 0]
     cnt = len(valid_acc)
-    avg_acc = valid_acc.mean() if cnt > 0 else 0
+    avg_acc = valid_acc.mean() if cnt > 0 else 0.0
     return acc, avg_acc, cnt
 
 
 def keypoint_auc(pred: np.ndarray,
                  gt: np.ndarray,
                  mask: np.ndarray,
                  norm_factor: np.ndarray,
@@ -314,7 +315,61 @@
     if pred.shape[0] == 0:
         acc = 0.0  # when no sample is with gt labels, set acc to 0.
     else:
         # The classification of a sample is regarded as correct
         # only if it's correct for all labels.
         acc = (((pred - thr) * (gt - thr)) > 0).all(axis=1).mean()
     return acc
+
+
+def keypoint_mpjpe(pred: np.ndarray,
+                   gt: np.ndarray,
+                   mask: np.ndarray,
+                   alignment: str = 'none'):
+    """Calculate the mean per-joint position error (MPJPE) and the error after
+    rigid alignment with the ground truth (P-MPJPE).
+
+    Note:
+        - batch_size: N
+        - num_keypoints: K
+        - keypoint_dims: C
+
+    Args:
+        pred (np.ndarray): Predicted keypoint location with shape [N, K, C].
+        gt (np.ndarray): Groundtruth keypoint location with shape [N, K, C].
+        mask (np.ndarray): Visibility of the target with shape [N, K].
+            False for invisible joints, and True for visible.
+            Invisible joints will be ignored for accuracy calculation.
+        alignment (str, optional): method to align the prediction with the
+            groundtruth. Supported options are:
+
+                - ``'none'``: no alignment will be applied
+                - ``'scale'``: align in the least-square sense in scale
+                - ``'procrustes'``: align in the least-square sense in
+                    scale, rotation and translation.
+
+    Returns:
+        tuple: A tuple containing joint position errors
+
+        - (float | np.ndarray): mean per-joint position error (mpjpe).
+        - (float | np.ndarray): mpjpe after rigid alignment with the
+            ground truth (p-mpjpe).
+    """
+    assert mask.any()
+
+    if alignment == 'none':
+        pass
+    elif alignment == 'procrustes':
+        pred = np.stack([
+            compute_similarity_transform(pred_i, gt_i)
+            for pred_i, gt_i in zip(pred, gt)
+        ])
+    elif alignment == 'scale':
+        pred_dot_pred = np.einsum('nkc,nkc->n', pred, pred)
+        pred_dot_gt = np.einsum('nkc,nkc->n', pred, gt)
+        scale_factor = pred_dot_gt / pred_dot_pred
+        pred = pred * scale_factor[:, None, None]
+    else:
+        raise ValueError(f'Invalid value for alignment: {alignment}')
+    error = np.linalg.norm(pred - gt, ord=2, axis=-1)[mask].mean()
+
+    return error
```

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/functional/nms.py` & `mmpose-1.1.0/mmpose/evaluation/functional/nms.py`

 * *Files 1% similar despite different names*

```diff
@@ -98,15 +98,15 @@
         xd = d[n_d, 0::3]
         yd = d[n_d, 1::3]
         vd = d[n_d, 2::3]
         dx = xd - xg
         dy = yd - yg
         e = (dx**2 + dy**2) / vars / ((a_g + a_d[n_d]) / 2 + np.spacing(1)) / 2
         if vis_thr is not None:
-            ind = list(vg > vis_thr) and list(vd > vis_thr)
+            ind = list((vg > vis_thr) & (vd > vis_thr))
             e = e[ind]
         ious[n_d] = np.sum(np.exp(-e)) / len(e) if len(e) != 0 else 0.0
     return ious
 
 
 def oks_nms(kpts_db: List[dict],
             thr: float,
```

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/metrics/__init__.py` & `mmpose-1.1.0/mmpose/evaluation/metrics/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .coco_metric import CocoMetric
 from .coco_wholebody_metric import CocoWholeBodyMetric
 from .keypoint_2d_metrics import (AUC, EPE, NME, JhmdbPCKAccuracy,
                                   MpiiPCKAccuracy, PCKAccuracy)
+from .keypoint_3d_metrics import MPJPE
 from .keypoint_partition_metric import KeypointPartitionMetric
 from .posetrack18_metric import PoseTrack18Metric
 
 __all__ = [
     'CocoMetric', 'PCKAccuracy', 'MpiiPCKAccuracy', 'JhmdbPCKAccuracy', 'AUC',
     'EPE', 'NME', 'PoseTrack18Metric', 'CocoWholeBodyMetric',
-    'KeypointPartitionMetric'
+    'KeypointPartitionMetric', 'MPJPE'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/metrics/coco_metric.py` & `mmpose-1.1.0/mmpose/evaluation/metrics/coco_metric.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 import os.path as osp
 import tempfile
 from collections import OrderedDict, defaultdict
 from typing import Dict, Optional, Sequence
 
 import numpy as np
 from mmengine.evaluator import BaseMetric
-from mmengine.fileio import dump, load
+from mmengine.fileio import dump, get_local_path, load
 from mmengine.logging import MMLogger
 from xtcocotools.coco import COCO
 from xtcocotools.cocoeval import COCOeval
 
 from mmpose.registry import METRICS
 from ..functional import oks_nms, soft_oks_nms
 
@@ -98,15 +98,16 @@
                  collect_device: str = 'cpu',
                  prefix: Optional[str] = None) -> None:
         super().__init__(collect_device=collect_device, prefix=prefix)
         self.ann_file = ann_file
         # initialize coco helper with the annotation json file
         # if ann_file is not specified, initialize with the converted dataset
         if ann_file is not None:
-            self.coco = COCO(ann_file)
+            with get_local_path(ann_file) as local_path:
+                self.coco = COCO(local_path)
         else:
             self.coco = None
 
         self.use_area = use_area
         self.iou_type = iou_type
 
         allowed_score_modes = ['bbox', 'bbox_keypoint', 'bbox_rle', 'keypoint']
@@ -174,21 +175,27 @@
             pred = dict()
             pred['id'] = data_sample['id']
             pred['img_id'] = data_sample['img_id']
             pred['keypoints'] = keypoints
             pred['keypoint_scores'] = keypoint_scores
             pred['category_id'] = data_sample.get('category_id', 1)
 
-            if ('bbox_scores' not in data_sample['gt_instances']
-                    or len(data_sample['gt_instances']['bbox_scores']) !=
-                    len(keypoints)):
+            if 'bbox_scores' in data_sample['pred_instances']:
+                # some one-stage models will predict bboxes and scores
+                # together with keypoints
+                bbox_scores = data_sample['pred_instances']['bbox_scores']
+            elif ('bbox_scores' not in data_sample['gt_instances']
+                  or len(data_sample['gt_instances']['bbox_scores']) !=
+                  len(keypoints)):
                 # bottom-up models might output different number of
                 # instances from annotation
                 bbox_scores = np.ones(len(keypoints))
             else:
+                # top-down models use detected bboxes, the scores of which
+                # are contained in the gt_instances
                 bbox_scores = data_sample['gt_instances']['bbox_scores']
             pred['bbox_scores'] = bbox_scores
 
             # get area information
             if 'bbox_scales' in data_sample['gt_instances']:
                 pred['areas'] = np.prod(
                     data_sample['gt_instances']['bbox_scales'], axis=1)
```

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/metrics/coco_wholebody_metric.py` & `mmpose-1.1.0/mmpose/evaluation/metrics/coco_wholebody_metric.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/metrics/keypoint_2d_metrics.py` & `mmpose-1.1.0/mmpose/evaluation/metrics/keypoint_2d_metrics.py`

 * *Files 1% similar despite different names*

```diff
@@ -756,14 +756,16 @@
         # coco_wholebody_face corresponding to `right-most` and `left-most`
         # eye keypoints
         'coco_wholebody_face': [36, 45],
         # cofw: corresponding to `right-most` and `left-most` eye keypoints
         'cofw': [8, 9],
         # wflw: corresponding to `right-most` and `left-most` eye keypoints
         'wflw': [60, 72],
+        # lapa: corresponding to `right-most` and `left-most` eye keypoints
+        'lapa': [66, 79],
     }
 
     def __init__(self,
                  norm_mode: str,
                  norm_item: Optional[str] = None,
                  keypoint_indices: Optional[Sequence[int]] = None,
                  collect_device: str = 'cpu',
```

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/metrics/keypoint_partition_metric.py` & `mmpose-1.1.0/mmpose/evaluation/metrics/keypoint_partition_metric.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/evaluation/metrics/posetrack18_metric.py` & `mmpose-1.1.0/mmpose/evaluation/metrics/posetrack18_metric.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/__init__.py` & `mmpose-1.1.0/mmpose/models/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/__init__.py` & `mmpose-1.1.0/mmpose/models/backbones/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/alexnet.py` & `mmpose-1.1.0/mmpose/models/backbones/alexnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/base_backbone.py` & `mmpose-1.1.0/mmpose/models/backbones/base_backbone.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/cpm.py` & `mmpose-1.1.0/mmpose/models/backbones/cpm.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/hourglass.py` & `mmpose-1.1.0/mmpose/models/backbones/hourglass.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/hourglass_ae.py` & `mmpose-1.1.0/mmpose/models/backbones/hourglass_ae.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/hrformer.py` & `mmpose-1.1.0/mmpose/models/backbones/hrformer.py`

 * *Files 2% similar despite different names*

```diff
@@ -295,25 +295,20 @@
             padding=1)
         self.act2 = build_activation_layer(dw_act_cfg)
         self.norm2 = build_norm_layer(norm_cfg, hidden_features)[1]
         self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1)
         self.act3 = build_activation_layer(act_cfg)
         self.norm3 = build_norm_layer(norm_cfg, out_features)[1]
 
-        # put the modules togather
-        self.layers = [
-            self.fc1, self.norm1, self.act1, self.dw3x3, self.norm2, self.act2,
-            self.fc2, self.norm3, self.act3
-        ]
-
     def forward(self, x, H, W):
         """Forward function."""
         x = nlc_to_nchw(x, (H, W))
-        for layer in self.layers:
-            x = layer(x)
+        x = self.act1(self.norm1(self.fc1(x)))
+        x = self.act2(self.norm2(self.dw3x3(x)))
+        x = self.act3(self.norm3(self.fc2(x)))
         x = nchw_to_nlc(x)
         return x
 
 
 class HRFormerBlock(BaseModule):
     """High-Resolution Block for HRFormer.
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/hrnet.py` & `mmpose-1.1.0/mmpose/models/backbones/hrnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/litehrnet.py` & `mmpose-1.1.0/mmpose/models/backbones/litehrnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/mobilenet_v2.py` & `mmpose-1.1.0/mmpose/models/backbones/mobilenet_v2.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/mobilenet_v3.py` & `mmpose-1.1.0/mmpose/models/backbones/mobilenet_v3.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/mspn.py` & `mmpose-1.1.0/mmpose/models/backbones/mspn.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/pvt.py` & `mmpose-1.1.0/mmpose/models/backbones/pvt.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/regnet.py` & `mmpose-1.1.0/mmpose/models/backbones/regnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/resnest.py` & `mmpose-1.1.0/mmpose/models/backbones/resnest.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/resnet.py` & `mmpose-1.1.0/mmpose/models/backbones/resnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/resnext.py` & `mmpose-1.1.0/mmpose/models/backbones/resnext.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/rsn.py` & `mmpose-1.1.0/mmpose/models/backbones/rsn.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/scnet.py` & `mmpose-1.1.0/mmpose/models/backbones/scnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/seresnet.py` & `mmpose-1.1.0/mmpose/models/backbones/seresnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/seresnext.py` & `mmpose-1.1.0/mmpose/models/backbones/seresnext.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/shufflenet_v1.py` & `mmpose-1.1.0/mmpose/models/backbones/shufflenet_v1.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/shufflenet_v2.py` & `mmpose-1.1.0/mmpose/models/backbones/shufflenet_v2.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/swin.py` & `mmpose-1.1.0/mmpose/models/backbones/swin.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/tcn.py` & `mmpose-1.1.0/mmpose/models/backbones/tcn.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/utils/channel_shuffle.py` & `mmpose-1.1.0/mmpose/models/backbones/utils/channel_shuffle.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/utils/ckpt_convert.py` & `mmpose-1.1.0/mmpose/models/backbones/utils/ckpt_convert.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/utils/inverted_residual.py` & `mmpose-1.1.0/mmpose/models/backbones/utils/inverted_residual.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/utils/make_divisible.py` & `mmpose-1.1.0/mmpose/models/backbones/utils/make_divisible.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/utils/se_layer.py` & `mmpose-1.1.0/mmpose/models/backbones/utils/se_layer.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/utils/utils.py` & `mmpose-1.1.0/mmpose/models/backbones/utils/utils.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/v2v_net.py` & `mmpose-1.1.0/mmpose/models/backbones/v2v_net.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/vgg.py` & `mmpose-1.1.0/mmpose/models/backbones/vgg.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/vipnas_mbv3.py` & `mmpose-1.1.0/mmpose/models/backbones/vipnas_mbv3.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/backbones/vipnas_resnet.py` & `mmpose-1.1.0/mmpose/models/backbones/vipnas_resnet.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/builder.py` & `mmpose-1.1.0/mmpose/models/builder.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/__init__.py` & `mmpose-1.1.0/mmpose/models/heads/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .base_head import BaseHead
 from .coord_cls_heads import RTMCCHead, SimCCHead
 from .heatmap_heads import (AssociativeEmbeddingHead, CIDHead, CPMHead,
                             HeatmapHead, MSPNHead, ViPNASHead)
-from .hybrid_heads import DEKRHead
+from .hybrid_heads import DEKRHead, VisPredictHead
 from .regression_heads import (DSNTHead, IntegralRegressionHead,
-                               RegressionHead, RLEHead)
+                               RegressionHead, RLEHead, TemporalRegressionHead,
+                               TrajectoryRegressionHead)
 
 __all__ = [
     'BaseHead', 'HeatmapHead', 'CPMHead', 'MSPNHead', 'ViPNASHead',
     'RegressionHead', 'IntegralRegressionHead', 'SimCCHead', 'RLEHead',
-    'DSNTHead', 'AssociativeEmbeddingHead', 'DEKRHead', 'CIDHead', 'RTMCCHead'
+    'DSNTHead', 'AssociativeEmbeddingHead', 'DEKRHead', 'VisPredictHead',
+    'CIDHead', 'RTMCCHead', 'TemporalRegressionHead',
+    'TrajectoryRegressionHead'
 ]
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/base_head.py` & `mmpose-1.1.0/mmpose/models/heads/regression_heads/regression_head.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,149 +1,146 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-import warnings
-from abc import ABCMeta, abstractmethod
-from typing import List, Sequence, Tuple, Union
+from typing import Optional, Sequence, Tuple, Union
 
+import numpy as np
 import torch
-import torch.nn.functional as F
-from mmengine.model import BaseModule
-from mmengine.structures import InstanceData
-from torch import Tensor
+from torch import Tensor, nn
 
-from mmpose.models.utils.ops import resize
+from mmpose.evaluation.functional import keypoint_pck_accuracy
+from mmpose.models.utils.tta import flip_coordinates
+from mmpose.registry import KEYPOINT_CODECS, MODELS
 from mmpose.utils.tensor_utils import to_numpy
-from mmpose.utils.typing import (Features, InstanceList, OptConfigType,
-                                 OptSampleList, Predictions)
+from mmpose.utils.typing import (ConfigType, OptConfigType, OptSampleList,
+                                 Predictions)
+from ..base_head import BaseHead
+
+OptIntSeq = Optional[Sequence[int]]
 
 
-class BaseHead(BaseModule, metaclass=ABCMeta):
-    """Base head. A subclass should override :meth:`predict` and :meth:`loss`.
+@MODELS.register_module()
+class RegressionHead(BaseHead):
+    """Top-down regression head introduced in `Deeppose`_ by Toshev et al
+    (2014). The head is composed of fully-connected layers to predict the
+    coordinates directly.
 
     Args:
-        init_cfg (dict, optional): The extra init config of layers.
-            Defaults to None.
-    """
-
-    @abstractmethod
-    def forward(self, feats: Tuple[Tensor]):
-        """Forward the network."""
-
-    @abstractmethod
-    def predict(self,
-                feats: Features,
-                batch_data_samples: OptSampleList,
-                test_cfg: OptConfigType = {}) -> Predictions:
-        """Predict results from features."""
+        in_channels (int | sequence[int]): Number of input channels
+        num_joints (int): Number of joints
+        loss (Config): Config for keypoint loss. Defaults to use
+            :class:`SmoothL1Loss`
+        decoder (Config, optional): The decoder config that controls decoding
+            keypoint coordinates from the network output. Defaults to ``None``
+        init_cfg (Config, optional): Config to control the initialization. See
+            :attr:`default_init_cfg` for default settings
 
-    @abstractmethod
-    def loss(self,
-             feats: Tuple[Tensor],
-             batch_data_samples: OptSampleList,
-             train_cfg: OptConfigType = {}) -> dict:
-        """Calculate losses from a batch of inputs and data samples."""
-
-    def _get_in_channels(self) -> Union[int, List[int]]:
-        """Get the input channel number of the network according to the feature
-        channel numbers and the input transform type."""
-
-        feat_channels = self.in_channels
-        if isinstance(feat_channels, int):
-            feat_channels = [feat_channels]
-
-        if self.input_transform == 'resize_concat':
-            if isinstance(self.input_index, int):
-                in_channels = feat_channels[self.input_index]
-            else:
-                in_channels = sum(feat_channels[i] for i in self.input_index)
-        elif self.input_transform == 'select':
-            if isinstance(self.input_index, int):
-                in_channels = feat_channels[self.input_index]
-            else:
-                in_channels = [feat_channels[i] for i in self.input_index]
-        else:
-            raise ValueError(
-                f'Invalid input transform mode "{self.input_transform}"')
+    .. _`Deeppose`: https://arxiv.org/abs/1312.4659
+    """
 
-        return in_channels
+    _version = 2
 
-    def _transform_inputs(
-        self,
-        feats: Union[Tensor, Sequence[Tensor]],
-    ) -> Union[Tensor, Tuple[Tensor]]:
-        """Transform multi scale features into the network input."""
-        if not isinstance(feats, Sequence):
-            warnings.warn(f'the input of {self._get_name()} is a tensor '
-                          f'instead of a tuple or list. The argument '
-                          f'`input_transform` will be ignored.')
-            return feats
-
-        if self.input_transform == 'resize_concat':
-            inputs = [feats[i] for i in self.input_index]
-            resized_inputs = [
-                F.interpolate(
-                    x,
-                    size=inputs[0].shape[2:],
-                    mode='bilinear',
-                    align_corners=self.align_corners) for x in inputs
-            ]
-            inputs = torch.cat(resized_inputs, dim=1)
-        elif self.input_transform == 'select':
-            if isinstance(self.input_index, int):
-                inputs = feats[self.input_index]
-                if hasattr(self, 'upsample') and self.upsample > 0:
-                    inputs = resize(
-                        input=F.relu(inputs),
-                        scale_factor=self.upsample,
-                        mode='bilinear',
-                        align_corners=self.align_corners)
-            else:
-                inputs = tuple(feats[i] for i in self.input_index)
+    def __init__(self,
+                 in_channels: Union[int, Sequence[int]],
+                 num_joints: int,
+                 loss: ConfigType = dict(
+                     type='SmoothL1Loss', use_target_weight=True),
+                 decoder: OptConfigType = None,
+                 init_cfg: OptConfigType = None):
+
+        if init_cfg is None:
+            init_cfg = self.default_init_cfg
+
+        super().__init__(init_cfg)
+
+        self.in_channels = in_channels
+        self.num_joints = num_joints
+        self.loss_module = MODELS.build(loss)
+        if decoder is not None:
+            self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
-            raise (ValueError,
-                   f'Invalid input transform mode "{self.input_transform}"')
+            self.decoder = None
 
-        return inputs
+        # Define fully-connected layers
+        self.fc = nn.Linear(in_channels, self.num_joints * 2)
 
-    def decode(self, batch_outputs: Union[Tensor,
-                                          Tuple[Tensor]]) -> InstanceList:
-        """Decode keypoints from outputs.
+    def forward(self, feats: Tuple[Tensor]) -> Tensor:
+        """Forward the network. The input is multi scale feature maps and the
+        output is the coordinates.
 
         Args:
-            batch_outputs (Tensor | Tuple[Tensor]): The network outputs of
-                a data batch
+            feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
-            List[InstanceData]: A list of InstanceData, each contains the
-            decoded pose information of the instances of one data sample.
+            Tensor: output coordinates(and sigmas[optional]).
         """
+        x = feats[-1]
 
-        def _pack_and_call(args, func):
-            if not isinstance(args, tuple):
-                args = (args, )
-            return func(*args)
-
-        if self.decoder is None:
-            raise RuntimeError(
-                f'The decoder has not been set in {self.__class__.__name__}. '
-                'Please set the decoder configs in the init parameters to '
-                'enable head methods `head.predict()` and `head.decode()`')
-
-        if self.decoder.support_batch_decoding:
-            batch_keypoints, batch_scores = _pack_and_call(
-                batch_outputs, self.decoder.batch_decode)
+        x = torch.flatten(x, 1)
+        x = self.fc(x)
 
+        return x.reshape(-1, self.num_joints, 2)
+
+    def predict(self,
+                feats: Tuple[Tensor],
+                batch_data_samples: OptSampleList,
+                test_cfg: ConfigType = {}) -> Predictions:
+        """Predict results from outputs."""
+
+        if test_cfg.get('flip_test', False):
+            # TTA: flip test -> feats = [orig, flipped]
+            assert isinstance(feats, list) and len(feats) == 2
+            flip_indices = batch_data_samples[0].metainfo['flip_indices']
+            input_size = batch_data_samples[0].metainfo['input_size']
+            _feats, _feats_flip = feats
+
+            _batch_coords = self.forward(_feats)
+            _batch_coords_flip = flip_coordinates(
+                self.forward(_feats_flip),
+                flip_indices=flip_indices,
+                shift_coords=test_cfg.get('shift_coords', True),
+                input_size=input_size)
+            batch_coords = (_batch_coords + _batch_coords_flip) * 0.5
         else:
-            batch_output_np = to_numpy(batch_outputs, unzip=True)
-            batch_keypoints = []
-            batch_scores = []
-            for outputs in batch_output_np:
-                keypoints, scores = _pack_and_call(outputs,
-                                                   self.decoder.decode)
-                batch_keypoints.append(keypoints)
-                batch_scores.append(scores)
-
-        preds = [
-            InstanceData(keypoints=keypoints, keypoint_scores=scores)
-            for keypoints, scores in zip(batch_keypoints, batch_scores)
-        ]
+            batch_coords = self.forward(feats)  # (B, K, D)
+
+        batch_coords.unsqueeze_(dim=1)  # (B, N, K, D)
+        preds = self.decode(batch_coords)
 
         return preds
+
+    def loss(self,
+             inputs: Tuple[Tensor],
+             batch_data_samples: OptSampleList,
+             train_cfg: ConfigType = {}) -> dict:
+        """Calculate losses from a batch of inputs and data samples."""
+
+        pred_outputs = self.forward(inputs)
+
+        keypoint_labels = torch.cat(
+            [d.gt_instance_labels.keypoint_labels for d in batch_data_samples])
+        keypoint_weights = torch.cat([
+            d.gt_instance_labels.keypoint_weights for d in batch_data_samples
+        ])
+
+        # calculate losses
+        losses = dict()
+        loss = self.loss_module(pred_outputs, keypoint_labels,
+                                keypoint_weights.unsqueeze(-1))
+
+        losses.update(loss_kpt=loss)
+
+        # calculate accuracy
+        _, avg_acc, _ = keypoint_pck_accuracy(
+            pred=to_numpy(pred_outputs),
+            gt=to_numpy(keypoint_labels),
+            mask=to_numpy(keypoint_weights) > 0,
+            thr=0.05,
+            norm_factor=np.ones((pred_outputs.size(0), 2), dtype=np.float32))
+
+        acc_pose = torch.tensor(avg_acc, device=keypoint_labels.device)
+        losses.update(acc_pose=acc_pose)
+
+        return losses
+
+    @property
+    def default_init_cfg(self):
+        init_cfg = [dict(type='Normal', layer=['Linear'], std=0.01, bias=0)]
+        return init_cfg
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/coord_cls_heads/rtmcc_head.py` & `mmpose-1.1.0/mmpose/models/heads/coord_cls_heads/rtmcc_head.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,17 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+import warnings
 from typing import Optional, Sequence, Tuple, Union
 
 import torch
+from mmengine.dist import get_dist_info
 from mmengine.structures import PixelData
 from torch import Tensor, nn
 
+from mmpose.codecs.utils import get_simcc_normalized
 from mmpose.evaluation.functional import simcc_pck_accuracy
 from mmpose.models.utils.rtmcc_block import RTMCCBlock, ScaleNorm
 from mmpose.models.utils.tta import flip_vectors
 from mmpose.registry import KEYPOINT_CODECS, MODELS
 from mmpose.utils.tensor_utils import to_numpy
 from mmpose.utils.typing import (ConfigType, InstanceList, OptConfigType,
                                  OptSampleList)
@@ -40,30 +43,14 @@
                 s=128,
                 expansion_factor=2,
                 dropout_rate=0.,
                 drop_path=0.,
                 act_fn='ReLU',
                 use_rel_bias=False,
                 pos_enc=False).
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
         loss (Config): Config of the keypoint loss. Defaults to use
             :class:`KLDiscretLoss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
     """
@@ -81,17 +68,14 @@
             s=128,
             expansion_factor=2,
             dropout_rate=0.,
             drop_path=0.,
             act_fn='ReLU',
             use_rel_bias=False,
             pos_enc=False),
-        input_transform: str = 'select',
-        input_index: Union[int, Sequence[int]] = -1,
-        align_corners: bool = False,
         loss: ConfigType = dict(type='KLDiscretLoss', use_target_weight=True),
         decoder: OptConfigType = None,
         init_cfg: OptConfigType = None,
     ):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
@@ -99,31 +83,26 @@
         super().__init__(init_cfg)
 
         self.in_channels = in_channels
         self.out_channels = out_channels
         self.input_size = input_size
         self.in_featuremap_size = in_featuremap_size
         self.simcc_split_ratio = simcc_split_ratio
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
 
         self.loss_module = MODELS.build(loss)
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
 
         if isinstance(in_channels, (tuple, list)):
             raise ValueError(
                 f'{self.__class__.__name__} does not support selecting '
                 'multiple input features.')
 
-        in_channels = self._get_in_channels()
-
         # Define SimCC layers
         flatten_dims = self.in_featuremap_size[0] * self.in_featuremap_size[1]
 
         self.final_layer = nn.Conv2d(
             in_channels,
             out_channels,
             kernel_size=final_layer_kernel_size,
@@ -151,25 +130,25 @@
 
         self.cls_x = nn.Linear(gau_cfg['hidden_dims'], W, bias=False)
         self.cls_y = nn.Linear(gau_cfg['hidden_dims'], H, bias=False)
 
     def forward(self, feats: Tuple[Tensor]) -> Tuple[Tensor, Tensor]:
         """Forward the network.
 
-        The input is multi scale feature maps and the
-        output is the heatmap.
+        The input is the featuremap extracted by backbone and the
+        output is the simcc representation.
 
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
             pred_x (Tensor): 1d representation of x.
             pred_y (Tensor): 1d representation of y.
         """
-        feats = self._transform_inputs(feats)
+        feats = feats[-1]
 
         feats = self.final_layer(feats)  # -> B, K, H, W
 
         # flatten the output heatmap
         feats = torch.flatten(feats, 2)
 
         feats = self.mlp(feats)  # -> B, K, hidden
@@ -229,14 +208,25 @@
             batch_pred_y = (_batch_pred_y + _batch_pred_y_flip) * 0.5
         else:
             batch_pred_x, batch_pred_y = self.forward(feats)
 
         preds = self.decode((batch_pred_x, batch_pred_y))
 
         if test_cfg.get('output_heatmaps', False):
+            rank, _ = get_dist_info()
+            if rank == 0:
+                warnings.warn('The predicted simcc values are normalized for '
+                              'visualization. This may cause discrepancy '
+                              'between the keypoint scores and the 1D heatmaps'
+                              '.')
+
+            # normalize the predicted 1d distribution
+            batch_pred_x = get_simcc_normalized(batch_pred_x)
+            batch_pred_y = get_simcc_normalized(batch_pred_y)
+
             B, K, _ = batch_pred_x.shape
             # B, K, Wx -> B, K, Wx, 1
             x = batch_pred_x.reshape(B, K, 1, -1)
             # B, K, Wy -> B, K, 1, Wy
             y = batch_pred_y.reshape(B, K, -1, 1)
             # B, K, Wx, Wy
             batch_heatmaps = torch.matmul(y, x)
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/coord_cls_heads/simcc_head.py` & `mmpose-1.1.0/mmpose/models/heads/coord_cls_heads/simcc_head.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,15 +1,18 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+import warnings
 from typing import Optional, Sequence, Tuple, Union
 
 import torch
 from mmcv.cnn import build_conv_layer
+from mmengine.dist import get_dist_info
 from mmengine.structures import PixelData
 from torch import Tensor, nn
 
+from mmpose.codecs.utils import get_simcc_normalized
 from mmpose.evaluation.functional import simcc_pck_accuracy
 from mmpose.models.utils.tta import flip_vectors
 from mmpose.registry import KEYPOINT_CODECS, MODELS
 from mmpose.utils.tensor_utils import to_numpy
 from mmpose.utils.typing import (ConfigType, InstanceList, OptConfigType,
                                  OptSampleList)
 from ..base_head import BaseHead
@@ -49,30 +52,16 @@
             deconv layer. Defaults to ``(16, 16, 16)``
         conv_out_channels (sequence[int], optional): The output channel number
             of each intermediate conv layer. ``None`` means no intermediate
             conv layer between deconv layers and the final conv layer.
             Defaults to ``None``
         conv_kernel_sizes (sequence[int | tuple], optional): The kernel size
             of each intermediate conv layer. Defaults to ``None``
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
+        final_layer (dict): Arguments of the final Conv2d layer.
+            Defaults to ``dict(kernel_size=1)``
         loss (Config): Config of the keypoint loss. Defaults to use
             :class:`KLDiscretLoss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
 
@@ -90,18 +79,15 @@
         simcc_split_ratio: float = 2.0,
         deconv_type: str = 'heatmap',
         deconv_out_channels: OptIntSeq = (256, 256, 256),
         deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
         deconv_num_groups: OptIntSeq = (16, 16, 16),
         conv_out_channels: OptIntSeq = None,
         conv_kernel_sizes: OptIntSeq = None,
-        has_final_layer: bool = True,
-        input_transform: str = 'select',
-        input_index: Union[int, Sequence[int]] = -1,
-        align_corners: bool = False,
+        final_layer: dict = dict(kernel_size=1),
         loss: ConfigType = dict(type='KLDiscretLoss', use_target_weight=True),
         decoder: OptConfigType = None,
         init_cfg: OptConfigType = None,
     ):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
@@ -115,17 +101,14 @@
                 '{"heatmap", "vipnas"}')
 
         self.in_channels = in_channels
         self.out_channels = out_channels
         self.input_size = input_size
         self.in_featuremap_size = in_featuremap_size
         self.simcc_split_ratio = simcc_split_ratio
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
         self.loss_module = MODELS.build(loss)
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
 
         num_deconv = len(deconv_out_channels) if deconv_out_channels else 0
@@ -139,122 +122,100 @@
                 out_channels=out_channels,
                 deconv_type=deconv_type,
                 deconv_out_channels=deconv_out_channels,
                 deconv_kernel_sizes=deconv_kernel_sizes,
                 deconv_num_groups=deconv_num_groups,
                 conv_out_channels=conv_out_channels,
                 conv_kernel_sizes=conv_kernel_sizes,
-                has_final_layer=has_final_layer,
-                input_transform=input_transform,
-                input_index=input_index,
-                align_corners=align_corners)
+                final_layer=final_layer)
 
-            if has_final_layer:
+            if final_layer is not None:
                 in_channels = out_channels
             else:
                 in_channels = deconv_out_channels[-1]
 
         else:
-            in_channels = self._get_in_channels()
             self.deconv_head = None
 
-            if has_final_layer:
+            if final_layer is not None:
                 cfg = dict(
                     type='Conv2d',
                     in_channels=in_channels,
                     out_channels=out_channels,
                     kernel_size=1)
+                cfg.update(final_layer)
                 self.final_layer = build_conv_layer(cfg)
             else:
                 self.final_layer = None
 
-            if self.input_transform == 'resize_concat':
-                if isinstance(in_featuremap_size, tuple):
-                    self.heatmap_size = in_featuremap_size
-                elif isinstance(in_featuremap_size, list):
-                    self.heatmap_size = in_featuremap_size[0]
-            elif self.input_transform == 'select':
-                if isinstance(in_featuremap_size, tuple):
-                    self.heatmap_size = in_featuremap_size
-                elif isinstance(in_featuremap_size, list):
-                    self.heatmap_size = in_featuremap_size[input_index]
-
-        if isinstance(in_channels, list):
-            raise ValueError(
-                f'{self.__class__.__name__} does not support selecting '
-                'multiple input features.')
+            self.heatmap_size = in_featuremap_size
 
         # Define SimCC layers
         flatten_dims = self.heatmap_size[0] * self.heatmap_size[1]
 
         W = int(self.input_size[0] * self.simcc_split_ratio)
         H = int(self.input_size[1] * self.simcc_split_ratio)
 
         self.mlp_head_x = nn.Linear(flatten_dims, W)
         self.mlp_head_y = nn.Linear(flatten_dims, H)
 
-    def _make_deconv_head(self,
-                          in_channels: Union[int, Sequence[int]],
-                          out_channels: int,
-                          deconv_type: str = 'heatmap',
-                          deconv_out_channels: OptIntSeq = (256, 256, 256),
-                          deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
-                          deconv_num_groups: OptIntSeq = (16, 16, 16),
-                          conv_out_channels: OptIntSeq = None,
-                          conv_kernel_sizes: OptIntSeq = None,
-                          has_final_layer: bool = True,
-                          input_transform: str = 'select',
-                          input_index: Union[int, Sequence[int]] = -1,
-                          align_corners: bool = False) -> nn.Module:
+    def _make_deconv_head(
+        self,
+        in_channels: Union[int, Sequence[int]],
+        out_channels: int,
+        deconv_type: str = 'heatmap',
+        deconv_out_channels: OptIntSeq = (256, 256, 256),
+        deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
+        deconv_num_groups: OptIntSeq = (16, 16, 16),
+        conv_out_channels: OptIntSeq = None,
+        conv_kernel_sizes: OptIntSeq = None,
+        final_layer: dict = dict(kernel_size=1)
+    ) -> nn.Module:
         """Create deconvolutional layers by given parameters."""
 
         if deconv_type == 'heatmap':
             deconv_head = MODELS.build(
                 dict(
                     type='HeatmapHead',
                     in_channels=self.in_channels,
                     out_channels=out_channels,
                     deconv_out_channels=deconv_out_channels,
                     deconv_kernel_sizes=deconv_kernel_sizes,
                     conv_out_channels=conv_out_channels,
                     conv_kernel_sizes=conv_kernel_sizes,
-                    has_final_layer=has_final_layer,
-                    input_transform=input_transform,
-                    input_index=input_index,
-                    align_corners=align_corners))
+                    final_layer=final_layer))
         else:
             deconv_head = MODELS.build(
                 dict(
                     type='ViPNASHead',
                     in_channels=in_channels,
                     out_channels=out_channels,
                     deconv_out_channels=deconv_out_channels,
                     deconv_num_groups=deconv_num_groups,
                     conv_out_channels=conv_out_channels,
                     conv_kernel_sizes=conv_kernel_sizes,
-                    has_final_layer=has_final_layer,
-                    input_transform=input_transform,
-                    input_index=input_index,
-                    align_corners=align_corners))
+                    final_layer=final_layer))
 
         return deconv_head
 
     def forward(self, feats: Tuple[Tensor]) -> Tuple[Tensor, Tensor]:
-        """Forward the network. The input is multi scale feature maps and the
-        output is the heatmap.
+        """Forward the network.
+
+        The input is the featuremap extracted by backbone and the
+        output is the simcc representation.
 
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
             pred_x (Tensor): 1d representation of x.
             pred_y (Tensor): 1d representation of y.
         """
         if self.deconv_head is None:
-            feats = self._transform_inputs(feats)
+            feats = feats[-1]
             if self.final_layer is not None:
                 feats = self.final_layer(feats)
         else:
             feats = self.deconv_head(feats)
 
         # flatten the output heatmap
         x = torch.flatten(feats, 2)
@@ -313,14 +274,26 @@
             batch_pred_y = (_batch_pred_y + _batch_pred_y_flip) * 0.5
         else:
             batch_pred_x, batch_pred_y = self.forward(feats)
 
         preds = self.decode((batch_pred_x, batch_pred_y))
 
         if test_cfg.get('output_heatmaps', False):
+            rank, _ = get_dist_info()
+            if rank == 0:
+                warnings.warn('The predicted simcc values are normalized for '
+                              'visualization. This may cause discrepancy '
+                              'between the keypoint scores and the 1D heatmaps'
+                              '.')
+
+            # normalize the predicted 1d distribution
+            sigma = self.decoder.sigma
+            batch_pred_x = get_simcc_normalized(batch_pred_x, sigma[0])
+            batch_pred_y = get_simcc_normalized(batch_pred_y, sigma[1])
+
             B, K, _ = batch_pred_x.shape
             # B, K, Wx -> B, K, Wx, 1
             x = batch_pred_x.reshape(B, K, 1, -1)
             # B, K, Wy -> B, K, 1, Wy
             y = batch_pred_y.reshape(B, K, -1, 1)
             # B, K, Wx, Wy
             batch_heatmaps = torch.matmul(y, x)
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/ae_head.py` & `mmpose-1.1.0/mmpose/models/heads/heatmap_heads/ae_head.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,18 +23,15 @@
                  num_keypoints: int,
                  tag_dim: int = 1,
                  tag_per_keypoint: bool = True,
                  deconv_out_channels: OptIntSeq = (256, 256, 256),
                  deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
                  conv_out_channels: OptIntSeq = None,
                  conv_kernel_sizes: OptIntSeq = None,
-                 has_final_layer: bool = True,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
+                 final_layer: dict = dict(kernel_size=1),
                  keypoint_loss: ConfigType = dict(type='KeypointMSELoss'),
                  tag_loss: ConfigType = dict(type='AssociativeEmbeddingLoss'),
                  decoder: OptConfigType = None,
                  init_cfg: OptConfigType = None):
 
         if tag_per_keypoint:
             out_channels = num_keypoints * (1 + tag_dim)
@@ -48,18 +45,15 @@
         super().__init__(
             in_channels=in_channels,
             out_channels=out_channels,
             deconv_out_channels=deconv_out_channels,
             deconv_kernel_sizes=deconv_kernel_sizes,
             conv_out_channels=conv_out_channels,
             conv_kernel_sizes=conv_kernel_sizes,
-            has_final_layer=has_final_layer,
-            input_transform=input_transform,
-            input_index=input_index,
-            align_corners=align_corners,
+            final_layer=final_layer,
             loss=loss,
             decoder=decoder,
             init_cfg=init_cfg)
 
         self.num_keypoints = num_keypoints
         self.tag_dim = tag_dim
         self.tag_per_keypoint = tag_per_keypoint
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/cid_head.py` & `mmpose-1.1.0/mmpose/models/heads/heatmap_heads/cid_head.py`

 * *Files 5% similar despite different names*

```diff
@@ -387,30 +387,14 @@
     Args:
         in_channels (int | Sequence[int]): Number of channels in the input
             feature map
         num_keypoints (int): Number of keypoints
         gfd_channels (int): Number of filters in GFD module
         max_train_instances (int): Maximum number of instances in a batch
             during training. Defaults to 200
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | Sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
         heatmap_loss (Config): Config of the heatmap loss. Defaults to use
             :class:`KeypointMSELoss`
         coupled_heatmap_loss (Config): Config of the loss for coupled heatmaps.
             Defaults to use :class:`SoftWeightSmoothL1Loss`
         decoupled_heatmap_loss (Config): Config of the loss for decoupled
             heatmaps. Defaults to use :class:`SoftWeightSmoothL1Loss`
         contrastive_loss (Config): Config of the contrastive loss for
@@ -428,17 +412,14 @@
     _version = 2
 
     def __init__(self,
                  in_channels: Union[int, Sequence[int]],
                  gfd_channels: int,
                  num_keypoints: int,
                  prior_prob: float = 0.01,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
                  coupled_heatmap_loss: OptConfigType = dict(
                      type='FocalHeatmapLoss'),
                  decoupled_heatmap_loss: OptConfigType = dict(
                      type='FocalHeatmapLoss'),
                  contrastive_loss: OptConfigType = dict(type='InfoNCELoss'),
                  decoder: OptConfigType = None,
                  init_cfg: OptConfigType = None):
@@ -446,29 +427,19 @@
         if init_cfg is None:
             init_cfg = self.default_init_cfg
 
         super().__init__(init_cfg)
 
         self.in_channels = in_channels
         self.num_keypoints = num_keypoints
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
 
-        # Get model input channels according to feature
-        in_channels = self._get_in_channels()
-        if isinstance(in_channels, list):
-            raise ValueError(
-                f'{self.__class__.__name__} does not support selecting '
-                'multiple input features.')
-
         # build sub-modules
         bias_value = -math.log((1 - prior_prob) / prior_prob)
         self.iia_module = IIAModule(
             in_channels,
             num_keypoints + 1,
             init_cfg=init_cfg + [
                 dict(
@@ -522,15 +493,15 @@
 
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
             Tensor: output heatmap.
         """
-        feats = self._transform_inputs(feats)
+        feats = feats[-1]
         instance_info = self.iia_module.forward_test(feats, {})
         instance_feats, instance_coords, instance_scores = instance_info
         instance_imgids = torch.zeros(
             instance_coords.size(0), dtype=torch.long, device=feats.device)
         instance_heatmaps = self.gfd_module(feats, instance_feats,
                                             instance_coords, instance_imgids)
 
@@ -570,20 +541,18 @@
                 - heatmaps (Tensor): The predicted heatmaps in shape (K, h, w)
         """
         metainfo = batch_data_samples[0].metainfo
 
         if test_cfg.get('flip_test', False):
             assert isinstance(feats, list) and len(feats) == 2
 
-            feats_flipped = flip_heatmaps(
-                self._transform_inputs(feats[1]), shift_heatmap=False)
-            feats = torch.cat(
-                (self._transform_inputs(feats[0]), feats_flipped))
+            feats_flipped = flip_heatmaps(feats[1][-1], shift_heatmap=False)
+            feats = torch.cat((feats[0][-1], feats_flipped))
         else:
-            feats = self._transform_inputs(feats)
+            feats = feats[-1]
 
         instance_info = self.iia_module.forward_test(feats, test_cfg)
         instance_feats, instance_coords, instance_scores = instance_info
         if len(instance_coords) > 0:
             instance_imgids = torch.zeros(
                 instance_coords.size(0), dtype=torch.long, device=feats.device)
             if test_cfg.get('flip_test', False):
@@ -674,15 +643,15 @@
 
         gt_instance_coords = torch.cat(gt_instance_coords, dim=0)
         gt_instance_heatmaps = torch.cat(gt_instance_heatmaps, dim=0)
         keypoint_weights = torch.cat(keypoint_weights, dim=0)
         instance_imgids = torch.cat(instance_imgids).to(gt_heatmaps.device)
 
         # feed-forward
-        feats = self._transform_inputs(feats)
+        feats = feats[-1]
         pred_instance_feats, pred_heatmaps = self.iia_module.forward_train(
             feats, gt_instance_coords, instance_imgids)
 
         # conpute contrastive loss
         contrastive_loss = 0
         for i in range(len(batch_data_samples)):
             pred_instance_feat = pred_instance_feats[instance_imgids == i]
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/cpm_head.py` & `mmpose-1.1.0/mmpose/models/heads/heatmap_heads/cpm_head.py`

 * *Files 3% similar despite different names*

```diff
@@ -32,16 +32,16 @@
         deconv_out_channels (Sequence[int], optional): The output channel
             number of each deconv layer. Defaults to ``(256, 256, 256)``
         deconv_kernel_sizes (Sequence[int | tuple], optional): The kernel size
             of each deconv layer. Each element should be either an integer for
             both height and width dimensions, or a tuple of two integers for
             the height and the width dimension respectively.
             Defaults to ``(4, 4, 4)``
-        has_final_layer (bool): Whether have the final 1x1 Conv2d layer.
-            Defaults to ``True``
+        final_layer (dict): Arguments of the final Conv2d layer.
+            Defaults to ``dict(kernel_size=1)``
         loss (Config | List[Config]): Config of the keypoint loss of different
             stages. Defaults to use :class:`KeypointMSELoss`.
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
 
@@ -53,15 +53,15 @@
 
     def __init__(self,
                  in_channels: Union[int, Sequence[int]],
                  out_channels: int,
                  num_stages: int,
                  deconv_out_channels: OptIntSeq = None,
                  deconv_kernel_sizes: OptIntSeq = None,
-                 has_final_layer: bool = True,
+                 final_layer: dict = dict(kernel_size=1),
                  loss: MultiConfig = dict(
                      type='KeypointMSELoss', use_target_weight=True),
                  decoder: OptConfigType = None,
                  init_cfg: OptConfigType = None):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
@@ -107,20 +107,21 @@
             in_channels = deconv_out_channels[-1]
         else:
             for _ in range(self.num_stages):
                 self.multi_deconv_layers.append(nn.Identity())
 
         # build multi-stage final layers
         self.multi_final_layers = nn.ModuleList([])
-        if has_final_layer:
+        if final_layer is not None:
             cfg = dict(
                 type='Conv2d',
                 in_channels=in_channels,
                 out_channels=out_channels,
                 kernel_size=1)
+            cfg.update(final_layer)
             for _ in range(self.num_stages):
                 self.multi_final_layers.append(build_conv_layer(cfg))
         else:
             for _ in range(self.num_stages):
                 self.multi_final_layers.append(nn.Identity())
 
     @property
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/heatmap_head.py` & `mmpose-1.1.0/mmpose/models/heads/heatmap_heads/heatmap_head.py`

 * *Files 13% similar despite different names*

```diff
@@ -36,32 +36,16 @@
             ``(4, 4, 4)``
         conv_out_channels (Sequence[int], optional): The output channel number
             of each intermediate conv layer. ``None`` means no intermediate
             conv layer between deconv layers and the final conv layer.
             Defaults to ``None``
         conv_kernel_sizes (Sequence[int | tuple], optional): The kernel size
             of each intermediate conv layer. Defaults to ``None``
-        has_final_layer (bool): Whether have the final 1x1 Conv2d layer.
-            Defaults to ``True``
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | Sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
+        final_layer (dict): Arguments of the final Conv2d layer.
+            Defaults to ``dict(kernel_size=1)``
         loss (Config): Config of the keypoint loss. Defaults to use
             :class:`KeypointMSELoss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
         extra (dict, optional): Extra configurations.
@@ -75,61 +59,32 @@
     def __init__(self,
                  in_channels: Union[int, Sequence[int]],
                  out_channels: int,
                  deconv_out_channels: OptIntSeq = (256, 256, 256),
                  deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
                  conv_out_channels: OptIntSeq = None,
                  conv_kernel_sizes: OptIntSeq = None,
-                 has_final_layer: bool = True,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
+                 final_layer: dict = dict(kernel_size=1),
                  loss: ConfigType = dict(
                      type='KeypointMSELoss', use_target_weight=True),
                  decoder: OptConfigType = None,
-                 init_cfg: OptConfigType = None,
-                 extra=None):
+                 init_cfg: OptConfigType = None):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
 
         super().__init__(init_cfg)
 
         self.in_channels = in_channels
         self.out_channels = out_channels
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
         self.loss_module = MODELS.build(loss)
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
-        self.upsample = 0
-
-        if extra is not None and not isinstance(extra, dict):
-            raise TypeError('extra should be dict or None.')
-
-        kernel_size = 1
-        padding = 0
-        if extra is not None:
-            if 'upsample' in extra:
-                self.upsample = extra['upsample']
-            if 'final_conv_kernel' in extra:
-                assert extra['final_conv_kernel'] in [1, 3]
-                if extra['final_conv_kernel'] == 3:
-                    padding = 1
-                kernel_size = extra['final_conv_kernel']
-
-        # Get model input channels according to feature
-        in_channels = self._get_in_channels()
-        if isinstance(in_channels, list):
-            raise ValueError(
-                f'{self.__class__.__name__} does not support selecting '
-                'multiple input features.')
 
         if deconv_out_channels:
             if deconv_kernel_sizes is None or len(deconv_out_channels) != len(
                     deconv_kernel_sizes):
                 raise ValueError(
                     '"deconv_out_channels" and "deconv_kernel_sizes" should '
                     'be integer sequences with the same length. Got '
@@ -158,21 +113,21 @@
                 in_channels=in_channels,
                 layer_out_channels=conv_out_channels,
                 layer_kernel_sizes=conv_kernel_sizes)
             in_channels = conv_out_channels[-1]
         else:
             self.conv_layers = nn.Identity()
 
-        if has_final_layer:
+        if final_layer is not None:
             cfg = dict(
                 type='Conv2d',
                 in_channels=in_channels,
                 out_channels=out_channels,
-                padding=padding,
-                kernel_size=kernel_size)
+                kernel_size=1)
+            cfg.update(final_layer)
             self.final_layer = build_conv_layer(cfg)
         else:
             self.final_layer = nn.Identity()
 
         # Register the hook to automatically convert old version state dicts
         self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)
 
@@ -251,15 +206,15 @@
 
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
             Tensor: output heatmap.
         """
-        x = self._transform_inputs(feats)
+        x = feats[-1]
 
         x = self.deconv_layers(x)
         x = self.conv_layers(x)
         x = self.final_layer(x)
 
         return x
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/mix_head.py` & `mmpose-1.1.0/mmpose/models/heads/hybrid_heads/dekr_head.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,408 +1,581 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Optional, Sequence, Tuple, Union
+from typing import Sequence, Tuple, Union
 
 import torch
-import torch.nn.functional as F
-from mmcv.cnn import build_conv_layer
-from torch import Tensor, nn
+from mmcv.cnn import (ConvModule, build_activation_layer, build_conv_layer,
+                      build_norm_layer)
+from mmengine.model import BaseModule, ModuleDict, Sequential
+from mmengine.structures import InstanceData, PixelData
+from torch import Tensor
 
-from mmpose.evaluation.functional import simcc_pck_accuracy
-from mmpose.models.utils.tta import flip_vectors
+from mmpose.evaluation.functional.nms import nearby_joints_nms
+from mmpose.models.utils.tta import flip_heatmaps
 from mmpose.registry import KEYPOINT_CODECS, MODELS
 from mmpose.utils.tensor_utils import to_numpy
-from mmpose.utils.typing import (ConfigType, InstanceList, OptConfigType,
-                                 OptSampleList)
+from mmpose.utils.typing import (ConfigType, Features, InstanceList,
+                                 OptConfigType, OptSampleList, Predictions)
+from ...backbones.resnet import BasicBlock
 from ..base_head import BaseHead
 
-OptIntSeq = Optional[Sequence[int]]
+try:
+    from mmcv.ops import DeformConv2d
+    has_mmcv_full = True
+except (ImportError, ModuleNotFoundError):
+    has_mmcv_full = False
+
+
+class AdaptiveActivationBlock(BaseModule):
+    """Adaptive activation convolution block. "Bottom-up human pose estimation
+    via disentangled keypoint regression", CVPR'2021.
+
+    Args:
+        in_channels (int): Number of input channels
+        out_channels (int): Number of output channels
+        groups (int): Number of groups. Generally equal to the
+            number of joints.
+        norm_cfg (dict): Config for normalization layers.
+        act_cfg (dict): Config for activation layers.
+    """
+
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 groups=1,
+                 norm_cfg=dict(type='BN'),
+                 act_cfg=dict(type='ReLU'),
+                 init_cfg=None):
+        super(AdaptiveActivationBlock, self).__init__(init_cfg=init_cfg)
+
+        assert in_channels % groups == 0 and out_channels % groups == 0
+        self.groups = groups
+
+        regular_matrix = torch.tensor([[-1, -1, -1, 0, 0, 0, 1, 1, 1],
+                                       [-1, 0, 1, -1, 0, 1, -1, 0, 1],
+                                       [1, 1, 1, 1, 1, 1, 1, 1, 1]])
+        self.register_buffer('regular_matrix', regular_matrix.float())
+
+        self.transform_matrix_conv = build_conv_layer(
+            dict(type='Conv2d'),
+            in_channels=in_channels,
+            out_channels=6 * groups,
+            kernel_size=3,
+            padding=1,
+            groups=groups,
+            bias=True)
+
+        if has_mmcv_full:
+            self.adapt_conv = DeformConv2d(
+                in_channels,
+                out_channels,
+                kernel_size=3,
+                padding=1,
+                bias=False,
+                groups=groups,
+                deform_groups=groups)
+        else:
+            raise ImportError('Please install the full version of mmcv '
+                              'to use `DeformConv2d`.')
+
+        self.norm = build_norm_layer(norm_cfg, out_channels)[1]
+        self.act = build_activation_layer(act_cfg)
+
+    def forward(self, x):
+        B, _, H, W = x.size()
+        residual = x
+
+        affine_matrix = self.transform_matrix_conv(x)
+        affine_matrix = affine_matrix.permute(0, 2, 3, 1).contiguous()
+        affine_matrix = affine_matrix.view(B, H, W, self.groups, 2, 3)
+        offset = torch.matmul(affine_matrix, self.regular_matrix)
+        offset = offset.transpose(4, 5).reshape(B, H, W, self.groups * 18)
+        offset = offset.permute(0, 3, 1, 2).contiguous()
+
+        x = self.adapt_conv(x, offset)
+        x = self.norm(x)
+        x = self.act(x + residual)
+
+        return x
+
+
+class RescoreNet(BaseModule):
+    """Rescore net used to predict the OKS score of predicted pose. We use the
+    off-the-shelf rescore net pretrained by authors of DEKR.
+
+    Args:
+        in_channels (int): Input channels
+        norm_indexes (Tuple(int)): Indices of torso in skeleton
+        init_cfg (dict, optional): Initialization config dict
+    """
+
+    def __init__(
+        self,
+        in_channels,
+        norm_indexes,
+        init_cfg=None,
+    ):
+        super(RescoreNet, self).__init__(init_cfg=init_cfg)
+
+        self.norm_indexes = norm_indexes
+
+        hidden = 256
+
+        self.l1 = torch.nn.Linear(in_channels, hidden, bias=True)
+        self.l2 = torch.nn.Linear(hidden, hidden, bias=True)
+        self.l3 = torch.nn.Linear(hidden, 1, bias=True)
+        self.relu = torch.nn.ReLU()
+
+    def make_feature(self, keypoints, keypoint_scores, skeleton):
+        """Combine original scores, joint distance and relative distance to
+        make feature.
+
+        Args:
+            keypoints (torch.Tensor): predicetd keypoints
+            keypoint_scores (torch.Tensor): predicetd keypoint scores
+            skeleton (list(list(int))): joint links
+
+        Returns:
+            torch.Tensor: feature for each instance
+        """
+        joint_1, joint_2 = zip(*skeleton)
+        num_link = len(skeleton)
+
+        joint_relate = (keypoints[:, joint_1] -
+                        keypoints[:, joint_2])[:, :, :2]
+        joint_length = joint_relate.norm(dim=2)
+
+        # To use the torso distance to normalize
+        normalize = (joint_length[:, self.norm_indexes[0]] +
+                     joint_length[:, self.norm_indexes[1]]) / 2
+        normalize = normalize.unsqueeze(1).expand(normalize.size(0), num_link)
+        normalize = normalize.clamp(min=1).contiguous()
+
+        joint_length = joint_length / normalize[:, :]
+        joint_relate = joint_relate / normalize.unsqueeze(-1)
+        joint_relate = joint_relate.flatten(1)
+
+        feature = torch.cat((joint_relate, joint_length, keypoint_scores),
+                            dim=1).float()
+        return feature
+
+    def forward(self, keypoints, keypoint_scores, skeleton):
+        feature = self.make_feature(keypoints, keypoint_scores, skeleton)
+        x = self.relu(self.l1(feature))
+        x = self.relu(self.l2(x))
+        x = self.l3(x)
+        return x.squeeze(1)
 
 
 @MODELS.register_module()
-class MixHead(BaseHead):
-    """Top-down heatmap head introduced in `SimCC`_ by Li et al (2022). The
-    head is composed of a few deconvolutional layers followed by a fully-
-    connected layer to generate 1d representation from low-resolution feature
-    maps.
+class DEKRHead(BaseHead):
+    """DisEntangled Keypoint Regression head introduced in `Bottom-up human
+    pose estimation via disentangled keypoint regression`_ by Geng et al
+    (2021). The head is composed of a heatmap branch and a displacement branch.
 
     Args:
-        in_channels (int | sequence[int]): Number of channels in the input
+        in_channels (int | Sequence[int]): Number of channels in the input
             feature map
-        out_channels (int): Number of channels in the output heatmap
-        input_size (tuple): Input image size in shape [w, h]
-        in_featuremap_size (int | sequence[int]): Size of input feature map
-        simcc_split_ratio (float): Split ratio of pixels
-        deconv_type (str, optional): The type of deconv head which should
-            be one of the following options:
-
-                - ``'Heatmap'``: make deconv layers in `HeatmapHead`
-                - ``'ViPNAS'``: make deconv layers in `ViPNASHead`
-
-            Defaults to ``'Heatmap'``
-        deconv_out_channels (sequence[int]): The output channel number of each
-            deconv layer. Defaults to ``(256, 256, 256)``
-        deconv_kernel_sizes (sequence[int | tuple], optional): The kernel size
-            of each deconv layer. Each element should be either an integer for
-            both height and width dimensions, or a tuple of two integers for
-            the height and the width dimension respectively.Defaults to
-            ``(4, 4, 4)``
-        deconv_num_groups (Sequence[int], optional): The group number of each
-            deconv layer. Defaults to ``(16, 16, 16)``
-        conv_out_channels (sequence[int], optional): The output channel number
-            of each intermediate conv layer. ``None`` means no intermediate
-            conv layer between deconv layers and the final conv layer.
-            Defaults to ``None``
-        conv_kernel_sizes (sequence[int | tuple], optional): The kernel size
-            of each intermediate conv layer. Defaults to ``None``
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
-        loss (Config): Config of the keypoint loss. Defaults to use
-            :class:`KLDiscretLoss`
+        num_joints (int): Number of joints
+        num_heatmap_filters (int): Number of filters for heatmap branch.
+            Defaults to 32
+        num_offset_filters_per_joint (int): Number of filters for each joint
+            in displacement branch. Defaults to 15
+        heatmap_loss (Config): Config of the heatmap loss. Defaults to use
+            :class:`KeypointMSELoss`
+        displacement_loss (Config): Config of the displacement regression loss.
+            Defaults to use :class:`SoftWeightSmoothL1Loss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
+        rescore_cfg (Config, optional): The config for rescore net which
+            estimates OKS via predicted keypoints and keypoint scores.
+            Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
 
-    .. _`SimCC`: https://arxiv.org/abs/2107.03332
+    .. _`Bottom-up human pose estimation via disentangled keypoint regression`:
+        https://arxiv.org/abs/2104.02300
     """
 
     _version = 2
 
-    def __init__(
-        self,
-        in_channels: Union[int, Sequence[int]],
-        out_channels: int,
-        input_size: Tuple[int, int],
-        in_featuremap_size: Tuple[int, int],
-        simcc_split_ratio: float = 2.0,
-        debias: bool = False,
-        beta: float = 1.,
-        deconv_type: str = 'Heatmap',
-        deconv_out_channels: OptIntSeq = (256, 256, 256),
-        deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
-        deconv_num_groups: OptIntSeq = (16, 16, 16),
-        conv_out_channels: OptIntSeq = None,
-        conv_kernel_sizes: OptIntSeq = None,
-        has_final_layer: bool = True,
-        input_transform: str = 'select',
-        input_index: Union[int, Sequence[int]] = -1,
-        align_corners: bool = False,
-        loss: ConfigType = dict(type='KLDiscretLoss', use_target_weight=True),
-        decoder: OptConfigType = None,
-        init_cfg: OptConfigType = None,
-    ):
+    def __init__(self,
+                 in_channels: Union[int, Sequence[int]],
+                 num_keypoints: int,
+                 num_heatmap_filters: int = 32,
+                 num_displacement_filters_per_keypoint: int = 15,
+                 heatmap_loss: ConfigType = dict(
+                     type='KeypointMSELoss', use_target_weight=True),
+                 displacement_loss: ConfigType = dict(
+                     type='SoftWeightSmoothL1Loss',
+                     use_target_weight=True,
+                     supervise_empty=False),
+                 decoder: OptConfigType = None,
+                 rescore_cfg: OptConfigType = None,
+                 init_cfg: OptConfigType = None):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
 
         super().__init__(init_cfg)
 
-        if deconv_type not in {'Heatmap', 'ViPNAS'}:
-            raise ValueError(
-                f'{self.__class__.__name__} got invalid `deconv_type` value'
-                f'{deconv_type}. Should be one of '
-                '{"Heatmap", "ViPNAS"}')
-
         self.in_channels = in_channels
-        self.out_channels = out_channels
-        self.input_size = input_size
-        self.in_featuremap_size = in_featuremap_size
-        self.simcc_split_ratio = simcc_split_ratio
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
-        self.debias = debias
-        self.beta = beta
-        self.loss_module = MODELS.build(loss)
+        self.num_keypoints = num_keypoints
+
+        # build heatmap branch
+        self.heatmap_conv_layers = self._make_heatmap_conv_layers(
+            in_channels=in_channels,
+            out_channels=1 + num_keypoints,
+            num_filters=num_heatmap_filters,
+        )
+
+        # build displacement branch
+        self.displacement_conv_layers = self._make_displacement_conv_layers(
+            in_channels=in_channels,
+            out_channels=2 * num_keypoints,
+            num_filters=num_keypoints * num_displacement_filters_per_keypoint,
+            groups=num_keypoints)
+
+        # build losses
+        self.loss_module = ModuleDict(
+            dict(
+                heatmap=MODELS.build(heatmap_loss),
+                displacement=MODELS.build(displacement_loss),
+            ))
+
+        # build decoder
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
 
-        num_deconv = len(deconv_out_channels) if deconv_out_channels else 0
-        if num_deconv != 0:
-            self.heatmap_size = tuple(
-                [s * (2**num_deconv) for s in in_featuremap_size])
+        # build rescore net
+        if rescore_cfg is not None:
+            self.rescore_net = RescoreNet(**rescore_cfg)
+        else:
+            self.rescore_net = None
 
-            # deconv layers + 1x1 conv
-            self.deconv_head = self._make_deconv_head(
-                in_channels=in_channels,
-                out_channels=out_channels,
-                deconv_type=deconv_type,
-                deconv_out_channels=deconv_out_channels,
-                deconv_kernel_sizes=deconv_kernel_sizes,
-                deconv_num_groups=deconv_num_groups,
-                conv_out_channels=conv_out_channels,
-                conv_kernel_sizes=conv_kernel_sizes,
-                has_final_layer=has_final_layer,
-                input_transform=input_transform,
-                input_index=input_index,
-                align_corners=align_corners)
+        # Register the hook to automatically convert old version state dicts
+        self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)
 
-            if has_final_layer:
-                in_channels = out_channels
-            else:
-                in_channels = deconv_out_channels[-1]
+    @property
+    def default_init_cfg(self):
+        init_cfg = [
+            dict(
+                type='Normal', layer=['Conv2d', 'ConvTranspose2d'], std=0.001),
+            dict(type='Constant', layer='BatchNorm2d', val=1)
+        ]
+        return init_cfg
 
-        else:
-            in_channels = self._get_in_channels()
-            self.deconv_head = None
+    def _make_heatmap_conv_layers(self, in_channels: int, out_channels: int,
+                                  num_filters: int):
+        """Create convolutional layers of heatmap branch by given
+        parameters."""
+        layers = [
+            ConvModule(
+                in_channels=in_channels,
+                out_channels=num_filters,
+                kernel_size=1,
+                norm_cfg=dict(type='BN')),
+            BasicBlock(num_filters, num_filters),
+            build_conv_layer(
+                dict(type='Conv2d'),
+                in_channels=num_filters,
+                out_channels=out_channels,
+                kernel_size=1),
+        ]
 
-            if has_final_layer:
-                cfg = dict(
-                    type='Conv2d',
-                    in_channels=in_channels,
-                    out_channels=out_channels,
-                    kernel_size=1)
-                self.final_layer = build_conv_layer(cfg)
-            else:
-                self.final_layer = None
+        return Sequential(*layers)
 
-            if self.input_transform == 'resize_concat':
-                if isinstance(in_featuremap_size, tuple):
-                    self.heatmap_size = in_featuremap_size
-                elif isinstance(in_featuremap_size, list):
-                    self.heatmap_size = in_featuremap_size[0]
-            elif self.input_transform == 'select':
-                if isinstance(in_featuremap_size, tuple):
-                    self.heatmap_size = in_featuremap_size
-                elif isinstance(in_featuremap_size, list):
-                    self.heatmap_size = in_featuremap_size[input_index]
-
-        if isinstance(in_channels, list):
-            raise ValueError(
-                f'{self.__class__.__name__} does not support selecting '
-                'multiple input features.')
-
-        # Define SimCC layers
-        flatten_dims = self.heatmap_size[0] * self.heatmap_size[1]
-
-        W = int(self.input_size[0] * self.simcc_split_ratio)
-        H = int(self.input_size[1] * self.simcc_split_ratio)
-
-        self.mlp_head_x = nn.Linear(flatten_dims, W)
-        self.mlp_head_y = nn.Linear(flatten_dims, H)
-
-        self.linspace_x = torch.arange(0.0, 1.0 * W, 1).reshape(1, 1, W) / W
-        self.linspace_y = torch.arange(0.0, 1.0 * H, 1).reshape(1, 1, H) / H
-
-        self.linspace_x = nn.Parameter(self.linspace_x, requires_grad=False)
-        self.linspace_y = nn.Parameter(self.linspace_y, requires_grad=False)
-
-    def _make_deconv_head(self,
-                          in_channels: Union[int, Sequence[int]],
-                          out_channels: int,
-                          deconv_type: str = 'Heatmap',
-                          deconv_out_channels: OptIntSeq = (256, 256, 256),
-                          deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
-                          deconv_num_groups: OptIntSeq = (16, 16, 16),
-                          conv_out_channels: OptIntSeq = None,
-                          conv_kernel_sizes: OptIntSeq = None,
-                          has_final_layer: bool = True,
-                          input_transform: str = 'select',
-                          input_index: Union[int, Sequence[int]] = -1,
-                          align_corners: bool = False) -> nn.Module:
-
-        if deconv_type == 'Heatmap':
-            deconv_head = MODELS.build(
-                dict(
-                    type='HeatmapHead',
-                    in_channels=self.in_channels,
-                    out_channels=out_channels,
-                    deconv_out_channels=deconv_out_channels,
-                    deconv_kernel_sizes=deconv_kernel_sizes,
-                    conv_out_channels=conv_out_channels,
-                    conv_kernel_sizes=conv_kernel_sizes,
-                    has_final_layer=has_final_layer,
-                    input_transform=input_transform,
-                    input_index=input_index,
-                    align_corners=align_corners))
-        else:
-            deconv_head = MODELS.build(
-                dict(
-                    type='ViPNASHead',
-                    in_channels=in_channels,
-                    out_channels=out_channels,
-                    deconv_out_channels=deconv_out_channels,
-                    deconv_num_groups=deconv_num_groups,
-                    conv_out_channels=conv_out_channels,
-                    conv_kernel_sizes=conv_kernel_sizes,
-                    has_final_layer=has_final_layer,
-                    input_transform=input_transform,
-                    input_index=input_index,
-                    align_corners=align_corners))
+    def _make_displacement_conv_layers(self, in_channels: int,
+                                       out_channels: int, num_filters: int,
+                                       groups: int):
+        """Create convolutional layers of displacement branch by given
+        parameters."""
+        layers = [
+            ConvModule(
+                in_channels=in_channels,
+                out_channels=num_filters,
+                kernel_size=1,
+                norm_cfg=dict(type='BN')),
+            AdaptiveActivationBlock(num_filters, num_filters, groups=groups),
+            AdaptiveActivationBlock(num_filters, num_filters, groups=groups),
+            build_conv_layer(
+                dict(type='Conv2d'),
+                in_channels=num_filters,
+                out_channels=out_channels,
+                kernel_size=1,
+                groups=groups)
+        ]
 
-        return deconv_head
+        return Sequential(*layers)
 
-    def forward(self, feats: Tuple[Tensor]) -> Tuple[Tensor, Tensor]:
+    def forward(self, feats: Tuple[Tensor]) -> Tensor:
         """Forward the network. The input is multi scale feature maps and the
-        output is the heatmap.
+        output is a tuple of heatmap and displacement.
 
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
-            pred_x (Tensor): 1d representation of x.
-            pred_y (Tensor): 1d representation of y.
+            Tuple[Tensor]: output heatmap and displacement.
         """
-        if self.deconv_head is None:
-            feats = self._transform_inputs(feats)
-            if self.final_layer is not None:
-                feats = self.final_layer(feats)
-        else:
-            feats = self.deconv_head(feats)
+        x = feats[-1]
 
-        # flatten the output heatmap
-        x = torch.flatten(feats, 2)
+        heatmaps = self.heatmap_conv_layers(x)
+        displacements = self.displacement_conv_layers(x)
 
-        simcc_x = self.mlp_head_x(x)
-        simcc_y = self.mlp_head_y(x)
+        return heatmaps, displacements
 
-        pred_x = F.softmax(simcc_x * self.beta, dim=-1)
-        pred_x = (pred_x * self.linspace_x).sum(dim=-1, keepdim=True)
+    def loss(self,
+             feats: Tuple[Tensor],
+             batch_data_samples: OptSampleList,
+             train_cfg: ConfigType = {}) -> dict:
+        """Calculate losses from a batch of inputs and data samples.
 
-        pred_y = F.softmax(simcc_y * self.beta, dim=-1)
-        pred_y = (pred_y * self.linspace_y).sum(dim=-1, keepdim=True)
+        Args:
+            feats (Tuple[Tensor]): The multi-stage features
+            batch_data_samples (List[:obj:`PoseDataSample`]): The batch
+                data samples
+            train_cfg (dict): The runtime config for training process.
+                Defaults to {}
 
-        if self.debias:
-            C_x = simcc_x.exp().sum(dim=-1, keepdim=True)
-            pred_x = C_x / (C_x - 1) * (pred_x - 1 / (2 * C_x))
+        Returns:
+            dict: A dictionary of losses.
+        """
+        pred_heatmaps, pred_displacements = self.forward(feats)
+        gt_heatmaps = torch.stack(
+            [d.gt_fields.heatmaps for d in batch_data_samples])
+        heatmap_weights = torch.stack(
+            [d.gt_fields.heatmap_weights for d in batch_data_samples])
+        gt_displacements = torch.stack(
+            [d.gt_fields.displacements for d in batch_data_samples])
+        displacement_weights = torch.stack(
+            [d.gt_fields.displacement_weights for d in batch_data_samples])
+
+        if 'heatmap_mask' in batch_data_samples[0].gt_fields.keys():
+            heatmap_mask = torch.stack(
+                [d.gt_fields.heatmap_mask for d in batch_data_samples])
+        else:
+            heatmap_mask = None
 
-            C_y = simcc_y.exp().sum(dim=-1, keepdim=True)
-            pred_y = C_x / (C_y - 1) * (pred_y - 1 / (2 * C_y))
+        # calculate losses
+        losses = dict()
+        heatmap_loss = self.loss_module['heatmap'](pred_heatmaps, gt_heatmaps,
+                                                   heatmap_weights,
+                                                   heatmap_mask)
+        displacement_loss = self.loss_module['displacement'](
+            pred_displacements, gt_displacements, displacement_weights)
+
+        losses.update({
+            'loss/heatmap': heatmap_loss,
+            'loss/displacement': displacement_loss,
+        })
 
-        pred = torch.cat([pred_x, pred_y], dim=-1)
-        return pred, simcc_x, simcc_y
+        return losses
 
-    def predict(
-        self,
-        feats: Tuple[Tensor],
-        batch_data_samples: OptSampleList,
-        test_cfg: OptConfigType = {},
-    ) -> InstanceList:
+    def predict(self,
+                feats: Features,
+                batch_data_samples: OptSampleList,
+                test_cfg: ConfigType = {}) -> Predictions:
         """Predict results from features.
 
         Args:
             feats (Tuple[Tensor] | List[Tuple[Tensor]]): The multi-stage
-                features (or multiple multi-stage features in TTA)
+                features (or multiple multi-scale features in TTA)
             batch_data_samples (List[:obj:`PoseDataSample`]): The batch
                 data samples
             test_cfg (dict): The runtime config for testing process. Defaults
                 to {}
 
         Returns:
-            List[InstanceData]: The pose predictions, each contains
+            Union[InstanceList | Tuple[InstanceList | PixelDataList]]: If
+            ``test_cfg['output_heatmap']==True``, return both pose and heatmap
+            prediction; otherwise only return the pose prediction.
+
+            The pose prediction is a list of ``InstanceData``, each contains
             the following fields:
 
                 - keypoints (np.ndarray): predicted keypoint coordinates in
                     shape (num_instances, K, D) where K is the keypoint number
                     and D is the keypoint dimension
                 - keypoint_scores (np.ndarray): predicted keypoint scores in
                     shape (num_instances, K)
-                - keypoint_x_labels (np.ndarray, optional): The predicted 1-D
-                    intensity distribution in the x direction
-                - keypoint_y_labels (np.ndarray, optional): The predicted 1-D
-                    intensity distribution in the y direction
+
+            The heatmap prediction is a list of ``PixelData``, each contains
+            the following fields:
+
+                - heatmaps (Tensor): The predicted heatmaps in shape (1, h, w)
+                    or (K+1, h, w) if keypoint heatmaps are predicted
+                - displacements (Tensor): The predicted displacement fields
+                    in shape (K*2, h, w)
         """
 
-        if test_cfg.get('flip_test', False):
-            # TTA: flip test -> feats = [orig, flipped]
-            assert isinstance(feats, list) and len(feats) == 2
-            flip_indices = batch_data_samples[0].metainfo['flip_indices']
-            _feats, _feats_flip = feats
-
-            _batch_pred_x, _batch_pred_y = self.forward(_feats)
-
-            _batch_pred_x_flip, _batch_pred_y_flip = self.forward(_feats_flip)
-            _batch_pred_x_flip, _batch_pred_y_flip = flip_vectors(
-                _batch_pred_x_flip,
-                _batch_pred_y_flip,
-                flip_indices=flip_indices)
+        assert len(batch_data_samples) == 1, f'DEKRHead only supports ' \
+            f'prediction with batch_size 1, but got {len(batch_data_samples)}'
+
+        multiscale_test = test_cfg.get('multiscale_test', False)
+        flip_test = test_cfg.get('flip_test', False)
+        metainfo = batch_data_samples[0].metainfo
+        aug_scales = [1]
 
-            batch_pred_x = (_batch_pred_x + _batch_pred_x_flip) * 0.5
-            batch_pred_y = (_batch_pred_y + _batch_pred_y_flip) * 0.5
+        if not multiscale_test:
+            feats = [feats]
         else:
-            batch_pred_x, batch_pred_y = self.forward(feats)
+            aug_scales = aug_scales + metainfo['aug_scales']
 
-        preds = self.decode((batch_pred_x, batch_pred_y))
+        heatmaps, displacements = [], []
+        for feat, s in zip(feats, aug_scales):
+            if flip_test:
+                assert isinstance(feat, list) and len(feat) == 2
+                flip_indices = metainfo['flip_indices']
+                _feat, _feat_flip = feat
+                _heatmaps, _displacements = self.forward(_feat)
+                _heatmaps_flip, _displacements_flip = self.forward(_feat_flip)
+
+                _heatmaps_flip = flip_heatmaps(
+                    _heatmaps_flip,
+                    flip_mode='heatmap',
+                    flip_indices=flip_indices + [len(flip_indices)],
+                    shift_heatmap=test_cfg.get('shift_heatmap', False))
+                _heatmaps = (_heatmaps + _heatmaps_flip) / 2.0
+
+                _displacements_flip = flip_heatmaps(
+                    _displacements_flip,
+                    flip_mode='offset',
+                    flip_indices=flip_indices,
+                    shift_heatmap=False)
+
+                # this is a coordinate amendment.
+                x_scale_factor = s * (
+                    metainfo['input_size'][0] / _heatmaps.shape[-1])
+                _displacements_flip[:, ::2] += (x_scale_factor - 1) / (
+                    x_scale_factor)
+                _displacements = (_displacements + _displacements_flip) / 2.0
 
-        if test_cfg.get('output_heatmaps', False):
-            for pred_instances, pred_x, pred_y in zip(preds,
-                                                      to_numpy(batch_pred_x),
-                                                      to_numpy(batch_pred_y)):
+            else:
+                _heatmaps, _displacements = self.forward(feat)
 
-                pred_instances.keypoint_x_labels = pred_x[None]
-                pred_instances.keypoint_y_labels = pred_y[None]
+            heatmaps.append(_heatmaps)
+            displacements.append(_displacements)
 
-        return preds
+        preds = self.decode(heatmaps, displacements, test_cfg, metainfo)
 
-    def loss(
-        self,
-        feats: Tuple[Tensor],
-        batch_data_samples: OptSampleList,
-        train_cfg: OptConfigType = {},
-    ) -> dict:
-        """Calculate losses from a batch of inputs and data samples."""
-
-        pred_x, pred_y = self.forward(feats)
-
-        gt_x = torch.cat([
-            d.gt_instance_labels.keypoint_x_labels for d in batch_data_samples
-        ],
-                         dim=0)
-        gt_y = torch.cat([
-            d.gt_instance_labels.keypoint_y_labels for d in batch_data_samples
-        ],
-                         dim=0)
-        keypoint_weights = torch.cat(
-            [
-                d.gt_instance_labels.keypoint_weights
-                for d in batch_data_samples
-            ],
-            dim=0,
-        )
+        if test_cfg.get('output_heatmaps', False):
+            heatmaps = [hm.detach() for hm in heatmaps]
+            displacements = [dm.detach() for dm in displacements]
+            B = heatmaps[0].shape[0]
+            pred_fields = []
+            for i in range(B):
+                pred_fields.append(
+                    PixelData(
+                        heatmaps=heatmaps[0][i],
+                        displacements=displacements[0][i]))
+            return preds, pred_fields
+        else:
+            return preds
 
-        pred_simcc = (pred_x, pred_y)
-        gt_simcc = (gt_x, gt_y)
+    def decode(self,
+               heatmaps: Tuple[Tensor],
+               displacements: Tuple[Tensor],
+               test_cfg: ConfigType = {},
+               metainfo: dict = {}) -> InstanceList:
+        """Decode keypoints from outputs.
 
-        # calculate losses
-        losses = dict()
-        loss = self.loss_module(pred_simcc, gt_simcc, keypoint_weights)
+        Args:
+            heatmaps (Tuple[Tensor]): The output heatmaps inferred from one
+                image or multi-scale images.
+            displacements (Tuple[Tensor]): The output displacement fields
+                inferred from one image or multi-scale images.
+            test_cfg (dict): The runtime config for testing process. Defaults
+                to {}
+            metainfo (dict): The metainfo of test dataset. Defaults to {}
 
-        losses.update(loss_kpt=loss)
+        Returns:
+            List[InstanceData]: A list of InstanceData, each contains the
+                decoded pose information of the instances of one data sample.
+        """
 
-        # calculate accuracy
-        _, avg_acc, _ = simcc_pck_accuracy(
-            output=to_numpy(pred_simcc),
-            target=to_numpy(gt_simcc),
-            simcc_split_ratio=self.simcc_split_ratio,
-            mask=to_numpy(keypoint_weights) > 0,
-        )
+        if self.decoder is None:
+            raise RuntimeError(
+                f'The decoder has not been set in {self.__class__.__name__}. '
+                'Please set the decoder configs in the init parameters to '
+                'enable head methods `head.predict()` and `head.decode()`')
+
+        multiscale_test = test_cfg.get('multiscale_test', False)
+        skeleton = metainfo.get('skeleton_links', None)
+
+        preds = []
+        batch_size = heatmaps[0].shape[0]
+
+        for b in range(batch_size):
+            if multiscale_test:
+                raise NotImplementedError
+            else:
+                keypoints, (root_scores,
+                            keypoint_scores) = self.decoder.decode(
+                                heatmaps[0][b], displacements[0][b])
+
+            # rescore each instance
+            if self.rescore_net is not None and skeleton and len(
+                    keypoints) > 0:
+                instance_scores = self.rescore_net(keypoints, keypoint_scores,
+                                                   skeleton)
+                instance_scores[torch.isnan(instance_scores)] = 0
+                root_scores = root_scores * instance_scores
+
+            # nms
+            keypoints, keypoint_scores = to_numpy((keypoints, keypoint_scores))
+            scores = to_numpy(root_scores)[..., None] * keypoint_scores
+            if len(keypoints) > 0 and test_cfg.get('nms_dist_thr', 0) > 0:
+                kpts_db = []
+                for i in range(len(keypoints)):
+                    kpts_db.append(
+                        dict(keypoints=keypoints[i], score=keypoint_scores[i]))
+                keep_instance_inds = nearby_joints_nms(
+                    kpts_db,
+                    test_cfg['nms_dist_thr'],
+                    test_cfg.get('nms_joints_thr', None),
+                    score_per_joint=True,
+                    max_dets=test_cfg.get('max_num_people', 30))
+                keypoints = keypoints[keep_instance_inds]
+                scores = scores[keep_instance_inds]
+
+            # pack outputs
+            preds.append(
+                InstanceData(keypoints=keypoints, keypoint_scores=scores))
 
-        acc_pose = torch.tensor(avg_acc, device=gt_x.device)
-        losses.update(acc_pose=acc_pose)
+        return preds
 
-        return losses
+    def _load_state_dict_pre_hook(self, state_dict, prefix, local_meta, *args,
+                                  **kwargs):
+        """A hook function to convert old-version state dict of
+        :class:`DEKRHead` (before MMPose v1.0.0) to a compatible format
+        of :class:`DEKRHead`.
 
-    @property
-    def default_init_cfg(self):
-        init_cfg = [
-            dict(
-                type='Normal', layer=['Conv2d', 'ConvTranspose2d'], std=0.001),
-            dict(type='Constant', layer='BatchNorm2d', val=1),
-            dict(type='Normal', layer=['Linear'], std=0.01, bias=0),
-        ]
-        return init_cfg
+        The hook will be automatically registered during initialization.
+        """
+        version = local_meta.get('version', None)
+        if version and version >= self._version:
+            return
+
+        # convert old-version state dict
+        keys = list(state_dict.keys())
+        for k in keys:
+            if 'offset_conv_layer' in k:
+                v = state_dict.pop(k)
+                k = k.replace('offset_conv_layers', 'displacement_conv_layers')
+                if 'displacement_conv_layers.3.' in k:
+                    # the source and target of displacement vectors are
+                    # opposite between two versions.
+                    v = -v
+                state_dict[k] = v
+
+            if 'heatmap_conv_layers.2' in k:
+                # root heatmap is at the first/last channel of the
+                # heatmap tensor in MMPose v0.x/1.x, respectively.
+                v = state_dict.pop(k)
+                state_dict[k] = torch.cat((v[1:], v[:1]))
+
+            if 'rescore_net' in k:
+                v = state_dict.pop(k)
+                k = k.replace('rescore_net', 'head.rescore_net')
+                state_dict[k] = v
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/mspn_head.py` & `mmpose-1.1.0/mmpose/models/heads/heatmap_heads/mspn_head.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/heatmap_heads/vipnas_head.py` & `mmpose-1.1.0/mmpose/models/heads/heatmap_heads/vipnas_head.py`

 * *Files 26% similar despite different names*

```diff
@@ -35,32 +35,16 @@
             deconv layer. Defaults to ``(16, 16, 16)``
         conv_out_channels (Sequence[int], optional): The output channel number
             of each intermediate conv layer. ``None`` means no intermediate
             conv layer between deconv layers and the final conv layer.
             Defaults to ``None``
         conv_kernel_sizes (Sequence[int | tuple], optional): The kernel size
             of each intermediate conv layer. Defaults to ``None``
-        has_final_layer (bool): Whether have the final 1x1 Conv2d layer.
-            Defaults to ``True``
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | Sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
+        final_layer (dict): Arguments of the final Conv2d layer.
+            Defaults to ``dict(kernel_size=1)``
         loss (Config): Config of the keypoint loss. Defaults to use
             :class:`KeypointMSELoss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
 
@@ -74,46 +58,33 @@
                  in_channels: Union[int, Sequence[int]],
                  out_channels: int,
                  deconv_out_channels: OptIntSeq = (144, 144, 144),
                  deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
                  deconv_num_groups: OptIntSeq = (16, 16, 16),
                  conv_out_channels: OptIntSeq = None,
                  conv_kernel_sizes: OptIntSeq = None,
-                 has_final_layer: bool = True,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
+                 final_layer: dict = dict(kernel_size=1),
                  loss: ConfigType = dict(
                      type='KeypointMSELoss', use_target_weight=True),
                  decoder: OptConfigType = None,
                  init_cfg: OptConfigType = None):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
 
         super(HeatmapHead, self).__init__(init_cfg)
 
         self.in_channels = in_channels
         self.out_channels = out_channels
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
         self.loss_module = MODELS.build(loss)
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
 
-        # Get model input channels according to feature
-        in_channels = self._get_in_channels()
-        if isinstance(in_channels, list):
-            raise ValueError(
-                f'{self.__class__.__name__} does not support selecting '
-                'multiple input features.')
-
         if deconv_out_channels:
             if deconv_kernel_sizes is None or len(deconv_out_channels) != len(
                     deconv_kernel_sizes):
                 raise ValueError(
                     '"deconv_out_channels" and "deconv_kernel_sizes" should '
                     'be integer sequences with the same length. Got '
                     f'mismatched lengths {deconv_out_channels} and '
@@ -149,20 +120,21 @@
                 in_channels=in_channels,
                 layer_out_channels=conv_out_channels,
                 layer_kernel_sizes=conv_kernel_sizes)
             in_channels = conv_out_channels[-1]
         else:
             self.conv_layers = nn.Identity()
 
-        if has_final_layer:
+        if final_layer is not None:
             cfg = dict(
                 type='Conv2d',
                 in_channels=in_channels,
                 out_channels=out_channels,
                 kernel_size=1)
+            cfg.update(final_layer)
             self.final_layer = build_conv_layer(cfg)
         else:
             self.final_layer = nn.Identity()
 
         # Register the hook to automatically convert old version state dicts
         self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/hybrid_heads/dekr_head.py` & `mmpose-1.1.0/mmpose/visualization/local_visualizer_3d.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,605 +1,564 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Sequence, Tuple, Union
+import math
+from typing import Dict, List, Optional, Tuple, Union
 
-import torch
-from mmcv.cnn import (ConvModule, build_activation_layer, build_conv_layer,
-                      build_norm_layer)
-from mmengine.model import BaseModule, ModuleDict, Sequential
-from mmengine.structures import InstanceData, PixelData
-from torch import Tensor
-
-from mmpose.evaluation.functional.nms import nearby_joints_nms
-from mmpose.models.utils.tta import flip_heatmaps
-from mmpose.registry import KEYPOINT_CODECS, MODELS
-from mmpose.utils.tensor_utils import to_numpy
-from mmpose.utils.typing import (ConfigType, Features, InstanceList,
-                                 OptConfigType, OptSampleList, Predictions)
-from ...backbones.resnet import BasicBlock
-from ..base_head import BaseHead
-
-try:
-    from mmcv.ops import DeformConv2d
-    has_mmcv_full = True
-except (ImportError, ModuleNotFoundError):
-    has_mmcv_full = False
-
-
-class AdaptiveActivationBlock(BaseModule):
-    """Adaptive activation convolution block. "Bottom-up human pose estimation
-    via disentangled keypoint regression", CVPR'2021.
+import cv2
+import mmcv
+import numpy as np
+from matplotlib import pyplot as plt
+from mmengine.dist import master_only
+from mmengine.structures import InstanceData
+
+from mmpose.registry import VISUALIZERS
+from mmpose.structures import PoseDataSample
+from . import PoseLocalVisualizer
+
+
+@VISUALIZERS.register_module()
+class Pose3dLocalVisualizer(PoseLocalVisualizer):
+    """MMPose 3d Local Visualizer.
 
     Args:
-        in_channels (int): Number of input channels
-        out_channels (int): Number of output channels
-        groups (int): Number of groups. Generally equal to the
-            number of joints.
-        norm_cfg (dict): Config for normalization layers.
-        act_cfg (dict): Config for activation layers.
-    """
-
-    def __init__(self,
-                 in_channels,
-                 out_channels,
-                 groups=1,
-                 norm_cfg=dict(type='BN'),
-                 act_cfg=dict(type='ReLU'),
-                 init_cfg=None):
-        super(AdaptiveActivationBlock, self).__init__(init_cfg=init_cfg)
-
-        assert in_channels % groups == 0 and out_channels % groups == 0
-        self.groups = groups
-
-        regular_matrix = torch.tensor([[-1, -1, -1, 0, 0, 0, 1, 1, 1],
-                                       [-1, 0, 1, -1, 0, 1, -1, 0, 1],
-                                       [1, 1, 1, 1, 1, 1, 1, 1, 1]])
-        self.register_buffer('regular_matrix', regular_matrix.float())
-
-        self.transform_matrix_conv = build_conv_layer(
-            dict(type='Conv2d'),
-            in_channels=in_channels,
-            out_channels=6 * groups,
-            kernel_size=3,
-            padding=1,
-            groups=groups,
-            bias=True)
-
-        if has_mmcv_full:
-            self.adapt_conv = DeformConv2d(
-                in_channels,
-                out_channels,
-                kernel_size=3,
-                padding=1,
-                bias=False,
-                groups=groups,
-                deform_groups=groups)
-        else:
-            raise ImportError('Please install the full version of mmcv '
-                              'to use `DeformConv2d`.')
-
-        self.norm = build_norm_layer(norm_cfg, out_channels)[1]
-        self.act = build_activation_layer(act_cfg)
-
-    def forward(self, x):
-        B, _, H, W = x.size()
-        residual = x
-
-        affine_matrix = self.transform_matrix_conv(x)
-        affine_matrix = affine_matrix.permute(0, 2, 3, 1).contiguous()
-        affine_matrix = affine_matrix.view(B, H, W, self.groups, 2, 3)
-        offset = torch.matmul(affine_matrix, self.regular_matrix)
-        offset = offset.transpose(4, 5).reshape(B, H, W, self.groups * 18)
-        offset = offset.permute(0, 3, 1, 2).contiguous()
-
-        x = self.adapt_conv(x, offset)
-        x = self.norm(x)
-        x = self.act(x + residual)
-
-        return x
-
-
-class RescoreNet(BaseModule):
-    """Rescore net used to predict the OKS score of predicted pose. We use the
-    off-the-shelf rescore net pretrained by authors of DEKR.
-
-    Args:
-        in_channels (int): Input channels
-        norm_indexes (Tuple(int)): Indices of torso in skeleton
-        init_cfg (dict, optional): Initialization config dict
+        name (str): Name of the instance. Defaults to 'visualizer'.
+        image (np.ndarray, optional): the origin image to draw. The format
+            should be RGB. Defaults to ``None``
+        vis_backends (list, optional): Visual backend config list. Defaults to
+            ``None``
+        save_dir (str, optional): Save file dir for all storage backends.
+            If it is ``None``, the backend storage will not save any data.
+            Defaults to ``None``
+        bbox_color (str, tuple(int), optional): Color of bbox lines.
+            The tuple of color should be in BGR order. Defaults to ``'green'``
+        kpt_color (str, tuple(tuple(int)), optional): Color of keypoints.
+            The tuple of color should be in BGR order. Defaults to ``'red'``
+        link_color (str, tuple(tuple(int)), optional): Color of skeleton.
+            The tuple of color should be in BGR order. Defaults to ``None``
+        line_width (int, float): The width of lines. Defaults to 1
+        radius (int, float): The radius of keypoints. Defaults to 4
+        show_keypoint_weight (bool): Whether to adjust the transparency
+            of keypoints according to their score. Defaults to ``False``
+        alpha (int, float): The transparency of bboxes. Defaults to ``0.8``
+        det_kpt_color (str, tuple(tuple(int)), optional): Keypoints color
+             info for detection. Defaults to ``None``
+        det_dataset_skeleton (list): Skeleton info for detection. Defaults to
+            ``None``
+        det_dataset_link_color (list): Link color for detection. Defaults to
+            ``None``
     """
 
     def __init__(
+            self,
+            name: str = 'visualizer',
+            image: Optional[np.ndarray] = None,
+            vis_backends: Optional[Dict] = None,
+            save_dir: Optional[str] = None,
+            bbox_color: Optional[Union[str, Tuple[int]]] = 'green',
+            kpt_color: Optional[Union[str, Tuple[Tuple[int]]]] = 'red',
+            link_color: Optional[Union[str, Tuple[Tuple[int]]]] = None,
+            text_color: Optional[Union[str, Tuple[int]]] = (255, 255, 255),
+            skeleton: Optional[Union[List, Tuple]] = None,
+            line_width: Union[int, float] = 1,
+            radius: Union[int, float] = 3,
+            show_keypoint_weight: bool = False,
+            backend: str = 'opencv',
+            alpha: float = 0.8,
+            det_kpt_color: Optional[Union[str, Tuple[Tuple[int]]]] = None,
+            det_dataset_skeleton: Optional[Union[str,
+                                                 Tuple[Tuple[int]]]] = None,
+            det_dataset_link_color: Optional[np.ndarray] = None):
+        super().__init__(name, image, vis_backends, save_dir, bbox_color,
+                         kpt_color, link_color, text_color, skeleton,
+                         line_width, radius, show_keypoint_weight, backend,
+                         alpha)
+        self.det_kpt_color = det_kpt_color
+        self.det_dataset_skeleton = det_dataset_skeleton
+        self.det_dataset_link_color = det_dataset_link_color
+
+    def _draw_3d_data_samples(
         self,
-        in_channels,
-        norm_indexes,
-        init_cfg=None,
+        image: np.ndarray,
+        pose_samples: PoseDataSample,
+        draw_gt: bool = True,
+        kpt_thr: float = 0.3,
+        num_instances=-1,
+        axis_azimuth: float = 70,
+        axis_limit: float = 1.7,
+        axis_dist: float = 10.0,
+        axis_elev: float = 15.0,
     ):
-        super(RescoreNet, self).__init__(init_cfg=init_cfg)
-
-        self.norm_indexes = norm_indexes
-
-        hidden = 256
-
-        self.l1 = torch.nn.Linear(in_channels, hidden, bias=True)
-        self.l2 = torch.nn.Linear(hidden, hidden, bias=True)
-        self.l3 = torch.nn.Linear(hidden, 1, bias=True)
-        self.relu = torch.nn.ReLU()
-
-    def make_feature(self, keypoints, keypoint_scores, skeleton):
-        """Combine original scores, joint distance and relative distance to
-        make feature.
+        """Draw keypoints and skeletons (optional) of GT or prediction.
 
         Args:
-            keypoints (torch.Tensor): predicetd keypoints
-            keypoint_scores (torch.Tensor): predicetd keypoint scores
-            skeleton (list(list(int))): joint links
+            image (np.ndarray): The image to draw.
+            instances (:obj:`InstanceData`): Data structure for
+                instance-level annotations or predictions.
+            draw_gt (bool): Whether to draw GT PoseDataSample. Default to
+                ``True``
+            kpt_thr (float, optional): Minimum threshold of keypoints
+                to be shown. Default: 0.3.
+            num_instances (int): Number of instances to be shown in 3D. If
+                smaller than 0, all the instances in the pose_result will be
+                shown. Otherwise, pad or truncate the pose_result to a length
+                of num_instances.
+            axis_azimuth (float): axis azimuth angle for 3D visualizations.
+            axis_dist (float): axis distance for 3D visualizations.
+            axis_elev (float): axis elevation view angle for 3D visualizations.
+            axis_limit (float): The axis limit to visualize 3d pose. The xyz
+                range will be set as:
+                - x: [x_c - axis_limit/2, x_c + axis_limit/2]
+                - y: [y_c - axis_limit/2, y_c + axis_limit/2]
+                - z: [0, axis_limit]
+                Where x_c, y_c is the mean value of x and y coordinates
 
         Returns:
-            torch.Tensor: feature for each instance
+            Tuple(np.ndarray): the drawn image which channel is RGB.
         """
-        joint_1, joint_2 = zip(*skeleton)
-        num_link = len(skeleton)
+        vis_height, vis_width, _ = image.shape
 
-        joint_relate = (keypoints[:, joint_1] -
-                        keypoints[:, joint_2])[:, :, :2]
-        joint_length = joint_relate.norm(dim=2)
-
-        # To use the torso distance to normalize
-        normalize = (joint_length[:, self.norm_indexes[0]] +
-                     joint_length[:, self.norm_indexes[1]]) / 2
-        normalize = normalize.unsqueeze(1).expand(normalize.size(0), num_link)
-        normalize = normalize.clamp(min=1).contiguous()
-
-        joint_length = joint_length / normalize[:, :]
-        joint_relate = joint_relate / normalize.unsqueeze(-1)
-        joint_relate = joint_relate.flatten(1)
-
-        feature = torch.cat((joint_relate, joint_length, keypoint_scores),
-                            dim=1).float()
-        return feature
-
-    def forward(self, keypoints, keypoint_scores, skeleton):
-        feature = self.make_feature(keypoints, keypoint_scores, skeleton)
-        x = self.relu(self.l1(feature))
-        x = self.relu(self.l2(x))
-        x = self.l3(x)
-        return x.squeeze(1)
-
-
-@MODELS.register_module()
-class DEKRHead(BaseHead):
-    """DisEntangled Keypoint Regression head introduced in `Bottom-up human
-    pose estimation via disentangled keypoint regression`_ by Geng et al
-    (2021). The head is composed of a heatmap branch and a displacement branch.
+        if 'pred_instances' in pose_samples:
+            pred_instances = pose_samples.pred_instances
+        else:
+            pred_instances = InstanceData()
+        if num_instances < 0:
+            if 'keypoints' in pred_instances:
+                num_instances = len(pred_instances)
+            else:
+                num_instances = 0
+        else:
+            if len(pred_instances) > num_instances:
+                pred_instances_ = InstanceData()
+                for k in pred_instances.keys():
+                    new_val = pred_instances[k][:num_instances]
+                    pred_instances_.set_field(new_val, k)
+                pred_instances = pred_instances_
+            elif num_instances < len(pred_instances):
+                num_instances = len(pred_instances)
+
+        num_fig = num_instances
+        if draw_gt:
+            vis_width *= 2
+            num_fig *= 2
+
+        plt.ioff()
+        fig = plt.figure(
+            figsize=(vis_width * num_instances * 0.01, vis_height * 0.01))
+
+        def _draw_3d_instances_kpts(keypoints,
+                                    scores,
+                                    keypoints_visible,
+                                    fig_idx,
+                                    title=None):
+
+            for idx, (kpts, score, visible) in enumerate(
+                    zip(keypoints, scores, keypoints_visible)):
+
+                valid = np.logical_and(score >= kpt_thr,
+                                       np.any(~np.isnan(kpts), axis=-1))
+
+                ax = fig.add_subplot(
+                    1, num_fig, fig_idx * (idx + 1), projection='3d')
+                ax.view_init(elev=axis_elev, azim=axis_azimuth)
+                ax.set_zlim3d([0, axis_limit])
+                ax.set_aspect('auto')
+                ax.set_xticks([])
+                ax.set_yticks([])
+                ax.set_zticks([])
+                ax.set_xticklabels([])
+                ax.set_yticklabels([])
+                ax.set_zticklabels([])
+                ax.scatter([0], [0], [0], marker='o', color='red')
+                if title:
+                    ax.set_title(f'{title} ({idx})')
+                ax.dist = axis_dist
+
+                x_c = np.mean(kpts[valid, 0]) if valid.any() else 0
+                y_c = np.mean(kpts[valid, 1]) if valid.any() else 0
+
+                ax.set_xlim3d([x_c - axis_limit / 2, x_c + axis_limit / 2])
+                ax.set_ylim3d([y_c - axis_limit / 2, y_c + axis_limit / 2])
+
+                kpts = np.array(kpts, copy=False)
+
+                if self.kpt_color is None or isinstance(self.kpt_color, str):
+                    kpt_color = [self.kpt_color] * len(kpts)
+                elif len(self.kpt_color) == len(kpts):
+                    kpt_color = self.kpt_color
+                else:
+                    raise ValueError(
+                        f'the length of kpt_color '
+                        f'({len(self.kpt_color)}) does not matches '
+                        f'that of keypoints ({len(kpts)})')
+
+                kpts = kpts[valid]
+                x_3d, y_3d, z_3d = np.split(kpts[:, :3], [1, 2], axis=1)
+
+                kpt_color = kpt_color[valid][..., ::-1] / 255.
+
+                ax.scatter(x_3d, y_3d, z_3d, marker='o', color=kpt_color)
+
+                for kpt_idx in range(len(x_3d)):
+                    ax.text(x_3d[kpt_idx][0], y_3d[kpt_idx][0],
+                            z_3d[kpt_idx][0], str(kpt_idx))
+
+                if self.skeleton is not None and self.link_color is not None:
+                    if self.link_color is None or isinstance(
+                            self.link_color, str):
+                        link_color = [self.link_color] * len(self.skeleton)
+                    elif len(self.link_color) == len(self.skeleton):
+                        link_color = self.link_color
+                    else:
+                        raise ValueError(
+                            f'the length of link_color '
+                            f'({len(self.link_color)}) does not matches '
+                            f'that of skeleton ({len(self.skeleton)})')
+
+                    for sk_id, sk in enumerate(self.skeleton):
+                        sk_indices = [_i for _i in sk]
+                        xs_3d = kpts[sk_indices, 0]
+                        ys_3d = kpts[sk_indices, 1]
+                        zs_3d = kpts[sk_indices, 2]
+                        kpt_score = score[sk_indices]
+                        if kpt_score.min() > kpt_thr:
+                            # matplotlib uses RGB color in [0, 1] value range
+                            _color = link_color[sk_id][::-1] / 255.
+                            ax.plot(
+                                xs_3d, ys_3d, zs_3d, color=_color, zdir='z')
+
+        if 'keypoints' in pred_instances:
+            keypoints = pred_instances.get('keypoints',
+                                           pred_instances.keypoints)
 
-    Args:
-        in_channels (int | Sequence[int]): Number of channels in the input
-            feature map
-        num_joints (int): Number of joints
-        num_heatmap_filters (int): Number of filters for heatmap branch.
-            Defaults to 32
-        num_offset_filters_per_joint (int): Number of filters for each joint
-            in displacement branch. Defaults to 15
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | Sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
-        heatmap_loss (Config): Config of the heatmap loss. Defaults to use
-            :class:`KeypointMSELoss`
-        displacement_loss (Config): Config of the displacement regression loss.
-            Defaults to use :class:`SoftWeightSmoothL1Loss`
-        decoder (Config, optional): The decoder config that controls decoding
-            keypoint coordinates from the network output. Defaults to ``None``
-        rescore_cfg (Config, optional): The config for rescore net which
-            estimates OKS via predicted keypoints and keypoint scores.
-            Defaults to ``None``
-        init_cfg (Config, optional): Config to control the initialization. See
-            :attr:`default_init_cfg` for default settings
+            if 'keypoint_scores' in pred_instances:
+                scores = pred_instances.keypoint_scores
+            else:
+                scores = np.ones(keypoints.shape[:-1])
 
-    .. _`Bottom-up human pose estimation via disentangled keypoint regression`:
-        https://arxiv.org/abs/2104.02300
-    """
+            if 'keypoints_visible' in pred_instances:
+                keypoints_visible = pred_instances.keypoints_visible
+            else:
+                keypoints_visible = np.ones(keypoints.shape[:-1])
 
-    _version = 2
+            _draw_3d_instances_kpts(keypoints, scores, keypoints_visible, 1,
+                                    'Prediction')
 
-    def __init__(self,
-                 in_channels: Union[int, Sequence[int]],
-                 num_keypoints: int,
-                 num_heatmap_filters: int = 32,
-                 num_displacement_filters_per_keypoint: int = 15,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
-                 heatmap_loss: ConfigType = dict(
-                     type='KeypointMSELoss', use_target_weight=True),
-                 displacement_loss: ConfigType = dict(
-                     type='SoftWeightSmoothL1Loss',
-                     use_target_weight=True,
-                     supervise_empty=False),
-                 decoder: OptConfigType = None,
-                 rescore_cfg: OptConfigType = None,
-                 init_cfg: OptConfigType = None):
-
-        if init_cfg is None:
-            init_cfg = self.default_init_cfg
-
-        super().__init__(init_cfg)
-
-        self.in_channels = in_channels
-        self.num_keypoints = num_keypoints
-        self.input_transform = input_transform
-        self.input_index = input_index
-        self.align_corners = align_corners
-
-        in_channels = self._get_in_channels()
-
-        # build heatmap branch
-        self.heatmap_conv_layers = self._make_heatmap_conv_layers(
-            in_channels=in_channels,
-            out_channels=1 + num_keypoints,
-            num_filters=num_heatmap_filters,
-        )
-
-        # build displacement branch
-        self.displacement_conv_layers = self._make_displacement_conv_layers(
-            in_channels=in_channels,
-            out_channels=2 * num_keypoints,
-            num_filters=num_keypoints * num_displacement_filters_per_keypoint,
-            groups=num_keypoints)
-
-        # build losses
-        self.loss_module = ModuleDict(
-            dict(
-                heatmap=MODELS.build(heatmap_loss),
-                displacement=MODELS.build(displacement_loss),
-            ))
-
-        # build decoder
-        if decoder is not None:
-            self.decoder = KEYPOINT_CODECS.build(decoder)
-        else:
-            self.decoder = None
+        if draw_gt and 'gt_instances' in pose_samples:
+            gt_instances = pose_samples.gt_instances
+            if 'lifting_target' in gt_instances:
+                keypoints = gt_instances.get('lifting_target',
+                                             gt_instances.lifting_target)
+                scores = np.ones(keypoints.shape[:-1])
+
+                if 'lifting_target_visible' in gt_instances:
+                    keypoints_visible = gt_instances.lifting_target_visible
+                else:
+                    keypoints_visible = np.ones(keypoints.shape[:-1])
+
+                _draw_3d_instances_kpts(keypoints, scores, keypoints_visible,
+                                        2, 'Ground Truth')
+
+        # convert figure to numpy array
+        fig.tight_layout()
+        fig.canvas.draw()
+
+        pred_img_data = fig.canvas.tostring_rgb()
+        pred_img_data = np.frombuffer(
+            fig.canvas.tostring_rgb(), dtype=np.uint8)
 
-        # build rescore net
-        if rescore_cfg is not None:
-            self.rescore_net = RescoreNet(**rescore_cfg)
+        if not pred_img_data.any():
+            pred_img_data = np.full((vis_height, vis_width, 3), 255)
         else:
-            self.rescore_net = None
-
-        # Register the hook to automatically convert old version state dicts
-        self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)
-
-    @property
-    def default_init_cfg(self):
-        init_cfg = [
-            dict(
-                type='Normal', layer=['Conv2d', 'ConvTranspose2d'], std=0.001),
-            dict(type='Constant', layer='BatchNorm2d', val=1)
-        ]
-        return init_cfg
-
-    def _make_heatmap_conv_layers(self, in_channels: int, out_channels: int,
-                                  num_filters: int):
-        """Create convolutional layers of heatmap branch by given
-        parameters."""
-        layers = [
-            ConvModule(
-                in_channels=in_channels,
-                out_channels=num_filters,
-                kernel_size=1,
-                norm_cfg=dict(type='BN')),
-            BasicBlock(num_filters, num_filters),
-            build_conv_layer(
-                dict(type='Conv2d'),
-                in_channels=num_filters,
-                out_channels=out_channels,
-                kernel_size=1),
-        ]
-
-        return Sequential(*layers)
-
-    def _make_displacement_conv_layers(self, in_channels: int,
-                                       out_channels: int, num_filters: int,
-                                       groups: int):
-        """Create convolutional layers of displacement branch by given
-        parameters."""
-        layers = [
-            ConvModule(
-                in_channels=in_channels,
-                out_channels=num_filters,
-                kernel_size=1,
-                norm_cfg=dict(type='BN')),
-            AdaptiveActivationBlock(num_filters, num_filters, groups=groups),
-            AdaptiveActivationBlock(num_filters, num_filters, groups=groups),
-            build_conv_layer(
-                dict(type='Conv2d'),
-                in_channels=num_filters,
-                out_channels=out_channels,
-                kernel_size=1,
-                groups=groups)
-        ]
-
-        return Sequential(*layers)
-
-    def forward(self, feats: Tuple[Tensor]) -> Tensor:
-        """Forward the network. The input is multi scale feature maps and the
-        output is a tuple of heatmap and displacement.
+            pred_img_data = pred_img_data.reshape(vis_height,
+                                                  vis_width * num_instances,
+                                                  -1)
+
+        plt.close(fig)
+
+        return pred_img_data
+
+    def _draw_instances_kpts(self,
+                             image: np.ndarray,
+                             instances: InstanceData,
+                             kpt_thr: float = 0.3,
+                             show_kpt_idx: bool = False,
+                             skeleton_style: str = 'mmpose'):
+        """Draw keypoints and skeletons (optional) of GT or prediction.
 
         Args:
-            feats (Tuple[Tensor]): Multi scale feature maps.
+            image (np.ndarray): The image to draw.
+            instances (:obj:`InstanceData`): Data structure for
+                instance-level annotations or predictions.
+            kpt_thr (float, optional): Minimum threshold of keypoints
+                to be shown. Default: 0.3.
+            show_kpt_idx (bool): Whether to show the index of keypoints.
+                Defaults to ``False``
+            skeleton_style (str): Skeleton style selection. Defaults to
+                ``'mmpose'``
 
         Returns:
-            Tuple[Tensor]: output heatmap and displacement.
+            np.ndarray: the drawn image which channel is RGB.
         """
-        x = self._transform_inputs(feats)
 
-        heatmaps = self.heatmap_conv_layers(x)
-        displacements = self.displacement_conv_layers(x)
+        self.set_image(image)
+        img_h, img_w, _ = image.shape
 
-        return heatmaps, displacements
+        if 'keypoints' in instances:
+            keypoints = instances.get('transformed_keypoints',
+                                      instances.keypoints)
 
-    def loss(self,
-             feats: Tuple[Tensor],
-             batch_data_samples: OptSampleList,
-             train_cfg: ConfigType = {}) -> dict:
-        """Calculate losses from a batch of inputs and data samples.
-
-        Args:
-            feats (Tuple[Tensor]): The multi-stage features
-            batch_data_samples (List[:obj:`PoseDataSample`]): The batch
-                data samples
-            train_cfg (dict): The runtime config for training process.
-                Defaults to {}
+            if 'keypoint_scores' in instances:
+                scores = instances.keypoint_scores
+            else:
+                scores = np.ones(keypoints.shape[:-1])
 
-        Returns:
-            dict: A dictionary of losses.
-        """
-        pred_heatmaps, pred_displacements = self.forward(feats)
-        gt_heatmaps = torch.stack(
-            [d.gt_fields.heatmaps for d in batch_data_samples])
-        heatmap_weights = torch.stack(
-            [d.gt_fields.heatmap_weights for d in batch_data_samples])
-        gt_displacements = torch.stack(
-            [d.gt_fields.displacements for d in batch_data_samples])
-        displacement_weights = torch.stack(
-            [d.gt_fields.displacement_weights for d in batch_data_samples])
-
-        if 'heatmap_mask' in batch_data_samples[0].gt_fields.keys():
-            heatmap_mask = torch.stack(
-                [d.gt_fields.heatmap_mask for d in batch_data_samples])
-        else:
-            heatmap_mask = None
+            if 'keypoints_visible' in instances:
+                keypoints_visible = instances.keypoints_visible
+            else:
+                keypoints_visible = np.ones(keypoints.shape[:-1])
 
-        # calculate losses
-        losses = dict()
-        heatmap_loss = self.loss_module['heatmap'](pred_heatmaps, gt_heatmaps,
-                                                   heatmap_weights,
-                                                   heatmap_mask)
-        displacement_loss = self.loss_module['displacement'](
-            pred_displacements, gt_displacements, displacement_weights)
-
-        losses.update({
-            'loss/heatmap': heatmap_loss,
-            'loss/displacement': displacement_loss,
-        })
-
-        return losses
-
-    def predict(self,
-                feats: Features,
-                batch_data_samples: OptSampleList,
-                test_cfg: ConfigType = {}) -> Predictions:
-        """Predict results from features.
+            if skeleton_style == 'openpose':
+                keypoints_info = np.concatenate(
+                    (keypoints, scores[..., None], keypoints_visible[...,
+                                                                     None]),
+                    axis=-1)
+                # compute neck joint
+                neck = np.mean(keypoints_info[:, [5, 6]], axis=1)
+                # neck score when visualizing pred
+                neck[:, 2:4] = np.logical_and(
+                    keypoints_info[:, 5, 2:4] > kpt_thr,
+                    keypoints_info[:, 6, 2:4] > kpt_thr).astype(int)
+                new_keypoints_info = np.insert(
+                    keypoints_info, 17, neck, axis=1)
+
+                mmpose_idx = [
+                    17, 6, 8, 10, 7, 9, 12, 14, 16, 13, 15, 2, 1, 4, 3
+                ]
+                openpose_idx = [
+                    1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17
+                ]
+                new_keypoints_info[:, openpose_idx] = \
+                    new_keypoints_info[:, mmpose_idx]
+                keypoints_info = new_keypoints_info
+
+                keypoints, scores, keypoints_visible = keypoints_info[
+                    ..., :2], keypoints_info[..., 2], keypoints_info[..., 3]
+
+            kpt_color = self.kpt_color
+            if self.det_kpt_color is not None:
+                kpt_color = self.det_kpt_color
+
+            for kpts, score, visible in zip(keypoints, scores,
+                                            keypoints_visible):
+                kpts = np.array(kpts, copy=False)
+
+                if kpt_color is None or isinstance(kpt_color, str):
+                    kpt_color = [kpt_color] * len(kpts)
+                elif len(kpt_color) == len(kpts):
+                    kpt_color = kpt_color
+                else:
+                    raise ValueError(f'the length of kpt_color '
+                                     f'({len(kpt_color)}) does not matches '
+                                     f'that of keypoints ({len(kpts)})')
+
+                # draw each point on image
+                for kid, kpt in enumerate(kpts):
+                    if score[kid] < kpt_thr or not visible[
+                            kid] or kpt_color[kid] is None:
+                        # skip the point that should not be drawn
+                        continue
+
+                    color = kpt_color[kid]
+                    if not isinstance(color, str):
+                        color = tuple(int(c) for c in color)
+                    transparency = self.alpha
+                    if self.show_keypoint_weight:
+                        transparency *= max(0, min(1, score[kid]))
+                    self.draw_circles(
+                        kpt,
+                        radius=np.array([self.radius]),
+                        face_colors=color,
+                        edge_colors=color,
+                        alpha=transparency,
+                        line_widths=self.radius)
+                    if show_kpt_idx:
+                        self.draw_texts(
+                            str(kid),
+                            kpt,
+                            colors=color,
+                            font_sizes=self.radius * 3,
+                            vertical_alignments='bottom',
+                            horizontal_alignments='center')
+
+                # draw links
+                skeleton = self.skeleton
+                if self.det_dataset_skeleton is not None:
+                    skeleton = self.det_dataset_skeleton
+                link_color = self.link_color
+                if self.det_dataset_link_color is not None:
+                    link_color = self.det_dataset_link_color
+                if skeleton is not None and link_color is not None:
+                    if link_color is None or isinstance(link_color, str):
+                        link_color = [link_color] * len(skeleton)
+                    elif len(link_color) == len(skeleton):
+                        link_color = link_color
+                    else:
+                        raise ValueError(
+                            f'the length of link_color '
+                            f'({len(link_color)}) does not matches '
+                            f'that of skeleton ({len(skeleton)})')
+
+                    for sk_id, sk in enumerate(skeleton):
+                        pos1 = (int(kpts[sk[0], 0]), int(kpts[sk[0], 1]))
+                        pos2 = (int(kpts[sk[1], 0]), int(kpts[sk[1], 1]))
+                        if not (visible[sk[0]] and visible[sk[1]]):
+                            continue
+
+                        if (pos1[0] <= 0 or pos1[0] >= img_w or pos1[1] <= 0
+                                or pos1[1] >= img_h or pos2[0] <= 0
+                                or pos2[0] >= img_w or pos2[1] <= 0
+                                or pos2[1] >= img_h or score[sk[0]] < kpt_thr
+                                or score[sk[1]] < kpt_thr
+                                or link_color[sk_id] is None):
+                            # skip the link that should not be drawn
+                            continue
+                        X = np.array((pos1[0], pos2[0]))
+                        Y = np.array((pos1[1], pos2[1]))
+                        color = link_color[sk_id]
+                        if not isinstance(color, str):
+                            color = tuple(int(c) for c in color)
+                        transparency = self.alpha
+                        if self.show_keypoint_weight:
+                            transparency *= max(
+                                0, min(1, 0.5 * (score[sk[0]] + score[sk[1]])))
+
+                        if skeleton_style == 'openpose':
+                            mX = np.mean(X)
+                            mY = np.mean(Y)
+                            length = ((Y[0] - Y[1])**2 + (X[0] - X[1])**2)**0.5
+                            angle = math.degrees(
+                                math.atan2(Y[0] - Y[1], X[0] - X[1]))
+                            stickwidth = 2
+                            polygons = cv2.ellipse2Poly(
+                                (int(mX), int(mY)),
+                                (int(length / 2), int(stickwidth)), int(angle),
+                                0, 360, 1)
+
+                            self.draw_polygons(
+                                polygons,
+                                edge_colors=color,
+                                face_colors=color,
+                                alpha=transparency)
+
+                        else:
+                            self.draw_lines(
+                                X, Y, color, line_widths=self.line_width)
+
+        return self.get_image()
+
+    @master_only
+    def add_datasample(self,
+                       name: str,
+                       image: np.ndarray,
+                       data_sample: PoseDataSample,
+                       det_data_sample: Optional[PoseDataSample] = None,
+                       draw_gt: bool = True,
+                       draw_pred: bool = True,
+                       draw_2d: bool = True,
+                       draw_bbox: bool = False,
+                       show_kpt_idx: bool = False,
+                       skeleton_style: str = 'mmpose',
+                       num_instances: int = -1,
+                       show: bool = False,
+                       wait_time: float = 0,
+                       out_file: Optional[str] = None,
+                       kpt_thr: float = 0.3,
+                       step: int = 0) -> None:
+        """Draw datasample and save to all backends.
+
+        - If GT and prediction are plotted at the same time, they are
+        displayed in a stitched image where the left image is the
+        ground truth and the right image is the prediction.
+        - If ``show`` is True, all storage backends are ignored, and
+        the images will be displayed in a local window.
+        - If ``out_file`` is specified, the drawn image will be
+        saved to ``out_file``. t is usually used when the display
+        is not available.
 
         Args:
-            feats (Tuple[Tensor] | List[Tuple[Tensor]]): The multi-stage
-                features (or multiple multi-scale features in TTA)
-            batch_data_samples (List[:obj:`PoseDataSample`]): The batch
-                data samples
-            test_cfg (dict): The runtime config for testing process. Defaults
-                to {}
-
-        Returns:
-            Union[InstanceList | Tuple[InstanceList | PixelDataList]]: If
-            ``test_cfg['output_heatmap']==True``, return both pose and heatmap
-            prediction; otherwise only return the pose prediction.
-
-            The pose prediction is a list of ``InstanceData``, each contains
-            the following fields:
-
-                - keypoints (np.ndarray): predicted keypoint coordinates in
-                    shape (num_instances, K, D) where K is the keypoint number
-                    and D is the keypoint dimension
-                - keypoint_scores (np.ndarray): predicted keypoint scores in
-                    shape (num_instances, K)
-
-            The heatmap prediction is a list of ``PixelData``, each contains
-            the following fields:
-
-                - heatmaps (Tensor): The predicted heatmaps in shape (1, h, w)
-                    or (K+1, h, w) if keypoint heatmaps are predicted
-                - displacements (Tensor): The predicted displacement fields
-                    in shape (K*2, h, w)
+            name (str): The image identifier
+            image (np.ndarray): The image to draw
+            data_sample (:obj:`PoseDataSample`): The 3d data sample
+                to visualize
+            det_data_sample (:obj:`PoseDataSample`, optional): The 2d detection
+                data sample to visualize
+            draw_gt (bool): Whether to draw GT PoseDataSample. Default to
+                ``True``
+            draw_pred (bool): Whether to draw Prediction PoseDataSample.
+                Defaults to ``True``
+            draw_2d (bool): Whether to draw 2d detection results. Defaults to
+                ``True``
+            draw_bbox (bool): Whether to draw bounding boxes. Default to
+                ``False``
+            show_kpt_idx (bool): Whether to show the index of keypoints.
+                Defaults to ``False``
+            skeleton_style (str): Skeleton style selection. Defaults to
+                ``'mmpose'``
+            num_instances (int): Number of instances to be shown in 3D. If
+                smaller than 0, all the instances in the pose_result will be
+                shown. Otherwise, pad or truncate the pose_result to a length
+                of num_instances. Defaults to -1
+            show (bool): Whether to display the drawn image. Default to
+                ``False``
+            wait_time (float): The interval of show (s). Defaults to 0
+            out_file (str): Path to output file. Defaults to ``None``
+            kpt_thr (float, optional): Minimum threshold of keypoints
+                to be shown. Default: 0.3.
+            step (int): Global step value to record. Defaults to 0
         """
 
-        assert len(batch_data_samples) == 1, f'DEKRHead only supports ' \
-            f'prediction with batch_size 1, but got {len(batch_data_samples)}'
+        det_img_data = None
+        gt_img_data = None
 
-        multiscale_test = test_cfg.get('multiscale_test', False)
-        flip_test = test_cfg.get('flip_test', False)
-        metainfo = batch_data_samples[0].metainfo
-        aug_scales = [1]
+        if draw_2d:
+            det_img_data = image.copy()
 
-        if not multiscale_test:
-            feats = [feats]
+            # draw bboxes & keypoints
+            if 'pred_instances' in det_data_sample:
+                det_img_data = self._draw_instances_kpts(
+                    det_img_data, det_data_sample.pred_instances, kpt_thr,
+                    show_kpt_idx, skeleton_style)
+                if draw_bbox:
+                    det_img_data = self._draw_instances_bbox(
+                        det_img_data, det_data_sample.pred_instances)
+
+        pred_img_data = self._draw_3d_data_samples(
+            image.copy(),
+            data_sample,
+            draw_gt=draw_gt,
+            num_instances=num_instances)
+
+        # merge visualization results
+        if det_img_data is not None and gt_img_data is not None:
+            drawn_img = np.concatenate(
+                (det_img_data, pred_img_data, gt_img_data), axis=1)
+        elif det_img_data is not None:
+            drawn_img = np.concatenate((det_img_data, pred_img_data), axis=1)
+        elif gt_img_data is not None:
+            drawn_img = np.concatenate((det_img_data, gt_img_data), axis=1)
         else:
-            aug_scales = aug_scales + metainfo['aug_scales']
+            drawn_img = pred_img_data
 
-        heatmaps, displacements = [], []
-        for feat, s in zip(feats, aug_scales):
-            if flip_test:
-                assert isinstance(feat, list) and len(feat) == 2
-                flip_indices = metainfo['flip_indices']
-                _feat, _feat_flip = feat
-                _heatmaps, _displacements = self.forward(_feat)
-                _heatmaps_flip, _displacements_flip = self.forward(_feat_flip)
-
-                _heatmaps_flip = flip_heatmaps(
-                    _heatmaps_flip,
-                    flip_mode='heatmap',
-                    flip_indices=flip_indices + [len(flip_indices)],
-                    shift_heatmap=test_cfg.get('shift_heatmap', False))
-                _heatmaps = (_heatmaps + _heatmaps_flip) / 2.0
-
-                _displacements_flip = flip_heatmaps(
-                    _displacements_flip,
-                    flip_mode='offset',
-                    flip_indices=flip_indices,
-                    shift_heatmap=False)
-
-                # this is a coordinate amendment.
-                x_scale_factor = s * (
-                    metainfo['input_size'][0] / _heatmaps.shape[-1])
-                _displacements_flip[:, ::2] += (x_scale_factor - 1) / (
-                    x_scale_factor)
-                _displacements = (_displacements + _displacements_flip) / 2.0
+        # It is convenient for users to obtain the drawn image.
+        # For example, the user wants to obtain the drawn image and
+        # save it as a video during video inference.
+        self.set_image(drawn_img)
 
-            else:
-                _heatmaps, _displacements = self.forward(feat)
-
-            heatmaps.append(_heatmaps)
-            displacements.append(_displacements)
-
-        preds = self.decode(heatmaps, displacements, test_cfg, metainfo)
+        if show:
+            self.show(drawn_img, win_name=name, wait_time=wait_time)
 
-        if test_cfg.get('output_heatmaps', False):
-            heatmaps = [hm.detach() for hm in heatmaps]
-            displacements = [dm.detach() for dm in displacements]
-            B = heatmaps[0].shape[0]
-            pred_fields = []
-            for i in range(B):
-                pred_fields.append(
-                    PixelData(
-                        heatmaps=heatmaps[0][i],
-                        displacements=displacements[0][i]))
-            return preds, pred_fields
+        if out_file is not None:
+            mmcv.imwrite(drawn_img[..., ::-1], out_file)
         else:
-            return preds
-
-    def decode(self,
-               heatmaps: Tuple[Tensor],
-               displacements: Tuple[Tensor],
-               test_cfg: ConfigType = {},
-               metainfo: dict = {}) -> InstanceList:
-        """Decode keypoints from outputs.
-
-        Args:
-            heatmaps (Tuple[Tensor]): The output heatmaps inferred from one
-                image or multi-scale images.
-            displacements (Tuple[Tensor]): The output displacement fields
-                inferred from one image or multi-scale images.
-            test_cfg (dict): The runtime config for testing process. Defaults
-                to {}
-            metainfo (dict): The metainfo of test dataset. Defaults to {}
+            # save drawn_img to backends
+            self.add_image(name, drawn_img, step)
 
-        Returns:
-            List[InstanceData]: A list of InstanceData, each contains the
-                decoded pose information of the instances of one data sample.
-        """
-
-        if self.decoder is None:
-            raise RuntimeError(
-                f'The decoder has not been set in {self.__class__.__name__}. '
-                'Please set the decoder configs in the init parameters to '
-                'enable head methods `head.predict()` and `head.decode()`')
-
-        multiscale_test = test_cfg.get('multiscale_test', False)
-        skeleton = metainfo.get('skeleton_links', None)
-
-        preds = []
-        batch_size = heatmaps[0].shape[0]
-
-        for b in range(batch_size):
-            if multiscale_test:
-                raise NotImplementedError
-            else:
-                keypoints, (root_scores,
-                            keypoint_scores) = self.decoder.decode(
-                                heatmaps[0][b], displacements[0][b])
-
-            # rescore each instance
-            if self.rescore_net is not None and skeleton and len(
-                    keypoints) > 0:
-                instance_scores = self.rescore_net(keypoints, keypoint_scores,
-                                                   skeleton)
-                instance_scores[torch.isnan(instance_scores)] = 0
-                root_scores = root_scores * instance_scores
-
-            # nms
-            keypoints, keypoint_scores = to_numpy((keypoints, keypoint_scores))
-            scores = to_numpy(root_scores)[..., None] * keypoint_scores
-            if len(keypoints) > 0 and test_cfg.get('nms_dist_thr', 0) > 0:
-                kpts_db = []
-                for i in range(len(keypoints)):
-                    kpts_db.append(
-                        dict(keypoints=keypoints[i], score=keypoint_scores[i]))
-                keep_instance_inds = nearby_joints_nms(
-                    kpts_db,
-                    test_cfg['nms_dist_thr'],
-                    test_cfg.get('nms_joints_thr', None),
-                    score_per_joint=True,
-                    max_dets=test_cfg.get('max_num_people', 30))
-                keypoints = keypoints[keep_instance_inds]
-                scores = scores[keep_instance_inds]
-
-            # pack outputs
-            preds.append(
-                InstanceData(keypoints=keypoints, keypoint_scores=scores))
-
-        return preds
-
-    def _load_state_dict_pre_hook(self, state_dict, prefix, local_meta, *args,
-                                  **kwargs):
-        """A hook function to convert old-version state dict of
-        :class:`DEKRHead` (before MMPose v1.0.0) to a compatible format
-        of :class:`DEKRHead`.
-
-        The hook will be automatically registered during initialization.
-        """
-        version = local_meta.get('version', None)
-        if version and version >= self._version:
-            return
-
-        # convert old-version state dict
-        keys = list(state_dict.keys())
-        for k in keys:
-            if 'offset_conv_layer' in k:
-                v = state_dict.pop(k)
-                k = k.replace('offset_conv_layers', 'displacement_conv_layers')
-                if 'displacement_conv_layers.3.' in k:
-                    # the source and target of displacement vectors are
-                    # opposite between two versions.
-                    v = -v
-                state_dict[k] = v
-
-            if 'heatmap_conv_layers.2' in k:
-                # root heatmap is at the first/last channel of the
-                # heatmap tensor in MMPose v0.x/1.x, respectively.
-                v = state_dict.pop(k)
-                state_dict[k] = torch.cat((v[1:], v[:1]))
-
-            if 'rescore_net' in k:
-                v = state_dict.pop(k)
-                k = k.replace('rescore_net', 'head.rescore_net')
-                state_dict[k] = v
+        return self.get_image()
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/dsnt_head.py` & `mmpose-1.1.0/mmpose/models/heads/regression_heads/dsnt_head.py`

 * *Files 15% similar despite different names*

```diff
@@ -44,30 +44,16 @@
             ``(4, 4, 4)``
         conv_out_channels (sequence[int], optional): The output channel number
             of each intermediate conv layer. ``None`` means no intermediate
             conv layer between deconv layers and the final conv layer.
             Defaults to ``None``
         conv_kernel_sizes (sequence[int | tuple], optional): The kernel size
             of each intermediate conv layer. Defaults to ``None``
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
+        final_layer (dict): Arguments of the final Conv2d layer.
+            Defaults to ``dict(kernel_size=1)``
         loss (Config): Config for keypoint loss. Defaults to use
             :class:`DSNTLoss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
 
@@ -83,18 +69,15 @@
                  lambda_t: int = -1,
                  debias: bool = False,
                  beta: float = 1.0,
                  deconv_out_channels: OptIntSeq = (256, 256, 256),
                  deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
                  conv_out_channels: OptIntSeq = None,
                  conv_kernel_sizes: OptIntSeq = None,
-                 has_final_layer: bool = True,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
+                 final_layer: dict = dict(kernel_size=1),
                  loss: ConfigType = dict(
                      type='MultipleLossWrapper',
                      losses=[
                          dict(type='SmoothL1Loss', use_target_weight=True),
                          dict(type='JSDiscretLoss', use_target_weight=True)
                      ]),
                  decoder: OptConfigType = None,
@@ -106,18 +89,15 @@
             num_joints=num_joints,
             debias=debias,
             beta=beta,
             deconv_out_channels=deconv_out_channels,
             deconv_kernel_sizes=deconv_kernel_sizes,
             conv_out_channels=conv_out_channels,
             conv_kernel_sizes=conv_kernel_sizes,
-            has_final_layer=has_final_layer,
-            input_transform=input_transform,
-            input_index=input_index,
-            align_corners=align_corners,
+            final_layer=final_layer,
             loss=loss,
             decoder=decoder,
             init_cfg=init_cfg)
 
         self.lambda_t = lambda_t
 
     def loss(self,
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/integral_regression_head.py` & `mmpose-1.1.0/mmpose/models/heads/regression_heads/integral_regression_head.py`

 * *Files 12% similar despite different names*

```diff
@@ -47,30 +47,16 @@
             ``(4, 4, 4)``
         conv_out_channels (sequence[int], optional): The output channel number
             of each intermediate conv layer. ``None`` means no intermediate
             conv layer between deconv layers and the final conv layer.
             Defaults to ``None``
         conv_kernel_sizes (sequence[int | tuple], optional): The kernel size
             of each intermediate conv layer. Defaults to ``None``
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
+        final_layer (dict): Arguments of the final Conv2d layer.
+            Defaults to ``dict(kernel_size=1)``
         loss (Config): Config for keypoint loss. Defaults to use
             :class:`SmoothL1Loss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
 
@@ -86,35 +72,29 @@
                  num_joints: int,
                  debias: bool = False,
                  beta: float = 1.0,
                  deconv_out_channels: OptIntSeq = (256, 256, 256),
                  deconv_kernel_sizes: OptIntSeq = (4, 4, 4),
                  conv_out_channels: OptIntSeq = None,
                  conv_kernel_sizes: OptIntSeq = None,
-                 has_final_layer: bool = True,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
+                 final_layer: dict = dict(kernel_size=1),
                  loss: ConfigType = dict(
                      type='SmoothL1Loss', use_target_weight=True),
                  decoder: OptConfigType = None,
                  init_cfg: OptConfigType = None):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
 
         super().__init__(init_cfg)
 
         self.in_channels = in_channels
         self.num_joints = num_joints
         self.debias = debias
         self.beta = beta
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
         self.loss_module = MODELS.build(loss)
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
 
         num_deconv = len(deconv_out_channels) if deconv_out_channels else 0
@@ -127,48 +107,36 @@
             self.simplebaseline_head = HeatmapHead(
                 in_channels=in_channels,
                 out_channels=num_joints,
                 deconv_out_channels=deconv_out_channels,
                 deconv_kernel_sizes=deconv_kernel_sizes,
                 conv_out_channels=conv_out_channels,
                 conv_kernel_sizes=conv_kernel_sizes,
-                has_final_layer=has_final_layer,
-                input_transform=input_transform,
-                input_index=input_index,
-                align_corners=align_corners)
+                final_layer=final_layer)
 
-            if has_final_layer:
+            if final_layer is not None:
                 in_channels = num_joints
             else:
                 in_channels = deconv_out_channels[-1]
 
         else:
-            in_channels = self._get_in_channels()
             self.simplebaseline_head = None
 
-            if has_final_layer:
+            if final_layer is not None:
                 cfg = dict(
                     type='Conv2d',
                     in_channels=in_channels,
                     out_channels=num_joints,
                     kernel_size=1)
+                cfg.update(final_layer)
                 self.final_layer = build_conv_layer(cfg)
             else:
                 self.final_layer = None
 
-            if self.input_transform == 'resize_concat':
-                if isinstance(in_featuremap_size, tuple):
-                    self.heatmap_size = in_featuremap_size
-                elif isinstance(in_featuremap_size, list):
-                    self.heatmap_size = in_featuremap_size[0]
-            elif self.input_transform == 'select':
-                if isinstance(in_featuremap_size, tuple):
-                    self.heatmap_size = in_featuremap_size
-                elif isinstance(in_featuremap_size, list):
-                    self.heatmap_size = in_featuremap_size[input_index]
+            self.heatmap_size = in_featuremap_size
 
         if isinstance(in_channels, list):
             raise ValueError(
                 f'{self.__class__.__name__} does not support selecting '
                 'multiple input features.')
 
         W, H = self.heatmap_size
@@ -207,15 +175,15 @@
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
             Tensor: output coordinates(and sigmas[optional]).
         """
         if self.simplebaseline_head is None:
-            feats = self._transform_inputs(feats)
+            feats = feats[-1]
             if self.final_layer is not None:
                 feats = self.final_layer(feats)
         else:
             feats = self.simplebaseline_head(feats)
 
         heatmaps = self._flat_softmax(feats * self.beta)
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/regression_head.py` & `mmpose-1.1.0/mmpose/models/heads/regression_heads/rle_head.py`

 * *Files 13% similar despite different names*

```diff
@@ -13,126 +13,106 @@
                                  Predictions)
 from ..base_head import BaseHead
 
 OptIntSeq = Optional[Sequence[int]]
 
 
 @MODELS.register_module()
-class RegressionHead(BaseHead):
-    """Top-down regression head introduced in `Deeppose`_ by Toshev et al
-    (2014). The head is composed of fully-connected layers to predict the
-    coordinates directly.
+class RLEHead(BaseHead):
+    """Top-down regression head introduced in `RLE`_ by Li et al(2021). The
+    head is composed of fully-connected layers to predict the coordinates and
+    sigma(the variance of the coordinates) together.
 
     Args:
         in_channels (int | sequence[int]): Number of input channels
         num_joints (int): Number of joints
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
         loss (Config): Config for keypoint loss. Defaults to use
-            :class:`SmoothL1Loss`
+            :class:`RLELoss`
         decoder (Config, optional): The decoder config that controls decoding
             keypoint coordinates from the network output. Defaults to ``None``
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
 
-    .. _`Deeppose`: https://arxiv.org/abs/1312.4659
+    .. _`RLE`: https://arxiv.org/abs/2107.11291
     """
 
     _version = 2
 
     def __init__(self,
                  in_channels: Union[int, Sequence[int]],
                  num_joints: int,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
                  loss: ConfigType = dict(
-                     type='SmoothL1Loss', use_target_weight=True),
+                     type='RLELoss', use_target_weight=True),
                  decoder: OptConfigType = None,
                  init_cfg: OptConfigType = None):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
 
         super().__init__(init_cfg)
 
         self.in_channels = in_channels
         self.num_joints = num_joints
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
         self.loss_module = MODELS.build(loss)
         if decoder is not None:
             self.decoder = KEYPOINT_CODECS.build(decoder)
         else:
             self.decoder = None
 
-        # Get model input channels according to feature
-        in_channels = self._get_in_channels()
-        if isinstance(in_channels, list):
-            raise ValueError(
-                f'{self.__class__.__name__} does not support selecting '
-                'multiple input features.')
-
         # Define fully-connected layers
-        self.fc = nn.Linear(in_channels, self.num_joints * 2)
+        self.fc = nn.Linear(in_channels, self.num_joints * 4)
+
+        # Register the hook to automatically convert old version state dicts
+        self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)
 
     def forward(self, feats: Tuple[Tensor]) -> Tensor:
         """Forward the network. The input is multi scale feature maps and the
         output is the coordinates.
 
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
             Tensor: output coordinates(and sigmas[optional]).
         """
-        x = self._transform_inputs(feats)
+        x = feats[-1]
 
         x = torch.flatten(x, 1)
         x = self.fc(x)
 
-        return x.reshape(-1, self.num_joints, 2)
+        return x.reshape(-1, self.num_joints, 4)
 
     def predict(self,
                 feats: Tuple[Tensor],
                 batch_data_samples: OptSampleList,
                 test_cfg: ConfigType = {}) -> Predictions:
         """Predict results from outputs."""
 
         if test_cfg.get('flip_test', False):
             # TTA: flip test -> feats = [orig, flipped]
             assert isinstance(feats, list) and len(feats) == 2
             flip_indices = batch_data_samples[0].metainfo['flip_indices']
             input_size = batch_data_samples[0].metainfo['input_size']
+
             _feats, _feats_flip = feats
 
             _batch_coords = self.forward(_feats)
+            _batch_coords[..., 2:] = _batch_coords[..., 2:].sigmoid()
+
             _batch_coords_flip = flip_coordinates(
                 self.forward(_feats_flip),
                 flip_indices=flip_indices,
                 shift_coords=test_cfg.get('shift_coords', True),
                 input_size=input_size)
+            _batch_coords_flip[..., 2:] = _batch_coords_flip[..., 2:].sigmoid()
+
             batch_coords = (_batch_coords + _batch_coords_flip) * 0.5
         else:
             batch_coords = self.forward(feats)  # (B, K, D)
+            batch_coords[..., 2:] = batch_coords[..., 2:].sigmoid()
 
         batch_coords.unsqueeze_(dim=1)  # (B, N, K, D)
         preds = self.decode(batch_coords)
 
         return preds
 
     def loss(self,
@@ -145,31 +125,63 @@
 
         keypoint_labels = torch.cat(
             [d.gt_instance_labels.keypoint_labels for d in batch_data_samples])
         keypoint_weights = torch.cat([
             d.gt_instance_labels.keypoint_weights for d in batch_data_samples
         ])
 
+        pred_coords = pred_outputs[:, :, :2]
+        pred_sigma = pred_outputs[:, :, 2:4]
+
         # calculate losses
         losses = dict()
-        loss = self.loss_module(pred_outputs, keypoint_labels,
+        loss = self.loss_module(pred_coords, pred_sigma, keypoint_labels,
                                 keypoint_weights.unsqueeze(-1))
 
         losses.update(loss_kpt=loss)
 
         # calculate accuracy
         _, avg_acc, _ = keypoint_pck_accuracy(
-            pred=to_numpy(pred_outputs),
+            pred=to_numpy(pred_coords),
             gt=to_numpy(keypoint_labels),
             mask=to_numpy(keypoint_weights) > 0,
             thr=0.05,
-            norm_factor=np.ones((pred_outputs.size(0), 2), dtype=np.float32))
+            norm_factor=np.ones((pred_coords.size(0), 2), dtype=np.float32))
 
         acc_pose = torch.tensor(avg_acc, device=keypoint_labels.device)
         losses.update(acc_pose=acc_pose)
 
         return losses
 
+    def _load_state_dict_pre_hook(self, state_dict, prefix, local_meta, *args,
+                                  **kwargs):
+        """A hook function to convert old-version state dict of
+        :class:`TopdownHeatmapSimpleHead` (before MMPose v1.0.0) to a
+        compatible format of :class:`HeatmapHead`.
+
+        The hook will be automatically registered during initialization.
+        """
+
+        version = local_meta.get('version', None)
+        if version and version >= self._version:
+            return
+
+        # convert old-version state dict
+        keys = list(state_dict.keys())
+        for _k in keys:
+            v = state_dict.pop(_k)
+            k = _k.lstrip(prefix)
+            # In old version, "loss" includes the instances of loss,
+            # now it should be renamed "loss_module"
+            k_parts = k.split('.')
+            if k_parts[0] == 'loss':
+                # loss.xxx -> loss_module.xxx
+                k_new = prefix + 'loss_module.' + '.'.join(k_parts[1:])
+            else:
+                k_new = _k
+
+            state_dict[k_new] = v
+
     @property
     def default_init_cfg(self):
         init_cfg = [dict(type='Normal', layer=['Linear'], std=0.01, bias=0)]
         return init_cfg
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/heads/regression_heads/rle_head.py` & `mmpose-1.1.0/mmpose/models/heads/hybrid_heads/vis_head.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,216 +1,229 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from typing import Optional, Sequence, Tuple, Union
+from typing import Tuple, Union
 
-import numpy as np
 import torch
 from torch import Tensor, nn
 
-from mmpose.evaluation.functional import keypoint_pck_accuracy
-from mmpose.models.utils.tta import flip_coordinates
-from mmpose.registry import KEYPOINT_CODECS, MODELS
+from mmpose.models.utils.tta import flip_visibility
+from mmpose.registry import MODELS
 from mmpose.utils.tensor_utils import to_numpy
-from mmpose.utils.typing import (ConfigType, OptConfigType, OptSampleList,
-                                 Predictions)
+from mmpose.utils.typing import (ConfigType, InstanceList, OptConfigType,
+                                 OptSampleList, Predictions)
 from ..base_head import BaseHead
 
-OptIntSeq = Optional[Sequence[int]]
-
 
 @MODELS.register_module()
-class RLEHead(BaseHead):
-    """Top-down regression head introduced in `RLE`_ by Li et al(2021). The
-    head is composed of fully-connected layers to predict the coordinates and
-    sigma(the variance of the coordinates) together.
+class VisPredictHead(BaseHead):
+    """VisPredictHead must be used together with other heads. It can predict
+    keypoints coordinates of and their visibility simultaneously. In the
+    current version, it only supports top-down approaches.
 
     Args:
-        in_channels (int | sequence[int]): Number of input channels
-        num_joints (int): Number of joints
-        input_transform (str): Transformation of input features which should
-            be one of the following options:
-
-                - ``'resize_concat'``: Resize multiple feature maps specified
-                    by ``input_index`` to the same size as the first one and
-                    concat these feature maps
-                - ``'select'``: Select feature map(s) specified by
-                    ``input_index``. Multiple selected features will be
-                    bundled into a tuple
-
-            Defaults to ``'select'``
-        input_index (int | sequence[int]): The feature map index used in the
-            input transformation. See also ``input_transform``. Defaults to -1
-        align_corners (bool): `align_corners` argument of
-            :func:`torch.nn.functional.interpolate` used in the input
-            transformation. Defaults to ``False``
-        loss (Config): Config for keypoint loss. Defaults to use
-            :class:`RLELoss`
-        decoder (Config, optional): The decoder config that controls decoding
-            keypoint coordinates from the network output. Defaults to ``None``
+        pose_cfg (Config): Config to construct keypoints prediction head
+        loss (Config): Config for visibility loss. Defaults to use
+            :class:`BCELoss`
+        use_sigmoid (bool): Whether to use sigmoid activation function
         init_cfg (Config, optional): Config to control the initialization. See
             :attr:`default_init_cfg` for default settings
-
-    .. _`RLE`: https://arxiv.org/abs/2107.11291
     """
 
-    _version = 2
-
     def __init__(self,
-                 in_channels: Union[int, Sequence[int]],
-                 num_joints: int,
-                 input_transform: str = 'select',
-                 input_index: Union[int, Sequence[int]] = -1,
-                 align_corners: bool = False,
+                 pose_cfg: ConfigType,
                  loss: ConfigType = dict(
-                     type='RLELoss', use_target_weight=True),
-                 decoder: OptConfigType = None,
+                     type='BCELoss', use_target_weight=False,
+                     with_logits=True),
+                 use_sigmoid: bool = False,
                  init_cfg: OptConfigType = None):
 
         if init_cfg is None:
             init_cfg = self.default_init_cfg
 
         super().__init__(init_cfg)
 
-        self.in_channels = in_channels
-        self.num_joints = num_joints
-        self.align_corners = align_corners
-        self.input_transform = input_transform
-        self.input_index = input_index
-        self.loss_module = MODELS.build(loss)
-        if decoder is not None:
-            self.decoder = KEYPOINT_CODECS.build(decoder)
+        self.in_channels = pose_cfg['in_channels']
+        if pose_cfg.get('num_joints', None) is not None:
+            self.out_channels = pose_cfg['num_joints']
+        elif pose_cfg.get('out_channels', None) is not None:
+            self.out_channels = pose_cfg['out_channels']
         else:
-            self.decoder = None
+            raise ValueError('VisPredictHead requires \'num_joints\' or'
+                             ' \'out_channels\' in the pose_cfg.')
+
+        self.loss_module = MODELS.build(loss)
 
-        # Get model input channels according to feature
-        in_channels = self._get_in_channels()
-        if isinstance(in_channels, list):
-            raise ValueError(
-                f'{self.__class__.__name__} does not support selecting '
-                'multiple input features.')
+        self.pose_head = MODELS.build(pose_cfg)
+        self.pose_cfg = pose_cfg
 
-        # Define fully-connected layers
-        self.fc = nn.Linear(in_channels, self.num_joints * 4)
+        self.use_sigmoid = use_sigmoid
 
-        # Register the hook to automatically convert old version state dicts
-        self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)
+        modules = [
+            nn.AdaptiveAvgPool2d(1),
+            nn.Flatten(),
+            nn.Linear(self.in_channels, self.out_channels)
+        ]
+        if use_sigmoid:
+            modules.append(nn.Sigmoid())
+
+        self.vis_head = nn.Sequential(*modules)
+
+    def vis_forward(self, feats: Tuple[Tensor]):
+        """Forward the vis_head. The input is multi scale feature maps and the
+        output is coordinates visibility.
 
-    def forward(self, feats: Tuple[Tensor]) -> Tensor:
+        Args:
+            feats (Tuple[Tensor]): Multi scale feature maps.
+
+        Returns:
+            Tensor: output coordinates visibility.
+        """
+        x = feats[-1]
+        while len(x.shape) < 4:
+            x.unsqueeze_(-1)
+        x = self.vis_head(x)
+        return x.reshape(-1, self.out_channels)
+
+    def forward(self, feats: Tuple[Tensor]):
         """Forward the network. The input is multi scale feature maps and the
-        output is the coordinates.
+        output is coordinates and coordinates visibility.
 
         Args:
             feats (Tuple[Tensor]): Multi scale feature maps.
 
         Returns:
-            Tensor: output coordinates(and sigmas[optional]).
+            Tuple[Tensor]: output coordinates and coordinates visibility.
         """
-        x = self._transform_inputs(feats)
+        x_pose = self.pose_head.forward(feats)
+        x_vis = self.vis_forward(feats)
+
+        return x_pose, x_vis
 
-        x = torch.flatten(x, 1)
-        x = self.fc(x)
+    def integrate(self, batch_vis: Tensor,
+                  pose_preds: Union[Tuple, Predictions]) -> InstanceList:
+        """Add keypoints visibility prediction to pose prediction.
 
-        return x.reshape(-1, self.num_joints, 4)
+        Overwrite the original keypoint_scores.
+        """
+        if isinstance(pose_preds, tuple):
+            pose_pred_instances, pose_pred_fields = pose_preds
+        else:
+            pose_pred_instances = pose_preds
+            pose_pred_fields = None
+
+        batch_vis_np = to_numpy(batch_vis, unzip=True)
+
+        assert len(pose_pred_instances) == len(batch_vis_np)
+        for index, _ in enumerate(pose_pred_instances):
+            pose_pred_instances[index].keypoint_scores = batch_vis_np[index]
+
+        return pose_pred_instances, pose_pred_fields
 
     def predict(self,
                 feats: Tuple[Tensor],
                 batch_data_samples: OptSampleList,
                 test_cfg: ConfigType = {}) -> Predictions:
-        """Predict results from outputs."""
+        """Predict results from features.
 
+        Args:
+            feats (Tuple[Tensor] | List[Tuple[Tensor]]): The multi-stage
+                features (or multiple multi-stage features in TTA)
+            batch_data_samples (List[:obj:`PoseDataSample`]): The batch
+                data samples
+            test_cfg (dict): The runtime config for testing process. Defaults
+                to {}
+
+        Returns:
+            Union[InstanceList | Tuple[InstanceList | PixelDataList]]: If
+            posehead's ``test_cfg['output_heatmap']==True``, return both
+            pose and heatmap prediction; otherwise only return the pose
+            prediction.
+
+            The pose prediction is a list of ``InstanceData``, each contains
+            the following fields:
+
+                - keypoints (np.ndarray): predicted keypoint coordinates in
+                    shape (num_instances, K, D) where K is the keypoint number
+                    and D is the keypoint dimension
+                - keypoint_scores (np.ndarray): predicted keypoint scores in
+                    shape (num_instances, K)
+                - keypoint_visibility (np.ndarray): predicted keypoints
+                    visibility in shape (num_instances, K)
+
+            The heatmap prediction is a list of ``PixelData``, each contains
+            the following fields:
+
+                - heatmaps (Tensor): The predicted heatmaps in shape (K, h, w)
+        """
         if test_cfg.get('flip_test', False):
             # TTA: flip test -> feats = [orig, flipped]
             assert isinstance(feats, list) and len(feats) == 2
             flip_indices = batch_data_samples[0].metainfo['flip_indices']
-            input_size = batch_data_samples[0].metainfo['input_size']
-
             _feats, _feats_flip = feats
 
-            _batch_coords = self.forward(_feats)
-            _batch_coords[..., 2:] = _batch_coords[..., 2:].sigmoid()
-
-            _batch_coords_flip = flip_coordinates(
-                self.forward(_feats_flip),
-                flip_indices=flip_indices,
-                shift_coords=test_cfg.get('shift_coords', True),
-                input_size=input_size)
-            _batch_coords_flip[..., 2:] = _batch_coords_flip[..., 2:].sigmoid()
-
-            batch_coords = (_batch_coords + _batch_coords_flip) * 0.5
+            _batch_vis = self.vis_forward(_feats)
+            _batch_vis_flip = flip_visibility(
+                self.vis_forward(_feats_flip), flip_indices=flip_indices)
+            batch_vis = (_batch_vis + _batch_vis_flip) * 0.5
         else:
-            batch_coords = self.forward(feats)  # (B, K, D)
-            batch_coords[..., 2:] = batch_coords[..., 2:].sigmoid()
+            batch_vis = self.vis_forward(feats)  # (B, K, D)
+
+        batch_vis.unsqueeze_(dim=1)  # (B, N, K, D)
 
-        batch_coords.unsqueeze_(dim=1)  # (B, N, K, D)
-        preds = self.decode(batch_coords)
+        if not self.use_sigmoid:
+            batch_vis = torch.sigmoid(batch_vis)
 
-        return preds
+        batch_pose = self.pose_head.predict(feats, batch_data_samples,
+                                            test_cfg)
+
+        return self.integrate(batch_vis, batch_pose)
+
+    def vis_accuracy(self, vis_pred_outputs, vis_labels):
+        """Calculate visibility prediction accuracy."""
+        probabilities = torch.sigmoid(torch.flatten(vis_pred_outputs))
+        threshold = 0.5
+        predictions = (probabilities >= threshold).int()
+        labels = torch.flatten(vis_labels)
+        correct = torch.sum(predictions == labels).item()
+        accuracy = correct / len(labels)
+        return torch.tensor(accuracy)
 
     def loss(self,
-             inputs: Tuple[Tensor],
+             feats: Tuple[Tensor],
              batch_data_samples: OptSampleList,
-             train_cfg: ConfigType = {}) -> dict:
-        """Calculate losses from a batch of inputs and data samples."""
+             train_cfg: OptConfigType = {}) -> dict:
+        """Calculate losses from a batch of inputs and data samples.
 
-        pred_outputs = self.forward(inputs)
+        Args:
+            feats (Tuple[Tensor]): The multi-stage features
+            batch_data_samples (List[:obj:`PoseDataSample`]): The batch
+                data samples
+            train_cfg (dict): The runtime config for training process.
+                Defaults to {}
 
-        keypoint_labels = torch.cat(
-            [d.gt_instance_labels.keypoint_labels for d in batch_data_samples])
-        keypoint_weights = torch.cat([
+        Returns:
+            dict: A dictionary of losses.
+        """
+        vis_pred_outputs = self.vis_forward(feats)
+        vis_labels = torch.cat([
             d.gt_instance_labels.keypoint_weights for d in batch_data_samples
         ])
 
-        pred_coords = pred_outputs[:, :, :2]
-        pred_sigma = pred_outputs[:, :, 2:4]
-
-        # calculate losses
+        # calculate vis losses
         losses = dict()
-        loss = self.loss_module(pred_coords, pred_sigma, keypoint_labels,
-                                keypoint_weights.unsqueeze(-1))
+        loss_vis = self.loss_module(vis_pred_outputs, vis_labels)
 
-        losses.update(loss_kpt=loss)
+        losses.update(loss_vis=loss_vis)
 
-        # calculate accuracy
-        _, avg_acc, _ = keypoint_pck_accuracy(
-            pred=to_numpy(pred_coords),
-            gt=to_numpy(keypoint_labels),
-            mask=to_numpy(keypoint_weights) > 0,
-            thr=0.05,
-            norm_factor=np.ones((pred_coords.size(0), 2), dtype=np.float32))
+        # calculate vis accuracy
+        acc_vis = self.vis_accuracy(vis_pred_outputs, vis_labels)
+        losses.update(acc_vis=acc_vis)
 
-        acc_pose = torch.tensor(avg_acc, device=keypoint_labels.device)
-        losses.update(acc_pose=acc_pose)
+        # calculate keypoints losses
+        loss_kpt = self.pose_head.loss(feats, batch_data_samples)
+        losses.update(loss_kpt)
 
         return losses
 
-    def _load_state_dict_pre_hook(self, state_dict, prefix, local_meta, *args,
-                                  **kwargs):
-        """A hook function to convert old-version state dict of
-        :class:`TopdownHeatmapSimpleHead` (before MMPose v1.0.0) to a
-        compatible format of :class:`HeatmapHead`.
-
-        The hook will be automatically registered during initialization.
-        """
-
-        version = local_meta.get('version', None)
-        if version and version >= self._version:
-            return
-
-        # convert old-version state dict
-        keys = list(state_dict.keys())
-        for _k in keys:
-            v = state_dict.pop(_k)
-            k = _k.lstrip(prefix)
-            # In old version, "loss" includes the instances of loss,
-            # now it should be renamed "loss_module"
-            k_parts = k.split('.')
-            if k_parts[0] == 'loss':
-                # loss.xxx -> loss_module.xxx
-                k_new = prefix + 'loss_module.' + '.'.join(k_parts[1:])
-            else:
-                k_new = _k
-
-            state_dict[k_new] = v
-
     @property
     def default_init_cfg(self):
         init_cfg = [dict(type='Normal', layer=['Linear'], std=0.01, bias=0)]
         return init_cfg
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/losses/__init__.py` & `mmpose-1.1.0/mmpose/models/losses/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/losses/ae_loss.py` & `mmpose-1.1.0/mmpose/models/losses/ae_loss.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/losses/classification_loss.py` & `mmpose-1.1.0/mmpose/models/losses/classification_loss.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,19 +10,24 @@
 class BCELoss(nn.Module):
     """Binary Cross Entropy loss.
 
     Args:
         use_target_weight (bool): Option to use weighted loss.
             Different joint types may have different target weights.
         loss_weight (float): Weight of the loss. Default: 1.0.
+        with_logits (bool): Whether to use BCEWithLogitsLoss. Default: False.
     """
 
-    def __init__(self, use_target_weight=False, loss_weight=1.):
+    def __init__(self,
+                 use_target_weight=False,
+                 loss_weight=1.,
+                 with_logits=False):
         super().__init__()
-        self.criterion = F.binary_cross_entropy
+        self.criterion = F.binary_cross_entropy if not with_logits\
+            else F.binary_cross_entropy_with_logits
         self.use_target_weight = use_target_weight
         self.loss_weight = loss_weight
 
     def forward(self, output, target, target_weight=None):
         """Forward function.
 
         Note:
@@ -135,51 +140,43 @@
         self.use_target_weight = use_target_weight
 
         self.log_softmax = nn.LogSoftmax(dim=1)
         self.kl_loss = nn.KLDivLoss(reduction='none')
 
     def criterion(self, dec_outs, labels):
         """Criterion function."""
-
-        scores = self.log_softmax(dec_outs * self.beta)
+        log_pt = self.log_softmax(dec_outs * self.beta)
         if self.label_softmax:
             labels = F.softmax(labels * self.beta, dim=1)
-        loss = torch.mean(self.kl_loss(scores, labels), dim=1)
+        loss = torch.mean(self.kl_loss(log_pt, labels), dim=1)
         return loss
 
     def forward(self, pred_simcc, gt_simcc, target_weight):
         """Forward function.
 
         Args:
             pred_simcc (Tuple[Tensor, Tensor]): Predicted SimCC vectors of
                 x-axis and y-axis.
             gt_simcc (Tuple[Tensor, Tensor]): Target representations.
             target_weight (torch.Tensor[N, K] or torch.Tensor[N]):
                 Weights across different labels.
         """
-        output_x, output_y = pred_simcc
-        target_x, target_y = gt_simcc
-        num_joints = output_x.size(1)
+        num_joints = pred_simcc[0].size(1)
         loss = 0
 
-        for idx in range(num_joints):
-            coord_x_pred = output_x[:, idx].squeeze()
-            coord_y_pred = output_y[:, idx].squeeze()
-            coord_x_gt = target_x[:, idx].squeeze()
-            coord_y_gt = target_y[:, idx].squeeze()
-
-            if self.use_target_weight:
-                weight = target_weight[:, idx].squeeze()
-            else:
-                weight = 1.
-
-            loss += (
-                self.criterion(coord_x_pred, coord_x_gt).mul(weight).sum())
-            loss += (
-                self.criterion(coord_y_pred, coord_y_gt).mul(weight).sum())
+        if self.use_target_weight:
+            weight = target_weight.reshape(-1)
+        else:
+            weight = 1.
+
+        for pred, target in zip(pred_simcc, gt_simcc):
+            pred = pred.reshape(-1, pred.size(-1))
+            target = target.reshape(-1, target.size(-1))
+
+            loss += self.criterion(pred, target).mul(weight).sum()
 
         return loss / num_joints
 
 
 @MODELS.register_module()
 class InfoNCELoss(nn.Module):
     """InfoNCE loss for training a discriminative representation space with a
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/losses/heatmap_loss.py` & `mmpose-1.1.0/mmpose/models/losses/heatmap_loss.py`

 * *Files 0% similar despite different names*

```diff
@@ -102,15 +102,15 @@
             if mask is None:
                 mask = _mask
             else:
                 mask = mask * _mask
 
         # Mask by ``skip_empty_channel``
         if self.skip_empty_channel:
-            _mask = (target != 0).flatten(2).any()
+            _mask = (target != 0).flatten(2).any(dim=2)
             ndim_pad = target.ndim - _mask.ndim
             _mask = _mask.view(_mask.shape + (1, ) * ndim_pad)
 
             if mask is None:
                 mask = _mask
             else:
                 mask = mask * _mask
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/losses/loss_wrappers.py` & `mmpose-1.1.0/mmpose/models/losses/loss_wrappers.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/losses/regression_loss.py` & `mmpose-1.1.0/mmpose/models/losses/regression_loss.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/necks/fpn.py` & `mmpose-1.1.0/mmpose/models/necks/fpn.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/necks/gap_neck.py` & `mmpose-1.1.0/mmpose/models/necks/gap_neck.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/necks/posewarper_neck.py` & `mmpose-1.1.0/mmpose/models/necks/posewarper_neck.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/pose_estimators/base.py` & `mmpose-1.1.0/mmpose/models/pose_estimators/base.py`

 * *Files 9% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 from typing import Tuple, Union
 
 import torch
 from mmengine.model import BaseModel
 from torch import Tensor
 
 from mmpose.datasets.datasets.utils import parse_pose_metainfo
+from mmpose.models.utils import check_and_update_config
 from mmpose.registry import MODELS
 from mmpose.utils.typing import (ConfigType, ForwardResults, OptConfigType,
                                  Optional, OptMultiConfig, OptSampleList,
                                  SampleList)
 
 
 class BasePoseEstimator(BaseModel, metaclass=ABCMeta):
@@ -20,15 +21,15 @@
         data_preprocessor (dict | ConfigDict, optional): The pre-processing
             config of :class:`BaseDataPreprocessor`. Defaults to ``None``
         init_cfg (dict | ConfigDict): The model initialization config.
             Defaults to ``None``
         metainfo (dict): Meta information for dataset, such as keypoints
             definition and properties. If set, the metainfo of the input data
             batch will be overridden. For more details, please refer to
-            https://mmpose.readthedocs.io/en/1.x/user_guides/
+            https://mmpose.readthedocs.io/en/latest/user_guides/
             prepare_datasets.html#create-a-custom-dataset-info-
             config-file-for-the-dataset. Defaults to ``None``
     """
     _version = 2
 
     def __init__(self,
                  backbone: ConfigType,
@@ -41,14 +42,20 @@
                  metainfo: Optional[dict] = None):
         super().__init__(
             data_preprocessor=data_preprocessor, init_cfg=init_cfg)
         self.metainfo = self._load_metainfo(metainfo)
 
         self.backbone = MODELS.build(backbone)
 
+        # the PR #2108 and #2126 modified the interface of neck and head.
+        # The following function automatically detects outdated
+        # configurations and updates them accordingly, while also providing
+        # clear and concise information on the changes made.
+        neck, head = check_and_update_config(neck, head)
+
         if neck is not None:
             self.neck = MODELS.build(neck)
 
         if head is not None:
             self.head = MODELS.build(head)
 
         self.train_cfg = train_cfg if train_cfg else {}
@@ -119,14 +126,16 @@
 
             - If ``mode='tensor'``, return a tensor or a tuple of tensors
             - If ``mode='predict'``, return a list of :obj:``PoseDataSample``
                 that contains the pose predictions
             - If ``mode='loss'``, return a dict of tensor(s) which is the loss
                 function value
         """
+        if isinstance(inputs, list):
+            inputs = torch.stack(inputs)
         if mode == 'loss':
             return self.loss(inputs, data_samples)
         elif mode == 'predict':
             # use customed metainfo to override the default metainfo
             if self.metainfo is not None:
                 for data_sample in data_samples:
                     data_sample.set_metainfo(self.metainfo)
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/pose_estimators/bottomup.py` & `mmpose-1.1.0/mmpose/models/pose_estimators/bottomup.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/pose_estimators/topdown.py` & `mmpose-1.1.0/mmpose/models/pose_estimators/topdown.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,15 +26,15 @@
             build the instance of :class:`BaseDataPreprocessor`. Defaults to
             ``None``
         init_cfg (dict, optional): The config to control the initialization.
             Defaults to ``None``
         metainfo (dict): Meta information for dataset, such as keypoints
             definition and properties. If set, the metainfo of the input data
             batch will be overridden. For more details, please refer to
-            https://mmpose.readthedocs.io/en/1.x/user_guides/
+            https://mmpose.readthedocs.io/en/latest/user_guides/
             prepare_datasets.html#create-a-custom-dataset-info-
             config-file-for-the-dataset. Defaults to ``None``
     """
 
     def __init__(self,
                  backbone: ConfigType,
                  neck: OptConfigType = None,
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/utils/ckpt_convert.py` & `mmpose-1.1.0/mmpose/models/utils/ckpt_convert.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/utils/geometry.py` & `mmpose-1.1.0/mmpose/models/utils/geometry.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/utils/realnvp.py` & `mmpose-1.1.0/mmpose/models/utils/realnvp.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/utils/regularizations.py` & `mmpose-1.1.0/mmpose/models/utils/regularizations.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/utils/rtmcc_block.py` & `mmpose-1.1.0/mmpose/models/utils/rtmcc_block.py`

 * *Files 8% similar despite different names*

```diff
@@ -101,15 +101,15 @@
         Args:
             x (torch.Tensor): Input tensor.
 
         Returns:
             torch.Tensor: The tensor after applying scale norm.
         """
 
-        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale
+        norm = torch.norm(x, dim=2, keepdim=True) * self.scale
         return x / norm.clamp(min=self.eps) * self.g
 
 
 class RTMCCBlock(nn.Module):
     """Gated Attention Unit (GAU) in RTMBlock.
 
     Args:
@@ -239,52 +239,58 @@
         if self.attn_type == 'self-attn':
             x = inputs
         else:
             x, k, v = inputs
 
         x = self.ln(x)
 
+        # [B, K, in_token_dims] -> [B, K, e + e + s]
         uv = self.uv(x)
+        uv = self.act_fn(uv)
 
         if self.attn_type == 'self-attn':
-            u, v, base = torch.split(
-                self.act_fn(uv), [self.e, self.e, self.s], dim=-1)
-
+            # [B, K, e + e + s] -> [B, K, e], [B, K, e], [B, K, s]
+            u, v, base = torch.split(uv, [self.e, self.e, self.s], dim=2)
+            # [B, K, 1, s] * [1, 1, 2, s] + [2, s] -> [B, K, 2, s]
             base = base.unsqueeze(2) * self.gamma[None, None, :] + self.beta
 
             if self.pos_enc:
                 base = rope(base, dim=1)
-
-            q, k = torch.unbind(base, dim=-2)
+            # [B, K, 2, s] -> [B, K, s], [B, K, s]
+            q, k = torch.unbind(base, dim=2)
 
         else:
-            u, q = torch.split(self.act_fn(uv), [self.e, self.s], dim=-1)
+            # [B, K, e + s] -> [B, K, e], [B, K, s]
+            u, q = torch.split(uv, [self.e, self.s], dim=2)
 
-            k = self.k_fc(k)
-            v = self.v_fc(v)
+            k = self.k_fc(k)  # -> [B, K, s]
+            v = self.v_fc(v)  # -> [B, K, e]
 
             if self.pos_enc:
                 q = rope(q, 1)
                 k = rope(k, 1)
 
+        # [B, K, s].permute() -> [B, s, K]
+        # [B, K, s] x [B, s, K] -> [B, K, K]
         qk = torch.bmm(q, k.permute(0, 2, 1))
 
         if self.use_rel_bias:
             if self.attn_type == 'self-attn':
                 bias = self.rel_pos_bias(q.size(1))
             else:
                 bias = self.rel_pos_bias(q.size(1), k.size(1))
             qk += bias[:, :q.size(1), :k.size(1)]
-
+        # [B, K, K]
         kernel = torch.square(F.relu(qk / self.sqrt_s))
 
         if self.dropout_rate > 0.:
             kernel = self.dropout(kernel)
-
+        # [B, K, K] x [B, K, e] -> [B, K, e]
         x = u * torch.bmm(kernel, v)
+        # [B, K, e] -> [B, K, out_token_dims]
         x = self.o(x)
 
         return x
 
     def forward(self, x):
         """Forward function."""
```

### Comparing `mmpose-1.0.0rc1/mmpose/models/utils/transformer.py` & `mmpose-1.1.0/mmpose/models/utils/transformer.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/models/utils/tta.py` & `mmpose-1.1.0/mmpose/models/utils/tta.py`

 * *Files 3% similar despite different names*

```diff
@@ -110,14 +110,29 @@
         img_width = input_size[0]
         coords[:, :, 0] -= 1.0 / img_width
 
     coords = coords[:, flip_indices]
     return coords
 
 
+def flip_visibility(vis: Tensor, flip_indices: List[int]):
+    """Flip keypoints visibility for test-time augmentation.
+
+    Args:
+        vis (Tensor): The keypoints visibility to flip. Should be a tensor
+            in shape [B, K]
+        flip_indices (List[int]): The indices of each keypoint's symmetric
+            keypoint
+    """
+    assert vis.ndim == 2
+
+    vis = vis[:, flip_indices]
+    return vis
+
+
 def aggregate_heatmaps(heatmaps: List[Tensor],
                        size: Optional[Tuple[int, int]],
                        align_corners: bool = False,
                        mode: str = 'average'):
     """Aggregate multiple heatmaps.
 
     Args:
```

### Comparing `mmpose-1.0.0rc1/mmpose/registry.py` & `mmpose-1.1.0/mmpose/registry.py`

 * *Files 4% similar despite different names*

```diff
@@ -72,32 +72,30 @@
     locations=['mmpose.models'])
 # manage all kinds of batch augmentations like Mixup and CutMix.
 BATCH_AUGMENTS = Registry('batch augment', locations=['mmpose.models'])
 
 # Registries For Optimizer and the related
 # manage all kinds of optimizers like `SGD` and `Adam`
 OPTIMIZERS = Registry(
-    'optimizer',
-    parent=MMENGINE_OPTIMIZERS,
-    locations=['mmpose.engine.optimizers'])
+    'optimizer', parent=MMENGINE_OPTIMIZERS, locations=['mmpose.engine'])
 # manage optimizer wrapper
 OPTIM_WRAPPERS = Registry(
     'optimizer_wrapper',
     parent=MMENGINE_OPTIM_WRAPPERS,
-    locations=['mmpose.engine.optimizers'])
+    locations=['mmpose.engine'])
 # manage constructors that customize the optimization hyperparameters.
 OPTIM_WRAPPER_CONSTRUCTORS = Registry(
     'optimizer wrapper constructor',
     parent=MMENGINE_OPTIM_WRAPPER_CONSTRUCTORS,
-    locations=['mmpose.engine.optimizers'])
+    locations=['mmpose.engine.optim_wrappers'])
 # manage all kinds of parameter schedulers like `MultiStepLR`
 PARAM_SCHEDULERS = Registry(
     'parameter scheduler',
     parent=MMENGINE_PARAM_SCHEDULERS,
-    locations=['mmpose.engine.schedulers'])
+    locations=['mmpose.engine'])
 
 # manage all kinds of metrics
 METRICS = Registry(
     'metric', parent=MMENGINE_METRICS, locations=['mmpose.evaluation.metrics'])
 # manage all kinds of evaluators
 EVALUATORS = Registry(
     'evaluator', parent=MMENGINE_EVALUATOR, locations=['mmpose.evaluation'])
```

### Comparing `mmpose-1.0.0rc1/mmpose/structures/__init__.py` & `mmpose-1.1.0/mmpose/structures/__init__.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/structures/bbox/transforms.py` & `mmpose-1.1.0/mmpose/structures/bbox/transforms.py`

 * *Files 1% similar despite different names*

```diff
@@ -107,24 +107,24 @@
 
     return center, scale
 
 
 def bbox_cs2xyxy(center: np.ndarray,
                  scale: np.ndarray,
                  padding: float = 1.) -> np.ndarray:
-    """Transform the bbox format from (center, scale) to (x,y,w,h).
+    """Transform the bbox format from (center, scale) to (x1,y1,x2,y2).
 
     Args:
         center (ndarray): BBox center (x, y) in shape (2,) or (n, 2)
         scale (ndarray): BBox scale (w, h) in shape (2,) or (n, 2)
         padding (float): BBox padding factor that will be multilied to scale.
             Default: 1.0
 
     Returns:
-        ndarray[float32]: BBox (x, y, w, h) in shape (4, ) or (n, 4)
+        ndarray[float32]: BBox (x1, y1, x2, y2) in shape (4, ) or (n, 4)
     """
 
     dim = center.ndim
     assert scale.ndim == dim
 
     if dim == 1:
         center = center[None, :]
```

### Comparing `mmpose-1.0.0rc1/mmpose/structures/multilevel_pixel_data.py` & `mmpose-1.1.0/mmpose/structures/multilevel_pixel_data.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/structures/pose_data_sample.py` & `mmpose-1.1.0/mmpose/structures/pose_data_sample.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/structures/utils.py` & `mmpose-1.1.0/mmpose/structures/utils.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/testing/_utils.py` & `mmpose-1.1.0/mmpose/testing/_utils.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/camera.py` & `mmpose-1.1.0/mmpose/utils/camera.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/config_utils.py` & `mmpose-1.1.0/mmpose/utils/config_utils.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/hooks.py` & `mmpose-1.1.0/mmpose/utils/hooks.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/logger.py` & `mmpose-1.1.0/mmpose/utils/logger.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/setup_env.py` & `mmpose-1.1.0/mmpose/utils/setup_env.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/tensor_utils.py` & `mmpose-1.1.0/mmpose/utils/tensor_utils.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/timer.py` & `mmpose-1.1.0/mmpose/utils/timer.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/utils/typing.py` & `mmpose-1.1.0/mmpose/utils/typing.py`

 * *Files identical despite different names*

### Comparing `mmpose-1.0.0rc1/mmpose/version.py` & `mmpose-1.1.0/mmpose/version.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # Copyright (c) Open-MMLab. All rights reserved.
 
-__version__ = '1.0.0rc1'
+__version__ = '1.1.0'
 short_version = __version__
 
 
 def parse_version_info(version_str):
     """Parse a version string into a tuple.
 
     Args:
```

### Comparing `mmpose-1.0.0rc1/mmpose/visualization/local_visualizer.py` & `mmpose-1.1.0/mmpose/visualization/local_visualizer.py`

 * *Files 19% similar despite different names*

```diff
@@ -4,18 +4,19 @@
 
 import cv2
 import mmcv
 import numpy as np
 import torch
 from mmengine.dist import master_only
 from mmengine.structures import InstanceData, PixelData
-from mmengine.visualization import Visualizer
 
+from mmpose.datasets.datasets.utils import parse_pose_metainfo
 from mmpose.registry import VISUALIZERS
 from mmpose.structures import PoseDataSample
+from .opencv_backend_visualizer import OpencvBackendVisualizer
 from .simcc_vis import SimCCVisualizer
 
 
 def _get_adaptive_scales(areas: np.ndarray,
                          min_area: int = 800,
                          max_area: int = 30000) -> np.ndarray:
     """Get adaptive scales according to areas.
@@ -37,15 +38,15 @@
     """
     scales = 0.5 + (areas - min_area) / (max_area - min_area)
     scales = np.clip(scales, 0.5, 1.0)
     return scales
 
 
 @VISUALIZERS.register_module()
-class PoseLocalVisualizer(Visualizer):
+class PoseLocalVisualizer(OpencvBackendVisualizer):
     """MMPose Local Visualizer.
 
     Args:
         name (str): Name of the instance. Defaults to 'visualizer'.
         image (np.ndarray, optional): the origin image to draw. The format
             should be RGB. Defaults to ``None``
         vis_backends (list, optional): Visual backend config list. Defaults to
@@ -59,15 +60,15 @@
             The tuple of color should be in BGR order. Defaults to ``'red'``
         link_color (str, tuple(tuple(int)), optional): Color of skeleton.
             The tuple of color should be in BGR order. Defaults to ``None``
         line_width (int, float): The width of lines. Defaults to 1
         radius (int, float): The radius of keypoints. Defaults to 4
         show_keypoint_weight (bool): Whether to adjust the transparency
             of keypoints according to their score. Defaults to ``False``
-        alpha (int, float): The transparency of bboxes. Defaults to ``0.8``
+        alpha (int, float): The transparency of bboxes. Defaults to ``1.0``
 
     Examples:
         >>> import numpy as np
         >>> from mmengine.structures import InstanceData
         >>> from mmpose.structures import PoseDataSample
         >>> from mmpose.visualization import PoseLocalVisualizer
 
@@ -110,37 +111,51 @@
                  link_color: Optional[Union[str, Tuple[Tuple[int]]]] = None,
                  text_color: Optional[Union[str,
                                             Tuple[int]]] = (255, 255, 255),
                  skeleton: Optional[Union[List, Tuple]] = None,
                  line_width: Union[int, float] = 1,
                  radius: Union[int, float] = 3,
                  show_keypoint_weight: bool = False,
-                 alpha: float = 0.8):
-        super().__init__(name, image, vis_backends, save_dir)
+                 backend: str = 'opencv',
+                 alpha: float = 1.0):
+        super().__init__(
+            name=name,
+            image=image,
+            vis_backends=vis_backends,
+            save_dir=save_dir,
+            backend=backend)
+
         self.bbox_color = bbox_color
         self.kpt_color = kpt_color
         self.link_color = link_color
         self.line_width = line_width
         self.text_color = text_color
         self.skeleton = skeleton
         self.radius = radius
         self.alpha = alpha
         self.show_keypoint_weight = show_keypoint_weight
         # Set default value. When calling
         # `PoseLocalVisualizer().set_dataset_meta(xxx)`,
         # it will override the default value.
         self.dataset_meta = {}
 
-    def set_dataset_meta(self, dataset_meta: Dict):
+    def set_dataset_meta(self,
+                         dataset_meta: Dict,
+                         skeleton_style: str = 'mmpose'):
         """Assign dataset_meta to the visualizer. The default visualization
         settings will be overridden.
 
         Args:
             dataset_meta (dict): meta information of dataset.
         """
+        if dataset_meta.get(
+                'dataset_name') == 'coco' and skeleton_style == 'openpose':
+            dataset_meta = parse_pose_metainfo(
+                dict(from_file='configs/_base_/datasets/coco_openpose.py'))
+
         if isinstance(dataset_meta, dict):
             self.dataset_meta = dataset_meta.copy()
             self.bbox_color = dataset_meta.get('bbox_color', self.bbox_color)
             self.kpt_color = dataset_meta.get('keypoint_colors',
                                               self.kpt_color)
             self.link_color = dataset_meta.get('skeleton_link_colors',
                                                self.link_color)
@@ -207,24 +222,29 @@
                     }])
 
         return self.get_image()
 
     def _draw_instances_kpts(self,
                              image: np.ndarray,
                              instances: InstanceData,
-                             kpt_score_thr: float = 0.3,
-                             show_kpt_idx: bool = False):
+                             kpt_thr: float = 0.3,
+                             show_kpt_idx: bool = False,
+                             skeleton_style: str = 'mmpose'):
         """Draw keypoints and skeletons (optional) of GT or prediction.
 
         Args:
             image (np.ndarray): The image to draw.
             instances (:obj:`InstanceData`): Data structure for
                 instance-level annotations or predictions.
-            kpt_score_thr (float, optional): Minimum score of keypoints
+            kpt_thr (float, optional): Minimum threshold of keypoints
                 to be shown. Default: 0.3.
+            show_kpt_idx (bool): Whether to show the index of keypoints.
+                Defaults to ``False``
+            skeleton_style (str): Skeleton style selection. Defaults to
+                ``'mmpose'``
 
         Returns:
             np.ndarray: the drawn image which channel is RGB.
         """
 
         self.set_image(image)
         img_h, img_w, _ = image.shape
@@ -232,20 +252,47 @@
         if 'keypoints' in instances:
             keypoints = instances.get('transformed_keypoints',
                                       instances.keypoints)
 
             if 'keypoint_scores' in instances:
                 scores = instances.keypoint_scores
             else:
-                scores = [np.ones(len(kpts)) for kpts in keypoints]
+                scores = np.ones(keypoints.shape[:-1])
 
             if 'keypoints_visible' in instances:
                 keypoints_visible = instances.keypoints_visible
             else:
-                keypoints_visible = [np.ones(len(kpts)) for kpts in keypoints]
+                keypoints_visible = np.ones(keypoints.shape[:-1])
+
+            if skeleton_style == 'openpose':
+                keypoints_info = np.concatenate(
+                    (keypoints, scores[..., None], keypoints_visible[...,
+                                                                     None]),
+                    axis=-1)
+                # compute neck joint
+                neck = np.mean(keypoints_info[:, [5, 6]], axis=1)
+                # neck score when visualizing pred
+                neck[:, 2:4] = np.logical_and(
+                    keypoints_info[:, 5, 2:4] > kpt_thr,
+                    keypoints_info[:, 6, 2:4] > kpt_thr).astype(int)
+                new_keypoints_info = np.insert(
+                    keypoints_info, 17, neck, axis=1)
+
+                mmpose_idx = [
+                    17, 6, 8, 10, 7, 9, 12, 14, 16, 13, 15, 2, 1, 4, 3
+                ]
+                openpose_idx = [
+                    1, 2, 3, 4, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17
+                ]
+                new_keypoints_info[:, openpose_idx] = \
+                    new_keypoints_info[:, mmpose_idx]
+                keypoints_info = new_keypoints_info
+
+                keypoints, scores, keypoints_visible = keypoints_info[
+                    ..., :2], keypoints_info[..., 2], keypoints_info[..., 3]
 
             for kpts, score, visible in zip(keypoints, scores,
                                             keypoints_visible):
                 kpts = np.array(kpts, copy=False)
 
                 if self.kpt_color is None or isinstance(self.kpt_color, str):
                     kpt_color = [self.kpt_color] * len(kpts)
@@ -253,43 +300,14 @@
                     kpt_color = self.kpt_color
                 else:
                     raise ValueError(
                         f'the length of kpt_color '
                         f'({len(self.kpt_color)}) does not matches '
                         f'that of keypoints ({len(kpts)})')
 
-                # draw each point on image
-                for kid, kpt in enumerate(kpts):
-                    if score[kid] < kpt_score_thr or not visible[
-                            kid] or kpt_color[kid] is None:
-                        # skip the point that should not be drawn
-                        continue
-
-                    color = kpt_color[kid]
-                    if not isinstance(color, str):
-                        color = tuple(int(c) for c in color)
-                    transparency = self.alpha
-                    if self.show_keypoint_weight:
-                        transparency *= max(0, min(1, score[kid]))
-                    self.draw_circles(
-                        kpt,
-                        radius=np.array([self.radius]),
-                        face_colors=color,
-                        edge_colors=color,
-                        alpha=transparency,
-                        line_widths=self.radius)
-                    if show_kpt_idx:
-                        self.draw_texts(
-                            str(kid),
-                            kpt,
-                            colors=color,
-                            font_sizes=self.radius * 3,
-                            vertical_alignments='bottom',
-                            horizontal_alignments='center')
-
                 # draw links
                 if self.skeleton is not None and self.link_color is not None:
                     if self.link_color is None or isinstance(
                             self.link_color, str):
                         link_color = [self.link_color] * len(self.skeleton)
                     elif len(self.link_color) == len(self.skeleton):
                         link_color = self.link_color
@@ -304,50 +322,82 @@
                         pos2 = (int(kpts[sk[1], 0]), int(kpts[sk[1], 1]))
                         if not (visible[sk[0]] and visible[sk[1]]):
                             continue
 
                         if (pos1[0] <= 0 or pos1[0] >= img_w or pos1[1] <= 0
                                 or pos1[1] >= img_h or pos2[0] <= 0
                                 or pos2[0] >= img_w or pos2[1] <= 0
-                                or pos2[1] >= img_h
-                                or score[sk[0]] < kpt_score_thr
-                                or score[sk[1]] < kpt_score_thr
+                                or pos2[1] >= img_h or score[sk[0]] < kpt_thr
+                                or score[sk[1]] < kpt_thr
                                 or link_color[sk_id] is None):
                             # skip the link that should not be drawn
                             continue
                         X = np.array((pos1[0], pos2[0]))
                         Y = np.array((pos1[1], pos2[1]))
                         color = link_color[sk_id]
                         if not isinstance(color, str):
                             color = tuple(int(c) for c in color)
+                        transparency = self.alpha
                         if self.show_keypoint_weight:
+                            transparency *= max(
+                                0, min(1, 0.5 * (score[sk[0]] + score[sk[1]])))
 
+                        if skeleton_style == 'openpose':
                             mX = np.mean(X)
                             mY = np.mean(Y)
                             length = ((Y[0] - Y[1])**2 + (X[0] - X[1])**2)**0.5
+                            transparency = 0.6
                             angle = math.degrees(
                                 math.atan2(Y[0] - Y[1], X[0] - X[1]))
-                            stickwidth = 2
                             polygons = cv2.ellipse2Poly(
                                 (int(mX), int(mY)),
-                                (int(length / 2), int(stickwidth)), int(angle),
-                                0, 360, 1)
-                            transparency = self.alpha
-                            transparency *= max(
-                                0, min(1, 0.5 * (score[sk[0]] + score[sk[1]])))
+                                (int(length / 2), int(self.line_width)),
+                                int(angle), 0, 360, 1)
+
                             self.draw_polygons(
                                 polygons,
                                 edge_colors=color,
                                 face_colors=color,
                                 alpha=transparency)
 
                         else:
                             self.draw_lines(
                                 X, Y, color, line_widths=self.line_width)
 
+                # draw each point on image
+                for kid, kpt in enumerate(kpts):
+                    if score[kid] < kpt_thr or not visible[
+                            kid] or kpt_color[kid] is None:
+                        # skip the point that should not be drawn
+                        continue
+
+                    color = kpt_color[kid]
+                    if not isinstance(color, str):
+                        color = tuple(int(c) for c in color)
+                    transparency = self.alpha
+                    if self.show_keypoint_weight:
+                        transparency *= max(0, min(1, score[kid]))
+                    self.draw_circles(
+                        kpt,
+                        radius=np.array([self.radius]),
+                        face_colors=color,
+                        edge_colors=color,
+                        alpha=transparency,
+                        line_widths=self.radius)
+                    if show_kpt_idx:
+                        kpt[0] += self.radius
+                        kpt[1] -= self.radius
+                        self.draw_texts(
+                            str(kid),
+                            kpt,
+                            colors=color,
+                            font_sizes=self.radius * 3,
+                            vertical_alignments='bottom',
+                            horizontal_alignments='center')
+
         return self.get_image()
 
     def _draw_instance_heatmap(
         self,
         fields: PixelData,
         overlaid_image: Optional[np.ndarray] = None,
     ):
@@ -406,18 +456,19 @@
                        image: np.ndarray,
                        data_sample: PoseDataSample,
                        draw_gt: bool = True,
                        draw_pred: bool = True,
                        draw_heatmap: bool = False,
                        draw_bbox: bool = False,
                        show_kpt_idx: bool = False,
+                       skeleton_style: str = 'mmpose',
                        show: bool = False,
                        wait_time: float = 0,
                        out_file: Optional[str] = None,
-                       kpt_score_thr: float = 0.3,
+                       kpt_thr: float = 0.3,
                        step: int = 0) -> None:
         """Draw datasample and save to all backends.
 
         - If GT and prediction are plotted at the same time, they are
         displayed in a stitched image where the left image is the
         ground truth and the right image is the prediction.
         - If ``show`` is True, all storage backends are ignored, and
@@ -435,35 +486,39 @@
                 ``True``
             draw_pred (bool): Whether to draw Prediction PoseDataSample.
                 Defaults to ``True``
             draw_bbox (bool): Whether to draw bounding boxes. Default to
                 ``False``
             draw_heatmap (bool): Whether to draw heatmaps. Defaults to
                 ``False``
+            show_kpt_idx (bool): Whether to show the index of keypoints.
+                Defaults to ``False``
+            skeleton_style (str): Skeleton style selection. Defaults to
+                ``'mmpose'``
             show (bool): Whether to display the drawn image. Default to
                 ``False``
             wait_time (float): The interval of show (s). Defaults to 0
             out_file (str): Path to output file. Defaults to ``None``
-            pred_score_thr (float): The threshold to visualize the bboxes
-                and masks. Defaults to 0.3
+            kpt_thr (float, optional): Minimum threshold of keypoints
+                to be shown. Default: 0.3.
             step (int): Global step value to record. Defaults to 0
         """
 
         gt_img_data = None
         pred_img_data = None
 
         if draw_gt:
             gt_img_data = image.copy()
             gt_img_heatmap = None
 
             # draw bboxes & keypoints
             if 'gt_instances' in data_sample:
                 gt_img_data = self._draw_instances_kpts(
-                    gt_img_data, data_sample.gt_instances, kpt_score_thr,
-                    show_kpt_idx)
+                    gt_img_data, data_sample.gt_instances, kpt_thr,
+                    show_kpt_idx, skeleton_style)
                 if draw_bbox:
                     gt_img_data = self._draw_instances_bbox(
                         gt_img_data, data_sample.gt_instances)
 
             # draw heatmaps
             if 'gt_fields' in data_sample and draw_heatmap:
                 gt_img_heatmap = self._draw_instance_heatmap(
@@ -475,16 +530,16 @@
         if draw_pred:
             pred_img_data = image.copy()
             pred_img_heatmap = None
 
             # draw bboxes & keypoints
             if 'pred_instances' in data_sample:
                 pred_img_data = self._draw_instances_kpts(
-                    pred_img_data, data_sample.pred_instances, kpt_score_thr,
-                    show_kpt_idx)
+                    pred_img_data, data_sample.pred_instances, kpt_thr,
+                    show_kpt_idx, skeleton_style)
                 if draw_bbox:
                     pred_img_data = self._draw_instances_bbox(
                         pred_img_data, data_sample.pred_instances)
 
             # draw heatmaps
             if 'pred_fields' in data_sample and draw_heatmap:
                 if 'keypoint_x_labels' in data_sample.pred_instances:
```

### Comparing `mmpose-1.0.0rc1/mmpose/visualization/simcc_vis.py` & `mmpose-1.1.0/mmpose/visualization/simcc_vis.py`

 * *Files 8% similar despite different names*

```diff
@@ -32,15 +32,15 @@
         K = K if K <= n else n
         blank_size = tuple(heatmap.size()[1:])
         maps = {'x': [], 'y': []}
         for i in xy_heatmap:
             x, y = self.draw_1d_heatmaps(i['x']), self.draw_1d_heatmaps(i['y'])
             maps['x'].append(x)
             maps['y'].append(y)
-        white = self.creat_blank(blank_size)
+        white = self.creat_blank(blank_size, K)
         map2d = self.draw_2d_heatmaps(heatmap2d)
         if mix:
             map2d = cv.addWeighted(overlaid_image, 1 - weight, map2d, weight,
                                    0)
         self.image_cover(white, map2d, int(blank_size[1] * 0.1),
                          int(blank_size[0] * 0.1))
         white = self.add_1d_heatmaps(maps, white, blank_size, K)
@@ -74,17 +74,24 @@
         if size[0] < size[1]:
             cv_img = cv.resize(cv_img, (length, 15))
         else:
             cv_img = cv.resize(cv_img, (15, length))
         single_map = cv.applyColorMap(cv_img, cv.COLORMAP_JET)
         return single_map
 
-    def creat_blank(self, size: Union[list, tuple]):
+    def creat_blank(self,
+                    size: Union[list, tuple],
+                    K: int = 20,
+                    interval: int = 10):
         """Create the background."""
-        blank = np.zeros((size[0] * 2, size[1] * 2, 3), np.uint8)
+        blank_height = int(
+            max(size[0] * 2, size[0] * 1.1 + (K + 1) * (15 + interval)))
+        blank_width = int(
+            max(size[1] * 2, size[1] * 1.1 + (K + 1) * (15 + interval)))
+        blank = np.zeros((blank_height, blank_width, 3), np.uint8)
         blank.fill(255)
         return blank
 
     def draw_2d_heatmaps(self, heatmap_2d):
         """Draw a two-dimensional heatmap fused with the original image."""
         np_heatmap = ToPILImage()(heatmap_2d).convert('RGB')
         cv_img = cv.cvtColor(np.asarray(np_heatmap), cv.COLOR_RGB2BGR)
```

### Comparing `mmpose-1.0.0rc1/mmpose.egg-info/PKG-INFO` & `mmpose-1.1.0/mmpose.egg-info/PKG-INFO`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mmpose
-Version: 1.0.0rc1
+Version: 1.1.0
 Summary: OpenMMLab Pose Estimation Toolbox and Benchmark.
 Home-page: https://github.com/open-mmlab/mmpose
 Author: MMPose Contributors
 Author-email: openmmlab@gmail.com
 License: Apache License 2.0
 Description: <div align="center">
           <img src="resources/mmpose-logo.png" width="450"/>
@@ -22,66 +22,72 @@
               <a href="https://platform.openmmlab.com">
                 <i>TRY IT OUT</i>
               </a>
             </sup>
           </div>
           <div>&nbsp;</div>
         
-        [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/1.x/?badge=latest)
+        [![Documentation](https://readthedocs.org/projects/mmpose/badge/?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest)
         [![actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)](https://github.com/open-mmlab/mmpose/actions)
-        [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/1.x/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose)
+        [![codecov](https://codecov.io/gh/open-mmlab/mmpose/branch/latest/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmpose)
         [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/project/mmpose/)
-        [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE)
+        [![LICENSE](https://img.shields.io/github/license/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/main/LICENSE)
         [![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
         [![Percentage of issues still open](https://isitmaintained.com/badge/open/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
         
-        [Documentation](https://mmpose.readthedocs.io/en/1.x/) |
-        [Installation](https://mmpose.readthedocs.io/en/1.x/installation.html) |
-        [Model Zoo](https://mmpose.readthedocs.io/en/1.x/model_zoo.html) |
-        [Papers](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html) |
-        [Update News](https://mmpose.readthedocs.io/en/1.x/notes/changelog.html) |
+        [Documentation](https://mmpose.readthedocs.io/en/latest/) |
+        [Installation](https://mmpose.readthedocs.io/en/latest/installation.html) |
+        [Model Zoo](https://mmpose.readthedocs.io/en/latest/model_zoo.html) |
+        [Papers](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html) |
+        [Update News](https://mmpose.readthedocs.io/en/latest/notes/changelog.html) |
         [Reporting Issues](https://github.com/open-mmlab/mmpose/issues/new/choose) |
         [RTMPose](/projects/rtmpose/)
         
         </div>
         
         <div align="center">
           <a href="https://openmmlab.medium.com/" style="text-decoration:none;">
-            <img src="https://user-images.githubusercontent.com/25839884/218352562-cdded397-b0f3-4ca1-b8dd-a60df8dca75b.png" width="3%" alt="" /></a>
+            <img src="https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png" width="3%" alt="" /></a>
           <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
-          <a href="https://discord.com/channels/1037617289144569886/1046608014234370059" style="text-decoration:none;">
+          <a href="https://discord.com/channels/1037617289144569886/1072798105428299817" style="text-decoration:none;">
             <img src="https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png" width="3%" alt="" /></a>
           <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
           <a href="https://twitter.com/OpenMMLab" style="text-decoration:none;">
             <img src="https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png" width="3%" alt="" /></a>
           <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
           <a href="https://www.youtube.com/openmmlab" style="text-decoration:none;">
             <img src="https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://space.bilibili.com/1293512903" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png" width="3%" alt="" /></a>
+          <img src="https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png" width="3%" alt="" />
+          <a href="https://www.zhihu.com/people/openmmlab" style="text-decoration:none;">
+            <img src="https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png" width="3%" alt="" /></a>
         </div>
         
         ## Introduction
         
         English | [](README_CN.md)
         
         MMPose is an open-source toolbox for pose estimation based on PyTorch.
         It is a part of the [OpenMMLab project](https://github.com/open-mmlab).
         
-        The master branch works with **PyTorch 1.6+**.
+        The main branch works with **PyTorch 1.8+**.
         
         https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-ded1-11eb-84f6-24eeddbf4d91.mp4
         
         <br/>
         
         <details close>
         <summary><b>Major Features</b></summary>
         
         - **Support diverse tasks**
         
           We support a wide spectrum of mainstream pose analysis tasks in current research community, including 2d multi-person human pose estimation, 2d hand pose estimation, 2d face landmark detection, 133 keypoint whole-body human pose estimation, 3d human mesh recovery, fashion landmark detection and animal pose estimation.
-          See [Demo](demo/docs/) for more information.
+          See [Demo](demo/docs/en) for more information.
         
         - **Higher efficiency and higher accuracy**
         
           MMPose implements multiple state-of-the-art (SOTA) deep learning models, including both top-down & bottom-up approaches. We achieve faster training speed and higher accuracy than other popular codebases, such as [HRNet](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch).
           See [benchmark.md](docs/en/notes/benchmark.md) for more information.
         
         - **Support for various datasets**
@@ -95,184 +101,238 @@
           pose estimation framework by combining different modules.
           We provide detailed documentation and API reference, as well as unittests.
         
         </details>
         
         ## What's New
         
-        - We are excited to release **RTMPose**, a real-time pose estimation framework including:
-        
-          - A family of lightweight pose estimation models with state-of-the-art performance
-          - Inference APIs for Python, C++, C#, Java, etc. Easy to integrate into your applications and empower real-time stable pose estimation
-          - Cross-platform deployment with various backends
-          - A step-by-step guide to training and deploying your own models
-        
-          Checkout our [project page](/projects/rtmpose/) and [technical report](https://arxiv.org/abs/2303.07399) for more information!
+        - We are glad to support 3 new datasets:
+          - (CVPR 2023) [Human-Art](https://github.com/IDEA-Research/HumanArt)
+          - (CVPR 2022) [Animal Kingdom](https://github.com/sutdcv/Animal-Kingdom)
+          - (AAAI 2020) [LaPa](https://github.com/JDAI-CV/lapa-dataset/)
         
-        ![rtmpose_intro](https://user-images.githubusercontent.com/13503330/219269619-935499e5-bdd9-49ea-8104-3c7796dbd862.png)
+        ![image](https://github.com/open-mmlab/mmpose/assets/13503330/c9171dbb-7e7a-4c39-98e3-c92932182efb)
         
         - Welcome to [*projects of MMPose*](/projects/README.md), where you can access to the latest features of MMPose, and share your ideas and codes with the community at once. Contribution to MMPose will be simple and smooth:
         
           - Provide an easy and agile way to integrate algorithms, features and applications into MMPose
           - Allow flexible code structure and style; only need a short code review process
           - Build individual projects with full power of MMPose but not bound up with heavy frameworks
           - Checkout new projects:
             - [RTMPose](/projects/rtmpose/)
-            - [YOLOX-Pose (coming soon)](<>)
-            - [MMPose4AIGC (coming soon)](<>)
+            - [YOLOX-Pose](/projects/yolox_pose/)
+            - [MMPose4AIGC](/projects/mmpose4aigc/)
+            - [Simple Keypoints](/projects/skps/)
           - Become a contributors and make MMPose greater. Start your journey from the [example project](/projects/example_project/)
         
         <br/>
         
-        - 2022-03-15: MMPose [v1.0.0rc1](https://github.com/open-mmlab/mmpose/releases/tag/v1.0.0rc1) is released. Major updates include:
+        - 2023-07-04: MMPose [v1.1.0](https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) is officially released, with the main updates including:
         
-          - Release [RTMPose](/projects/rtmpose/), a high-performance real-time pose estimation framework based on MMPose
-          - Support [ViTPose](/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.md) (NeurIPS'22), [CID](/configs/body_2d_keypoint/cid/coco/hrnet_coco.md) (CVPR'22) and [DEKR](/configs/body_2d_keypoint/dekr/) (CVPR'21)
-          - Add [*Inferencer*](/docs/en/user_guides/inference.md#out-of-the-box-inferencer), a convenient interface for inference and visualization
+          - Support new datasets: Human-Art, Animal Kingdom and LaPa.
+          - Support new config type that is more user-friendly and flexible.
+          - Improve RTMPose with better performance.
+          - Migrate 3D pose estimation models on h36m.
+          - Inference speedup and webcam inference with all demo scripts.
         
-          See the full [release note](https://github.com/open-mmlab/mmpose/releases/tag/v1.0.0rc1) for more exciting updates brought by MMPose v1.0.0rc1!
+          Please refer to the [release notes](https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) for more updates brought by MMPose v1.1.0!
         
-        ## Installation
+        ## 0.x / 1.x Migration
         
-        Below are quick steps for installation:
+        MMPose v1.0.0 is a major update, including many API and config file changes. Currently, a part of the algorithms have been migrated to v1.0.0, and the remaining algorithms will be completed in subsequent versions. We will show the migration progress in the following list.
         
-        ```shell
-        conda create -n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -c pytorch -y
-        conda activate open-mmlab
-        pip install openmim
-        git clone -b 1.x https://github.com/open-mmlab/mmpose.git
-        cd mmpose
-        mim install -e .
-        ```
+        <details close>
+        <summary><b>Migration Progress</b></summary>
         
-        Please refer to [installation.md](https://mmpose.readthedocs.io/en/1.x/installation.html) for more detailed installation and dataset preparation.
+        | Algorithm                         |   Status    |
+        | :-------------------------------- | :---------: |
+        | MTUT (CVPR 2019)                  |             |
+        | MSPN (ArXiv 2019)                 |    done     |
+        | InterNet (ECCV 2020)              |             |
+        | DEKR (CVPR 2021)                  |    done     |
+        | HigherHRNet (CVPR 2020)           |             |
+        | DeepPose (CVPR 2014)              |    done     |
+        | RLE (ICCV 2021)                   |    done     |
+        | SoftWingloss (TIP 2021)           |    done     |
+        | VideoPose3D (CVPR 2019)           |    done     |
+        | Hourglass (ECCV 2016)             |    done     |
+        | LiteHRNet (CVPR 2021)             |    done     |
+        | AdaptiveWingloss (ICCV 2019)      |    done     |
+        | SimpleBaseline2D (ECCV 2018)      |    done     |
+        | PoseWarper (NeurIPS 2019)         |             |
+        | SimpleBaseline3D (ICCV 2017)      |    done     |
+        | HMR (CVPR 2018)                   |             |
+        | UDP (CVPR 2020)                   |    done     |
+        | VIPNAS (CVPR 2021)                |    done     |
+        | Wingloss (CVPR 2018)              |    done     |
+        | DarkPose (CVPR 2020)              |    done     |
+        | Associative Embedding (NIPS 2017) | in progress |
+        | VoxelPose (ECCV 2020)             |             |
+        | RSN (ECCV 2020)                   |    done     |
+        | CID (CVPR 2022)                   |    done     |
+        | CPM (CVPR 2016)                   |    done     |
+        | HRNet (CVPR 2019)                 |    done     |
+        | HRNetv2 (TPAMI 2019)              |    done     |
+        | SCNet (CVPR 2020)                 |    done     |
+        
+        </details>
+        
+        If your algorithm has not been migrated, you can continue to use the [0.x branch](https://github.com/open-mmlab/mmpose/tree/0.x) and [old documentation](https://mmpose.readthedocs.io/en/0.x/).
+        
+        ## Installation
+        
+        Please refer to [installation.md](https://mmpose.readthedocs.io/en/latest/installation.html) for more detailed installation and dataset preparation.
         
         ## Getting Started
         
         We provided a series of tutorials about the basic usage of MMPose for new users:
         
-        - [About Configs](https://mmpose.readthedocs.io/en/1.x/user_guides/configs.html)
-        - [Add New Dataset](https://mmpose.readthedocs.io/en/1.x/user_guides/prepare_datasets.html)
-        - [Keypoint Encoding & Decoding](https://mmpose.readthedocs.io/en/1.x/user_guides/codecs.html)
-        - [Inference with Existing Models](https://mmpose.readthedocs.io/en/1.x/user_guides/inference.html)
-        - [Train and Test](https://mmpose.readthedocs.io/en/1.x/user_guides/train_and_test.html)
-        - [Visualization Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/visualization.html)
-        - [Other Useful Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/useful_tools.html)
+        1. For the basic usage of MMPose:
+        
+           - [A 20-minute Tour to MMPose](https://mmpose.readthedocs.io/en/latest/guide_to_framework.html)
+           - [Demos](https://mmpose.readthedocs.io/en/latest/demos.html)
+           - [Inference](https://mmpose.readthedocs.io/en/latest/user_guides/inference.html)
+           - [Configs](https://mmpose.readthedocs.io/en/latest/user_guides/configs.html)
+           - [Prepare Datasets](https://mmpose.readthedocs.io/en/latest/user_guides/prepare_datasets.html)
+           - [Train and Test](https://mmpose.readthedocs.io/en/latest/user_guides/train_and_test.html)
+        
+        2. For developers who wish to develop based on MMPose:
+        
+           - [Learn about Codecs](https://mmpose.readthedocs.io/en/latest/advanced_guides/codecs.html)
+           - [Dataflow in MMPose](https://mmpose.readthedocs.io/en/latest/advanced_guides/dataflow.html)
+           - [Implement New Models](https://mmpose.readthedocs.io/en/latest/advanced_guides/implement_new_models.html)
+           - [Customize Datasets](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_datasets.html)
+           - [Customize Data Transforms](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_transforms.html)
+           - [Customize Optimizer](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_optimizer.html)
+           - [Customize Logging](https://mmpose.readthedocs.io/en/latest/advanced_guides/customize_logging.html)
+           - [How to Deploy](https://mmpose.readthedocs.io/en/latest/advanced_guides/how_to_deploy.html)
+           - [Model Analysis](https://mmpose.readthedocs.io/en/latest/advanced_guides/model_analysis.html)
+           - [Migration Guide](https://mmpose.readthedocs.io/en/latest/migration.html)
+        
+        3. For researchers and developers who are willing to contribute to MMPose:
+        
+           - [Contribution Guide](https://mmpose.readthedocs.io/en/latest/contribution_guide.html)
+        
+        4. For some common issues, we provide a FAQ list:
+        
+           - [FAQ](https://mmpose.readthedocs.io/en/latest/faq.html)
         
         ## Model Zoo
         
         Results and models are available in the **README.md** of each method's config directory.
-        A summary can be found in the [Model Zoo](https://mmpose.readthedocs.io/en/1.x/modelzoo.html) page.
+        A summary can be found in the [Model Zoo](https://mmpose.readthedocs.io/en/latest/model_zoo.html) page.
         
-        <details open>
+        <details close>
         <summary><b>Supported algorithms:</b></summary>
         
-        - [x] [DeepPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#deeppose-cvpr-2014) (CVPR'2014)
-        - [x] [CPM](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#cpm-cvpr-2016) (CVPR'2016)
-        - [x] [Hourglass](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016)
-        - [ ] [SimpleBaseline3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017)
-        - [ ] [Associative Embedding](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017)
-        - [x] [SimpleBaseline2D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018)
-        - [x] [DSNT](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#dsnt-2018) (ArXiv'2021)
-        - [x] [HRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019)
-        - [x] [IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#ipr-eccv-2018) (ECCV'2018)
-        - [ ] [VideoPose3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019)
-        - [x] [HRNetv2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrnetv2-tpami-2019) (TPAMI'2019)
-        - [x] [MSPN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mspn-arxiv-2019) (ArXiv'2019)
-        - [x] [SCNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020)
-        - [ ] [HigherHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#higherhrnet-cvpr-2020) (CVPR'2020)
-        - [x] [RSN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#rsn-eccv-2020) (ECCV'2020)
-        - [ ] [InterNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020)
-        - [ ] [VoxelPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#voxelpose-eccv-2020) (ECCV'2020)
-        - [x] [LiteHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#litehrnet-cvpr-2021) (CVPR'2021)
-        - [x] [ViPNAS](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021)
-        - [x] [Debias-IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#debias-ipr-iccv-2021) (ICCV'2021)
-        - [x] [SimCC](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simcc-eccv-2022) (ECCV'2022)
+        - [x] [DeepPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#deeppose-cvpr-2014) (CVPR'2014)
+        - [x] [CPM](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#cpm-cvpr-2016) (CVPR'2016)
+        - [x] [Hourglass](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016)
+        - [x] [SimpleBaseline3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017)
+        - [ ] [Associative Embedding](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017)
+        - [x] [SimpleBaseline2D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018)
+        - [x] [DSNT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#dsnt-2018) (ArXiv'2021)
+        - [x] [HRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019)
+        - [x] [IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#ipr-eccv-2018) (ECCV'2018)
+        - [x] [VideoPose3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019)
+        - [x] [HRNetv2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrnetv2-tpami-2019) (TPAMI'2019)
+        - [x] [MSPN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mspn-arxiv-2019) (ArXiv'2019)
+        - [x] [SCNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020)
+        - [ ] [HigherHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#higherhrnet-cvpr-2020) (CVPR'2020)
+        - [x] [RSN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#rsn-eccv-2020) (ECCV'2020)
+        - [ ] [InterNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020)
+        - [ ] [VoxelPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#voxelpose-eccv-2020) (ECCV'2020)
+        - [x] [LiteHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#litehrnet-cvpr-2021) (CVPR'2021)
+        - [x] [ViPNAS](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021)
+        - [x] [Debias-IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#debias-ipr-iccv-2021) (ICCV'2021)
+        - [x] [SimCC](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simcc-eccv-2022) (ECCV'2022)
         
         </details>
         
-        <details open>
+        <details close>
         <summary><b>Supported techniques:</b></summary>
         
-        - [ ] [FPN](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017)
-        - [ ] [FP16](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017)
-        - [ ] [Wingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018)
-        - [ ] [AdaptiveWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019)
-        - [x] [DarkPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#darkpose-cvpr-2020) (CVPR'2020)
-        - [x] [UDP](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020)
-        - [ ] [Albumentations](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#albumentations-information-2020) (Information'2020)
-        - [ ] [SoftWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#softwingloss-tip-2021) (TIP'2021)
-        - [x] [RLE](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#rle-iccv-2021) (ICCV'2021)
+        - [x] [FPN](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017)
+        - [x] [FP16](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017)
+        - [x] [Wingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018)
+        - [x] [AdaptiveWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019)
+        - [x] [DarkPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#darkpose-cvpr-2020) (CVPR'2020)
+        - [x] [UDP](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020)
+        - [x] [Albumentations](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#albumentations-information-2020) (Information'2020)
+        - [x] [SoftWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#softwingloss-tip-2021) (TIP'2021)
+        - [x] [RLE](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#rle-iccv-2021) (ICCV'2021)
         
         </details>
         
-        <details open>
-        <summary><b>Supported <a href="https://mmpose.readthedocs.io/en/1.x/dataset_zoo.html">datasets</a>:</b></summary>
+        <details close>
+        <summary><b>Supported datasets:</b></summary>
         
-        - [x] [AFLW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\] (ICCVW'2011)
-        - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013)
-        - [x] [COFW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cofw-iccv-2013) \[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013)
-        - [x] [MPII](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\] (CVPR'2014)
-        - [x] [Human3.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014)
-        - [x] [COCO](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-eccv-2014) \[[homepage](http://cocodataset.org/)\] (ECCV'2014)
-        - [x] [CMU Panoptic](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015)
-        - [x] [DeepFashion](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016)
-        - [x] [300W](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#300w-imavis-2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
-        - [x] [RHD](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017)
-        - [x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017)
-        - [x] [AI Challenger](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/AI_Challenger_2017)\] (ArXiv'2017)
-        - [x] [MHP](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018)
-        - [x] [WFLW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018)
-        - [x] [PoseTrack18](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/download.php)\] (CVPR'2018)
-        - [x] [OCHuman](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
-        - [x] [CrowdPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019)
-        - [x] [MPII-TRB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019)
-        - [x] [FreiHand](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-freiburg.de/projects/freihand/)\] (ICCV'2019)
-        - [x] [Animal-Pose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#animal-pose-iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019)
-        - [x] [OneHand10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019)
-        - [x] [Vinegar Fly](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Nature Methods'2019)
-        - [x] [Desert Locust](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
-        - [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
-        - [x] [ATRW](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020)
-        - [x] [Halpe](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/Halpe-FullBody/)\] (CVPR'2020)
-        - [x] [COCO-WholeBody](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-wholebody-eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\] (ECCV'2020)
-        - [x] [MacaquePose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020)
-        - [x] [InterHand2.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/InterHand2.6M/)\] (ECCV'2020)
-        - [x] [AP-10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021)
-        - [x] [Horse-10](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#horse-10-wacv-2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
+        - [x] [AFLW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\] (ICCVW'2011)
+        - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013)
+        - [x] [COFW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cofw-iccv-2013) \[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013)
+        - [x] [MPII](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\] (CVPR'2014)
+        - [x] [Human3.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014)
+        - [x] [COCO](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-eccv-2014) \[[homepage](http://cocodataset.org/)\] (ECCV'2014)
+        - [x] [CMU Panoptic](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015)
+        - [x] [DeepFashion](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016)
+        - [x] [300W](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#300w-imavis-2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
+        - [x] [RHD](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017)
+        - [x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017)
+        - [x] [AI Challenger](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/AI_Challenger_2017)\] (ArXiv'2017)
+        - [x] [MHP](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018)
+        - [x] [WFLW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018)
+        - [x] [PoseTrack18](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/download.php)\] (CVPR'2018)
+        - [x] [OCHuman](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
+        - [x] [CrowdPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019)
+        - [x] [MPII-TRB](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019)
+        - [x] [FreiHand](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-freiburg.de/projects/freihand/)\] (ICCV'2019)
+        - [x] [Animal-Pose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#animal-pose-iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019)
+        - [x] [OneHand10K](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019)
+        - [x] [Vinegar Fly](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Nature Methods'2019)
+        - [x] [Desert Locust](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
+        - [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019)
+        - [x] [ATRW](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020)
+        - [x] [Halpe](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/Halpe-FullBody/)\] (CVPR'2020)
+        - [x] [COCO-WholeBody](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-wholebody-eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\] (ECCV'2020)
+        - [x] [MacaquePose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020)
+        - [x] [InterHand2.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/InterHand2.6M/)\] (ECCV'2020)
+        - [x] [AP-10K](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021)
+        - [x] [Horse-10](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#horse-10-wacv-2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
+        - [x] [Human-Art](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#human-art-cvpr-2023) \[[homepage](https://idea-research.github.io/HumanArt/)\] (CVPR'2023)
+        - [x] [LaPa](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#lapa-aaai-2020) \[[homepage](https://github.com/JDAI-CV/lapa-dataset)\] (AAAI'2020)
         
         </details>
         
-        <details open>
+        <details close>
         <summary><b>Supported backbones:</b></summary>
         
-        - [x] [AlexNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012)
-        - [x] [VGG](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vgg-iclr-2015) (ICLR'2015)
-        - [x] [ResNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016)
-        - [x] [ResNext](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017)
-        - [x] [SEResNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018)
-        - [x] [ShufflenetV1](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018)
-        - [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018)
-        - [x] [MobilenetV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018)
-        - [x] [ResNetV1D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnetv1d-cvpr-2019) (CVPR'2019)
-        - [x] [ResNeSt](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020)
-        - [x] [Swin](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021)
-        - [x] [HRFormer](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021)
-        - [x] [PVT](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021)
-        - [x] [PVTV2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvtv2-cvmj-2022) (CVMJ'2022)
+        - [x] [AlexNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012)
+        - [x] [VGG](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#vgg-iclr-2015) (ICLR'2015)
+        - [x] [ResNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016)
+        - [x] [ResNext](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017)
+        - [x] [SEResNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018)
+        - [x] [ShufflenetV1](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018)
+        - [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018)
+        - [x] [MobilenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018)
+        - [x] [ResNetV1D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnetv1d-cvpr-2019) (CVPR'2019)
+        - [x] [ResNeSt](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020)
+        - [x] [Swin](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021)
+        - [x] [HRFormer](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021)
+        - [x] [PVT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021)
+        - [x] [PVTV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvtv2-cvmj-2022) (CVMJ'2022)
         
         </details>
         
         ### Model Request
         
-        We will keep up with the latest progress of the community, and support more popular algorithms and frameworks. If you have any feature requests, please feel free to leave a comment in [MMPose Roadmap](https://github.com/open-mmlab/mmpose/issues/9).
+        We will keep up with the latest progress of the community, and support more popular algorithms and frameworks. If you have any feature requests, please feel free to leave a comment in [MMPose Roadmap](https://github.com/open-mmlab/mmpose/issues/2258).
         
         ## Contributing
         
-        We appreciate all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://mmpose.readthedocs.io/en/1.x/notes/contribution_guide.html) for the contributing guideline.
+        We appreciate all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://mmpose.readthedocs.io/en/latest/contribution_guide.html) for the contributing guideline.
         
         ## Acknowledgement
         
         MMPose is an open source project that is contributed by researchers and engineers from various colleges and companies.
         We appreciate all the contributors who implement their methods or add new features, as well as users who give valuable feedbacks.
         We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new models.
         
@@ -293,32 +353,31 @@
         
         This project is released under the [Apache 2.0 license](LICENSE).
         
         ## Projects in OpenMMLab
         
         - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab foundational library for training deep learning models.
         - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer vision.
-        - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
-        - [MMClassification](https://github.com/open-mmlab/mmclassification): OpenMMLab image classification toolbox and benchmark.
+        - [MMPreTrain](https://github.com/open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark.
+        - [MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced, **G**enerative and **I**ntelligent **C**reation toolbox.
         - [MMDetection](https://github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
         - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's next-generation platform for general 3D object detection.
         - [MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and benchmark.
+        - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
         - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab semantic segmentation toolbox and benchmark.
         - [MMOCR](https://github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
         - [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation toolbox and benchmark.
         - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox and benchmark.
-        - [MMSelfSup](https://github.com/open-mmlab/mmselfsup): OpenMMLab self-supervised learning toolbox and benchmark.
-        - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
         - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and benchmark.
         - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and benchmark.
-        - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab video perception toolbox and benchmark.
         - [MMFlow](https://github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark.
-        - [MMEditing](https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing toolbox.
-        - [MMGeneration](https://github.com/open-mmlab/mmgeneration): OpenMMLab image and video generative models toolbox.
         - [MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment Framework.
+        - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and benchmark.
+        - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab packages.
+        - [Playground](https://github.com/open-mmlab/playground): A central hub for gathering and showcasing amazing projects built upon OpenMMLab.
         
 Keywords: computer vision,pose estimation
 Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
```

#### html2text {}

```diff
@@ -1,302 +1,334 @@
-Metadata-Version: 2.1 Name: mmpose Version: 1.0.0rc1 Summary: OpenMMLab Pose
+Metadata-Version: 2.1 Name: mmpose Version: 1.1.0 Summary: OpenMMLab Pose
 Estimation Toolbox and Benchmark. Home-page: https://github.com/open-mmlab/
 mmpose Author: MMPose Contributors Author-email: openmmlab@gmail.com License:
 Apache License 2.0 Description:
                           [resources/mmpose-logo.png]
                                        
            OpenMMLab website HOT  OpenMMLab platform TRY_IT_OUT
                                        
        [![Documentation](https://readthedocs.org/projects/mmpose/badge/
-   ?version=latest)](https://mmpose.readthedocs.io/en/1.x/?badge=latest) [!
+  ?version=latest)](https://mmpose.readthedocs.io/en/latest/?badge=latest) [!
   [actions](https://github.com/open-mmlab/mmpose/workflows/build/badge.svg)]
 (https://github.com/open-mmlab/mmpose/actions) [![codecov](https://codecov.io/
- gh/open-mmlab/mmpose/branch/1.x/graph/badge.svg)](https://codecov.io/gh/open-
-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://pypi.org/
-project/mmpose/) [![LICENSE](https://img.shields.io/github/license/open-mmlab/
-   mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/master/LICENSE) [!
-[Average time to resolve an issue](https://isitmaintained.com/badge/resolution/
-    open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues) [!
- [Percentage of issues still open](https://isitmaintained.com/badge/open/open-
+  gh/open-mmlab/mmpose/branch/latest/graph/badge.svg)](https://codecov.io/gh/
+  open-mmlab/mmpose) [![PyPI](https://img.shields.io/pypi/v/mmpose)](https://
+ pypi.org/project/mmpose/) [![LICENSE](https://img.shields.io/github/license/
+open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/blob/main/LICENSE)
+    [![Average time to resolve an issue](https://isitmaintained.com/badge/
+resolution/open-mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
+[![Percentage of issues still open](https://isitmaintained.com/badge/open/open-
        mmlab/mmpose.svg)](https://github.com/open-mmlab/mmpose/issues)
-         [Documentation](https://mmpose.readthedocs.io/en/1.x/) |
-[Installation](https://mmpose.readthedocs.io/en/1.x/installation.html) |
-    [Model Zoo](https://mmpose.readthedocs.io/en/1.x/model_zoo.html) |
-      [Papers](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-  algorithms.html) | [Update News](https://mmpose.readthedocs.io/en/1.x/
- notes/changelog.html) | [Reporting Issues](https://github.com/open-mmlab/
-         mmpose/issues/new/choose) | [RTMPose](/projects/rtmpose/)
+       [Documentation](https://mmpose.readthedocs.io/en/latest/) |
+        [Installation](https://mmpose.readthedocs.io/en/latest/
+ installation.html) | [Model Zoo](https://mmpose.readthedocs.io/en/latest/
+    model_zoo.html) | [Papers](https://mmpose.readthedocs.io/en/latest/
+        model_zoo_papers/algorithms.html) | [Update News](https://
+mmpose.readthedocs.io/en/latest/notes/changelog.html) | [Reporting Issues]
+  (https://github.com/open-mmlab/mmpose/issues/new/choose) | [RTMPose](/
+                              projects/rtmpose/)
 
 ## Introduction English | [](README_CN.md) MMPose is an open-source
 toolbox for pose estimation based on PyTorch. It is a part of the [OpenMMLab
-project](https://github.com/open-mmlab). The master branch works with **PyTorch
-1.6+**. https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-
+project](https://github.com/open-mmlab). The main branch works with **PyTorch
+1.8+**. https://user-images.githubusercontent.com/15977946/124654387-0fd3c500-
 ded1-11eb-84f6-24eeddbf4d91.mp4
  Major Features - **Support diverse tasks** We support a wide spectrum of
 mainstream pose analysis tasks in current research community, including 2d
 multi-person human pose estimation, 2d hand pose estimation, 2d face landmark
 detection, 133 keypoint whole-body human pose estimation, 3d human mesh
 recovery, fashion landmark detection and animal pose estimation. See [Demo]
-(demo/docs/) for more information. - **Higher efficiency and higher accuracy**
-MMPose implements multiple state-of-the-art (SOTA) deep learning models,
-including both top-down & bottom-up approaches. We achieve faster training
-speed and higher accuracy than other popular codebases, such as [HRNet](https:/
-/github.com/leoxiaobin/deep-high-resolution-net.pytorch). See [benchmark.md]
-(docs/en/notes/benchmark.md) for more information. - **Support for various
-datasets** The toolbox directly supports multiple popular and representative
-datasets, COCO, AIC, MPII, MPII-TRB, OCHuman etc. See [dataset_zoo](docs/en/
-dataset_zoo) for more information. - **Well designed, tested and documented**
-We decompose MMPose into different components and one can easily construct a
-customized pose estimation framework by combining different modules. We provide
-detailed documentation and API reference, as well as unittests.  ## What's New
-- We are excited to release **RTMPose**, a real-time pose estimation framework
-including: - A family of lightweight pose estimation models with state-of-the-
-art performance - Inference APIs for Python, C++, C#, Java, etc. Easy to
-integrate into your applications and empower real-time stable pose estimation -
-Cross-platform deployment with various backends - A step-by-step guide to
-training and deploying your own models Checkout our [project page](/projects/
-rtmpose/) and [technical report](https://arxiv.org/abs/2303.07399) for more
-information! ![rtmpose_intro](https://user-images.githubusercontent.com/
-13503330/219269619-935499e5-bdd9-49ea-8104-3c7796dbd862.png) - Welcome to
+(demo/docs/en) for more information. - **Higher efficiency and higher
+accuracy** MMPose implements multiple state-of-the-art (SOTA) deep learning
+models, including both top-down & bottom-up approaches. We achieve faster
+training speed and higher accuracy than other popular codebases, such as
+[HRNet](https://github.com/leoxiaobin/deep-high-resolution-net.pytorch). See
+[benchmark.md](docs/en/notes/benchmark.md) for more information. - **Support
+for various datasets** The toolbox directly supports multiple popular and
+representative datasets, COCO, AIC, MPII, MPII-TRB, OCHuman etc. See
+[dataset_zoo](docs/en/dataset_zoo) for more information. - **Well designed,
+tested and documented** We decompose MMPose into different components and one
+can easily construct a customized pose estimation framework by combining
+different modules. We provide detailed documentation and API reference, as well
+as unittests.  ## What's New - We are glad to support 3 new datasets: - (CVPR
+2023) [Human-Art](https://github.com/IDEA-Research/HumanArt) - (CVPR 2022)
+[Animal Kingdom](https://github.com/sutdcv/Animal-Kingdom) - (AAAI 2020) [LaPa]
+(https://github.com/JDAI-CV/lapa-dataset/) ![image](https://github.com/open-
+mmlab/mmpose/assets/13503330/c9171dbb-7e7a-4c39-98e3-c92932182efb) - Welcome to
 [*projects of MMPose*](/projects/README.md), where you can access to the latest
 features of MMPose, and share your ideas and codes with the community at once.
 Contribution to MMPose will be simple and smooth: - Provide an easy and agile
 way to integrate algorithms, features and applications into MMPose - Allow
 flexible code structure and style; only need a short code review process -
 Build individual projects with full power of MMPose but not bound up with heavy
 frameworks - Checkout new projects: - [RTMPose](/projects/rtmpose/) - [YOLOX-
-Pose (coming soon)](<>) - [MMPose4AIGC (coming soon)](<>) - Become a
-contributors and make MMPose greater. Start your journey from the [example
-project](/projects/example_project/)
-- 2022-03-15: MMPose [v1.0.0rc1](https://github.com/open-mmlab/mmpose/releases/
-tag/v1.0.0rc1) is released. Major updates include: - Release [RTMPose](/
-projects/rtmpose/), a high-performance real-time pose estimation framework
-based on MMPose - Support [ViTPose](/configs/body_2d_keypoint/topdown_heatmap/
-coco/vitpose_coco.md) (NeurIPS'22), [CID](/configs/body_2d_keypoint/cid/coco/
-hrnet_coco.md) (CVPR'22) and [DEKR](/configs/body_2d_keypoint/dekr/) (CVPR'21)
-- Add [*Inferencer*](/docs/en/user_guides/inference.md#out-of-the-box-
-inferencer), a convenient interface for inference and visualization See the
-full [release note](https://github.com/open-mmlab/mmpose/releases/tag/
-v1.0.0rc1) for more exciting updates brought by MMPose v1.0.0rc1! ##
-Installation Below are quick steps for installation: ```shell conda create -
-n open-mmlab python=3.8 pytorch==1.10.1 torchvision==0.11.2 cudatoolkit=11.3 -
-c pytorch -y conda activate open-mmlab pip install openmim git clone -b 1.x
-https://github.com/open-mmlab/mmpose.git cd mmpose mim install -e . ``` Please
-refer to [installation.md](https://mmpose.readthedocs.io/en/1.x/
-installation.html) for more detailed installation and dataset preparation. ##
-Getting Started We provided a series of tutorials about the basic usage of
-MMPose for new users: - [About Configs](https://mmpose.readthedocs.io/en/1.x/
-user_guides/configs.html) - [Add New Dataset](https://mmpose.readthedocs.io/en/
-1.x/user_guides/prepare_datasets.html) - [Keypoint Encoding & Decoding](https:/
-/mmpose.readthedocs.io/en/1.x/user_guides/codecs.html) - [Inference with
-Existing Models](https://mmpose.readthedocs.io/en/1.x/user_guides/
-inference.html) - [Train and Test](https://mmpose.readthedocs.io/en/1.x/
-user_guides/train_and_test.html) - [Visualization Tools](https://
-mmpose.readthedocs.io/en/1.x/user_guides/visualization.html) - [Other Useful
-Tools](https://mmpose.readthedocs.io/en/1.x/user_guides/useful_tools.html) ##
-Model Zoo Results and models are available in the **README.md** of each
-method's config directory. A summary can be found in the [Model Zoo](https://
-mmpose.readthedocs.io/en/1.x/modelzoo.html) page.  Supported algorithms: - [x]
-[DeepPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+Pose](/projects/yolox_pose/) - [MMPose4AIGC](/projects/mmpose4aigc/) - [Simple
+Keypoints](/projects/skps/) - Become a contributors and make MMPose greater.
+Start your journey from the [example project](/projects/example_project/)
+- 2023-07-04: MMPose [v1.1.0](https://github.com/open-mmlab/mmpose/releases/
+tag/v1.1.0) is officially released, with the main updates including: - Support
+new datasets: Human-Art, Animal Kingdom and LaPa. - Support new config type
+that is more user-friendly and flexible. - Improve RTMPose with better
+performance. - Migrate 3D pose estimation models on h36m. - Inference speedup
+and webcam inference with all demo scripts. Please refer to the [release notes]
+(https://github.com/open-mmlab/mmpose/releases/tag/v1.1.0) for more updates
+brought by MMPose v1.1.0! ## 0.x / 1.x Migration MMPose v1.0.0 is a major
+update, including many API and config file changes. Currently, a part of the
+algorithms have been migrated to v1.0.0, and the remaining algorithms will be
+completed in subsequent versions. We will show the migration progress in the
+following list.  Migration Progress | Algorithm | Status | | :-----------------
+--------------- | :---------: | | MTUT (CVPR 2019) | | | MSPN (ArXiv 2019) |
+done | | InterNet (ECCV 2020) | | | DEKR (CVPR 2021) | done | | HigherHRNet
+(CVPR 2020) | | | DeepPose (CVPR 2014) | done | | RLE (ICCV 2021) | done | |
+SoftWingloss (TIP 2021) | done | | VideoPose3D (CVPR 2019) | done | | Hourglass
+(ECCV 2016) | done | | LiteHRNet (CVPR 2021) | done | | AdaptiveWingloss (ICCV
+2019) | done | | SimpleBaseline2D (ECCV 2018) | done | | PoseWarper (NeurIPS
+2019) | | | SimpleBaseline3D (ICCV 2017) | done | | HMR (CVPR 2018) | | | UDP
+(CVPR 2020) | done | | VIPNAS (CVPR 2021) | done | | Wingloss (CVPR 2018) |
+done | | DarkPose (CVPR 2020) | done | | Associative Embedding (NIPS 2017) | in
+progress | | VoxelPose (ECCV 2020) | | | RSN (ECCV 2020) | done | | CID (CVPR
+2022) | done | | CPM (CVPR 2016) | done | | HRNet (CVPR 2019) | done | |
+HRNetv2 (TPAMI 2019) | done | | SCNet (CVPR 2020) | done |  If your algorithm
+has not been migrated, you can continue to use the [0.x branch](https://
+github.com/open-mmlab/mmpose/tree/0.x) and [old documentation](https://
+mmpose.readthedocs.io/en/0.x/). ## Installation Please refer to
+[installation.md](https://mmpose.readthedocs.io/en/latest/installation.html)
+for more detailed installation and dataset preparation. ## Getting Started We
+provided a series of tutorials about the basic usage of MMPose for new users:
+1. For the basic usage of MMPose: - [A 20-minute Tour to MMPose](https://
+mmpose.readthedocs.io/en/latest/guide_to_framework.html) - [Demos](https://
+mmpose.readthedocs.io/en/latest/demos.html) - [Inference](https://
+mmpose.readthedocs.io/en/latest/user_guides/inference.html) - [Configs](https:/
+/mmpose.readthedocs.io/en/latest/user_guides/configs.html) - [Prepare Datasets]
+(https://mmpose.readthedocs.io/en/latest/user_guides/prepare_datasets.html) -
+[Train and Test](https://mmpose.readthedocs.io/en/latest/user_guides/
+train_and_test.html) 2. For developers who wish to develop based on MMPose: -
+[Learn about Codecs](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+codecs.html) - [Dataflow in MMPose](https://mmpose.readthedocs.io/en/latest/
+advanced_guides/dataflow.html) - [Implement New Models](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/implement_new_models.html) -
+[Customize Datasets](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+customize_datasets.html) - [Customize Data Transforms](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/customize_transforms.html) -
+[Customize Optimizer](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+customize_optimizer.html) - [Customize Logging](https://mmpose.readthedocs.io/
+en/latest/advanced_guides/customize_logging.html) - [How to Deploy](https://
+mmpose.readthedocs.io/en/latest/advanced_guides/how_to_deploy.html) - [Model
+Analysis](https://mmpose.readthedocs.io/en/latest/advanced_guides/
+model_analysis.html) - [Migration Guide](https://mmpose.readthedocs.io/en/
+latest/migration.html) 3. For researchers and developers who are willing to
+contribute to MMPose: - [Contribution Guide](https://mmpose.readthedocs.io/en/
+latest/contribution_guide.html) 4. For some common issues, we provide a FAQ
+list: - [FAQ](https://mmpose.readthedocs.io/en/latest/faq.html) ## Model Zoo
+Results and models are available in the **README.md** of each method's config
+directory. A summary can be found in the [Model Zoo](https://
+mmpose.readthedocs.io/en/latest/model_zoo.html) page.  Supported algorithms: -
+[x] [DeepPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#deeppose-cvpr-2014) (CVPR'2014) - [x] [CPM](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#cpm-cvpr-2016)
-(CVPR'2016) - [x] [Hourglass](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016) - [ ]
-[SimpleBaseline3D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#cpm-cvpr-2016)
+(CVPR'2016) - [x] [Hourglass](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#hourglass-eccv-2016) (ECCV'2016) - [x]
+[SimpleBaseline3D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#simplebaseline3d-iccv-2017) (ICCV'2017) - [ ] [Associative
-Embedding](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+Embedding](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#associative-embedding-nips-2017) (NeurIPS'2017) - [x]
-[SimpleBaseline2D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[SimpleBaseline2D](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#simplebaseline2d-eccv-2018) (ECCV'2018) - [x] [DSNT](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#dsnt-2018)
-(ArXiv'2021) - [x] [HRNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#dsnt-2018)
+(ArXiv'2021) - [x] [HRNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#hrnet-cvpr-2019) (CVPR'2019) - [x] [IPR](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#ipr-eccv-2018)
-(ECCV'2018) - [ ] [VideoPose3D](https://mmpose.readthedocs.io/en/1.x/
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#ipr-eccv-
+2018) (ECCV'2018) - [x] [VideoPose3D](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/algorithms.html#videopose3d-cvpr-2019) (CVPR'2019) - [x]
-[HRNetv2](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[HRNetv2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 backbones.html#hrnetv2-tpami-2019) (TPAMI'2019) - [x] [MSPN](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#mspn-arxiv-2019)
-(ArXiv'2019) - [x] [SCNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#mspn-arxiv-
+2019) (ArXiv'2019) - [x] [SCNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#scnet-cvpr-2020) (CVPR'2020) - [ ]
-[HigherHRNet](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[HigherHRNet](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 backbones.html#higherhrnet-cvpr-2020) (CVPR'2020) - [x] [RSN](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#rsn-eccv-2020)
-(ECCV'2020) - [ ] [InterNet](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#rsn-eccv-2020)
+(ECCV'2020) - [ ] [InterNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/algorithms.html#internet-eccv-2020) (ECCV'2020) - [ ]
-[VoxelPose](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[VoxelPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#voxelpose-eccv-2020) (ECCV'2020) - [x] [LiteHRNet](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#litehrnet-cvpr-
-2021) (CVPR'2021) - [x] [ViPNAS](https://mmpose.readthedocs.io/en/1.x/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#litehrnet-cvpr-
+2021) (CVPR'2021) - [x] [ViPNAS](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#vipnas-cvpr-2021) (CVPR'2021) - [x] [Debias-
-IPR](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+IPR](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 algorithms.html#debias-ipr-iccv-2021) (ICCV'2021) - [x] [SimCC](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/algorithms.html#simcc-eccv-2022)
-(ECCV'2022)   Supported techniques: - [ ] [FPN](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/techniques.html#fpn-cvpr-2017) (CVPR'2017) - [ ] [FP16]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#fp16-
-arxiv-2017) (ArXiv'2017) - [ ] [Wingloss](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/techniques.html#wingloss-cvpr-2018) (CVPR'2018) - [ ]
-[AdaptiveWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/algorithms.html#simcc-eccv-
+2022) (ECCV'2022)   Supported techniques: - [x] [FPN](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#fpn-cvpr-2017)
+(CVPR'2017) - [x] [FP16](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/techniques.html#fp16-arxiv-2017) (ArXiv'2017) - [x] [Wingloss]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+techniques.html#wingloss-cvpr-2018) (CVPR'2018) - [x] [AdaptiveWingloss](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/
 techniques.html#adaptivewingloss-iccv-2019) (ICCV'2019) - [x] [DarkPose](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#darkpose-cvpr-
-2020) (CVPR'2020) - [x] [UDP](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020) - [ ]
-[Albumentations](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-techniques.html#albumentations-information-2020) (Information'2020) - [ ]
-[SoftWingloss](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#darkpose-
+cvpr-2020) (CVPR'2020) - [x] [UDP](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/techniques.html#udp-cvpr-2020) (CVPR'2020) - [x]
+[Albumentations](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+techniques.html#albumentations-information-2020) (Information'2020) - [x]
+[SoftWingloss](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 techniques.html#softwingloss-tip-2021) (TIP'2021) - [x] [RLE](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/techniques.html#rle-iccv-2021)
+mmpose.readthedocs.io/en/latest/model_zoo_papers/techniques.html#rle-iccv-2021)
 (ICCV'2021)   Supported datasets: - [x] [AFLW](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://
+en/latest/model_zoo_papers/datasets.html#aflw-iccvw-2011) \[[homepage](https://
 www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/)\]
-(ICCVW'2011) - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/1.x/
+(ICCVW'2011) - [x] [sub-JHMDB](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#jhmdb-iccv-2013) \[[homepage](http://
 jhmdb.is.tue.mpg.de/dataset)\] (ICCV'2013) - [x] [COFW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cofw-iccv-2013) \[
-[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013) - [x]
-[MPII](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cofw-iccv-2013)
+\[[homepage](http://www.vision.caltech.edu/xpburgos/ICCV13/)\] (ICCV'2013) -
+[x] [MPII](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#mpii-cvpr-2014) \[[homepage](http://human-pose.mpi-inf.mpg.de/)\]
-(CVPR'2014) - [x] [Human3.6M](https://mmpose.readthedocs.io/en/1.x/
+(CVPR'2014) - [x] [Human3.6M](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#human3-6m-tpami-2014) \[[homepage](http://
 vision.imar.ro/human3.6m/description.php)\] (TPAMI'2014) - [x] [COCO](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-eccv-2014) \[
-[homepage](http://cocodataset.org/)\] (ECCV'2014) - [x] [CMU Panoptic](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#cmu-panoptic-iccv-
-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015) - [x]
-[DeepFashion](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-eccv-2014)
+\[[homepage](http://cocodataset.org/)\] (ECCV'2014) - [x] [CMU Panoptic](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#cmu-panoptic-
+iccv-2015) \[[homepage](http://domedb.perception.cs.cmu.edu/)\] (ICCV'2015) -
+[x] [DeepFashion](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#deepfashion-cvpr-2016) \[[homepage](http://mmlab.ie.cuhk.edu.hk/
 projects/DeepFashion/LandmarkDetection.html)\] (CVPR'2016) - [x] [300W](https:/
-/mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#300w-imavis-2016)
-\[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016) - [x]
-[RHD](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#rhd-
-iccv-2017) \[[homepage](https://lmb.informatik.uni-freiburg.de/resources/
-datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017) - [x] [CMU Panoptic
-HandDB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage](http://
-domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017) - [x] [AI Challenger]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#ai-
-challenger-arxiv-2017) \[[homepage](https://github.com/AIChallenger/
-AI_Challenger_2017)\] (ArXiv'2017) - [x] [MHP](https://mmpose.readthedocs.io/
-en/1.x/model_zoo_papers/datasets.html#mhp-acm-mm-2018) \[[homepage](https://lv-
-mhp.github.io/dataset)\] (ACM MM'2018) - [x] [WFLW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#wflw-cvpr-2018) \[
-[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\] (CVPR'2018) - [x]
-[PoseTrack18](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#posetrack18-cvpr-2018) \[[homepage](https://posetrack.net/users/
-download.php)\] (CVPR'2018) - [x] [OCHuman](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#ochuman-cvpr-2019) \[[homepage](https://
-github.com/liruilong940607/OCHumanApi)\] (CVPR'2019) - [x] [CrowdPose](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#crowdpose-cvpr-
-2019) \[[homepage](https://github.com/Jeff-sjtu/CrowdPose)\] (CVPR'2019) - [x]
-[MPII-TRB](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#mpii-trb-iccv-2019) \[[homepage](https://github.com/
-kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019) - [x]
-[FreiHand](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+/mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#300w-imavis-
+2016) \[[homepage](https://ibug.doc.ic.ac.uk/resources/300-W/)\] (IMAVIS'2016)
+- [x] [RHD](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#rhd-iccv-2017) \[[homepage](https://lmb.informatik.uni-
+freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\] (ICCV'2017) -
+[x] [CMU Panoptic HandDB](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#cmu-panoptic-handdb-cvpr-2017) \[[homepage]
+(http://domedb.perception.cs.cmu.edu/handdb.html)\] (CVPR'2017) - [x] [AI
+Challenger](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#ai-challenger-arxiv-2017) \[[homepage](https://github.com/
+AIChallenger/AI_Challenger_2017)\] (ArXiv'2017) - [x] [MHP](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#mhp-acm-mm-2018)
+\[[homepage](https://lv-mhp.github.io/dataset)\] (ACM MM'2018) - [x] [WFLW]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#wflw-
+cvpr-2018) \[[homepage](https://wywu.github.io/projects/LAB/WFLW.html)\]
+(CVPR'2018) - [x] [PoseTrack18](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#posetrack18-cvpr-2018) \[[homepage](https://
+posetrack.net/users/download.php)\] (CVPR'2018) - [x] [OCHuman](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#ochuman-cvpr-
+2019) \[[homepage](https://github.com/liruilong940607/OCHumanApi)\] (CVPR'2019)
+- [x] [CrowdPose](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#crowdpose-cvpr-2019) \[[homepage](https://github.com/Jeff-sjtu/
+CrowdPose)\] (CVPR'2019) - [x] [MPII-TRB](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/datasets.html#mpii-trb-iccv-2019) \[[homepage](https://
+github.com/kennymckormick/Triplet-Representation-of-human-Body)\] (ICCV'2019) -
+[x] [FreiHand](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#freihand-iccv-2019) \[[homepage](https://lmb.informatik.uni-
 freiburg.de/projects/freihand/)\] (ICCV'2019) - [x] [Animal-Pose](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#animal-pose-iccv-
-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\] (ICCV'2019) -
-[x] [OneHand10K](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://www.yangangwang.com/
-papers/WANG-MCC-2018-10.html)\] (TCSVT'2019) - [x] [Vinegar Fly](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#vinegar-fly-nature-
-methods-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\]
-(Nature Methods'2019) - [x] [Desert Locust](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#desert-locust-elife-2019) \[[homepage]
-(https://github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [Grvys
-Zebra](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-datasets.html#grevys-zebra-elife-2019) \[[homepage](https://github.com/
-jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [ATRW](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#atrw-acm-mm-2020) \
-[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020) - [x]
-[Halpe](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#animal-pose-
+iccv-2019) \[[homepage](https://sites.google.com/view/animal-pose/)\]
+(ICCV'2019) - [x] [OneHand10K](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#onehand10k-tcsvt-2019) \[[homepage](https://
+www.yangangwang.com/papers/WANG-MCC-2018-10.html)\] (TCSVT'2019) - [x] [Vinegar
+Fly](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#vinegar-fly-nature-methods-2019) \[[homepage](https://github.com/
+jgraving/DeepPoseKit-Data)\] (Nature Methods'2019) - [x] [Desert Locust](https:
+//mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#desert-locust-
+elife-2019) \[[homepage](https://github.com/jgraving/DeepPoseKit-Data)\]
+(Elife'2019) - [x] [Grvys Zebra](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/datasets.html#grevys-zebra-elife-2019) \[[homepage](https://
+github.com/jgraving/DeepPoseKit-Data)\] (Elife'2019) - [x] [ATRW](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#atrw-acm-mm-
+2020) \[[homepage](https://cvwc2019.github.io/challenge.html)\] (ACM MM'2020) -
+[x] [Halpe](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#halpe-cvpr-2020) \[[homepage](https://github.com/Fang-Haoshu/
 Halpe-FullBody/)\] (CVPR'2020) - [x] [COCO-WholeBody](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#coco-wholebody-
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#coco-wholebody-
 eccv-2020) \[[homepage](https://github.com/jin-s13/COCO-WholeBody/)\]
-(ECCV'2020) - [x] [MacaquePose](https://mmpose.readthedocs.io/en/1.x/
+(ECCV'2020) - [x] [MacaquePose](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/datasets.html#macaquepose-biorxiv-2020) \[[homepage](http://
 www.pri.kyoto-u.ac.jp/datasets/macaquepose/index.html)\] (bioRxiv'2020) - [x]
-[InterHand2.6M](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
+[InterHand2.6M](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
 datasets.html#interhand2-6m-eccv-2020) \[[homepage](https://mks0601.github.io/
 InterHand2.6M/)\] (ECCV'2020) - [x] [AP-10K](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https://
-github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021) - [x] [Horse-10](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/datasets.html#horse-10-wacv-2021)
-\[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021)
-Supported backbones: - [x] [AlexNet](https://mmpose.readthedocs.io/en/1.x/
+latest/model_zoo_papers/datasets.html#ap-10k-neurips-2021) \[[homepage](https:/
+/github.com/AlexTheBad/AP-10K)\] (NeurIPS'2021) - [x] [Horse-10](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#horse-10-wacv-
+2021) \[[homepage](http://www.mackenziemathislab.org/horse10)\] (WACV'2021) -
+[x] [Human-Art](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+datasets.html#human-art-cvpr-2023) \[[homepage](https://idea-
+research.github.io/HumanArt/)\] (CVPR'2023) - [x] [LaPa](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/datasets.html#lapa-aaai-2020)
+\[[homepage](https://github.com/JDAI-CV/lapa-dataset)\] (AAAI'2020)   Supported
+backbones: - [x] [AlexNet](https://mmpose.readthedocs.io/en/latest/
 model_zoo_papers/backbones.html#alexnet-neurips-2012) (NeurIPS'2012) - [x]
-[VGG](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#vgg-
-iclr-2015) (ICLR'2015) - [x] [ResNet](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#resnet-cvpr-2016) (CVPR'2016) - [x] [ResNext]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnext-
-cvpr-2017) (CVPR'2017) - [x] [SEResNet](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#seresnet-cvpr-2018) (CVPR'2018) - [x]
-[ShufflenetV1](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-backbones.html#shufflenetv1-cvpr-2018) (CVPR'2018) - [x] [ShufflenetV2](https:/
-/mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#shufflenetv2-
-eccv-2018) (ECCV'2018) - [x] [MobilenetV2](https://mmpose.readthedocs.io/en/
-1.x/model_zoo_papers/backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018) - [x]
-[ResNetV1D](https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/
-backbones.html#resnetv1d-cvpr-2019) (CVPR'2019) - [x] [ResNeSt](https://
-mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#resnest-arxiv-
-2020) (ArXiv'2020) - [x] [Swin](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#swin-cvpr-2021) (CVPR'2021) - [x] [HRFormer]
-(https://mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#hrformer-
-nips-2021) (NIPS'2021) - [x] [PVT](https://mmpose.readthedocs.io/en/1.x/
-model_zoo_papers/backbones.html#pvt-iccv-2021) (ICCV'2021) - [x] [PVTV2](https:
-//mmpose.readthedocs.io/en/1.x/model_zoo_papers/backbones.html#pvtv2-cvmj-2022)
-(CVMJ'2022)  ### Model Request We will keep up with the latest progress of the
-community, and support more popular algorithms and frameworks. If you have any
-feature requests, please feel free to leave a comment in [MMPose Roadmap]
-(https://github.com/open-mmlab/mmpose/issues/9). ## Contributing We appreciate
-all contributions to improve MMPose. Please refer to [CONTRIBUTING.md](https://
-mmpose.readthedocs.io/en/1.x/notes/contribution_guide.html) for the
-contributing guideline. ## Acknowledgement MMPose is an open source project
-that is contributed by researchers and engineers from various colleges and
-companies. We appreciate all the contributors who implement their methods or
-add new features, as well as users who give valuable feedbacks. We wish that
-the toolbox and benchmark could serve the growing research community by
-providing a flexible toolkit to reimplement existing methods and develop their
-own new models. ## Citation If you find this project useful in your research,
-please consider cite: ```bibtex @misc{mmpose2020, title={OpenMMLab Pose
-Estimation Toolbox and Benchmark}, author={MMPose Contributors}, howpublished =
-{\url{https://github.com/open-mmlab/mmpose}}, year={2020} } ``` ## License This
-project is released under the [Apache 2.0 license](LICENSE). ## Projects in
-OpenMMLab - [MMEngine](https://github.com/open-mmlab/mmengine): OpenMMLab
-foundational library for training deep learning models. - [MMCV](https://
-github.com/open-mmlab/mmcv): OpenMMLab foundational library for computer
-vision. - [MIM](https://github.com/open-mmlab/mim): MIM installs OpenMMLab
-packages. - [MMClassification](https://github.com/open-mmlab/mmclassification):
-OpenMMLab image classification toolbox and benchmark. - [MMDetection](https://
-github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and benchmark.
-- [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): OpenMMLab's
-next-generation platform for general 3D object detection. - [MMRotate](https://
-github.com/open-mmlab/mmrotate): OpenMMLab rotated object detection toolbox and
-benchmark. - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation):
-OpenMMLab semantic segmentation toolbox and benchmark. - [MMOCR](https://
-github.com/open-mmlab/mmocr): OpenMMLab text detection, recognition, and
-understanding toolbox. - [MMPose](https://github.com/open-mmlab/mmpose):
-OpenMMLab pose estimation toolbox and benchmark. - [MMHuman3D](https://
-github.com/open-mmlab/mmhuman3d): OpenMMLab 3D human parametric model toolbox
-and benchmark. - [MMSelfSup](https://github.com/open-mmlab/mmselfsup):
-OpenMMLab self-supervised learning toolbox and benchmark. - [MMRazor](https://
-github.com/open-mmlab/mmrazor): OpenMMLab model compression toolbox and
-benchmark. - [MMFewShot](https://github.com/open-mmlab/mmfewshot): OpenMMLab
-fewshot learning toolbox and benchmark. - [MMAction2](https://github.com/open-
-mmlab/mmaction2): OpenMMLab's next-generation action understanding toolbox and
-benchmark. - [MMTracking](https://github.com/open-mmlab/mmtracking): OpenMMLab
-video perception toolbox and benchmark. - [MMFlow](https://github.com/open-
-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. - [MMEditing]
-(https://github.com/open-mmlab/mmediting): OpenMMLab image and video editing
-toolbox. - [MMGeneration](https://github.com/open-mmlab/mmgeneration):
-OpenMMLab image and video generative models toolbox. - [MMDeploy](https://
-github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment Framework.
-Keywords: computer vision,pose estimation Platform: UNKNOWN Classifier:
-Development Status :: 4 - Beta Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Programming Language :: Python :: 3 Classifier: Programming Language :: Python
-:: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
-Programming Language :: Python :: 3.9 Requires-Python: >=3.7 Description-
-Content-Type: text/markdown Provides-Extra: all Provides-Extra: tests Provides-
-Extra: optional Provides-Extra: mim
+[VGG](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#vgg-iclr-2015) (ICLR'2015) - [x] [ResNet](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnet-cvpr-
+2016) (CVPR'2016) - [x] [ResNext](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#resnext-cvpr-2017) (CVPR'2017) - [x] [SEResNet]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#seresnet-cvpr-2018) (CVPR'2018) - [x] [ShufflenetV1](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#shufflenetv1-
+cvpr-2018) (CVPR'2018) - [x] [ShufflenetV2](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/backbones.html#shufflenetv2-eccv-2018) (ECCV'2018) -
+[x] [MobilenetV2](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#mobilenetv2-cvpr-2018) (CVPR'2018) - [x] [ResNetV1D](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#resnetv1d-cvpr-
+2019) (CVPR'2019) - [x] [ResNeSt](https://mmpose.readthedocs.io/en/latest/
+model_zoo_papers/backbones.html#resnest-arxiv-2020) (ArXiv'2020) - [x] [Swin]
+(https://mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#swin-
+cvpr-2021) (CVPR'2021) - [x] [HRFormer](https://mmpose.readthedocs.io/en/
+latest/model_zoo_papers/backbones.html#hrformer-nips-2021) (NIPS'2021) - [x]
+[PVT](https://mmpose.readthedocs.io/en/latest/model_zoo_papers/
+backbones.html#pvt-iccv-2021) (ICCV'2021) - [x] [PVTV2](https://
+mmpose.readthedocs.io/en/latest/model_zoo_papers/backbones.html#pvtv2-cvmj-
+2022) (CVMJ'2022)  ### Model Request We will keep up with the latest progress
+of the community, and support more popular algorithms and frameworks. If you
+have any feature requests, please feel free to leave a comment in [MMPose
+Roadmap](https://github.com/open-mmlab/mmpose/issues/2258). ## Contributing We
+appreciate all contributions to improve MMPose. Please refer to
+[CONTRIBUTING.md](https://mmpose.readthedocs.io/en/latest/
+contribution_guide.html) for the contributing guideline. ## Acknowledgement
+MMPose is an open source project that is contributed by researchers and
+engineers from various colleges and companies. We appreciate all the
+contributors who implement their methods or add new features, as well as users
+who give valuable feedbacks. We wish that the toolbox and benchmark could serve
+the growing research community by providing a flexible toolkit to reimplement
+existing methods and develop their own new models. ## Citation If you find this
+project useful in your research, please consider cite: ```bibtex @misc
+{mmpose2020, title={OpenMMLab Pose Estimation Toolbox and Benchmark}, author=
+{MMPose Contributors}, howpublished = {\url{https://github.com/open-mmlab/
+mmpose}}, year={2020} } ``` ## License This project is released under the
+[Apache 2.0 license](LICENSE). ## Projects in OpenMMLab - [MMEngine](https://
+github.com/open-mmlab/mmengine): OpenMMLab foundational library for training
+deep learning models. - [MMCV](https://github.com/open-mmlab/mmcv): OpenMMLab
+foundational library for computer vision. - [MMPreTrain](https://github.com/
+open-mmlab/mmpretrain): OpenMMLab pre-training toolbox and benchmark. -
+[MMagic](https://github.com/open-mmlab/mmagic): Open**MM**Lab **A**dvanced,
+**G**enerative and **I**ntelligent **C**reation toolbox. - [MMDetection](https:
+//github.com/open-mmlab/mmdetection): OpenMMLab detection toolbox and
+benchmark. - [MMDetection3D](https://github.com/open-mmlab/mmdetection3d):
+OpenMMLab's next-generation platform for general 3D object detection. -
+[MMRotate](https://github.com/open-mmlab/mmrotate): OpenMMLab rotated object
+detection toolbox and benchmark. - [MMTracking](https://github.com/open-mmlab/
+mmtracking): OpenMMLab video perception toolbox and benchmark. -
+[MMSegmentation](https://github.com/open-mmlab/mmsegmentation): OpenMMLab
+semantic segmentation toolbox and benchmark. - [MMOCR](https://github.com/open-
+mmlab/mmocr): OpenMMLab text detection, recognition, and understanding toolbox.
+- [MMPose](https://github.com/open-mmlab/mmpose): OpenMMLab pose estimation
+toolbox and benchmark. - [MMHuman3D](https://github.com/open-mmlab/mmhuman3d):
+OpenMMLab 3D human parametric model toolbox and benchmark. - [MMFewShot](https:
+//github.com/open-mmlab/mmfewshot): OpenMMLab fewshot learning toolbox and
+benchmark. - [MMAction2](https://github.com/open-mmlab/mmaction2): OpenMMLab's
+next-generation action understanding toolbox and benchmark. - [MMFlow](https://
+github.com/open-mmlab/mmflow): OpenMMLab optical flow toolbox and benchmark. -
+[MMDeploy](https://github.com/open-mmlab/mmdeploy): OpenMMLab Model Deployment
+Framework. - [MMRazor](https://github.com/open-mmlab/mmrazor): OpenMMLab model
+compression toolbox and benchmark. - [MIM](https://github.com/open-mmlab/mim):
+MIM installs OpenMMLab packages. - [Playground](https://github.com/open-mmlab/
+playground): A central hub for gathering and showcasing amazing projects built
+upon OpenMMLab. Keywords: computer vision,pose estimation Platform: UNKNOWN
+Classifier: Development Status :: 4 - Beta Classifier: License :: OSI Approved
+:: Apache Software License Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3 Classifier: Programming
+Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9 Requires-Python: >=3.7
+Description-Content-Type: text/markdown Provides-Extra: all Provides-Extra:
+tests Provides-Extra: optional Provides-Extra: mim
```

### Comparing `mmpose-1.0.0rc1/mmpose.egg-info/SOURCES.txt` & `mmpose-1.1.0/mmpose.egg-info/SOURCES.txt`

 * *Files 16% similar despite different names*

```diff
@@ -7,41 +7,49 @@
 mmpose/version.py
 mmpose.egg-info/PKG-INFO
 mmpose.egg-info/SOURCES.txt
 mmpose.egg-info/dependency_links.txt
 mmpose.egg-info/not-zip-safe
 mmpose.egg-info/requires.txt
 mmpose.egg-info/top_level.txt
+mmpose/.mim/dataset-index.yml
 mmpose/.mim/model-index.yml
 mmpose/.mim/configs/_base_/default_runtime.py
 mmpose/.mim/configs/_base_/datasets/300w.py
 mmpose/.mim/configs/_base_/datasets/aflw.py
 mmpose/.mim/configs/_base_/datasets/aic.py
+mmpose/.mim/configs/_base_/datasets/ak.py
 mmpose/.mim/configs/_base_/datasets/animalpose.py
 mmpose/.mim/configs/_base_/datasets/ap10k.py
 mmpose/.mim/configs/_base_/datasets/atrw.py
 mmpose/.mim/configs/_base_/datasets/campus.py
 mmpose/.mim/configs/_base_/datasets/coco.py
 mmpose/.mim/configs/_base_/datasets/coco_aic.py
+mmpose/.mim/configs/_base_/datasets/coco_openpose.py
 mmpose/.mim/configs/_base_/datasets/coco_wholebody.py
 mmpose/.mim/configs/_base_/datasets/coco_wholebody_face.py
 mmpose/.mim/configs/_base_/datasets/coco_wholebody_hand.py
 mmpose/.mim/configs/_base_/datasets/cofw.py
 mmpose/.mim/configs/_base_/datasets/crowdpose.py
+mmpose/.mim/configs/_base_/datasets/deepfashion2.py
 mmpose/.mim/configs/_base_/datasets/deepfashion_full.py
 mmpose/.mim/configs/_base_/datasets/deepfashion_lower.py
 mmpose/.mim/configs/_base_/datasets/deepfashion_upper.py
 mmpose/.mim/configs/_base_/datasets/fly.py
 mmpose/.mim/configs/_base_/datasets/freihand2d.py
 mmpose/.mim/configs/_base_/datasets/h36m.py
 mmpose/.mim/configs/_base_/datasets/halpe.py
+mmpose/.mim/configs/_base_/datasets/halpe26.py
 mmpose/.mim/configs/_base_/datasets/horse10.py
+mmpose/.mim/configs/_base_/datasets/humanart.py
+mmpose/.mim/configs/_base_/datasets/humanart_aic.py
 mmpose/.mim/configs/_base_/datasets/interhand2d.py
 mmpose/.mim/configs/_base_/datasets/interhand3d.py
 mmpose/.mim/configs/_base_/datasets/jhmdb.py
+mmpose/.mim/configs/_base_/datasets/lapa.py
 mmpose/.mim/configs/_base_/datasets/locust.py
 mmpose/.mim/configs/_base_/datasets/macaque.py
 mmpose/.mim/configs/_base_/datasets/mhp.py
 mmpose/.mim/configs/_base_/datasets/mpi_inf_3dhp.py
 mmpose/.mim/configs/_base_/datasets/mpii.py
 mmpose/.mim/configs/_base_/datasets/mpii_trb.py
 mmpose/.mim/configs/_base_/datasets/ochuman.py
@@ -50,75 +58,141 @@
 mmpose/.mim/configs/_base_/datasets/panoptic_hand2d.py
 mmpose/.mim/configs/_base_/datasets/posetrack18.py
 mmpose/.mim/configs/_base_/datasets/rhd2d.py
 mmpose/.mim/configs/_base_/datasets/shelf.py
 mmpose/.mim/configs/_base_/datasets/wflw.py
 mmpose/.mim/configs/_base_/datasets/zebra.py
 mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/rtmpose-m_8xb64-210e_ap10k-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/rtmpose/ap10k/rtmpose_ap10k.yml
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/hrnet_animalkingdom.yml
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P1-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P2-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_amphibian-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_bird-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_fish-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_mammal-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ak/td-hm_hrnet-w32_8xb32-300e_animalkingdom_P3_reptile-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/hrnet_animalpose.yml
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/resnet_animalpose.yml
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w32_8xb64-210e_animalpose-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_hrnet-w48_8xb64-210e_animalpose-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res101_8xb64-210e_animalpose-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res152_8xb32-210e_animalpose-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/animalpose/td-hm_res50_8xb64-210e_animalpose-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/cspnext-m_udp_8xb64-210e_ap10k-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/cspnext_udp_ap10k.yml
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/hrnet_ap10k.yml
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/resnet_ap10k.yml
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w32_8xb64-210e_ap10k-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_hrnet-w48_8xb64-210e_ap10k-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res101_8xb64-210e_ap10k-256x256.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/ap10k/td-hm_res50_8xb64-210e_ap10k-256x256.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/resnet_locust.yml
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res101_8xb64-210e_locust-160x160.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res152_8xb32-210e_locust-160x160.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/locust/td-hm_res50_8xb64-210e_locust-160x160.py
+mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/resnet_zebra.yml
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res101_8xb64-210e_zebra-160x160.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res152_8xb32-210e_zebra-160x160.py
 mmpose/.mim/configs/animal_2d_keypoint/topdown_heatmap/zebra/td-hm_res50_8xb64-210e_zebra-160x160.py
 mmpose/.mim/configs/body_2d_keypoint/associative_embedding/coco/ae_hrnet-w32_8xb24-300e_coco-512x512.py
 mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w32_8xb20-140e_coco-512x512.py
 mmpose/.mim/configs/body_2d_keypoint/cid/coco/cid_hrnet-w48_8xb20-140e_coco-512x512.py
+mmpose/.mim/configs/body_2d_keypoint/cid/coco/hrnet_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w32_8xb10-140e_coco-512x512.py
 mmpose/.mim/configs/body_2d_keypoint/dekr/coco/dekr_hrnet-w48_8xb10-140e_coco-640x640.py
+mmpose/.mim/configs/body_2d_keypoint/dekr/coco/hrnet_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w32_8xb10-300e_crowdpose-512x512.py
 mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/dekr_hrnet-w48_8xb5-300e_crowdpose-640x640.py
+mmpose/.mim/configs/body_2d_keypoint/dekr/crowdpose/hrnet_crowdpose.yml
 mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_8xb64-210e_coco-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_debias-8xb64-210e_coco-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/ipr_res50_dsnt-8xb64-210e_coco-256x256.py
+mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/resnet_debias_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/resnet_dsnt_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/integral_regression/coco/resnet_ipr_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb256-420e_body8-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb256-420e_body8-384x288.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb512-700e_body8-halpe26-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-l_8xb512-700e_body8-halpe26-384x288.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb256-420e_body8-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb256-420e_body8-384x288.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb512-700e_body8-halpe26-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-m_8xb512-700e_body8-halpe26-384x288.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-s_8xb1024-700e_body8-halpe26-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-s_8xb256-420e_body8-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-t_8xb1024-700e_body8-halpe26-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-t_8xb256-420e_body8-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose-x_8xb256-700e_body8-halpe26-384x288.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose_body8-coco.yml
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/body8/rtmpose_body8-halpe26.yml
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_aic-coco-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-l_8xb256-420e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_aic-coco-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-m_8xb256-420e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-s_8xb256-420e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose-t_8xb256-420e_coco-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/coco/rtmpose_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/rtmpose-m_8xb64-210e_crowdpose-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/crowdpose/rtmpose_crowdpose.yml
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-l_8xb256-420e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-m_8xb256-420e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-s_8xb256-420e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose-t_8xb256-420e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/humanart/rtmpose_humanart.yml
 mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/rtmpose-m_8xb64-210e_mpii-256x256.py
+mmpose/.mim/configs/body_2d_keypoint/rtmpose/mpii/rtmpose_mpii.yml
 mmpose/.mim/configs/body_2d_keypoint/simcc/coco/mobilenetv2_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/simcc/coco/resnet_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_mobilenetv2_wo-deconv-8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb32-140e_coco-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_res50_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/simcc/coco/simcc_vipnas-mbv3_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/simcc/coco/vipnas_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/simcc/mpii/simcc_res50_wo-deconv-8xb64-210e_mpii-256x256.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/hrnet_aic.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/resnet_aic.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_hrnet-w32_8xb64-210e_aic-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/aic/td-hm_res101_8xb64-210e_aic-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/alexnet_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cpm_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-l_udp_8xb256-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-m_udp_8xb256-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-s_udp_8xb256-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_aic-coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext-tiny_udp_8xb256-210e_coco-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/cspnext_udp_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hourglass_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrformer_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_augmentation_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_dark_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/hrnet_udp_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/litehrnet_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/mobilenetv2_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/mspn_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/pvt_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnest_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnet_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnet_dark_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnetv1d_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/resnext_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/rsn_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/scnet_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/seresnet_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/shufflenetv1_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/shufflenetv2_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/swin_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xmspn50_8xb32-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_2xrsn50_8xb32-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xmspn50_8xb32-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_3xrsn50_8xb32-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_4xmspn50_8xb32-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base-simple_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_ViTPose-base_8xb64-210e_coco-256x192.py
@@ -218,31 +292,60 @@
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-b-p4-w7_8xb32-210e_coco-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-l-p4-w7_8xb32-210e_coco-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_swin-t-p4-w7_8xb32-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vgg16-bn_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-mbv3_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_vipnas-res50_8xb64-210e_coco-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vgg_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vipnas_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/coco/vitpose_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/cspnext-m_udp_8xb64-210e_crowpose-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/cspnext_udp_crowdpose.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/hrnet_crowdpose.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/resnet_crowdpose.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_hrnet-w32_8xb64-210e_crowdpose-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res101_8xb64-210e_crowdpose-320x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res152_8xb64-210e_crowdpose-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/crowdpose/td-hm_res50_8xb64-210e_crowdpose-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/hrnet_humanart.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-base_8xb64-210e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-huge_8xb64-210e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-large_8xb64-210e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_ViTPose-small_8xb64-210e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_hrnet-w32_8xb64-210e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/td-hm_hrnet-w48_8xb32-210e_humanart-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/humanart/vitpose_humanart.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/cpm_jhmdb.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/resnet_jhmdb.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub1-368x368.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub2-368x368.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_cpm_8xb32-40e_jhmdb-sub3-368x368.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub1-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub2-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50-2deconv_8xb64-40e_jhmdb-sub3-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub1-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub2-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/jhmdb/td-hm_res50_8xb64-20e_jhmdb-sub3-256x256.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cpm_mpii.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cspnext-m_udp_8xb64-210e_mpii-256x256.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/cspnext_udp_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/hourglass_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/hrnet_dark_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/hrnet_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/litehrnet_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/mobilenetv2_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/resnet_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/resnetv1d_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/resnext_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/scnet_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/seresnet_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/shufflenetv1_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/shufflenetv2_mpii.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_cpm_8xb64-210e_mpii-368x368.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb32-210e_mpii-384x384.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hourglass52_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w32_dark-8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_hrnet-w48_dark-8xb64-210e_mpii-256x256.py
@@ -259,75 +362,159 @@
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet101_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_scnet50_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet101_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet152_8xb32-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_seresnet50_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv1_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/mpii/td-hm_shufflenetv2_8xb64-210e_mpii-256x256.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/hrnet_posetrack18.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/resnet_posetrack18.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w32_8xb64-20e_posetrack18-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_hrnet-w48_8xb64-20e_posetrack18-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_heatmap/posetrack18/td-hm_res50_8xb64-20e_posetrack18-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/mobilenetv2_rle_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/resnet_coco.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/resnet_rle_coco.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_mobilenetv2_rle-pretrained-8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res101_rle-8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res152_rle-8xb64-210e_coco-384x288.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-8xb64-210e_coco-256x192.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/coco/td-reg_res50_rle-pretrained-8xb64-210e_coco-256x192.py
+mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/resnet_mpii.yml
+mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/resnet_rle_mpii.yml
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res101_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res152_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_8xb64-210e_mpii-256x256.py
 mmpose/.mim/configs/body_2d_keypoint/topdown_regression/mpii/td-reg_res50_rle-8xb64-210e_mpii-256x256.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_simplebaseline3d_8xb64-200e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-1frm-supv-cpn-ft_8xb128-80e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-243frm-supv-cpn-ft_8xb128-200e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-243frm-supv_8xb128-80e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-semi-supv-cpn-ft_8xb64-200e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-semi-supv_8xb64-200e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-27frm-supv_8xb128-80e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/pose-lift_videopose3d-81frm-supv_8xb128-80e_h36m.py
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/simplebaseline3d_h36m.yml
+mmpose/.mim/configs/body_3d_keypoint/pose_lift/h36m/videopose3d_h36m.yml
 mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/rtmpose-m_8xb32-60e_coco-wholebody-face-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/coco_wholebody_face/rtmpose_coco_wholebody_face.yml
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose-m_8xb256-120e_face6-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose-s_8xb256-120e_face6-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose-t_8xb256-120e_face6-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/face6/rtmpose_face6.yml
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/lapa/rtmpose-m_8xb64-120e_lapa-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/lapa/rtmpose_lapa.yml
 mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/rtmpose-m_8xb64-60e_wflw-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/rtmpose/wflw/rtmpose_wflw.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/hrnetv2_300w.yml
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/300w/td-hm_hrnetv2-w18_8xb64-60e_300w-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/hrnetv2_aflw.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/hrnetv2_dark_aflw.yml
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_8xb64-60e_aflw-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/aflw/td-hm_hrnetv2-w18_dark-8xb64-60e_aflw-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/hourglass_coco_wholebody_face.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/hrnetv2_coco_wholebody_face.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/hrnetv2_dark_coco_wholebody_face.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/mobilenetv2_coco_wholebody_face.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/resnet_coco_wholebody_face.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/scnet_coco_wholebody_face.yml
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hourglass52_8xb32-60e_coco-wholebody-face-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_8xb32-60e_coco-wholebody-face-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_hrnetv2-w18_dark-8xb32-60e_coco-wholebody-face-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_mobilenetv2_8xb32-60e_coco-wholebody-face-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_res50_8xb32-60e_coco-wholebody-face-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/coco_wholebody_face/td-hm_scnet50_8xb32-60e_coco-wholebody-face-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/hrnetv2_cofw.yml
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/cofw/td-hm_hrnetv2-w18_8xb64-60e_cofw-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_awing_wflw.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_dark_wflw.yml
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/hrnetv2_wflw.yml
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_8xb64-60e_wflw-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_awing-8xb64-60e_wflw-256x256.py
 mmpose/.mim/configs/face_2d_keypoint/topdown_heatmap/wflw/td-hm_hrnetv2-w18_dark-8xb64-60e_wflw-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/resnet_softwingloss_wflw.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/resnet_wflw.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/resnet_wingloss_wflw.yml
+mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_8xb64-210e_wflw-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_softwingloss_8xb64-210e_wflw-256x256.py
+mmpose/.mim/configs/face_2d_keypoint/topdown_regression/wflw/td-reg_res50_wingloss_8xb64-210e_wflw-256x256.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/res50_deepfasion2.yml
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-long-sleeved-dress-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-skirt-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_1xb64-210e_deepfasion2-vest-dress-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_2xb64-210e_deepfasion2-trousers-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_3xb64-210e_deepfasion2-shorts-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-short-sleeved-dress-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-sling-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-sling-dress-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_4xb64-210e_deepfasion2-vest-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_6xb64-210e_deepfasion2-short-sleeved-shirt-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-long-sleeved-outwear-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-long-sleeved-shirt-256x192.py
+mmpose/.mim/configs/fashion_2d_keypoint/topdown_heatmap/deepfashion2/td-hm_res50_8xb64-210e_deepfasion2-short-sleeved-outwear-256x192.py
 mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose-m_8xb32-210e_coco-wholebody-hand-256x256.py
+mmpose/.mim/configs/hand_2d_keypoint/rtmpose/coco_wholebody_hand/rtmpose_coco_wholebody_hand.yml
+mmpose/.mim/configs/hand_2d_keypoint/rtmpose/hand5/rtmpose-m_8xb256-210e_hand5-256x256.py
+mmpose/.mim/configs/hand_2d_keypoint/rtmpose/hand5/rtmpose_hand5.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/hourglass_coco_wholebody_hand.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/hrnetv2_coco_wholebody_hand.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/hrnetv2_dark_coco_wholebody_hand.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/litehrnet_coco_wholebody_hand.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/mobilenetv2_coco_wholebody_hand.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/resnet_coco_wholebody_hand.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/scnet_coco_wholebody_hand.yml
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hourglass52_8xb32-210e_coco-wholebody-hand-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_8xb32-210e_coco-wholebody-hand-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_hrnetv2-w18_dark-8xb32-210e_coco-wholebody-hand-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_litehrnet-w18_8xb32-210e_coco-wholebody-hand-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_mobilenetv2_8xb32-210e_coco-wholebody-hand-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_res50_8xb32-210e_coco-wholebody-hand-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/coco_wholebody_hand/td-hm_scnet50_8xb32-210e_coco-wholebody-hand-256x256.py
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/resnet_freihand2d.yml
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/freihand2d/td-hm_res50_8xb64-100e_freihand2d-224x224.py
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/hrnetv2_dark_onehand10k.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/hrnetv2_onehand10k.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/hrnetv2_udp_onehand10k.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/mobilenetv2_onehand10k.yml
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/resnet_onehand10k.yml
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_8xb64-210e_onehand10k-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_dark-8xb64-210e_onehand10k-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_hrnetv2-w18_udp-8xb64-210e_onehand10k-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_mobilenetv2_8xb64-210e_onehand10k-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/onehand10k/td-hm_res50_8xb32-210e_onehand10k-256x256.py
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/hrnetv2_dark_rhd2d.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/hrnetv2_rhd2d.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/hrnetv2_udp_rhd2d.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/mobilenetv2_rhd2d.yml
+mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/resnet_rhd2d.yml
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_8xb64-210e_rhd2d-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_dark-8xb64-210e_rhd2d-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_hrnetv2-w18_udp-8xb64-210e_rhd2d-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_mobilenetv2_8xb64-210e_rhd2d-256x256.py
 mmpose/.mim/configs/hand_2d_keypoint/topdown_heatmap/rhd2d/td-hm_res50_8xb64-210e_rhd2d-256x256.py
+mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/resnet_onehand10k.yml
 mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/onehand10k/td-reg_res50_8xb64-210e_onehand10k-256x256.py
+mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/resnet_rhd2d.yml
 mmpose/.mim/configs/hand_2d_keypoint/topdown_regression/rhd2d/td-reg_res50_8xb64-210e_rhd2d-256x256.py
 mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb32-270e_coco-wholebody-384x288.py
 mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose-m_8xb64-270e_coco-wholebody-256x192.py
+mmpose/.mim/configs/wholebody_2d_keypoint/rtmpose/coco-wholebody/rtmpose_coco-wholebody.yml
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-l_udp_8xb64-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext-m_udp_8xb64-210e_coco-wholebody-256x192.py
+mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/cspnext_udp_coco-wholebody.yml
+mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/hrnet_coco-wholebody.yml
+mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/hrnet_dark_coco-wholebody.yml
+mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/resnet_coco-wholebody.yml
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_8xb64-210e_coco-wholebody-384x288.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w32_dark-8xb64-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_8xb32-210e_coco-wholebody-384x288.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_hrnet-w48_dark-8xb32-210e_coco-wholebody-384x288.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res101_8xb32-210e_coco-wholebody-256x192.py
@@ -336,139 +523,142 @@
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res152_8xb32-210e_coco-wholebody-384x288.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_res50_8xb64-210e_coco-wholebody-384x288.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_8xb64-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-mbv3_dark-8xb64-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_8xb64-210e_coco-wholebody-256x192.py
 mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/td-hm_vipnas-res50_dark-8xb64-210e_coco-wholebody-256x192.py
+mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/vipnas_coco-wholebody.yml
+mmpose/.mim/configs/wholebody_2d_keypoint/topdown_heatmap/coco-wholebody/vipnas_dark_coco-wholebody.yml
+mmpose/.mim/demo/body3d_pose_lifter_demo.py
 mmpose/.mim/demo/bottomup_demo.py
 mmpose/.mim/demo/image_demo.py
 mmpose/.mim/demo/inferencer_demo.py
 mmpose/.mim/demo/topdown_demo_with_mmdet.py
-mmpose/.mim/demo/webcam_demo.py
 mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_1class.py
 mmpose/.mim/demo/mmdetection_cfg/cascade_rcnn_x101_64x4d_fpn_coco.py
 mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_1class.py
 mmpose/.mim/demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py
 mmpose/.mim/demo/mmdetection_cfg/mask_rcnn_r50_fpn_2x_coco.py
 mmpose/.mim/demo/mmdetection_cfg/ssdlite_mobilenetv2-scratch_8xb24-600e_coco.py
+mmpose/.mim/demo/mmdetection_cfg/ssdlite_mobilenetv2_scratch_600e_onehand.py
 mmpose/.mim/demo/mmdetection_cfg/yolov3_d53_320_273e_coco.py
 mmpose/.mim/demo/mmdetection_cfg/yolox-s_8xb8-300e_coco-face.py
 mmpose/.mim/demo/mmtracking_cfg/deepsort_faster-rcnn_fpn_4e_mot17-private-half.py
 mmpose/.mim/demo/mmtracking_cfg/tracktor_faster-rcnn_r50_fpn_4e_mot17-private.py
-mmpose/.mim/demo/webcam_cfg/pose_estimation.py
-mmpose/.mim/demo/webcam_cfg/test_camera.py
 mmpose/.mim/tools/dist_test.sh
 mmpose/.mim/tools/dist_train.sh
 mmpose/.mim/tools/slurm_test.sh
 mmpose/.mim/tools/slurm_train.sh
 mmpose/.mim/tools/test.py
 mmpose/.mim/tools/train.py
 mmpose/.mim/tools/analysis_tools/analyze_logs.py
 mmpose/.mim/tools/analysis_tools/get_flops.py
 mmpose/.mim/tools/analysis_tools/print_config.py
 mmpose/.mim/tools/dataset_converters/h36m_to_coco.py
+mmpose/.mim/tools/dataset_converters/labelstudio2coco.py
+mmpose/.mim/tools/dataset_converters/lapa2coco.py
 mmpose/.mim/tools/dataset_converters/mat2json.py
 mmpose/.mim/tools/dataset_converters/parse_animalpose_dataset.py
 mmpose/.mim/tools/dataset_converters/parse_cofw_dataset.py
 mmpose/.mim/tools/dataset_converters/parse_deepposekit_dataset.py
 mmpose/.mim/tools/dataset_converters/parse_macaquepose_dataset.py
 mmpose/.mim/tools/dataset_converters/preprocess_h36m.py
 mmpose/.mim/tools/dataset_converters/preprocess_mpi_inf_3dhp.py
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_300w.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_aic.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_ap10k.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_coco2017.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_crowdpose.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_freihand.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_hagrid.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_halpe.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_lapa.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_mpii.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_onehand10k.sh
+mmpose/.mim/tools/dataset_converters/scripts/preprocess_wflw.sh
 mmpose/.mim/tools/misc/browse_dataset.py
 mmpose/.mim/tools/misc/keypoints2coco_without_mmdet.py
 mmpose/.mim/tools/misc/publish_model.py
 mmpose/.mim/tools/torchserve/mmpose2torchserve.py
 mmpose/.mim/tools/torchserve/mmpose_handler.py
 mmpose/.mim/tools/torchserve/test_torchserver.py
 mmpose/apis/__init__.py
 mmpose/apis/inference.py
+mmpose/apis/inference_3d.py
+mmpose/apis/inference_tracking.py
 mmpose/apis/inferencers/__init__.py
 mmpose/apis/inferencers/base_mmpose_inferencer.py
 mmpose/apis/inferencers/mmpose_inferencer.py
 mmpose/apis/inferencers/pose2d_inferencer.py
+mmpose/apis/inferencers/pose3d_inferencer.py
 mmpose/apis/inferencers/utils/__init__.py
 mmpose/apis/inferencers/utils/default_det_models.py
-mmpose/apis/webcam/__init__.py
-mmpose/apis/webcam/webcam_executor.py
-mmpose/apis/webcam/nodes/__init__.py
-mmpose/apis/webcam/nodes/base_visualizer_node.py
-mmpose/apis/webcam/nodes/node.py
-mmpose/apis/webcam/nodes/registry.py
-mmpose/apis/webcam/nodes/helper_nodes/__init__.py
-mmpose/apis/webcam/nodes/helper_nodes/monitor_node.py
-mmpose/apis/webcam/nodes/helper_nodes/object_assigner_node.py
-mmpose/apis/webcam/nodes/helper_nodes/recorder_node.py
-mmpose/apis/webcam/nodes/model_nodes/__init__.py
-mmpose/apis/webcam/nodes/model_nodes/detector_node.py
-mmpose/apis/webcam/nodes/model_nodes/pose_estimator_node.py
-mmpose/apis/webcam/nodes/visualizer_nodes/__init__.py
-mmpose/apis/webcam/nodes/visualizer_nodes/bigeye_effect_node.py
-mmpose/apis/webcam/nodes/visualizer_nodes/notice_board_node.py
-mmpose/apis/webcam/nodes/visualizer_nodes/object_visualizer_node.py
-mmpose/apis/webcam/nodes/visualizer_nodes/sunglasses_effect_node.py
-mmpose/apis/webcam/utils/__init__.py
-mmpose/apis/webcam/utils/buffer.py
-mmpose/apis/webcam/utils/event.py
-mmpose/apis/webcam/utils/image_capture.py
-mmpose/apis/webcam/utils/message.py
-mmpose/apis/webcam/utils/misc.py
-mmpose/apis/webcam/utils/pose.py
+mmpose/apis/inferencers/utils/get_model_alias.py
 mmpose/codecs/__init__.py
 mmpose/codecs/associative_embedding.py
 mmpose/codecs/base.py
 mmpose/codecs/decoupled_heatmap.py
+mmpose/codecs/image_pose_lifting.py
 mmpose/codecs/integral_regression_label.py
 mmpose/codecs/megvii_heatmap.py
 mmpose/codecs/msra_heatmap.py
 mmpose/codecs/regression_label.py
 mmpose/codecs/simcc_label.py
 mmpose/codecs/spr.py
 mmpose/codecs/udp_heatmap.py
+mmpose/codecs/video_pose_lifting.py
 mmpose/codecs/utils/__init__.py
 mmpose/codecs/utils/gaussian_heatmap.py
 mmpose/codecs/utils/instance_property.py
 mmpose/codecs/utils/offset_heatmap.py
 mmpose/codecs/utils/post_processing.py
 mmpose/codecs/utils/refinement.py
 mmpose/datasets/__init__.py
 mmpose/datasets/builder.py
 mmpose/datasets/dataset_wrappers.py
 mmpose/datasets/samplers.py
 mmpose/datasets/datasets/__init__.py
 mmpose/datasets/datasets/utils.py
 mmpose/datasets/datasets/animal/__init__.py
+mmpose/datasets/datasets/animal/animalkingdom_dataset.py
 mmpose/datasets/datasets/animal/animalpose_dataset.py
 mmpose/datasets/datasets/animal/ap10k_dataset.py
 mmpose/datasets/datasets/animal/atrw_dataset.py
 mmpose/datasets/datasets/animal/fly_dataset.py
 mmpose/datasets/datasets/animal/horse10_dataset.py
 mmpose/datasets/datasets/animal/locust_dataset.py
 mmpose/datasets/datasets/animal/macaque_dataset.py
 mmpose/datasets/datasets/animal/zebra_dataset.py
 mmpose/datasets/datasets/base/__init__.py
 mmpose/datasets/datasets/base/base_coco_style_dataset.py
+mmpose/datasets/datasets/base/base_mocap_dataset.py
 mmpose/datasets/datasets/body/__init__.py
 mmpose/datasets/datasets/body/aic_dataset.py
 mmpose/datasets/datasets/body/coco_dataset.py
 mmpose/datasets/datasets/body/crowdpose_dataset.py
+mmpose/datasets/datasets/body/humanart_dataset.py
 mmpose/datasets/datasets/body/jhmdb_dataset.py
 mmpose/datasets/datasets/body/mhp_dataset.py
 mmpose/datasets/datasets/body/mpii_dataset.py
 mmpose/datasets/datasets/body/mpii_trb_dataset.py
 mmpose/datasets/datasets/body/ochuman_dataset.py
 mmpose/datasets/datasets/body/posetrack18_dataset.py
 mmpose/datasets/datasets/body/posetrack18_video_dataset.py
+mmpose/datasets/datasets/body3d/__init__.py
+mmpose/datasets/datasets/body3d/h36m_dataset.py
 mmpose/datasets/datasets/face/__init__.py
 mmpose/datasets/datasets/face/aflw_dataset.py
 mmpose/datasets/datasets/face/coco_wholebody_face_dataset.py
 mmpose/datasets/datasets/face/cofw_dataset.py
 mmpose/datasets/datasets/face/face_300w_dataset.py
+mmpose/datasets/datasets/face/lapa_dataset.py
 mmpose/datasets/datasets/face/wflw_dataset.py
 mmpose/datasets/datasets/fashion/__init__.py
+mmpose/datasets/datasets/fashion/deepfashion2_dataset.py
 mmpose/datasets/datasets/fashion/deepfashion_dataset.py
 mmpose/datasets/datasets/hand/__init__.py
 mmpose/datasets/datasets/hand/coco_wholebody_hand_dataset.py
 mmpose/datasets/datasets/hand/freihand_dataset.py
 mmpose/datasets/datasets/hand/onehand10k_dataset.py
 mmpose/datasets/datasets/hand/panoptic_hand2d_dataset.py
 mmpose/datasets/datasets/hand/rhd2d_dataset.py
@@ -477,29 +667,32 @@
 mmpose/datasets/datasets/wholebody/halpe_dataset.py
 mmpose/datasets/transforms/__init__.py
 mmpose/datasets/transforms/bottomup_transforms.py
 mmpose/datasets/transforms/common_transforms.py
 mmpose/datasets/transforms/converting.py
 mmpose/datasets/transforms/formatting.py
 mmpose/datasets/transforms/loading.py
+mmpose/datasets/transforms/pose3d_transforms.py
 mmpose/datasets/transforms/topdown_transforms.py
 mmpose/engine/__init__.py
 mmpose/engine/hooks/__init__.py
 mmpose/engine/hooks/ema_hook.py
 mmpose/engine/hooks/visualization_hook.py
 mmpose/engine/optim_wrappers/__init__.py
 mmpose/engine/optim_wrappers/layer_decay_optim_wrapper.py
 mmpose/evaluation/__init__.py
 mmpose/evaluation/functional/__init__.py
 mmpose/evaluation/functional/keypoint_eval.py
+mmpose/evaluation/functional/mesh_eval.py
 mmpose/evaluation/functional/nms.py
 mmpose/evaluation/metrics/__init__.py
 mmpose/evaluation/metrics/coco_metric.py
 mmpose/evaluation/metrics/coco_wholebody_metric.py
 mmpose/evaluation/metrics/keypoint_2d_metrics.py
+mmpose/evaluation/metrics/keypoint_3d_metrics.py
 mmpose/evaluation/metrics/keypoint_partition_metric.py
 mmpose/evaluation/metrics/posetrack18_metric.py
 mmpose/models/__init__.py
 mmpose/models/builder.py
 mmpose/models/backbones/__init__.py
 mmpose/models/backbones/alexnet.py
 mmpose/models/backbones/base_backbone.py
@@ -544,39 +737,44 @@
 mmpose/models/heads/coord_cls_heads/rtmcc_head.py
 mmpose/models/heads/coord_cls_heads/simcc_head.py
 mmpose/models/heads/heatmap_heads/__init__.py
 mmpose/models/heads/heatmap_heads/ae_head.py
 mmpose/models/heads/heatmap_heads/cid_head.py
 mmpose/models/heads/heatmap_heads/cpm_head.py
 mmpose/models/heads/heatmap_heads/heatmap_head.py
-mmpose/models/heads/heatmap_heads/mix_head.py
 mmpose/models/heads/heatmap_heads/mspn_head.py
 mmpose/models/heads/heatmap_heads/vipnas_head.py
 mmpose/models/heads/hybrid_heads/__init__.py
 mmpose/models/heads/hybrid_heads/dekr_head.py
+mmpose/models/heads/hybrid_heads/vis_head.py
 mmpose/models/heads/regression_heads/__init__.py
 mmpose/models/heads/regression_heads/dsnt_head.py
 mmpose/models/heads/regression_heads/integral_regression_head.py
 mmpose/models/heads/regression_heads/regression_head.py
 mmpose/models/heads/regression_heads/rle_head.py
+mmpose/models/heads/regression_heads/temporal_regression_head.py
+mmpose/models/heads/regression_heads/trajectory_regression_head.py
 mmpose/models/losses/__init__.py
 mmpose/models/losses/ae_loss.py
 mmpose/models/losses/classification_loss.py
 mmpose/models/losses/heatmap_loss.py
 mmpose/models/losses/loss_wrappers.py
 mmpose/models/losses/regression_loss.py
 mmpose/models/necks/__init__.py
+mmpose/models/necks/fmap_proc_neck.py
 mmpose/models/necks/fpn.py
 mmpose/models/necks/gap_neck.py
 mmpose/models/necks/posewarper_neck.py
 mmpose/models/pose_estimators/__init__.py
 mmpose/models/pose_estimators/base.py
 mmpose/models/pose_estimators/bottomup.py
+mmpose/models/pose_estimators/pose_lifter.py
 mmpose/models/pose_estimators/topdown.py
 mmpose/models/utils/__init__.py
+mmpose/models/utils/check_and_update_config.py
 mmpose/models/utils/ckpt_convert.py
 mmpose/models/utils/geometry.py
 mmpose/models/utils/ops.py
 mmpose/models/utils/realnvp.py
 mmpose/models/utils/regularizations.py
 mmpose/models/utils/rtmcc_block.py
 mmpose/models/utils/transformer.py
@@ -598,15 +796,18 @@
 mmpose/utils/hooks.py
 mmpose/utils/logger.py
 mmpose/utils/setup_env.py
 mmpose/utils/tensor_utils.py
 mmpose/utils/timer.py
 mmpose/utils/typing.py
 mmpose/visualization/__init__.py
+mmpose/visualization/fast_visualizer.py
 mmpose/visualization/local_visualizer.py
+mmpose/visualization/local_visualizer_3d.py
+mmpose/visualization/opencv_backend_visualizer.py
 mmpose/visualization/simcc_vis.py
 requirements/albu.txt
 requirements/build.txt
 requirements/docs.txt
 requirements/mminstall.txt
 requirements/optional.txt
 requirements/poseval.txt
```

### Comparing `mmpose-1.0.0rc1/setup.py` & `mmpose-1.1.0/setup.py`

 * *Files 15% similar despite different names*

```diff
@@ -2,14 +2,20 @@
 import os.path as osp
 import platform
 import shutil
 import sys
 import warnings
 from setuptools import find_packages, setup
 
+try:
+    import google.colab  # noqa
+    ON_COLAB = True
+except ImportError:
+    ON_COLAB = False
+
 
 def readme():
     with open('README.md', encoding='utf-8') as f:
         content = f.read()
     return content
 
 
@@ -74,14 +80,24 @@
                         # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies
                         version, platform_deps = map(str.strip,
                                                      rest.split(';'))
                         info['platform_deps'] = platform_deps
                     else:
                         version = rest  # NOQA
                     info['version'] = (op, version)
+
+            if ON_COLAB and info['package'] == 'xtcocotools':
+                # Due to an incompatibility between the Colab platform and the
+                # pre-built xtcocotools PyPI package, it is necessary to
+                # compile xtcocotools from source on Colab.
+                info = dict(
+                    line=info['line'],
+                    package='xtcocotools@'
+                    'git+https://github.com/jin-s13/xtcocoapi')
+
             yield info
 
     def parse_require_file(fpath):
         with open(fpath, 'r') as f:
             for line in f.readlines():
                 line = line.strip()
                 if line and not line.startswith('#'):
@@ -124,15 +140,17 @@
     elif 'sdist' in sys.argv or 'bdist_wheel' in sys.argv:
         # installed by `pip install .`
         # or create source distribution by `python setup.py sdist`
         mode = 'copy'
     else:
         return
 
-    filenames = ['tools', 'configs', 'demo', 'model-index.yml']
+    filenames = [
+        'tools', 'configs', 'demo', 'model-index.yml', 'dataset-index.yml'
+    ]
     repo_path = osp.dirname(__file__)
     mim_path = osp.join(repo_path, 'mmpose', '.mim')
     os.makedirs(mim_path, exist_ok=True)
 
     for filename in filenames:
         if osp.exists(filename):
             src_path = osp.join(repo_path, filename)
```

