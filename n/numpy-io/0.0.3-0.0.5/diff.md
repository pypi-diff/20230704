# Comparing `tmp/numpy_io-0.0.3-py3-none-any.whl.zip` & `tmp/numpy_io-0.0.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,32 @@
-Zip file size: 25278 bytes, number of entries: 30
--rw-rw-rw-  2.0 fat       77 b- defN 23-Apr-26 15:07 numpy_io/__init__.py
--rw-rw-rw-  2.0 fat      623 b- defN 23-May-25 13:30 numpy_io/setup.py
--rw-rw-rw-  2.0 fat       80 b- defN 23-Apr-26 14:34 numpy_io/core/__init__.py
--rw-rw-rw-  2.0 fat    12381 b- defN 23-Apr-29 16:46 numpy_io/core/numpyadapter.py
--rw-rw-rw-  2.0 fat     5457 b- defN 23-Feb-18 01:47 numpy_io/core/parallel.py
--rw-rw-rw-  2.0 fat     1894 b- defN 23-Apr-26 14:48 numpy_io/core/reader.py
--rw-rw-rw-  2.0 fat     1366 b- defN 23-Apr-26 14:49 numpy_io/core/writer.py
--rw-rw-rw-  2.0 fat       54 b- defN 23-Apr-26 14:27 numpy_io/examples/__init__.py
--rw-rw-rw-  2.0 fat     2708 b- defN 23-Apr-26 15:08 numpy_io/examples/auto_parallel_writer.py
--rw-rw-rw-  2.0 fat     3241 b- defN 23-Apr-26 15:11 numpy_io/examples/auto_writer.py
--rw-rw-rw-  2.0 fat     1778 b- defN 23-Apr-26 14:27 numpy_io/examples/leveldb_readwriter_example.py
--rw-rw-rw-  2.0 fat     2218 b- defN 23-Apr-29 16:46 numpy_io/examples/lmdb_readwriter_example.py
--rw-rw-rw-  2.0 fat     1198 b- defN 23-Apr-26 14:27 numpy_io/examples/memory_raw_readwriter_example.py
--rw-rw-rw-  2.0 fat     1285 b- defN 23-Apr-26 14:27 numpy_io/examples/memory_readwriter_example.py
--rw-rw-rw-  2.0 fat     1586 b- defN 23-Apr-26 14:27 numpy_io/examples/record_numpywriter_example.py
--rw-rw-rw-  2.0 fat     1353 b- defN 23-Apr-26 14:27 numpy_io/examples/record_reader_example.py
--rw-rw-rw-  2.0 fat     1398 b- defN 23-Apr-26 14:27 numpy_io/examples/record_shuffle_example.py
--rw-rw-rw-  2.0 fat     3056 b- defN 23-Apr-26 14:27 numpy_io/examples/record_writer_example.py
--rw-rw-rw-  2.0 fat       56 b- defN 23-Apr-29 16:46 numpy_io/examples/testing/__init__.py
--rw-rw-rw-  2.0 fat     2658 b- defN 23-Apr-29 16:46 numpy_io/examples/testing/lmdb_test.py
--rw-rw-rw-  2.0 fat      613 b- defN 23-Apr-29 16:46 numpy_io/examples/testing/test_mem.py
--rw-rw-rw-  2.0 fat     2907 b- defN 23-Apr-29 16:46 numpy_io/examples/testing/test_mutiprocess.py
--rw-rw-rw-  2.0 fat       73 b- defN 23-Apr-29 16:46 numpy_io/pytorch_loader/__init__.py
--rw-rw-rw-  2.0 fat     4528 b- defN 23-May-25 13:57 numpy_io/pytorch_loader/data_helper.py
--rw-rw-rw-  2.0 fat     8631 b- defN 23-May-25 13:37 numpy_io/pytorch_loader/dataloaders.py
--rw-rw-rw-  2.0 fat     3780 b- defN 23-May-25 13:30 numpy_io/pytorch_loader/tokenizer_config_helper.py
--rw-rw-rw-  2.0 fat      401 b- defN 23-May-25 14:10 numpy_io-0.0.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-25 14:10 numpy_io-0.0.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        9 b- defN 23-May-25 14:10 numpy_io-0.0.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     2714 b- defN 23-May-25 14:10 numpy_io-0.0.3.dist-info/RECORD
-30 files, 68215 bytes uncompressed, 20826 bytes compressed:  69.5%
+Zip file size: 27385 bytes, number of entries: 30
+-rw-rw-rw-  2.0 fat       77 b- defN 23-Apr-27 06:30 numpy_io/__init__.py
+-rw-rw-rw-  2.0 fat      629 b- defN 23-Jul-04 02:07 numpy_io/setup.py
+-rw-rw-rw-  2.0 fat       80 b- defN 23-Apr-27 06:30 numpy_io/core/__init__.py
+-rw-rw-rw-  2.0 fat    19443 b- defN 23-Jul-04 03:27 numpy_io/core/numpyadapter.py
+-rw-rw-rw-  2.0 fat     5457 b- defN 23-Apr-27 06:30 numpy_io/core/parallel.py
+-rw-rw-rw-  2.0 fat     1894 b- defN 23-Apr-27 06:30 numpy_io/core/reader.py
+-rw-rw-rw-  2.0 fat     2244 b- defN 23-Jul-04 02:33 numpy_io/core/writer.py
+-rw-rw-rw-  2.0 fat       54 b- defN 23-Apr-27 06:30 numpy_io/examples/__init__.py
+-rw-rw-rw-  2.0 fat     3236 b- defN 23-Jul-03 23:35 numpy_io/examples/auto_parallel_writer.py
+-rw-rw-rw-  2.0 fat     3241 b- defN 23-Apr-27 06:30 numpy_io/examples/auto_writer.py
+-rw-rw-rw-  2.0 fat     1778 b- defN 23-Apr-27 07:23 numpy_io/examples/leveldb_readwriter_example.py
+-rw-rw-rw-  2.0 fat     2218 b- defN 23-Apr-28 00:25 numpy_io/examples/lmdb_readwriter_example.py
+-rw-rw-rw-  2.0 fat     1198 b- defN 23-Apr-27 06:30 numpy_io/examples/memory_raw_readwriter_example.py
+-rw-rw-rw-  2.0 fat     1285 b- defN 23-Apr-27 06:30 numpy_io/examples/memory_readwriter_example.py
+-rw-rw-rw-  2.0 fat     1586 b- defN 23-Apr-27 06:30 numpy_io/examples/record_numpywriter_example.py
+-rw-rw-rw-  2.0 fat     1353 b- defN 23-Apr-27 06:30 numpy_io/examples/record_reader_example.py
+-rw-rw-rw-  2.0 fat     1398 b- defN 23-Apr-27 06:30 numpy_io/examples/record_shuffle_example.py
+-rw-rw-rw-  2.0 fat     3056 b- defN 23-Apr-27 06:30 numpy_io/examples/record_writer_example.py
+-rw-rw-rw-  2.0 fat       56 b- defN 23-Apr-27 06:30 numpy_io/examples/testing/__init__.py
+-rw-rw-rw-  2.0 fat     2658 b- defN 23-Apr-27 06:30 numpy_io/examples/testing/lmdb_test.py
+-rw-rw-rw-  2.0 fat      613 b- defN 23-Apr-27 06:30 numpy_io/examples/testing/test_mem.py
+-rw-rw-rw-  2.0 fat     2907 b- defN 23-Apr-27 07:08 numpy_io/examples/testing/test_mutiprocess.py
+-rw-rw-rw-  2.0 fat       73 b- defN 23-Apr-28 00:25 numpy_io/pytorch_loader/__init__.py
+-rw-rw-rw-  2.0 fat    10309 b- defN 23-Jul-04 03:24 numpy_io/pytorch_loader/data_helper.py
+-rw-rw-rw-  2.0 fat     8940 b- defN 23-Jul-04 01:02 numpy_io/pytorch_loader/dataloaders.py
+-rw-rw-rw-  2.0 fat     3780 b- defN 23-May-25 05:42 numpy_io/pytorch_loader/tokenizer_config_helper.py
+-rw-rw-rw-  2.0 fat      390 b- defN 23-Jul-04 06:35 numpy_io-0.0.5.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-04 06:35 numpy_io-0.0.5.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        9 b- defN 23-Jul-04 06:35 numpy_io-0.0.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     2715 b- defN 23-Jul-04 06:35 numpy_io-0.0.5.dist-info/RECORD
+30 files, 82769 bytes uncompressed, 22933 bytes compressed:  72.3%
```

## zipnote {}

```diff
@@ -72,20 +72,20 @@
 
 Filename: numpy_io/pytorch_loader/dataloaders.py
 Comment: 
 
 Filename: numpy_io/pytorch_loader/tokenizer_config_helper.py
 Comment: 
 
-Filename: numpy_io-0.0.3.dist-info/METADATA
+Filename: numpy_io-0.0.5.dist-info/METADATA
 Comment: 
 
-Filename: numpy_io-0.0.3.dist-info/WHEEL
+Filename: numpy_io-0.0.5.dist-info/WHEEL
 Comment: 
 
-Filename: numpy_io-0.0.3.dist-info/top_level.txt
+Filename: numpy_io-0.0.5.dist-info/top_level.txt
 Comment: 
 
-Filename: numpy_io-0.0.3.dist-info/RECORD
+Filename: numpy_io-0.0.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## numpy_io/setup.py

```diff
@@ -3,22 +3,22 @@
 
 from setuptools import setup, find_packages
 
 ignore = []
 
 setup(
     name='numpy-io',
-    version='0.0.3',
+    version='0.0.5',
     description='an easy training architecture',
     long_description='numpy-io: https://github.com/ssbuild/numpy-io.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/numpy-io',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=[
-        'fastdatasets>=0.9.7 , <= 1',
+        'fastdatasets>=0.9.11 , <= 0.9.13',
         'numpy',
         'tqdm',
         'six'
     ],
     packages=[p for p in find_packages() if p not in ignore]
 )
```

## numpy_io/core/numpyadapter.py

```diff
@@ -3,19 +3,23 @@
 # @Author: tk
 # @File：numpyadapter
 
 import copy
 import typing
 import warnings
 from enum import Enum
+
+import numpy as np
 from fastdatasets.utils.py_features import Final
 from fastdatasets.record import writer as record_writer, RECORD, load_dataset as record_loader
 from fastdatasets.leveldb import writer as leveldb_writer, LEVELDB, load_dataset as leveldb_loader
 from fastdatasets.lmdb import writer as lmdb_writer, LMDB, load_dataset as lmdb_loader
 from fastdatasets.memory import writer as memory_writer, MEMORY, load_dataset as memory_loader
+from fastdatasets.arrow import writer as arrow_writer,load_dataset as arrow_loader
+from fastdatasets.parquet import writer as parquet_writer,load_dataset as parquet_loader
 from .parallel import ParallelStruct, parallel_apply
 
 
 __all__ = [
     'E_file_backend',
     'NumpyWriterAdapter',
     'NumpyReaderAdapter',
@@ -27,145 +31,211 @@
 
 class E_file_backend(Enum):
     record = 0
     leveldb = 1
     lmdb = 2
     memory = 3
     memory_raw = 4
+    arrow_stream = 5
+    arrow_file = 6
+    parquet = 7
+
 
     @staticmethod
     def from_string(b: str):
         b = b.lower()
         if b == 'record':
             return E_file_backend.record
         elif b == 'leveldb':
             return E_file_backend.leveldb
         elif b == 'lmdb':
             return E_file_backend.lmdb
         elif b == 'memory':
             return E_file_backend.memory
         elif b == 'memory_raw':
             return E_file_backend.memory_raw
+        elif b == 'arrow_stream':
+            return E_file_backend.arrow_stream
+        elif b == 'arrow_file':
+            return E_file_backend.arrow_file
+        elif b == 'parquet':
+            return E_file_backend.parquet
         return None
 
     def to_string(self, b):
         if b == E_file_backend.record:
             return 'record'
         if b == E_file_backend.leveldb:
             return 'leveldb'
         if b == E_file_backend.lmdb:
             return 'lmdb'
         if b == E_file_backend.memory:
             return 'memory'
         if b == E_file_backend.memory_raw:
             return 'memory_raw'
+        if b == E_file_backend.arrow_stream:
+            return 'arrow_stream'
+        if b == E_file_backend.arrow_file:
+            return 'arrow_file'
+        if b == E_file_backend.parquet:
+            return 'parquet'
         return None
 
 
 class NumpyWriterAdapter:
     def __init__(self, filename: typing.Union[str, typing.List],
                  backend: typing.Union[E_file_backend, str],
                  options: typing.Union[
-                     RECORD.TFRecordOptions, LEVELDB.LeveldbOptions, LMDB.LmdbOptions, MEMORY.MemoryOptions] = None,
+                     RECORD.TFRecordOptions,
+                     LEVELDB.LeveldbOptions,
+                     LMDB.LmdbOptions,
+                     MEMORY.MemoryOptions,
+                     typing.Dict,
+                     typing.AnyStr
+                 ] = None,
+                 parquet_options: typing.Optional = None,
+                 schema : typing.Optional[typing.Dict] = None,
                  leveldb_write_buffer_size=1024 * 1024 * 512,
                  leveldb_max_file_size=10 * 1024 * 1024 * 1024,
-                 lmdb_map_size=1024 * 1024 * 1024 * 150):
+                 lmdb_map_size=1024 * 1024 * 1024 * 150,
+                 batch_size=None):
 
         self.filename = filename
+        self.schema = schema
         if isinstance(backend, E_file_backend):
             self._backend_type = E_file_backend.to_string(backend)
             self._backend = backend
         else:
             self._backend_type = backend
             self._backend = E_file_backend.from_string(backend)
-        self._batch_buffer_num = 2000
+        self._buffer_batch_size = 2000
         self._kv_flag = True
+        self._is_table = False
         if self._backend == E_file_backend.record:
             self._kv_flag = False
-            self._batch_buffer_num = 2000
+            self._buffer_batch_size = 2000
             if options is None:
                 options = RECORD.TFRecordOptions(compression_type='GZIP')
             self._f_writer = record_writer.NumpyWriter(filename, options=options)
 
         elif self._backend == E_file_backend.leveldb:
-            self._batch_buffer_num = 100000
+            self._buffer_batch_size = 100000
             if options is None:
                 options = LEVELDB.LeveldbOptions(create_if_missing=True,
                                                  error_if_exists=False,
                                                  write_buffer_size=leveldb_write_buffer_size,
                                                  max_file_size=leveldb_max_file_size)
             self._f_writer = leveldb_writer.NumpyWriter(filename, options=options)
         elif self._backend == E_file_backend.lmdb:
-            self._batch_buffer_num = 100000
+            self._buffer_batch_size = 100000
             if options is None:
                 options = LMDB.LmdbOptions(env_open_flag=0,
                                            env_open_mode=0o664,  # 8进制表示
                                            txn_flag=0,
                                            dbi_flag=0,
                                            put_flag=0)
             self._f_writer = lmdb_writer.NumpyWriter(filename, options=options,
                                                      map_size=lmdb_map_size)
         elif self._backend == E_file_backend.memory:
             self._kv_flag = False
-            self._batch_buffer_num = 100000
+            self._buffer_batch_size = 100000
             if options is None:
                 options = MEMORY.MemoryOptions()
             self._f_writer = memory_writer.NumpyWriter(filename, options=options)
         elif self._backend == E_file_backend.memory_raw:
             self._kv_flag = False
-            self._batch_buffer_num = 100000
+            self._buffer_batch_size = 100000
             if options is None:
                 options = MEMORY.MemoryOptions()
             self._f_writer = memory_writer.WriterObject(filename, options=options)
+
+        # table
+        elif self._backend == E_file_backend.arrow_stream:
+            self._kv_flag = False
+            self._buffer_batch_size = 1024
+            self._is_table = True
+            self._f_writer = arrow_writer.PythonWriter(filename,with_stream=True,schema=schema, options=options)
+
+        elif self._backend == E_file_backend.arrow_file:
+            self._kv_flag = False
+            self._buffer_batch_size = 1024
+            self._is_table = True
+            self._f_writer = arrow_writer.PythonWriter(filename, with_stream=False, schema=schema, options=options)
+        elif self._backend == E_file_backend.parquet:
+            self._kv_flag = False
+            self._buffer_batch_size = 1024
+            self._is_table = True
+            self._f_writer = parquet_writer.PythonWriter(filename,
+                                                         schema=schema,
+                                                         arrow_options=options,
+                                                         parquet_options = parquet_options)
+
         else:
             raise ValueError(
                 'NumpyWriterAdapter does not support backend={} , not in record,leveldb,lmdb,memory,meory_raw'.format(
                     backend))
 
+        if batch_size is not None:
+            self._buffer_batch_size = batch_size
+        assert self._buffer_batch_size > 0
     def __del__(self):
         self.close()
 
     def close(self):
         if self._f_writer is not None:
             self._f_writer.close()
             self._f_writer = None
 
     @property
     def writer(self):
         return self._f_writer
 
     @property
+    def is_table(self):
+        return self._is_table
+    @property
     def is_kv_writer(self):
         return self._kv_flag
 
     @property
     def backend(self):
         return self._backend
 
     @property
     def backend_type(self):
         return self._backend_type
 
     @property
-    def advice_batch_buffer_size(self):
-        return self._batch_buffer_num
+    def buffer_batch_size(self):
+        return self._buffer_batch_size
 
+    @buffer_batch_size.setter
+    def buffer_batch_size(self,batch_size):
+        self._buffer_batch_size = batch_size
 
 class NumpyReaderAdapter:
     @staticmethod
     def load(input_files: typing.Union[typing.List[str], str, typing.List[typing.Any]],
              backend: typing.Union[E_file_backend, str],
              options: typing.Union[
-                 RECORD.TFRecordOptions, LEVELDB.LeveldbOptions, LMDB.LmdbOptions, MEMORY.MemoryOptions] = None,
+                 RECORD.TFRecordOptions,
+                 LEVELDB.LeveldbOptions,
+                 LMDB.LmdbOptions,
+                 MEMORY.MemoryOptions,
+                 typing.Dict,
+                 typing.AnyStr
+             ] = None,
+             col_names: typing.Optional[typing.Dict] = None,
              data_key_prefix_list=('input',),
              num_key='total_num',
              cycle_length=1,
              block_length=1,
              with_record_iterable_dataset=True,
-             with_parse_from_numpy=True):
+             with_parse_from_numpy=True,
+             with_share_memory=True):
         '''
             input_files: 文件列表
             backend: 存储引擎类型
             options: 存储引擎选项
             data_key_prefix_list: 键值数据库 键值前缀
             num_key: 键值数据库，记录数据总数建
             with_record_iterable_dataset 打开iterable_dataset
@@ -174,18 +244,23 @@
 
         parse_flag = True
         data_backend = backend if isinstance(backend, E_file_backend) else E_file_backend.from_string(backend)
         if data_backend == E_file_backend.record:
             if options is None:
                 options = RECORD.TFRecordOptions(compression_type='GZIP')
             if with_record_iterable_dataset:
-                dataset = record_loader.IterableDataset(input_files, cycle_length=cycle_length,
-                                                        block_length=block_length, options=options)
+                dataset = record_loader.IterableDataset(input_files,
+                                                        cycle_length=cycle_length,
+                                                        block_length=block_length,
+                                                        options=options,
+                                                        with_share_memory=with_share_memory)
             else:
-                dataset = record_loader.RandomDataset(input_files, options=options)
+                dataset = record_loader.RandomDataset(input_files,
+                                                      options=options,
+                                                      with_share_memory=with_share_memory)
 
         elif data_backend == E_file_backend.leveldb:
             if options is None:
                 options = LEVELDB.LeveldbOptions(create_if_missing=True, error_if_exists=False)
             dataset = leveldb_loader.RandomDataset(input_files,
                                                    data_key_prefix_list=data_key_prefix_list,
                                                    num_key=num_key,
@@ -206,16 +281,58 @@
                 options = MEMORY.MemoryOptions()
             dataset = memory_loader.RandomDataset(input_files, options=options)
         elif data_backend == E_file_backend.memory_raw:
             parse_flag = False
             if options is None:
                 options = MEMORY.MemoryOptions()
             dataset = memory_loader.RandomDataset(input_files, options=options)
+        elif data_backend == E_file_backend.arrow_stream:
+            parse_flag = False
+            if with_record_iterable_dataset:
+                dataset = arrow_loader.IterableDataset(input_files,
+                                                       cycle_length=cycle_length,
+                                                       block_length=block_length,
+                                                       options=options,
+                                                       col_names=col_names,
+                                                       with_share_memory=False)
+            else:
+                dataset = arrow_loader.RandomDataset(input_files,
+                                                     options=options,
+                                                     col_names=col_names,
+                                                     with_share_memory=False)
+        elif data_backend == E_file_backend.arrow_file:
+            parse_flag = False
+            if with_record_iterable_dataset:
+                dataset = arrow_loader.IterableDataset(input_files,
+                                                       cycle_length=cycle_length,
+                                                       block_length=block_length,
+                                                       options=options,
+                                                       col_names=col_names,
+                                                       with_share_memory=True)
+            else:
+                dataset = arrow_loader.RandomDataset(input_files,
+                                                     options=options,
+                                                     col_names=col_names,
+                                                     with_share_memory=True)
+        elif data_backend == E_file_backend.parquet:
+            parse_flag = False
+            if with_record_iterable_dataset:
+                dataset = parquet_loader.IterableDataset(input_files,
+                                                         cycle_length=cycle_length,
+                                                         block_length=block_length,
+                                                         options=options,
+                                                         with_share_memory=True)
+            else:
+                dataset = parquet_loader.RandomDataset(input_files,
+                                                       options=options,
+                                                       col_names=col_names,
+                                                       with_share_memory=True)
         else:
             dataset = None
+            warnings.warn('no support databackend')
         if with_parse_from_numpy and parse_flag:
             dataset = dataset.parse_from_numpy_writer()
         return dataset
 
 
 class ParallelNumpyWriter(ParallelStruct, metaclass=Final):
     def __init__(self, *args, **kwargs):
@@ -224,74 +341,100 @@
         self.batch_values = []
         self.total_num = 0
         self.numpy_writer = None
 
     def open(self, outfile: typing.Union[str, typing.List],
              backend: typing.Union[E_file_backend, str],
              options: typing.Union[
-                 RECORD.TFRecordOptions, LEVELDB.LeveldbOptions, LMDB.LmdbOptions, MEMORY.MemoryOptions] = None,
+                 RECORD.TFRecordOptions,
+                 LEVELDB.LeveldbOptions,
+                 LMDB.LmdbOptions,
+                 MEMORY.MemoryOptions,
+                 typing.Dict,
+                 typing.AnyStr
+             ] = None,
+             parquet_options: typing.Optional = None,
+             schema: typing.Optional[typing.Dict] = None,
              leveldb_write_buffer_size=1024 * 1024 * 512,
-             lmdb_map_size=1024 * 1024 * 1024 * 150):
-
-        self.numpy_writer = NumpyWriterAdapter(outfile, backend, options, leveldb_write_buffer_size, lmdb_map_size)
+             leveldb_max_file_size=10 * 1024 * 1024 * 1024,
+             lmdb_map_size=1024 * 1024 * 1024 * 150,
+             batch_size=None):
+
+        self.numpy_writer = NumpyWriterAdapter(outfile,
+                                               backend = backend,
+                                               options=options,
+                                               parquet_options=parquet_options,
+                                               schema=schema,
+                                               leveldb_write_buffer_size = leveldb_write_buffer_size,
+                                               leveldb_max_file_size=leveldb_max_file_size,
+                                               lmdb_map_size = lmdb_map_size,
+                                               batch_size=batch_size)
         self.backend = self.numpy_writer.backend
         self.backend_type = self.numpy_writer.backend_type
         self.is_kv_writer = self.numpy_writer.is_kv_writer
-        self.write_batch_size = self.numpy_writer.advice_batch_buffer_size
+        self.is_table = self.numpy_writer.is_table
+        self.schema = self.numpy_writer.schema
+        self.write_batch_size = self.numpy_writer.buffer_batch_size
 
     def write(self, data, input_hook_fn: typing.Callable, fn_args: typing.Union[typing.Tuple, typing.Dict],
               write_batch_size=None):
         self.input_hook_fn = input_hook_fn
         self.fn_args = fn_args
 
         assert self.numpy_writer is not None
         assert self.input_hook_fn is not None
 
         if write_batch_size is None or write_batch_size <= 0:
-            write_batch_size = self.numpy_writer.advice_batch_buffer_size
+            write_batch_size = self.numpy_writer.buffer_batch_size
             if write_batch_size >= len(data):
                 write_batch_size = len(data) // 2
 
         if write_batch_size <= 0:
             write_batch_size = 1
 
         self.write_batch_size = write_batch_size
         parallel_apply(data, self)
 
     def flush(self):
-        if not self.is_kv_writer:
+        if self.is_table:
+            values  = {k: [] for k in self.schema.keys()}
+            for d in self.batch_values:
+                for k,v in values.items():
+                    if isinstance(d[k],np.ndarray):
+                        data = d[k].tolist()
+                    else:
+                        data = d[k]
+                    v.append(data)
+            self.numpy_writer.writer.write_batch(list(values.keys()),list(values.values()))
+
+        elif not self.is_kv_writer:
             if self.backend == E_file_backend.memory_raw:
                 self.numpy_writer.writer.write_batch([d for d in self.batch_values])
             else:
                 self.numpy_writer.writer.write_batch(self.batch_values)
-
         else:
             self.numpy_writer.writer.put_batch(self.batch_keys, self.batch_values)
         self.batch_keys.clear()
         self.batch_values.clear()
 
     # 继承
     def on_input_process(self, x):
         return self.input_hook_fn(x, self.fn_args)
 
     # 继承
     def on_output_process(self, x):
         #忽略None数据
         if x is None:
             return
-        # 返回多个结果
-        if isinstance(x, (list, tuple)):
-            for one in x:
-                self.batch_keys.append('input{}'.format(self.total_num))
-                self.batch_values.append(one)
-                self.total_num += 1
-        # 返回一个结果
-        else:
+
+        if not isinstance(x, (list, tuple)):
+            x = [x]
+        for one in x:
             self.batch_keys.append('input{}'.format(self.total_num))
-            self.batch_values.append(x)
+            self.batch_values.append(one)
             self.total_num += 1
 
         if len(self.batch_values) > 0 and len(self.batch_values) % self.write_batch_size == 0:
             self.flush()
 
     # 继承
     def on_output_cleanup(self):
```

## numpy_io/core/writer.py

```diff
@@ -1,14 +1,14 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2023/4/24 9:16
 
 
 import typing
 
-from .numpyadapter import NumpyWriterAdapter, ParallelNumpyWriter
+from .numpyadapter import NumpyWriterAdapter, ParallelNumpyWriter,E_file_backend
 
 __all__ = [
     'DataWriteHelper',
     'NumpyWriterAdapter',
     'ParallelNumpyWriter'
 ]
 
@@ -16,15 +16,15 @@
     def __init__(self,
                  input_fn: typing.Callable[[typing.Any, tuple], typing.Union[typing.Dict, typing.List, typing.Tuple]],
                  input_fn_args: typing.Union[typing.Tuple,typing.Dict],
                  outfile: typing.Union[str,list],
                  backend='record',
                  num_process_worker=0,
                  shuffle=True):
-        assert backend in ['record', 'lmdb', 'leveldb','memory','memory_raw']
+        assert E_file_backend.from_string(backend) is not None
 
         self.input_fn = input_fn
         self.input_fn_args = input_fn_args
         self.outfile = outfile
         self._backend_type = backend
         self._parallel_writer = ParallelNumpyWriter(num_process_worker=num_process_worker,shuffle=shuffle)
 
@@ -33,10 +33,27 @@
         return self._backend_type
 
     @backend_type.setter
     def backend_type(self, value):
         self._backend_type = value
 
     # 多进程写大文件
-    def save(self,data: list):
-        self._parallel_writer.open(self.outfile ,self.backend_type)
+    def save(self,data: list,
+             options=None,
+             parquet_options: typing.Optional = None,
+             schema: typing.Optional[typing.Dict] = None,
+             leveldb_write_buffer_size=1024 * 1024 * 512,
+             leveldb_max_file_size=10 * 1024 * 1024 * 1024,
+             lmdb_map_size=1024 * 1024 * 1024 * 150,
+             batch_size=None
+             ):
+
+        self._parallel_writer.open(self.outfile ,
+                                   backend=self.backend_type,
+                                   schema=schema,
+                                   parquet_options=parquet_options,
+                                   options=options,
+                                   leveldb_write_buffer_size=leveldb_write_buffer_size,
+                                   leveldb_max_file_size=leveldb_max_file_size,
+                                   lmdb_map_size = lmdb_map_size,
+                                   batch_size=batch_size)
         self._parallel_writer.write(data,self.input_fn, self.input_fn_args)
```

## numpy_io/examples/auto_parallel_writer.py

```diff
@@ -32,23 +32,33 @@
         'attention_mask': attention_mask,
         'token_type_ids': token_type_ids,
         'seqlen': input_length
     }
     return node
 
 
-def make_dataset(tokenizer,data,data_backend,outputfile):
+def make_dataset(tokenizer,data,data_backend,schema,outputfile):
     parallel_writer = ParallelNumpyWriter(num_process_worker=0)
-    parallel_writer.open(outputfile,data_backend)
+    parallel_writer.open(outputfile,data_backend,schema=schema)
     parallel_writer.write(data,tokenize_data, (tokenizer,64))
 
 
 
 def test(tokenizer,data,data_backend,output):
-    make_dataset(tokenizer,data,data_backend,output)
+    print('*' * 30,data_backend)
+    schema = None
+    if data_backend.find('arrow') != -1 or data_backend.find('parquet') != -1:
+        schema = {
+            'input_ids': "int32",
+            'attention_mask': "int32",
+            'token_type_ids': "int32",
+            'seqlen': "int32",
+        }
+
+    make_dataset(tokenizer,data,data_backend,schema,output)
     dataset = NumpyReaderAdapter.load(output, data_backend)
     if isinstance(dataset, typing.Iterator):
         for d in dataset:
             print(d)
             break
     else:
         for i in range(len(dataset)):
@@ -61,8 +71,11 @@
     # data = DataReadLoader.read_from_file(filename)
     data = [str(i) + 'fastdatasets numpywriter demo' for i in range(1000)]
 
     test(tokenizer,data, 'memory_raw', [])
     test(tokenizer,data, 'memory',  [])
     test(tokenizer,data,'record','./data.record')
     test(tokenizer,data,'leveldb', './data.leveldb')
-    test(tokenizer,data,'lmdb', './data.lmdb')
+    test(tokenizer,data,'lmdb', './data.lmdb')
+    test(tokenizer, data, 'arrow_stream', './data.arrow_stream')
+    test(tokenizer, data, 'arrow_file', './data.arrow_file')
+    test(tokenizer, data, 'parquet', './data.arrow_file')
```

## numpy_io/pytorch_loader/data_helper.py

```diff
@@ -1,11 +1,12 @@
 # -*- coding: utf-8 -*-
 # @Author  : tk
 # @Time    : 2023/5/25 9:37
 import json
+import logging
 import os
 import typing
 from ..core.writer import DataWriteHelper
 from .dataloaders import load_distributed_random_sampler, load_random_sampler
 from .tokenizer_config_helper import *
 
 __all__ = [
@@ -63,14 +64,26 @@
                     if not line: continue
                     D.append(line)
         return D
 
 
 class DataHelperBase(DataPreprocessCallback):
     backend = 'record'
+
+    def __init__(self,backend,convert_file,cache_dir,intermediate_name,):
+        self.train_files = []
+        self.eval_files = []
+        self.test_files = []
+
+        self.backend = backend if backend else 'record'
+        self.convert_file = convert_file
+        self.intermediate_name = intermediate_name
+        self.cache_dir = cache_dir
+
+
     def load_distributed_random_sampler(self,*args,**kwargs):
         if 'backend' not in kwargs:
             kwargs.update({"backend": getattr(self,'backend','record')})
         kwargs.update({
             "shuffle": True,
         })
         return load_distributed_random_sampler(*args,**kwargs)
@@ -104,35 +117,163 @@
 
 
 
     def make_dataset(self,outfile: typing.Union[str,list],
                      data,
                      input_fn_args: typing.Any,
                      num_process_worker: int = 0,
-                     shuffle: bool=True):
+                     shuffle: bool=True,
+                     options=None,
+                     parquet_options: typing.Optional = None,
+                     schema: typing.Optional[typing.Dict] = None,
+                     leveldb_write_buffer_size=1024 * 1024 * 512,
+                     leveldb_max_file_size=10 * 1024 * 1024 * 1024,
+                     lmdb_map_size=1024 * 1024 * 1024 * 150,
+                     batch_size=None):
 
         #初始化
         self.on_data_ready()
         #创建写对象上下文
         fw = DataWriteHelper(self.on_data_process,
-                             input_fn_args,
-                             outfile,
-                             getattr(self,'backend','record'),
+                             input_fn_args=input_fn_args,
+                             outfile=outfile,
+                             backend=getattr(self,'backend','record'),
                              num_process_worker=num_process_worker,
                              shuffle=shuffle)
         #写数据回调 on_data_process
-        fw.save(data)
+        fw.save(data,
+                options=options,
+                parquet_options = parquet_options,
+                schema = schema,
+                leveldb_write_buffer_size = leveldb_write_buffer_size,
+                leveldb_max_file_size =leveldb_max_file_size,
+                lmdb_map_size = lmdb_map_size,
+                batch_size = batch_size)
         #写数据完成
         self.on_data_finalize()
 
+        # 返回制作特征数据的中间文件
+
+    def get_intermediate_file(self, intermediate_name, mode):
+        if self.backend.startswith('memory'):
+            # 内存数据: list
+            intermediate_output = []
+            logging.info('make data {} {}...'.format(self.cache_dir,
+                                                     intermediate_name + '-' + mode + '.' + self.backend))
+        else:
+            # 本地文件数据: 文件名
+            intermediate_output = os.path.join(self.cache_dir,
+                                               intermediate_name + '-' + mode + '.' + self.backend)
+            logging.info('make data {}...'.format(intermediate_output))
+        return intermediate_output
+
+    def make_dataset_with_args(self, input_files,
+                               mode,
+                               shuffle=False,
+                               num_process_worker: int = 0,
+                               overwrite: bool = False,
+                               mixed_data=True,
+                               dupe_factor=1,
+                               **dataset_args):
+        '''
+            mode: one of [ train , eval , test]
+            shuffle: whether shuffle data
+            num_process_worker: the number of mutiprocess
+            overwrite: whether overwrite data
+            mixed_data: Whether the mixed data
+        '''
+        logging.info('make_dataset {} {}...'.format(','.join(input_files), mode))
+        if mode == 'train':
+            contain_objs = self.train_files
+        elif mode == 'eval' or mode == 'val':
+            contain_objs = self.eval_files
+        elif mode == 'test' or mode == 'predict':
+            contain_objs = self.test_files
+        else:
+            raise ValueError('{} invalid '.format(mode))
+
+        if not input_files:
+            logging.info('input_files empty!')
+            return
+
+        for i in range(dupe_factor):
+            if self.convert_file:
+                if mixed_data:
+                    intermediate_name = self.intermediate_name + '_dupe_factor_{}'.format(i)
+                    intermediate_output = self.get_intermediate_file(intermediate_name, mode)
+
+                    if isinstance(intermediate_output, list) or not os.path.exists(intermediate_output) or overwrite:
+                        data = self.on_get_corpus(input_files, mode)
+                        self.make_dataset(intermediate_output,
+                                          data,
+                                          mode,
+                                          num_process_worker=num_process_worker,
+                                          shuffle=shuffle,
+                                          **dataset_args)
+                    contain_objs.append(intermediate_output)
+                else:
+                    for fid, input_item in enumerate(input_files):
+                        intermediate_name = self.intermediate_name + '_file_{}_dupe_factor_{}'.format(fid, i)
+                        intermediate_output = self.get_intermediate_file(intermediate_name, mode)
+
+                        if isinstance(intermediate_output, list) or not os.path.exists(
+                                intermediate_output) or overwrite:
+                            data = self.on_get_corpus([input_item], mode)
+                            self.make_dataset(intermediate_output,
+                                              data,
+                                              mode,
+                                              num_process_worker=num_process_worker,
+                                              shuffle=shuffle,
+                                              **dataset_args)
+                        contain_objs.append(intermediate_output)
+
+            else:
+                for input_item in input_files:
+                    contain_objs.append(input_item)
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 
 def make_dataset(data: typing.List,
                input_fn:typing.Callable[[int,typing.Any,tuple],typing.Union[typing.Dict,typing.List,typing.Tuple]],
                input_fn_args:typing.Tuple,
                outfile:str,
                backend: str,
                overwrite = False,
-               num_process_worker:int = 8):
+               num_process_worker:int = 8,
+               options=None,
+               parquet_options=None,
+               schema=None,
+               leveldb_write_buffer_size=None,
+               leveldb_max_file_size=None,
+               lmdb_map_size=None,
+               batch_size=None
+                 ):
 
     if not os.path.exists(outfile) or overwrite:
         fw = DataWriteHelper(input_fn,input_fn_args,outfile,backend,num_process_worker)
-        fw.save(data)
+        fw.save(data,
+                options=options,
+                parquet_options=parquet_options,
+                schema=schema,
+                leveldb_write_buffer_size=leveldb_write_buffer_size,
+                leveldb_max_file_size=leveldb_max_file_size,
+                lmdb_map_size=lmdb_map_size,
+                batch_size=batch_size
+                )
```

## numpy_io/pytorch_loader/dataloaders.py

```diff
@@ -43,14 +43,15 @@
                  cycle_length: int = 4,
                  block_length: int = 10,
                  num_processes: int = 1,
                  process_index: int = 0,
                  backend='record',
                  with_record_iterable_dataset: bool = False,
                  with_load_memory: bool = False,
+                 with_arrow_copy_to_memory=False,
                  with_torchdataset: bool = True,
                  transform_fn: typing.Callable = None,
                  check_dataset_file_fn=None,
                  limit_start: typing.Optional[int] = None,
                  limit_count: typing.Optional[int] = None,
                  dataset_loader_filter_fn: typing.Callable = None,
                  ) -> typing.Optional[typing.Union[torch.utils.data.Dataset, torch.utils.data.IterableDataset]]:
@@ -65,25 +66,31 @@
                                  block_length=block_length,
                                  with_record_iterable_dataset=with_record_iterable_dataset,
                                  with_parse_from_numpy=not with_load_memory,
                                  backend=backend,
                                  limit_start=limit_start,
                                  limit_count=limit_count,
                                  dataset_loader_filter_fn=dataset_loader_filter_fn)
+
+    if backend.startswith('arrow') or backend.startswith('parquet'):
+        with_load_memory = False
+        if with_arrow_copy_to_memory:
+            with_load_memory = True
+
     # 加载至内存
     if with_load_memory:
         logging.info('load dataset to memory...')
         if isinstance(dataset, typing.Iterator):
             raw_data = [i for i in dataset]
         else:
             raw_data = [dataset[i] for i in range(len(dataset))]
 
         dataset = MEMORY.load_dataset.SingleRandomDataset(raw_data)
         # 解析numpy数据
-        if backend != 'memory_raw':
+        if backend != 'memory_raw' and not backend.startswith('arrow') and not backend.startswith('parquet'):
             dataset = dataset.parse_from_numpy_writer()
 
     if isinstance(dataset, typing.Iterator):
         dataset: IterableDatasetBase
         if num_processes > 1:
             dataset = dataset.mutiprocess(num_processes, process_index)
```

## Comparing `numpy_io-0.0.3.dist-info/RECORD` & `numpy_io-0.0.5.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 numpy_io/__init__.py,sha256=aCtpF76c1Jz1-xvsEqeV_ze4a40-KqaqUtIrFy3BTcU,77
-numpy_io/setup.py,sha256=FpSWP9evO_39j97Ojgd_T8wUYdKMwZyPdX9sxeBAbds,623
+numpy_io/setup.py,sha256=onT5bLye1NON71-1jHsfSUBZI7f2ZNV2qwyCaTmm47w,629
 numpy_io/core/__init__.py,sha256=gvmkt5S6jF7SlAsFM1e9qGGXV96a5yTB5lBavIVjuKo,80
-numpy_io/core/numpyadapter.py,sha256=REFrhsPVHfQmdSu73_1j1NBfA929Y9xtaxx5cNxlJxM,12381
+numpy_io/core/numpyadapter.py,sha256=snEnLu1oEEcn15fr7-FDaYYktCvhKVQPTAzAZdRFQ0Q,19443
 numpy_io/core/parallel.py,sha256=7pxBQ7w26H5wo1gHEMJwFwrkl57PDUA7rF3y8GD5kjM,5457
 numpy_io/core/reader.py,sha256=-i7NiW3s1gPFwaM03tDWT0NLVG6FEkz6fOuo_nWB3r8,1894
-numpy_io/core/writer.py,sha256=ck68384PW4RY6osFhl7XFjqszztoW0zuSN_DEhUOuqY,1366
+numpy_io/core/writer.py,sha256=h0aobGbE2_QvM0bNVBg6gWz0uSR8gPrd8At6j56OJOA,2244
 numpy_io/examples/__init__.py,sha256=H6PH5pOOHNUW3_RuUpgWfpv6_Pijqyy_3U1JCTLl6jg,54
-numpy_io/examples/auto_parallel_writer.py,sha256=IBxAiuxCdouZAlWVoc98U7taEnnl17iFn6y9rafYMes,2708
+numpy_io/examples/auto_parallel_writer.py,sha256=geSpDCFHxe4skPd3Vk45lhc3azuJEPviGhcYtUlTid4,3236
 numpy_io/examples/auto_writer.py,sha256=vyvCXOJ5zroUweOOT0CFJnA8M_5nC8qJR0pxRv_8p88,3241
 numpy_io/examples/leveldb_readwriter_example.py,sha256=SKE4a3Wx67GPMbTmIlg8tIWT6JSxVc7QIRAi_gjOGvQ,1778
 numpy_io/examples/lmdb_readwriter_example.py,sha256=ZmZGfjy1o8c_O61JxXvKUrf6N_WSBGHHU1QCTK6DwH4,2218
 numpy_io/examples/memory_raw_readwriter_example.py,sha256=3LJzlLFFtDQdHyXM1dP7PupfnPb_0nM3qG4i7MF0MZg,1198
 numpy_io/examples/memory_readwriter_example.py,sha256=QAlov1poEZOYxmWNYgQkundKtST3VsFH10hTy3TE92w,1285
 numpy_io/examples/record_numpywriter_example.py,sha256=4VMeVDiA-0IwuBt0B_a0NNcs4OFyjkk1FOEmZlMavmc,1586
 numpy_io/examples/record_reader_example.py,sha256=Mv0uZyAVobFq6wGBJ-GQ0KvvEe9EjBZmGz5tXM9ChsQ,1353
 numpy_io/examples/record_shuffle_example.py,sha256=TFFnJOqgtOZZ8s8j4bZ2Ju16PyAt3Q5X17anhbfI9mg,1398
 numpy_io/examples/record_writer_example.py,sha256=VtgDxRxbs-tXVbNRo78TImVe48rioqvoKtOhTmwA1F8,3056
 numpy_io/examples/testing/__init__.py,sha256=Jj2ycuBs45i9qAcrC6iCm22QOY46Xujv9lzOSbeeZ_o,56
 numpy_io/examples/testing/lmdb_test.py,sha256=rQvokJAIDW9esAesab9wHs03OcG2TPImAuybnCO-LBA,2658
 numpy_io/examples/testing/test_mem.py,sha256=gteaDLS79vwwfJETTeL-JCxn3VhRT3So59y5AV4ah9w,613
 numpy_io/examples/testing/test_mutiprocess.py,sha256=rgO758tNAycWGYxyJl2AM2jkffNQs-AjRU9peSotGzQ,2907
 numpy_io/pytorch_loader/__init__.py,sha256=oRU1cnNHyp22vA9HkEYfoPtpdQdU4D9scv9KtgxYVdE,73
-numpy_io/pytorch_loader/data_helper.py,sha256=FOrVkAHUXml3M4rQGFXkHz9VYLlw2jY6ilxS5tLLwnw,4528
-numpy_io/pytorch_loader/dataloaders.py,sha256=Ub_ojjmjxQESDmf25ZPjCqGtIjqzz01kElLe7tvUzZk,8631
+numpy_io/pytorch_loader/data_helper.py,sha256=oVjtyWRobAYx1DMc5FePqQRyiQnoCpS60iidwHxN6k4,10309
+numpy_io/pytorch_loader/dataloaders.py,sha256=ov1DvKCv8GO4QGuoj09-BdUlDFop_4DScXMNv4B-jl0,8940
 numpy_io/pytorch_loader/tokenizer_config_helper.py,sha256=8hoofhL7uMyE8pT-U_3WNKDyy5DBOqQhDwJMZri-InY,3780
-numpy_io-0.0.3.dist-info/METADATA,sha256=3pZ1FSxw4LPkn4iFYAIBeQWxNWlXXf-yI_VbljsTUX0,401
-numpy_io-0.0.3.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-numpy_io-0.0.3.dist-info/top_level.txt,sha256=mV0ZVKt8HA3kBLuj9iZkff1sq-cR9hE36TgSw_7Mr4E,9
-numpy_io-0.0.3.dist-info/RECORD,,
+numpy_io-0.0.5.dist-info/METADATA,sha256=ADm7d7mavjUI5NsgUNr0d1Q8kQsdRIzBYlSZr7VrvLY,390
+numpy_io-0.0.5.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+numpy_io-0.0.5.dist-info/top_level.txt,sha256=mV0ZVKt8HA3kBLuj9iZkff1sq-cR9hE36TgSw_7Mr4E,9
+numpy_io-0.0.5.dist-info/RECORD,,
```

